
CS 6740: Advanced Language Technologies

February 25, 2010

Lecture 9: Language Models: More on Query Likelihood

Lecturer: Lillian Lee

Scribes: Navin Sivakumar, Lakshmi Ganesh, Taiyang Chen

Abstract

The Language Modeling approach to document retrieval scores the relevance of

a document d with respect to a query q as the likelihood that a language model

induced from d would generate q. This query-generative model is counter-intuitive;

after all, we are looking for a retrieval system that takes a query as input and

generates relevant documents; yet, here we appear to be trying to generate the

query itself. In this lecture we examine the query-generative perspective of the

language modeling approach in more detail and attempt to justify it.

1

Review of the Language Modeling Approach

First, we brieﬂy review the language-modeling approach to scoring documents

[PC98]. Recall that if ⃗x denotes an m-dimensional vector, then x[j] denotes the

jth entry and we write x[·] for �m

j=1 x[j]. Given a document d, we consider the

language model (a probability distribution over strings of terms) induced by the

document. The score of a document d with respect to a query q is given by the

probability assigned to q by the language model induced by d. More formally, a

document d induces a vector ⃗θd of parameters. We then score d with respect to a

query q by P⃗θd(⃗q), where P⃗θd denotes the probability distribution speciﬁed by the

parameters ⃗θd, and ⃗q is the vector of term counts1 in the query q. Typically, we

consider multinomial distributions P⃗θ, which are parametrized by a length param-

eter L, specifying the number of trials (in our case, the length of the string being

generated), and the probabilities θ[j] of term vj occurring in each trial. This gives

us a scoring function of the form

P⃗θ(⃗q) = k

�

j

θ[j]q[j]

(1)

where k is a constant (independent of the θ[j] parameters) giving the number of

possible rearrangements of the terms in q. Since k is independent of the parameters

that arise from the document d, the following is equivalent under rank:

P⃗θ(⃗q) rank

=

�

j

θ[j]q[j]

(2)

1From time to time we are somewhat loose in interpreting a language model as either a distribution

on strings of terms or a distribution on term-count vectors.

9-1


From a document d we induce the parameter θd[j] through the rate of occurrence

of term vj in d:

θd[j] = fd[j]

fd[·]

(3)

With Dirichlet smoothing [MP95], we can induce the TF, IDF and length normal-

ization terms.

2

Why “Query Likelihood”?

Let us discuss why it is appropriate to consider the query as the object being

generated by our model. As a point of comparison, we consider the alternative

approach of inducing a language model from a query and considering the document

as the object being generated.2 This approach to scoring documents has a plausible

interpretation as giving documents a high score if they are likely to be generated

by the language model given by a query.

However, there are two immediate weaknesses to this approach:

• One practical concern is that queries are typically much shorter than docu-

ments; therefore, queries provide much less data than documents for us to

use in constructing a language model. Despite this, we can generate a term

probability θ[j] for each term vj based on the frequency of vj in q (possibly

with some smoothing).

• Another problem is that, for a given query, the query length is ﬁxed, whereas

document lengths vary over the corpus. This means that a query-generative

model is easier to work with than a document-generative one. For example, if

the query q consisted of the single term v1, then the (unsmoothed) probabil-

ity distribution over terms induced from this query is given by: ⃗θq[1] = 1.

Now we run up against an inconsistency:

P⃗θq(v1) = P⃗θq(v1v1) = . . . =

P⃗θq(v11v12 . . . v1|d|) = 1. Our probabilities sum to more than one! What this

example reveals is that in our query-generative model there was an implicit

length parameter, which becomes explicit (and problematic) in the document-

generative world.

We can attempt to resolve the latter issue by using an alternative, more com-

plicated language model. Consider the ﬁnite Markov chain shown in Fig 1. We can

interpret the document generation process as follows:

1. The document ends with probability pstop; otherwise proceed to step 2.

2. Add a term to the document with probabilities given by ⃗θq; return to step 1.

2Recall from the last lecture the post-hoc justiﬁcation in [LZ02] for query likelihood as the outcome

of performing a diﬀerent Bayes ﬂip of r and q in the derivation of the probabilistic model of [RJ76]; the

“document likelihood” approach we are presently considering can be thought of as the result of instead

performing a Bayes ﬂip of d and r.

9-2




START



OUTPUT 



term v



j



 (prob 



θ



[j])



STOP

PSTOP

1-PSTOP

1

Figure 1: Language Model Based On a Finite Markov Model

We can then explicitly write the scoring function for a document d:

P⃗θq,pstop(d)

=



�

j

θq[j]fd[j](1 − pstop)fd[j]



 pstop

=

pstop(1 − pstop)fd[.] �

j

θq[j]fd[j]

(4)

However, this scoring function heavily penalizes long documents, which have larger

fd[.] values. If we want to avoid penalizing long documents, we must take a diﬀerent

approach.

3

Relevance Models

One of the criticisms of the language modeling approach is its counter-intuitive

query-generative perspective. We have discussed the issues with using a document-

generative model instead. An alternative is to drop generative models altogether

and return to an approach where relevance, not the query, takes the forefront (as

in the Robertson-Sp¨arck Jones probabilistic model [RJ76]). On that note, let us

consider the model of [LC01], a relevance-based and empirically eﬀective model.

We will see that even when we begin from a relevance-centric perspective, we are

naturally led back to the query-generative world; thus, this exercise provides further

evidence for the legitimacy of the query-generative approach.

Recall that in the derivation of the probabilistic scoring function for [RJ76] with

binary attributes, we had the following weight for terms vj such that both fd[j] &gt; 0

and q[j] &gt; 0 (1 in the case of binary attributes):

P(F[j] = 1 | r, ⃗q)

P(F[j] = 1)

.

Analogously, [LC01] uses a term weight of the form

P(vj | r, ⃗q)

P(vj)

,

9-3


where P(vj | r, ⃗q) should be interpreted as the rate of occurrence of term vj in

relevant documents.

How should we estimate P(vj | r, ⃗q)? Consider a ﬁxed query q. If we knew

the set of relevant documents Rel, we could approximate the probability with the

average rate of occurrence of term vj in all relevant documents as follows:

P(vj | r, ⃗q)

≈

�

d∈Rel

P(d | Rel)# (vj in d)

length of d

=

1

|Rel|

�

d∈Rel

# (vj in d)

length of d

=

1

|Rel|

�

d∈Rel

P⃗θd(vj)

(5)

where we have made the observation that language models induced by documents

occur naturally in this approximation.

Of course, we typically do not know the set of relevant documents Rel. In

this case, we make the assumption that there is a common underlying language

model that generates the query and relevant documents. Then we would like to

weight term vj according to the probability of vj in this underlying model. Since

we know nothing about the set of relevant documents, the only evidence we can

use to construct this underlying model is the fact that it can generate the query q.

We can capture this intuition as follows:

P(vj | Rel)

≈

P(vj is next word | previous words were q)

=

P(qvj)

P(q)

(6)

where P(q) = �

j′ P(qvj′). Note that all the strings qvj′ have the same length,

so we can use multinomial distributions to approximate the probabilities without

worrying about the length parameter. Two suggested approaches for computing

P(qvj) are given below (let L denote the length of the query):

1. Average over all documents the rate of occurrence of the string qvj:

P(qvj) =

�

d

P(d)P⃗θd(qvj) =

�

d

P(d)P⃗θd(vj)

L

�

k=1

P⃗θd(qk)

(7)

2. For each query term qk, average over documents (according to probabilities

given by vj) the rate of occurrence of qk. We then compute the probability

of qvj by multiplying the prior probability of vj by the averaged rates of

occurrence of each query term qk:

P(qvj) = P(vj)

L

�

k=1

�

d

P(d | vj)P⃗θd(qk)

(8)

In either case, we return to the notion of query-generation by language models

induced by documents.

9-4


4

Exercises

1. Recall that the language-modeling approach [PC98] scores a document d with

respect to a query q by the probability that the language model induced by

d generates the query q, sometimes written as P(q | d). The fact that we

use a probability measure over queries3 to score documents forces the scoring

function to satisfy certain properties. For example, consider two documents

d1 and d2; if there is a query q such that d1 scores higher than d2, then there

must be another query q′ such that d1 scores lower than d2, i.e.

P(q | d1) &gt; P(q | d2) ⇒ ∃q′.P(q′ | d1) &lt; P(q′ | d2).

(a) Prove that the property above holds.

(b) Discuss whether this is a desirable property for a scoring function.

2. (a) Recall the query-likelihood scoring function for a document d with respect

to query q:

�

j

θd[j]q[j],

where θd[j] depends on the frequency of term vj in d. Note that d is

rewarded only for containing terms that occur in q; if a term does not

occur in q, then q[j] = 0, so θd[j]q[j] contributes nothing to the product.

Explain why the relevance model scoring function does not have this

property; that is, if we use a scoring function based on weighting term

vj by P(vj | q), then it is conceivable for a document’s score to be

increased by the occurrence of terms that are not in q. Can you give

an interpretation for why it might be desirable to allow terms that do

not occur in the query to increase a document’s score?

(b) One approach to resolving this issue while still using a language-modeling

scoring function is query expansion: the scoring system adds terms to

the query, thereby allowing terms that were not in the original query to

contribute to the document score [XC00]. What might be a plausible

approach for performing query expansion?

3. Recall the term weight function of the [LC01] relevance model. Through the

derivation in (5) and the elimination of Rel in (6), we have reduced the equa-

tion to a ﬁnal term P(qvj). Two ways (both query-generative) – (7) and (8) –

are proposed to evaluate this quantity and thus the scoring function. In this

exercise we contrast these two methods.

Suppose we have the following vocabulary:

v1

v2

v3

v4

a

big

machine

super

3In the language modeling approach as described in the notes, there is a slight complication involving

the length parameter of the multinomial distribution induced by d; for now, assume that a document d

induces a probability distribution over all queries (of any length).

9-5


Consider the following corpus of documents:

d1

big super machine

d2

big big big big machine

d3

a machine super machine

Assume that we use the simple multinomial language model (2) with proba-

bilities deﬁned by (3).

For the following queries,

q1

a super machine

q2

super big

compare the relevance scores derived using (7) and (8). Discuss whether the

results correspond to your intuition about which documents are relevant to

which queries. Does the example illustrate any diﬀerences between the two

scoring methods?

5

Solutions

1. (a) We are given that P(q | d1) &gt; P(q | d2). Suppose that for all q′ distinct

from q, P(q′ | d1) ≥ P(q′ | d2). Then we have

1 =

�

q′

P(q′ | d1)

= P(q | d1) +

�

q′̸=q

P(q′ | d1)

&gt; P(q | d2) +

�

q′̸=q

P(q′ | d2)

=

�

q′

P(q′ | d2) = 1

which is a contradiction. Hence there exists q′ such that

P(q′ | d1) &lt; P(q′ | d2).

(b) If one believes that some documents are absolutely more relevant (re-

gardless of the query) than other documents, then this is an undesirable

property for a scoring function. Consider the case of an original docu-

ment d and a modiﬁed document d′ resulting from a factual correction in

d; it seems plausible that one would want d′ to score higher than d regard-

less of the query. A more extreme example is a “spam” document that

a reader would not ﬁnd relevant to any query; this might be considered

less relevant with respect to any query than a coherent document.

In addition, recall our discussion in an earlier lecture on the relationship

between length and relevance. We found that at least in one annotated

9-6


corpus, there was some evidence that longer documents were more rel-

evant (with respect to a collection of queries) than shorter documents

[SBM96]. While this is not an example of the strict dominance of one

document compared to another, it does suggest that there may be an

issue with the “zero-sum” property of a query-likelihood scoring function

(i.e. any increase in a document’s score with respect to one query must

be oﬀset by a decrease in score with respect to another query), since some

documents seem to deserve a greater overall “scoring mass” than others.

2. (a) Consider the weight assigned to term vj in the relevance model:

P(vj | q).

If term vj occurs often in the corpus in association with the query q, then

vj receives a large weight, even if it does not occur in q. In that case, a

document’s score is increased by occurrences of term vj, which is not a

query term.

If a P(vj | q) is large, then vj is likely to be a word associated with the

topic expressed by the query q. Therefore, it is reasonable to reward

documents for containing vj.

(b) Here are a couple of reasonable approaches to query expansion. The basic

principle is to add terms to the query that occur frequently in relevant

documents. One idea is to use a baseline retrieval method to retrieve

documents based on the query q and add terms to q based on their

occurrence in the retrieved documents, since the retrieved documents

ought to be relevant to the query. This is the basic idea used in [XC00].4

Note that this approach is dependent on the baseline retrieval method

being reasonably eﬀective at identifying relevant documents.

An alternative approach might be to combine the relevance modeling

approach with the query-likelihood score function.

Note that [LC01]

already provides us with a way to identify terms that are related to

the query q; namely, we have the weight function P(vj | q), as well as

some empirically good estimates for this quantity. Intuitively, this weight

ought to tell us the extent to which the presence of vj in a document d

is evidence that d is relevant to q. It seems reasonable then to instead

modify the query q by augmenting q[j] by a quantity that is based on

(an estimate of) P(vj | q).

3. Note: all the logarithms in the calculations are base 10. Let us use P1() to

denote the probability deﬁned by method 1 (7) and P2() to denote the prob-

ability deﬁned by method 2 (8). Recall that the scoring function is given by,

4In fact, this approach has much earlier origins; [XC00] oﬀers some improvements to the earlier

attempts and applies them in the context of a language modeling score function.

9-7


scoreq(d)

=

�

vj∈d∧vj∈q

log

�P(vj | r, ⃗q)

P(vj)

�

=

�

vj∈d∧vj∈q

log

















P(qvj)



�

j′

P(qvj′)



 P(vj)

















(9)

First we ﬁnd the rate of occurrence P(vj) for each term in document by

averaging the rate of occurrence in each document:

vj

P(vj)

v1

1

3(0) + 1

3(0) +

� 1

3 × 1

4

�

= 1

12

v2

� 1

3 × 1

3

�

+

� 1

3 × 4

5

�

+ 1

3(0) = 17

45

v3

� 1

3 × 1

3

�

+

� 1

3 × 1

5

�

+

� 1

3 × 1

2

�

= 31

90

v4

� 1

3 × 1

3

�

+ 1

3(0) +

� 1

3 × 1

4

�

= 7

36

Let q′ = qvj. Using method 1 (7), we get:

P1(q′)

=

�

d

P(d)P⃗θd(q′)

=

�

d

P(d)

�

j

�fd[j]

fd[·]

�q′[j]

=

1

3 ×

�

0q′[1]

3q′[2]+q′[3]+q′[4] + 4q′[2] × 0q′[1]+q′[4]

5q′[2]+q′[3]

+ 0q′[2] × 2q′[3]

4q′[1]+q′[3]+q′[4]

�

where we adopt the convention 00 = 1.

Plugging in the query terms from our corpus, we get the following results:

q

P1(qv1)

P1(qv2)

P1(qv3)

P1(qv4)

�

j′ P1(qv′

j)

q1

0.002604

0

0.005208

0.002604

0.01042

q2

0

0.01235

0.01235

0.01235

0.03704

Now compute the

P1(qvj)

�

j′ P1(qvj′) terms:

q

P1(qv1)

P

j′ P1(qvj′)

P1(qvj)

P

j′ P1(qvj′)

P1(qvj)

P

j′ P1(qvj′)

P1(qvj)

P

j′ P1(qvj′)

q1

0.2500

0

0.5000

0.2500

q2

0

0.3333

0.3333

0.3333

We can calculate the individual log terms in the scoring function:

9-8


vj

log

� P1(vj|r, ⃗q1)

P(vj)

�

log

� P1(vj|r, ⃗q2)

P(vj)

�

v1

log

�

0.2500

1

12

�

= 0.4771

log

�

0

1

12

�

= −∞

v2

log

�

0

17

45

�

= −∞

log

�

0.3333

17

45

�

= −0.05436

v3

log

�

0.5000

31

90

�

= 0.1619

log

�

0.3333

31

90

�

= −0.01424

v4

log

�

0.2500

7

36

�

= 0.1091

log

�

0.3333

7

36

�

= 0.2341

And so we can ﬁnally compute the document scores by adding the term

weights from the table above for each term vj that occurs in both the docu-

ment and the query:

q

scoreq(d1)

scoreq(d2)

scoreq(d3)

q1

0.1619 + 0.1091 = 0.2710

0.1619

0.4771 + 0.1619 + 0.1091 = 0.7481

q2

−0.05436 + 0.2341 = 0.1797

−0.05436

0.2341

Now let’s use method 2, represented by equation (8):

P2(qvj)

=

P(vj)

L

�

k=1

�

d

P(d | vj)P⃗θd(qk)

=

P(vj)

L

�

k=1

�

d

P(d | vj)

�

j

�fd[j]

fd[·]

�qk[j]

=

P(vj)

L

�

k=1

�

0qk[1]P(d1 | vj)

3qk[2]+qk[3]+qk[4] + 4qk[2]0qk[1]+qk[4]P(d2 | vj)

5qk[2]+qk[3]

+

2qk[3]0qk[2]P(d3 | vj)

4qk[1]+qk[3]+qk[4]

�

where qk[k] = q[k], and qk[j] = 0 if j ̸= k. Again, we adopt the convention

00 = 1.

Note that

P(d | vj) =

�

1

# of documents containing vj ,

if d contains vj

0,

otherwise

The values of P(d | vj) are given below:

d

P(d | v1)

P(d | v2)

P(d | v3)

P(d | v4)

d1

0

1

2

1

3

1

2

d2

0

1

2

1

3

0

d3

1

0

1

3

1

2

9-9


For the given corpus and query terms, we get the following results:

q

P2(qv1)

P2(qv2)

P2(qv3)

P2(qv4)

�

j′ P2(qv′

j)

q1

0.002604

0

0.001922

0.002954

0.007480

q2

0

0.03568

0.02530

0.009452

0.07043

Now compute the

P2(qvj)

�

j′ P2(qvj′) terms:

q

P2(qv1)

P

j′ P2(qvj′)

P2(qvj)

P

j′ P2(qvj′)

P2(qvj)

P

j′ P2(qvj′)

P2(qvj)

P

j′ P2(qvj′)

q1

0.3481

0

0.2570

0.3949

q2

0

0.5066

0.3592

0.1342

Similar to the method 1 calculations above, we now calculate the term weights:

vj

log

� P2(vj|r, ⃗q1)

P(vj)

�

log

� P2(vj|r, ⃗q2)

P(vj)

�

v1

log

�

0.3481

1

12

�

= 0.6209

log

�

0

1

12

�

= −∞

v2

log

�

0

17

45

�

= −∞

log

�

0.5066

17

45

�

= 0.1274

v3

log

�

0.2570

31

90

�

= −0.1272

log

�

0.3592

31

90

�

= 0.01822

v4

log

�

0.3949

7

36

�

= 0.3077

log

�

0.1342

7

36

�

= −0.1610

giving us document scores as follows:

q

scoreq(d1)

scoreq(d2)

scoreq(d3)

q1

−0.1272 + 0.3077 = 0.1805

−0.1272

0.6209 − 0.1272 + 0.3077 = 0.8041

q2

0.1274 − 0.1610 = −0.0336

0.1274

−0.1610

In terms of the rankings of the documents with respect to query q1, we see

no diﬀerence between the two methods for this particular example: both

methods rank d3 highest, then d1, and d2 lowest. Intuitively, this seems like

a good ranking with respect to q1. Recall query q1 was the string “a super

machine”. Documents d3 (“a machine super machine”) and d2 (“big super

machine”) seem to have content about “super machines”. One interesting

point is that to a human reader the article “a” in q1 seems to carry little

information, whereas the scoring functions assign the highest weight to “a”

(term v1); however, this is probably explained by the fact that we have a

rather unusual corpus where the term “a” has a high IDF, since it only

occurs in one document.

With respect to query q2 (“super big”), the two methods provide very diﬀerent

rankings. Method 1 gives the same ranking for q2 as for q1: document d3 is

highest, d1 comes second, and d2 is lowest; method 2 reverses the ranking:

d2 is highest, d1 is second, and d3 is lowest. In this case, method 2 seems

9-10


to perform better, although it can be argued that neither ranking performs

optimally.

Intuitively, d1 is probably most relevant to the query, since it

contains both query terms. One can argue that document d2 is more relevant

to the query than d3, on the basis that “super” is being used as an intensiﬁer,

so the repetition of “big” in d2 implicitly carries the meaning of the term

“super,” whereas document d3, although it contains “super,” does not seem

to have any connection with “big.” Moreover, “big” seems to be the more

important term in the query, since “super” is a modiﬁer applied to “big.”

Note that method 2 accurately captures this intuition by assigning a higher

term-weight to “big” than “super,” while method 1 assigns a higher weight

to “super” than ”big.”

Let us examine why this is the case. First consider method 1. Recall the basic

procedure to ﬁnd the probability P(qvj) in method 1: generate the complete

string qvj with the same document-induced language model, and average the

results over all documents.

Note that documents that do not contain all

query terms cannot generate qvj; however, they also cannot generate any

other qvj′, so they do not contribute to the normalization constant P1(q)

either. Therefore, we are only concerned with documents that contain all

query terms, and the weight assigned to vj by method 1 increases if vj occurs

frequently in documents that contain all query terms, so that documents that

contain all query terms are likely to generate qvj.

Now consider method 2. In method 2, the procedure to compute the probabil-

ity P(qvj) is essentially as follows: for each query term, compute the average

over language models induced by documents containing vj of the probabil-

ity of generating the term; now use these probabilities to compute the total

probability of generating the query string. Therefore, in method 2, P(qvj)

(and consequently the weight on term vj) increases if query terms are highly

likely to occur in documents that contain term vj. Note that this is a sort

of ﬂipped version of the condition for high term-weights in method 1. The

example in this exercise illustrates that the two approaches are in fact diﬀer-

ent: from the perspective of method 1, document d1 is the only document to

contain all terms of q2. Since d1 has a higher than average rate of occurrence

of term “super” and lower than average rate of term “big,” the term “super”

receives a positive weight and “big” a negative weight. From the perspective

of method 2, documents d1 and d2 contain “big,” and those documents have

very high rates of occurrence for terms in q2, giving “big” a large weight;

documents d1 and d3 contain “super,” and those documents have lower rates

of occurrence for query terms, giving “super” a smaller weight. Note that

method 2 captures the intuition that vj should receive a high weight if the

presence of vj in document d is evidence of a high frequency of query terms in

d, while it is not quite as clear how to interpret a high term-weight in method

1.

9-11


References

[LC01]

Victor Lavrenko and W. Bruce Croft. Relevance based language models.

In SIGIR ’01: Proceedings of the 24th Annual International ACM SIGIR

Conference on Research and Development in Information Retrieval, pages

120–127, New York, NY, USA, 2001. ACM.

[LZ02]

John Laﬀerty and Chengxiang Zhai. Probabilistic relevance models based

on document and query generation. In Language Modeling and Informa-

tion Retrieval, pages 1–10. Kluwer Academic Publishers, 2002.

[MP95]

David MacKay and Linda Peto. A hierarchical Dirichlet language model.

Natural Language Engineering, 1(3):289–307, 1995.

[PC98]

Jay M. Ponte and W. Bruce Croft. A language modeling approach to

information retrieval. pages 275–281, 1998.

[RJ76]

Stephen Robertson and Karen Sp¨arck Jones.

Relevance weighting of

search terms. Journal of the American Society for Information Science,

1976.

[SBM96] Amit Singhal, Chris Buckley, and Mandar Mitra.

Pivoted document

length normalization.

In SIGIR ’96: Proceedings of the 19th Annual

International ACM SIGIR Conference on Research and Development in

Information Retrieval, pages 21–29, New York, NY, USA, 1996. ACM.

[XC00]

Jinxi Xu and W. Bruce Croft. Improving the eﬀectiveness of informa-

tional retrieval with local context analysis. ACM Transactions on Infor-

mation Systems, 18:79–112, 2000.

9-12

