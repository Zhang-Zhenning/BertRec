
N-gram language models for speech 

recognition

g


The Bayes classifier for speech recognition

 The Bayes classification rule for speech recognition:

,..)}

,

(

,..)

,

|

(

{

max

arg

,...

,

2

1

2

1

,...

,

2

1

2

1

w

w

P

w

w

X

P

word

word

w

w



 P(X | w1, w2, …) measures the likelihood that speaking the word sequence 

w1, w2 … could result in the data (feature vector sequence) X

 P(w1, w2 … ) measures the probability that a person might actually utter 

the word sequence w1, w2 ….

Thi

ill b 0 f

i

ibl

d

 This will be 0 for impossible word sequences

 In theory, the probability term on the right hand side of the equation must 

be computed for every possible word sequence

be computed for every possible word sequence

 It will be 0 for impossible word sequences

 In practice this is often impossible

 In practice this is often impossible

 There are infinite word sequences


S

h

iti

t

l

Speech recognition system solves

word word

word

P signal wd wd

wd

P wd wd

wd

N

wd wd

wd

N

N

1

2

1

2

1

2

,

,...,

argmax

{ (

|

,

,...,

) (

,

,...,

)}



g

wd wd

wd

N

N

N

1

2

1

2

1

2

g

{ (

|

,

,

,

) (

,

,

,

)}

,

,...,

Acoustic model

Lanugage model

For HMM-based systems

this is an HMM


Bayes’ Classification: A Graphical View

echo has an additive 

accepted terminology for the

with the unusual title 

Cross Cepstrum and Saphe Cracking

logarithm of the power 

th

ll d thi

f

ti

th

t

on the time side and vice versa

Begin sentence marker

End sentence marker

the term cepstrum was introduced by Bogert et al and has come to be 

inverse Fourier transform of the logarithm of the power spectrum 

spectrum should exhibit a peak at the echo delay 

they called this function the cepstrum

interchanging letters in the word spectrum because 

systems for processing signals that have been combined by convolution

&lt;s&gt;

&lt;/s&gt;

of a signal in nineteen sixty three Bogert Healy and Tukey published a paper 

they observed that the logarithm of the power spectrum of a signal containing an 

periodic component due to the echo and thus the Fourier transform of the

in general, we find ourselves operating on the frequency side in ways customary 

Bogert et al went on to define an extensive vocabulary to describe this new 

systems for processing signals that have been combined by convolution

The Quefrency Analysis of Time Series for Echoes Cepstrum Pseudoautocovariance 

signal processing technique however only the term cepstrum has been widely used

the transformation of a signal into its cepstrum is a homomorphic transformation

and the concept of the cepstrum is a fundamental part of the theory of homomorphic 

 There will be one path for every possible word sequence

 A priori probabilitiy for a word sequence can be applied anywhere along 

the path representing that word sequence.

. . . . . . .

p

p

g

q

 It is the structure and size of this graph that determines the feasibility of the 

recognition task 


 A factored representation of the a priori probability of a word sequence

A left-to-right model for the langauge

 A factored representation of the a priori probability of a word sequence

P(&lt;s&gt; word1 word2 word3 word4…&lt;/s&gt;) = 

P(&lt;s&gt;) P(word1 | &lt;s&gt;) P(word2 | &lt;s&gt; word1) P(word3 | &lt;s&gt; word1 word2)…

 This is a left-to-right factorization of the probability of the word sequence

 The probability of a word is assumed to be dependent only on the words preceding it

 This probability model for word sequences is as accurate as the earlier whole-word-

 This probability model for word sequences is as accurate as the earlier whole word

sequence model, in theory

 It has the advantage that the probabilities of words are applied left to right – this is 

f

t f

h

iti

perfect for speech recognition

 P(word1 word2 word3 word4 … ) is incrementally obtained :

word1

word1 word2

word1 word2 word3

word1 word2 word3

word1 word2 word3 word4

…..


The left to right model: A Graphical View







sing



sing

•Assuming a two word













sing



sing





song



sing

•Assuming a two-word

vocabulary: “sing” and

“song”































song



&lt;s&gt;



song



sing



&lt;/s&gt;



















song



sing



sing



song









song



sing



song

 A priori probabilities for word sequences are spread through the graph

 They are applied on every edge

 This is a much more compact representation of the language than the full



song

 This is a much more compact representation of the language than the full 

graph shown earlier

 But is still inifinitely large in size






sing











sing



sing





song















sing



song



sing

































&lt;s&gt;



song



sing



&lt;/s&gt;



P(&lt;/s&gt;|&lt;s&gt;)















song



sing



g



song











song



song





sing





g



song


 The N gram assumption

Left-to-right language probabilities and the N-gram model

 The N-gram assumption

P(wK | w1,w2,w3,…wK-1) = P(wK | wK-(N-1), wK-(N-2),…,wK-1)

 The probability of a word is assumed to be dependent only on 

the past N-1 words

 For a 4-gram model the probability that a person will follow “two

 For a 4-gram model, the probability that  a person will follow two 

times two is” with “four” is assumed to be identical to the probability 

that  they will follow “seven times two is” with “four”.

 This is not such a poor assumption

 Surprisingly, the words we speak (or write) at any time are largely 

(but not entirely) dependent on the previous 3 4 words

(but not entirely) dependent on the previous 3-4 words.


 An N-gram language model is a generative model

The validity of the N-gram assumption

 An N-gram language model is a generative model

 One can generate word sequences randomly from it

 In a good generative model, randomly generated word sequences should 

be similar to word sequences that occur naturally in the language

 Word sequences that are more common in the language should be generated 

more frequently

 Is an N-gram language model a good model?

 If randomly generated word sequences are plausible in the language, it is a 

reasonable model

 If more common word sequences in the language are generated more 

frequently it is a good model

 If the relative frequency of generated word sequences is exactly that in the 

language, it is a perfect model

 Thought exercise: how would you generate word sequences from an N-

gram LM ?

gram LM ?

 Clue: Remember that N-gram LMs include the probability of a sentence end 

marker 


 1 gram LM:

Examples of sentences synthesized with N-gram LMs

 1-gram LM:

 The and the figure a of interval compared and 

 Involved the a at if states next a a the of producing of too

 In out the digits right the the to of or parameters endpoint to right

 Finding likelihood with find a we see values distribution can the a is

 2-gram LM:

 2 gram LM:

 Give an indication of figure shows the source and human

 Process of most papers deal with an HMM based on the next

i h h

d d

d

h

d

h

h

i

d

f

i

li i

 Eight hundred and other data show that in order for simplicity

 From this paper we observe that is not a technique applies to model

 3-gram LM:

g

 Because in the next experiment shows that a statistical model

 Models have recently been shown that a small amount

Finding an pper bo nd on the data on the other e periments ha e been

 Finding an upper bound on the data on the other experiments have been

 Exact Hessian is not used in the distribution with the sample values  


 N gram models are reasonably good models for the language

N-gram LMs

 N-gram models are reasonably good models for the language 

at higher N

 As N increases, they become better models

 For lower N (N=1, N=2), they are not so good as generative 

models

 Nevertheless, they are quite effective for analyzing the 

relative validity of word sequences

hi h f

i

f

d

i

lik l

b

lid

 Which of a given set of word sequences is more likely to be valid

 They usually assign higher probabilities to plausible word sequences 

than to implausible ones

 This, and the fact that they are left-to-right (Markov) models 

makes them very popular in speech recognition

 They have found to be the most effective language models for large 

vocabulary speech recognition


 By restricting the order of an N-gram LM the inifinitely

N-gram LMs and compact graphs

 By restricting the order of an N gram LM, the inifinitely 

sized tree-shaped graph representing the language can be 

collapsed into finite-sized graphs.

 Best explained with an example

 Best explained with an example

 Consider a simple 2-word example with the words “Sing” 

and “Song”

 Word sequences are

 Sing

 Sing sing

g

g

 Sing song sing

 Sing sing song

 Song

Song

 Song sing sing sing  sing sing song

 ….

 …

 There are  infinite possible sequences






sing











sing



sing





song















sing



song



sing

































&lt;s&gt;



song



sing



&lt;/s&gt;



P(&lt;/s&gt;|&lt;s&gt;)















song



sing



g



song











song



song





sing





g



song


The two-word example as a full tree with a unigram LM





sing











sing



sing





song















sing



song



sing

































&lt;s&gt;



song



sing



&lt;/s&gt;



P(&lt;/s&gt;)















song



sing



g



song











song



song





sing

 The structure is recursive and can be collapsed





g



song






sing













sing



sing





song





















sing



song



sing









































song



sing



&lt;/s&gt;











&lt;s&gt;



P(&lt;/s&gt;)















song



sing



g



song

















song



song





sing











g



song








sing













sing



sing





song





















sing



song



sing









































song



sing



&lt;/s&gt;











&lt;s&gt;



P(&lt;/s&gt;)















song



sing



g



song

















song



song





sing











g



song








sing













sing



sing





song





















sing



song



sing









































song



sing



&lt;/s&gt;











&lt;s&gt;



P(&lt;/s&gt;)















song



sing



g



song

















song



song





sing











g



song






sing



sing



&lt;/s&gt;







&lt;s&gt;

P(&lt;/s&gt;)



song



song






sing



The two-word example as a full tree with a bigram LM















sing



sing





song



















sing



song



sing







































song



sing



&lt;/s&gt;











&lt;s&gt;



P(&lt;/s&gt;|&lt;s&gt;)

















song



sing



g



song















song



song





sing











g



song



 The structure is recursive and can be collapsed






sing

















sing



sing





song



















sing



song



sing







































song



sing



&lt;/s&gt;











&lt;s&gt;



P(&lt;/s&gt;|&lt;s&gt;)

















song



sing



g



song















song



song





sing











g



song








sing

















sing



sing





song



P(sing | sing)

















sing



song



sing







































song



sing



&lt;/s&gt;











&lt;s&gt;



P(&lt;/s&gt;|&lt;s&gt;)

















song



sing



g



song















song



song





sing







P(song | song)





g



song










sing

P(sing | sing)







sing







&lt;/s&gt;







&lt;s&gt;

P(&lt;/s&gt; | &lt;s&gt;)





song







song

P(song | song)








sing

The two-word example as a full tree with a trigram LM











sing



sing





song















sing



song



sing

































&lt;s&gt;



song



sing



&lt;/s&gt;

















song



sing



g



song











song



song





sing





g



song

 The structure is recursive and can be collapsed






sing

P(sing|sing sing)











sing



sing





song

|sing sing)















sing



song



sing



P(song

P

)































&lt;s&gt;



song



sing



&lt;/s&gt;



P(sing|sing song

(song|song sing)















song



sing



g



song

g)

g song)

P(











song



song





sing

P(sing|song





g



song

P(song|song song)


P(sing|sing sing)







sing



sing

|sing sing)









sing



song

P(song

P

)















&lt;s&gt;



&lt;/s&gt;



P(sing|sing song

(song|song sing)









song



sing

g)

g song)

P(







song



song

P(sing|song



g

P(song|song song)


 Th

d

b l

“th ” “

k” “ t ”

Trigram representations

 Three word vocabulary “the”, “rock”, “star”

 The graph initially begins with bigrams of &lt;s&gt;

 There are edges from every node to “&lt;/s&gt;”, that are not shown

 Trigrams of “&lt;s&gt; the”..

an HMM

th

P(the | &lt;s&gt; the)

This is wrong! This would apply the probability

P(the | &lt;s&gt; the) to instances of “the the the”

(for which the correct probability value is

P(the | the the)

Each word is 





the

/

rock

P(&lt;/s&gt; | &lt;s&gt;)

P(the | the the)



P(the | &lt;/s&gt;)

&lt;s&gt;

&lt;/s&gt;

star

P(rock | &lt;/s&gt;)

P(star | &lt;/s&gt;)


Trigram representations

 Trigrams for all “&lt;s&gt; word” sequences

 A new instance of every word is required to ensure that the two preceding 

symbols are “&lt;s&gt; word”

the

P(the | &lt;s&gt; the)

P(rock | &lt;s&gt; the)

the

rock

star

P(rock | s  the)

h

P(star | &lt;s&gt; the)

&lt;s&gt;

rock

the

rock

star

&lt;/s&gt;

star

star

the

rock

P(the | &lt;s&gt; star)

P(rock | &lt;s&gt; star)

star

P(rock | &lt;s&gt; star)

P(star | &lt;s&gt; star)


Trigram representations



Each word in the second level represents a specific set of two terminal words in a 

partial word sequence

h

the

rock

star

an HMM

the

rock

star

the

rock

P(star | the rock)

Each word is 

&lt;s&gt;

rock

star

rock

star

P(star | star  rock)

P(star | rock rock)

the

rock

This always represents a partial

sequence ending with “rock star”

Any edge coming out of this 

star

y

g

g

instance of STAR will have the

word pair context “ROCK STAR”


Trigram representations

the

rock

Edges coming out of this wrongly

t d STAR

ld h

d

rock

star

connected STAR could have word

pair contexts that are either 

“THE STAR” or “ROCK STAR”.

This is amibiguous. A word cannot have 

incoming edges from two or more



the

k

the

k

incoming edges from two or more 

different words





&lt;s&gt;

rock

star

rock

star

star

the

rock

star


Generic N-gram representations

 The logic can be extended:

 A trigram decoding structure for a vocabulary of D words

 A trigram decoding structure for a vocabulary of D words 

needs D word instances at the first level and D2 word 

instances at the second level

T t l f D(D+1)

d

d l

t b i

t

ti t d

 Total of D(D+1) word models must be instantiated

 Other, more expensive structures are also possible

 An N-gram decoding structure will need

 D + D2 +D3… DN-1 word instances

 Arcs must be incorporated such that the exit from a word instance 

p

in the (N-1)th level always represents a word sequence with the 

same trailing sequence of  N-1 words


 N-gram probabilities must be estimated from data

Estimating N-gram probabilities



g

p

 Probabilities can be estimated simply by counting words in training text

 E.g. the training corpus has 1000 words in 50 sentences, of which 400 are 

“sing” and 600 are “song”

g

g

 count(sing)=400; count(song)=600; count(&lt;/s&gt;)=50

 There are a total of 1050 tokens, including the 50 “end-of-sentence” markers

 UNIGRAM MODEL: 

 P(sing) = 400/1050;  P(song) = 600/1050;  P(&lt;/s&gt;) = 50/1050

 BIGRAM MODEL: finer counting is needed. For example:

 30 sentences begin with sing, 20 with song

 We have 50 counts of &lt;s&gt;

 P(sing | &lt;s&gt;) = 30/50;   P(song|&lt;s&gt;) = 20/50

 10 sentences end with sing, 40 with song

 P(&lt;/s&gt; | sing) = 10/400; P(&lt;/s&gt;|song) = 40/600

 P( /s  | sing)  10/400;  P( /s |song)  40/600

 300 instances of sing are followed by sing, 90 are followed by song

 P(sing | sing) = 300/400; P(song | sing) = 90/400;

 500 instances of song are followed by song, 60 by sing

 P(song | song) = 500/600;  P(sing|song) = 60/600


 Note that “&lt;/s&gt;” is considered to be equivalent to a word. The probability

Estimating N-gram probabilities

 Note that 

/s

 is considered to be equivalent to a word. The probability 

for “&lt;/s&gt;” are counted exactly like that of other words

 For N-gram probabilities, we count not only words, but also word 

g

p

,

y

,

sequences of length N

 E.g. we count word sequences of length 2 for bigram LMs, and word 

sequences of length 3 for trigram LMs

 For N-gram probabilities of order N&gt;1, we also count word sequences 

that include the word beginning and word end markers

 E.g. counts of sequences of the kind “&lt;s&gt; wa wb” and “wc wd &lt;/s&gt;”

 The N-gram probability of a word wd given a context “wa wb wc” is 

computed as

 P(wd | wa wb wc)  =  Count(wa wb wc wd) / Count(wa wb wc)

 For unigram probabilities the count in the denominator is simply the count of 

ll

d

k

(

h b

i

i

f

k

)

d

all word tokens (except the beginning of sentence marker &lt;s&gt;). We do not 

explicitly compute the probability of P(&lt;s&gt;).


 Such direct estimation is however not possible in all cases

Estimating N-gram probabilities



p

 If we had only a 1000 words in our vocabulary, there are 1001*1001 

possible bigrams (including the &lt;s&gt; and &lt;/s&gt; markers)

 We are unlikely to encounter all 1002001 word pairs in any given corpus 

of training data

 i e many of the corresponding bigrams will have 0 count

 i.e. many of the corresponding bigrams will have 0 count

 However, this does not mean that the bigrams will never occur during 

recognition

 E.g., we may never see “sing sing” in the training corpus

 P(sing | sing) will be estimated as 0

 If a speaker says “sing sing” as part of any word sequence, at least the “sing 

sing” portion of it will never be recognized

sing  portion of it will never be recognized

 The problem gets worse as the order (N) of the N-gram model increases

 For the 1000 word vocabulary there are more than 109 possible trigrams

y

p

g

 Most of them will never been seen in any training corpus

 Yet they may actually be spoken during recognition


 We must assign a small non-zero probability to all N-grams

Discounting

 We must assign a small non-zero probability to all N-grams 

that were never seen in the training data

 However, this means we will have to reduce the probability 

of other terms, to compensate

 Example:   We see 100 instances of sing, 90 of which are followed by 

sing, and 10 by &lt;/s&gt; (the sentence end marker).

g,

y

(

)

 The bigram probabilities computed directly are P(sing|sing) = 90/100, 

P(&lt;s/&gt;|sing) = 10/100

 We never observed sing followed by song.

 We never observed sing followed by song.

 Let us attribute a small probability X (X &gt; 0) to P(song|sing)

 But 90/100 + 10/100 + X &gt; 1.0

T

t

bt

t

l

Y f

P( i

| i

)

d

 To compensate we subtract a value Y from P(sing|sing) and some 

value Z from P(&lt;/s&gt;|sing) such that

 P(sing | sing) = 90 / 100 – Y

P( /

| i

)

10 / 100

Z

 P(&lt;/s&gt; | sing) = 10 / 100 – Z

 P(sing | sing) + P(&lt;/s&gt; | sing) + P(song | sing) = 90/100-Y+10/100-Z+X=1


 The reduction of the probability estimates for seen Ngrams, in order to 

Discounting and smoothing



p

y

g

,

assign non-zero probabilities to unseen Ngrams is called discounting

 The process of modifying probability estimates to be more generalizable is 

called smoothing

 Discounting and smoothing techniques:

 Absolute discounting

 Jelinek-Mercer smoothing

 Good Turing discounting

 Other methods

 All discounting techniques follow the same basic principle: they modify

 All discounting techniques follow the same basic principle: they modify 

the counts of Ngrams that are seen in the training data

 The modification usually reduces the counts of seen Ngrams

 The withdrawn counts are reallocated to unseen Ngrams

 Probabilities of seen Ngrams are computed from the modified counts

 The resulting Ngram probabilities are discounted probability estimates

 Non-zero probability estimates are derived for unseen Ngrams, from the

 Non zero probability estimates are derived for unseen Ngrams, from the 

counts that are reallocated to unseen Ngrams


 Subtract a constant from all counts

Absolute Discounting

 Subtract a constant from all counts

 E.g., we have a vocabulary of K words, w1, w2,w3…wK

 Unigram:

C

t f

d

C(i)

 Count of word wi = C(i)

 Count of end-of-sentence markers (&lt;/s&gt;) = Cend

 Total count Ctotal = iC(i) + Cend

 Discounted Unigram Counts

 Cdiscount(i) = C(i) – 

 Cdiscountend = Cend – 

 Discounted probability for seen words

 P(i) = Cdiscount(i) / Ctotal

 Note that the denominator is the total of the undiscounted counts

 If Ko words are seen in the training corpus, K – Ko words are unseen

 A total count of Kox, representing a probability Kox / Ctotal remains 

unaccounted for

 This is distributed among the K – Ko words that were never seen in training

 We will discuss how this distribution is performed later


 Bigrams: We now have counts of the kind

Absolute Discounting: Higher order N-grams

 Bigrams:  We now have counts of the kind

 Contexts: Count(w1), Count(w2), … , Count(&lt;s&gt;)

 Note &lt;s&gt; is also counted; but it is used only as a context

 Context does not incoroporate &lt;/s&gt;

p

 Word pairs:  Count (&lt;s&gt; w1), Count(&lt;s&gt;,w2),…,Count(&lt;s&gt; &lt;/s&gt;),…,

Count(w1 w1), …,Count(w1 &lt;/s&gt;) … Count(wK wK), Count(wK &lt;/s&gt;)

 Word pairs ending in &lt;/s&gt; are also counted

 Discounted counts:

 DiscountedCount(wi wj) = Count(wi wj) – 

 Discounted probability:  

 P(wj | wi) = DiscountedCount(wi wj) / Count(wi)

 Note that the discounted count is used only in the numerator

 For each context wi, the probability Ko(wi)x / Count(wi) is left over

 Ko(wi) is the number of words that were seen following wi in the training corpus

 K (w )x / Count(w ) will be distributed over bigrams P(w | w ) for words w such

 Ko(wi)x / Count(wi) will be distributed over bigrams P(wj | wi), for words wj such 

that the word pair wi wj was never seen in the training data


 Trigrams: Word triplets and word pair contexts are counted

Absolute Discounting

 Trigrams:  Word triplets and word pair contexts are counted

 Context Counts: Count(&lt;s&gt; w1), Count(&lt;s&gt; w2), …

 Word triplets:  Count (&lt;s&gt; w1w1),…, Count(wK wK, &lt;/s&gt;)

 DiscountedCount(wi wj wk) = Count(wi wj wk) – 

 Trigram probabilities are computed as the ratio of discounted 

g

p

p

word triplet counts and undiscounted context counts

 The same procedure can be extended to estimate higher-order 

N-grams

N grams

 The value of : The most common value for  is 1

 However when the training text is small this can lead to allocation of

 However, when the training text is small, this can lead to allocation of 

a disproportionately large fraction of the probability to unseen events

 In these cases,  is set to be smaller than 1.0, e.g. 0.5 or 0.1

 The optimal value of  can also be derived from data

 Via K-fold cross validation


 Split training data into K equal parts

K-fold cross validation for estimating 

 Split training data into K equal parts

 Create K different groupings of the K parts by holding out one of the K 

parts and merging the rest of the K-1 parts together. The held out part is a 

p

g g

p

g

p

validation set, and the merged parts form a training set

 This gives us K different partitions of the training data into training and 

validation sets

 For several values of 

 Compute K different language models with each of the K training sets

 Compute the total probability Pvalidation(i) of the ith validation set on the LM 

trained from the ith training set

 Compute the total probability 

P alidation

P alidation(1)*P alidation(2)**P alidation(K)

Pvalidation = Pvalidation(1)*Pvalidation(2)**Pvalidation(K)

 Select the  for which Pvalidation is maximum

 Retrain the LM using the entire training data, using the chosen value of 


 Jelinek-Mercer smoothing returns the probability of an N-gram as a weighted 

The Jelinek Mercer Smoothing Technique



g

p

y

g

g

combination of maximum likelihood N-gram and smoothed N-1 gram 

probabilities





...)

 

 

|

(

...)

 

 

(

...)

 

 

|

(

wc

wb

wa

word

P

wc

wb

wa

wc

wb

wa

word

P

ML

smooth



 Psmooth(word | wa wb wc..) is the N-gram probability used during recognition

The higher order (N gram) term on the right hand side P

(word | wa wb wc ) is

ML

smooth





...)

 

 |

(

...)

 

 

(

0.1

wc

wb

word

P

wc

wb

wa

smooth





 The higher order (N-gram) term on the right hand side, PML(word | wa wb wc..) is 

simply a maximum likelihood (counting-based) estimate of P(word | wa wb wc..)

 The lower order ((N-1)-gram term ) Psmooth(word | wb wc..) is recursively obtained 

by interpolation between the ML estimate PML(word | wb wc..) and the smoothed 

estimate for the (N-2)-gram Psmooth(word | wc..)

 All  values lie between 0 and 1

 Unigram probabilities are interpolated with a uniform probability distribution

 The  values must be estimated using held-out data

 A combination of K-fold cross validation and the expectation maximization 

algorithms must be used

W

ill

h d

il

f h l

i

l

i h

i

hi

lk

 We will not present the details of the learning algorithm in this talk

 Often, an arbitrarily chosen value of , such as  = 0.5 is also very effective


 Zipf’s law: The number of events that occur often is small

Good-Turing discounting: Zipf’s law

 Zipf s law: The number of events that occur often is small, 

but the number of events that occur very rarely is very large.

 If n represents the number of times an event occurs in a unit

 If n represents the number of times an event occurs in a unit 

interval, the number of events that occur n times per unit time 

is proportional to 1/n, where  is greater than 1

 George Kingsley Zipf originally postulated that  = 1. 

 Later studies have shown that  is 1 + , where  is slightly greater 

than 0

 Zipf’s law is true for words in a language: the probability of 

occurrence of words starts high and tapers off. A few words 

f

hil

h

l

occur very often while many others occur rarely.


 A plot of the count of counts of words in a training corpus typically looks like this:

Good-Turing discounting

p

g

p

yp

y

Count of counts curve (Zipf’s law)

words

No. of w

probability mass

 In keeping with Zipf’s law, the number of words that occur n times in the 

training corpus is typically more than the number of words that occur n+1

n=1

2

3

4

5

6

7

8

9

10 11 12 13 14

training corpus is typically more than the number of words that occur n+1 

times


 A plot of the count of counts of words in a training corpus typically looks like this:

Total Probability Mass

p

g

p

yp

y

Count of counts curve (Zipf’s law)

words

No. of w

probability mass

 Black line: Count of counts

 Black line value at N = No. of words that occur N times

n=1

2

3

4

5

6

7

8

9

10 11 12 13 14



ac

e va ue at N

No. o wo ds t at occu N t

es

 Red line:   Total probability mass of all events with that count

 Red line value at 1 = (No. of words that occur once) / Total words

 Red line value at 2 = 2 * (No of words that occur twice) / Total words

 Red line value at 2  2   (No. of words that occur twice) / Total  words

 Red line value at N = N * (No. of words that occur N times) / Total words


 A plot of the count of counts of words in a training corpus typically looks like this:

Total Probability Mass

p

g

p

yp

y

Count of counts curve (Zipf’s law)

words

No. of w

probability mass

 Red Line

n=1

2

3

4

5

6

7

8

9

10 11 12 13 14

 P(K) =  K * NK / N

 K = No. of times word was seen

 N i

f

d

K ti

 NK is no. of words seen K times

 N: Total words


 A plot of the count of counts of words in a training corpus typically looks like this:

Good-Turing discounting

p

g

p

yp

y

Count of counts curve (Zipf’s law)

words

No. of w

probability mass

 In keeping with Zipf’s law, the number of words that occur n times in the 

training corpus is typically more than the number of words that occur n+1

n=1

2

3

4

5

6

7

8

9

10 11 12 13 14

training corpus is typically more than the number of words that occur n+1 

times

 The total probability mass of words that occur n times falls slowly

 Surprisingly the total probability mass of rare words is greater than the total

 Surprisingly, the total probability mass of rare words is greater than the total 

probability mass of common words, because of the large number of rare 

words


Good-Turing discounting

 A plot of the count of counts of words in a training corpus typically looks like this:

Count of counts curve (Zipf’s law)

words



p o o

e cou

o cou s o wo ds

g co pus yp c

y oo s

e

s:

probability mass

No. of w

Reallocated probability mass

 Good Turing discounting reallocates probabilities

Th

l

b bili

f ll

d

h

d

i

i

n=1

2

3

4

5

6

7

8

9

10 11 12 13 14

 The total probability mass of all words that occurred n times is 

assigned to words that occurred n-1 times

 The total probability mass of words that occurred once is reallocated 

d

h

b

d i

i i

to words that were never observed in training


Good Turing Discounting

 Assign probability mass of  events seen 2 times to events seen 

once.

 Before discounting:  P(word seen once) =  1 / N

 N = total words

 After discounting:

 After discounting:

P(word seen once) = (2*N2 / N) / N1

 N2 is no. of words seen twice

 N1 is no of words seen once

 N1 is no. of words seen once

 P(word seen once) = (2*N2 / N1) / N

 Discounted count for words seen once is: 

 N1,discounted =  (2*N2 / N1)

 Modified probability:  Use discounted count as the count for the word


Good-Turing discounting

Count of counts curve

of words

This point moves left!

New probability mass

for these words is 0!!

probability mass

No. o

n=1

2

3

4

5

6

7

8

9

10 11 12 13 14

Reallocated probability mass

 The probability mass curve cannot simply be shifted left directly due to 

two potential problems

two potential problems

 Directly shifting the probability mass curve assigns 0 probability to the 

most frequently occurring words

q

y

g


Good-Turing discounting

True count of counts curve

words

No words observed with this count

No. of 

No words observed with this count.

Total probability mass here is 0.

Discounted probabilities (after left shift)

L+1

L

Discounted probabilities (after left shift)

Look like this!

 The count of counts curve is often not continuous

 We may have words that occurred L times and words that occurred L+2

L+1

L

 We may have words that occurred L times, and words that occurred L+2 

times, but none that ocurred L+1 times

 By simply reassigning probability masses backward, words that occurred L 

times are assigned the total probability of words that ocurred L+1 times = 0!

g

p

y


Good-Turing discounting

True count of counts curve

of words

S

th d

d

t

l t d

t

f

t

No. o

Smoothed and extrapolated count of counts curve

 The count of counts curve is smoothed and extrapolated

 Smoothing fills in “holes” – intermediate counts for which the curve went to 0

Smoothing may also vary the counts of events that were observed

 Smoothing may also vary the counts of events that were observed

 Extrapolation extends the curve to one step beyond the maximum count 

observed in the data

 Smoothing and extrapolation can be done by linear interpolation and 

 S oot

g a d e t apo at o ca be do e by

ea

te po at o a d

extrapolation, or by fitting polynomials or splines

 Probability masses are computed from the smoothed count-of-counts and 

reassigned


Good-Turing discounting

True count of counts curve

of words

S

th d

d

t

l t d

t

f

t

No. o

Smoothed and extrapolated count of counts curve

 Step 1: Compute count-of-counts curve

 Let r(i) be the number of words that occurred i times

 Step 2: Smooth and extend count-of-count curve

 Let r’(i) be the smoothed count of the number of words that occurred i times. 

 The total smoothed count of all words that occurred i times is r’(i) * i. 

 We operate entirely with the smoothed counts from here on


Good-Turing discounting

Smoothed count of counts curve

of words

Total probability curve

obtained from smoothed

Shifted total probability

mass curve

No. o

obtained from smoothed

counts

mass curve

n=1

2

3

4

5

6

7

8

9

10 11 12 13 14

 Step 3: Reassign the total smoothed counts r’(i)*i to words that occurred i-1

times. 

 reassignedcount(i-1) = r’(i)*i / r’(i-1)

g

(

)

( )

(

)

 Step 4: Compute modified total count from smoothed counts

 totalreassignedcount = i smoothedprobabilitymass(i)

 Step 5: A word w with count i is assigned probability

P(w| context) = reassignedcount(i) / totalreassignedcount


Good-Turing discounting

of words

Total probability mass assigned to unseen events

No. o

n=1

2

3

4

5

6

7

8

9

10 11 12 13 14

 Step 6: Compute a probability for unseen terms!!!!

 A probability mass Pleftover = r’(1)*N1 / totalreassignedcount is left over

p

y

leftover

( )

1

g

 Reminder: r’(1) is the smoothed count of words that occur once

 The left-over probability mass is reassigned to words that were not seen in the 

training corpus

 P(any unseen word) =  Pleftover / Nunseen


 UNIGRAMS:

Good-Turing estimation of LM probabilities

 UNIGRAMS:

 The count-of-counts curve is derived by counting the words (including &lt;/s&gt;) 

in the training corpus

 The count-of-counts curve is smoothed and extrapolated

p

 Word probabilities are computed for observed words are computed from the 

smoothed, reassigned counts

 The left-over probability is reassigned to unseen words

 BIGRAMS:

 For each word context W, (where W can also be &lt;s&gt;), the same procedure 

given above is followed: the count-of-counts for all words that occur 

immediately after W is obtained, smoothed and extrapolated, and bigram 

probabilities for words seen after W are computed.

 The left over probability is reassigned to the bigram probabilities of words

 The left-over probability is reassigned to the bigram probabilities of words 

that were never seen following W in the training corpus

 Higher order N-grams: The same procedure is followed for every word 

g

g

p

y

context W1 W2… WN-1


 All discounting techniques result in a some left-over

Reassigning left-over probability to unseen words

 All discounting techniques result in a some left-over 

probability to reassign to unseen words and N-grams

F

i

hi

b bili

i

if

l di

ib

d

 For unigrams, this probability is uniformly distributed over 

all unseen words

 The vocabulary for the LM must be prespecified

y

p

p

 The probability will be reassigned uniformly to words from this 

vocabulary that were not seen in the training corpus

 For higher-order N-grams, the reassignment is done 

differently

B

d

l

d

N

i

(N 1)

b biliti

 Based on lower-order N-gram, i.e. (N-1)-gram probabilities

 The process by which probabilities for unseen N-grams is computed 

from (N-1)-gram probabilities is referred to as “backoff”


Dealing with Unseen Ngrams

s

o. of words

Total probability mass assigned to unseen events

No

 UNIGRAMS: A probability mass Pleftover = r’(1)*N1 / totalreassignedcount is left 

n=1

2

3

4

5

6

7

8

9

10 11 12 13 14

over and distributed uniformly over unseen words

 P(any unseen word) =  Pleftover / Nunseen

 BIGRAMS: We only count over all words in a particular context

 BIGRAMS:  We only count over all words in a particular context

 E.g. all words that followed word “w3”

 We count words and smooth word counts only over this set (e.g. words that followed w3)

 We can use the same discounting principle as above to compute probabilities of unseen

 We can use the same discounting principle as above to compute probabilities of unseen 

bigrams of w3 (i.e bigram probabilities that a word will follow w3, although it was never 

observed to follow w3 in the training set)

 CAN WE DO BETTER THAN THIS?


Unseen Ngrams: BACKOFF

 Example:  Words  w5 and w6 were never observed to follow w3 in 

the training data

 E.g. we never saw “dog” or “bear” follow the word “the”

 Backoff assumption:  Relative frequencies of w5 and w6 will be the 

p

q

same in the context of w3 (bigram) as they are in the language in 

general (Unigrams)

 If the number of times we saw “dog” in the entire training corpus was 10x

 If the number of times we saw dog  in the entire training corpus was 10x 

the no. of times we saw “bear”, then we assume that the number of times we 

will see “dog” after “the” is also 10x the no. of times we will see “bear” after 

“the”

 Generalizing:  Ngram probabilities of words that are never seen (in 

the training data) in the given N-gram context follow the same 

g

)

g

g

distribution pattern observed in the N-1 gram context


 Explanation with a bigram example

N-gram LM: Backoff



p a at o w t a b g a

e a p e

ord|w3)

w1

w2

w3

w4

w5

w6

&lt;/s&gt;

Bigram(wo

Unigram

 Unigram probabilities are computed and known before bigram probabilities

w1

w2

w3

w4

w5

w6

&lt;/s&gt;

U

 Unigram probabilities are computed and known before bigram probabilities 

are computed

 Bigrams for P(w1 | w3), P(w2 | w3) and P(w3 | w3) were computed from 

discounted counts w4 w5 w6 and &lt;/s&gt; were never seen after w3 in the

discounted counts. w4, w5, w6 and &lt;/s&gt; were never seen after w3 in the 

training corpus


 Explanation with a bigram example

N-gram LM: Backoff



p a at o w t a b g a

e a p e

m(w3)

w1

w2

w3

w4

w5

w6

&lt;/s&gt;

Bigram

Unigram

 The probabilities P(w4|w3) P(w5|w3) P(w6|w3) and P(&lt;/s&gt;|w3) are assumed to

w1

w2

w3

w4

w5

w6

&lt;/s&gt;

U

 The probabilities P(w4|w3),  P(w5|w3), P(w6|w3) and P(&lt;/s&gt;|w3) are assumed to 

follow the same pattern as the unigram probabilities P(w4), P(w5), P(w6) and 

P(&lt;/s&gt;)

 They must however be scaled such that

 They must, however be scaled such that 

P(w1|w3) + P(w2|w3) + P(w3|w3) + scale*(P(w4)+P(w5)+P(w6)+P(&lt;/s&gt;)) = 1.0

 The backoff bigram probability for the unseen bigram P(w4 | w3) = scale*P(w4)


N-gram LM: Backoff

3)

Bigram(w3

gram

w1

w2

w3

w4

w5

w6

&lt;/s&gt;

w1

w2

w3

w4

w5

w6

&lt;/s&gt;

Unig

 P(w1|w3) + P(w2|w3) + P(w3|w3) + scale*(P(w4)+P(w5)+P(w6)+P(&lt;/s&gt;)) = 1.0

 The backoff bigram probability for the unseen bigram P(w4 | w3) = scale*P(w4)

 The scale term is called the backoff term. It is specific to w3

 Scale = backoff(w3)

 Specificity is because the various terms used to compute scale are specific to w3

)

/

(

)

(

)

(

)

(

)

|

(

)

|

(

)

|

(

1

)

(

6

5

4

3

3

3

2

3

1

3



















s

P

w

P

w

P

w

P

w

w

P

w

w

P

w

w

P

w

backoff


N-gram LM (Katz  Models): Backoff from N-gram to (N-1)-gram

ord|w3)

w1

w2

w3

w4

w5

w6

&lt;/s&gt;

Bigram(wo

 A

ti

Wh

ti

ti

N

b biliti

l

d h

t

ll

 Assumption: When estimating N-gram probabilities, we already have access to all 

N-1 gram probabilities

 Let w1 … wK be the words in the vocabulary (includes &lt;/s&gt;)

1

K

y (

)

 Let WN-1 be the context for which we are trying to estimate N-gram probabilities



Will be some sequence of N-1 words (for N-gram probabilities)

i

i h t

t

ll

b biliti

P(

d | W

)

 i.e we wish to compute all probabilities P(word | WN-1)

 E.g W3 = “wa wb wc”.  We wish to compute all 4-gram probabilities P(word | wa wb wc)


 Step 1: Compute leftover probability mass for unseen N grams (of the form

N-gram LM (Katz  Models): Backoff from N-gram to (N-1)-gram

 Step 1:  Compute leftover probability mass for unseen N-grams (of the form 

P(word| WN-1)) using Good Turing discounting

 Pleftover(WN-1) – this is specific to context WN-1 as we are only counting words that 

follow word sequence WN 1

follow word sequence WN-1

 Step 2: Compute backoff weight





1)

|

(

1

N

w

P

W







text

training

in the

 

following

seen 

NOT

was

2

 text

 training

in the

 

 

following

seen 

 

 was

1

1

1

1

)

|

(

)

|

(

1

)

(

N-

N-

w

N-

w

N-

N-

w

P

w

P

backoff

W

W

W

W

W

g

g

1

N

 Note WN-2 in the denominator.  If WN-1 is “wa wb wc”, WN-2 is “wb wc” 



The trailing N-2 words only



We already have N 1 gram probabilities of the form P(w | W

)



We already have N-1 gram probabilities of the form P(w | WN-2)

 Step 3: We can now compute N-gram probabilities for unseen Ngrams

)

|

(

)

(

)

|

(

P

b

k ff

P

W

W

W

 Actually, this is done “on demand” – there’s no need to store them explicitly.

)

|

(

)

(

)

|

(

2

1

1

N-

N-

N-

w

P

backoff

w

P

W

W

W




 In order to estimate the backoff weight needed to compute

Backoff is recursive

 In order to estimate the backoff weight needed to compute

N-gram probabilities for unseen N-grams, the corresponding N-1 

grams are required (as in the following 4-gram example)

 The corresponding N-1 grams might also not have been seen in the training 

)

|

(

)

(

)

|

(

c

b

c

b

a

c

b

a

w

w

w

P

w

w

w

backoff

w

w

w

w

P



data

 If  the backoff N-1 grams are also unseen, they must in turn be 

computed by backing off to N-2 grams

 The backoff weight for the unseen N-1 gram must also be known

 i.e. it must also have been computed already

p

y

 All lower order N-gram parameters (including probabilities and 

backoff weights) must be computed before higher-order N-gram 

g

)

p

g

g

parameters can be estimated


 First compute Unigrams

Learning Backoff Ngram models



s co pu e U g

s

 Count words, perform discounting, estimate discounted probabilities for all seen 

words

 Uniformly distribute the left-over probability over unseen unigrams

y

p

y

g

 Next, compute bigrams. For each word W seen in the training data:

 Count words that follow that W. Estimate discounted probabilities P(word | W) for 

all words that were seen after W

all words that were seen after W. 

 Compute the backoff weight (W) for the context W.

 The set of explicity estimated P(word | W) terms, and the backoff weight (W) 

together permit us to compute all bigram probabilities of the kind: P(word | W)

together permit us to compute all bigram probabilities of the kind: P(word | W)

 Next, compute trigrams: For each word pair “wa wb” seen in the training data:

 Count words that follow that “wa wb”. Estimate discounted probabilities

 Count words that follow that wa wb . Estimate discounted probabilities

P(word | wa wb) for all words that were seen after “wa wb”. 

 Compute the backoff weight (wa wb) for the context “wa wb”.

 The process can be continued to compute higher order N-gram probabilities.


 An N-gram backoff language model contains

The contents of a completely trained N-gram language model

 An N-gram backoff language model contains

 Unigram probabilities for all words in the vocabulary

 Backoff weights for all words in the vocabulary

 Bigram probabilities for some, but not all bigrams

 i.e. for all bigrams that were seen in the training data

 If N&gt;2, then: backoff weights for all seen word pairs

 If the word pair was never seen in the training corpus, it will not have a backoff 

weight. The backoff weight for all word pairs that were not seen in the training 

corpus is implicitly set to 1

 …

 N-gram probabilities for some, but not all N-grams

 N-grams seen in training data

 Note that backoff weights are not required for N-length word sequences in 

an N-gram LM

 Since backoff weights for N-length word sequences are only useful to compute 

backed off N+1 gram probabilities


An Example Backoff Trigram LM

\1-grams:

-1.2041 &lt;UNK&gt;

0.0000

-1.2041 &lt;/s&gt;

0.0000

-1.2041 &lt;s&gt;

-0.2730

-0.4260 one

-0.5283

-1.2041 three

-0.2730

-0.4260 two

-0.5283

\2-grams:

-0.1761 &lt;s&gt; one      0.0000

-0.4771 one three    0.1761

-0.3010 one two      0.3010

-0.1761 three two    0.0000

-0.3010 two one      0.3010

-0.4771 two three    0.1761

\3-grams:

-0.3010 &lt;s&gt; one two 

-0.3010 one three two 

-0.4771 one two one 

-0.4771 one two three 

-0.3010 three two one 

-0.4771 two one three 

-0.4771 two one two 

-0.3010 two three two


 To retrieve a probability P(word | wa wb wc

)

Obtaining an N-gram probability from a backoff N-gram LM

 To retrieve a probability P(word | wa wb wc …)

 How would a function written for returning N-gram probabilities work?

L

k f

h

b bili

P(

d |

b

) i

h LM

 Look for the probability P(word | wa wb wc …) in the LM

 If it is explicitly stored, return it

 If P(word | wa wb wc …)  is not explicitly stored in the LM retrive

it by backoff to lower order probabilities:

 Retrieve backoff weight backoff(wa wb wc ) for word sequence wa wb wc

 Retrieve backoff weight backoff(wa wb wc..) for word sequence wa wb wc

 If it is stored in the LM, return it

 Otherwise return 1

Retrie e P( ord |

b

c

) from the LM

 Retrieve P(word | wb wc …) from the LM

 If P(word | wb wc .. ) is not explicitly stored in the LM, derive it backing off

 This will be a recursive procedure

R

P(

d |

b

) * b

k ff(

b

)

 Return P(word | wb wc …)  * backoff(wa wb wc..)


Toolkits for training Ngram LMs

 CMU-Cambridge LM Toolkit

 SRI LM Toolkit

 MSR LM toolkit

 Good for large vocabularies

 ..

 Your own toolkit here


http://mi.eng.cam.ac.uk/~prc14/toolkit.html

htt

//

h

d /SLM i f

ht

l

Training a language model using CMU-Cambridge LM toolkit

http://www.speech.cs.cmu.edu/SLM_info.html

Contents of textfile

&lt;s&gt;  the term cepstrum was introduced by Bogert et al and has come to be 

vocabulary

&lt;s&gt;  

accepted terminology for the

inverse Fourier transform of the logarithm of the power spectrum 

of a signal in nineteen sixty three Bogert Healy and Tukey published a paper 

with the unusual title 

The Quefrency Analysis of Time Series for Echoes Cepstrum Pseudoautocovariance 

Cross Cepstrum and Saphe Cracking

&lt;/s&gt;

the 

term 

cepstrum 

was 

Cross Cepstrum and Saphe Cracking

they observed that the logarithm of the power spectrum of a signal containing an 

echo has an additive 

periodic component due to the echo and thus the Fourier transform of the

logarithm of the power 

spectrum should exhibit a peak at the echo delay 

introduced 

by 

Bogert 

et 

al 

and

they called this function the cepstrum

interchanging letters in the word spectrum because 

in general, we find ourselves operating on the frequency side in ways customary 

on the time side and vice versa

Bogert et al went on to define an extensive vocabulary to describe this new 

i

l

i

t

h i

h

l

th

t

t

h

b

id l

d

and 

has 

come 

to 

be 

accepted 

signal processing technique however only the term cepstrum has been widely used

The transformation of a signal into its cepstrum is a homomorphic transformation

and the concept of the cepstrum is a fundamental part of the theory of homomorphic 

systems for processing signals that have been combined by convolution

&lt;/s&gt;

terminology 

for

inverse 

Fourier 

transform 

f

of 

logarithm 

Power

. . .

Contents of contextfile

&lt;s&gt;


Training a language model using CMU-Cambridge LM toolkit

To train a bigram LM (n=2):

$bin/text2idngram -vocab vocabulary -n 2 -write_ascii &lt; textfile &gt; idngm.tempfile

$bin/idngram2lm -idngram idngm.tempfile -vocab vocabulary -arpa MYarpaLM -context 

contextfile -absolute -ascii_input -n 2 (optional: -cutoffs 0 0 or –cutoffs 1 1 ….)

OR

$bin/idngram2lm -idngram idngm.tempfile -vocab vocabulary -arpa MYarpaLM  -context 

g

g

g

p

y

p

p

contextfile  -good_turing -ascii_input -n 2

….


Representing N-gram LMs as graphs 

 F

iti

th N

 For recognition, the N-

gram LM can be 

represented as a finite 

h





sing

P(sing|sing sing)

sing)

state graph



Recognition can be 

performed exactly as we 

ld

f









sing



song

P(song|sing s

P

)

would perform 

recognition with 

grammars















sing



&lt;s&gt;



&lt;/s&gt;



P(sing|sing song)

P(song|song sing)

 Problem: This graph can 

get enormously large



There is an arc for every









song



sing



P(sing|song song)



There is an arc for every 

single N-gram 

probability!



Also for every single N-





song

P

P(song|song song)

y

g

1, N-2 .. 1-gram 

probabilities


The representation is wasteful

 In a typical N-gram LM,  the vast majority of bigrams, 

trigrams (and higher-order N-grams) are computed by 

b

k ff

backoff

 They are not seen in training data, however large

)

|

(

)

(

)

|

(

P

b

k ff

P

 The backed-off probability for an N-gram is obtained from 

th N 1

!

)

|

(

)

(

)

|

(

c

b

c

b

a

c

b

a

w

w

w

P

w

w

w

backoff

w

w

w

w

P



the N-1 gram!

 So for N-grams computed by backoff it should be sufficient 

to store only the N-1 gram in the graph

 Only have arcs for P(w | wb wc);  not necessary to have explicit arcs 

for P(w | wa wb wc)

(

|

a

b

c)

 This will reduce the size of the graph greatly


Ngram LMs as FSGs: accounting for backoff

 N-Gram language models with back-off can be represented 

as finite state grammars

as finite state grammars

 That explicitly account for backoff!

Thi

l

i

b

d

i

 This also permits us to use grammar-based recognizers to 

perform recognition with Ngram LMs

 There are a few precautions to take, however


Ngram to FSG conversion: Trigram LM



\1-grams:

-1.2041 &lt;UNK&gt;

0.0000

-1.2041 &lt;/s&gt;

0.0000

-1.2041 &lt;s&gt; -0.2730

0 4260

0 5283

-0.4260 one -0.5283

-1.2041 three

-0.2730

-0.4260 two -0.5283



\2-grams:

0 1761 &lt; &gt;

0 0000

-0.1761 &lt;s&gt; one      0.0000

-0.4771 one three    0.1761

-0.3010 one two      0.3010

-0.1761 three two    0.0000

-0.3010 two one      0.3010

-0.4771 two three

0.1761

0.4771 two three    0.1761



\3-grams:

-0.3010 &lt;s&gt; one two 

-0.3010 one three two

0.3010 one three two 

-0.4771 one two one 

-0.4771 one two three 

-0.3010 three two one 

-0.4771 two one three 

-0.4771 two one two 

-0.3010 two three two 


Step1: Add Explicit Ngrams:



\1-grams:

-1.2041 &lt;UNK&gt;

0.0000

-1.2041 &lt;/s&gt;

0.0000

-1.2041 &lt;s&gt; -0.2730

0 4260

0 5283

P(2|3)

P(1 | 3 2)

UG word 

history level

BG word 

history level

-0.4260 one -0.5283

-1.2041 three

-0.2730

-0.4260 two -0.5283



\2-grams:

0 1761 &lt; &gt;

0 0000

2

3

3

unk



&lt;s&gt;

2

P(3)

P(2|3)

P(2 | 2 3)

-0.1761 &lt;s&gt; one      0.0000

-0.4771 one three    0.1761

-0.3010 one two      0.3010

-0.1761 three two    0.0000

-0.3010 two one      0.3010

-0.4771 two three

0.1761

1

3

1



&lt;s&gt;

2

0.4771 two three    0.1761



\3-grams:

-0.3010 &lt;s&gt; one two 

-0.3010 one three two

2

1

&lt;/s&gt;

P(1|&lt;s&gt;)

0.3010 one three two 

-0.4771 one two one 

-0.4771 one two three 

-0.3010 three two one 

-0.4771 two one three 

-0.4771 two one two 

1

Note: The two-word history out of 

every node in the bigram word 

hi t

 l

l i  

i

 Note “EPSILON” Node for Unigram Probs

-0.3010 two three two 

history level is unique


Step2: Add Backoffs



\1-grams:

-1.2041 &lt;UNK&gt;

0.0000

-1.2041 &lt;/s&gt;

0.0000

-1.2041 &lt;s&gt; -0.2730

0 4260

0 5283

BO(3)

BO(3 2)

BO(2 3)

-0.4260 one -0.5283

-1.2041 three

-0.2730

-0.4260 two -0.5283



\2-grams:

0 1761 &lt; &gt;

0 0000

2

3

3

unk

 (1)

&lt;s&gt;

2

BO(2 3)

BO(1 3)

-0.1761 &lt;s&gt; one      0.0000

-0.4771 one three    0.1761

-0.3010 one two      0.3010

-0.1761 three two    0.0000

-0.3010 two one      0.3010

-0.4771 two three

0.1761

1

3

1

 (1)

&lt;s&gt;

2

BO(1 3)

0.4771 two three    0.1761



\3-grams:

-0.3010 &lt;s&gt; one two 

-0.3010 one three two

2

1

&lt;/s&gt;

BO(1)



From any node representing a word 

history  “wa” (unigram) add BO arc to epsilon



With score Backoff(wa)



From any node representing a word history “wa wb” add a BO arc to

0.3010 one three two 

-0.4771 one two one 

-0.4771 one two three 

-0.3010 three two one 

-0.4771 two one three 

-0.4771 two one two 

1



From any node representing a word history wa wb  add a BO arc to 

wb



With score Backoff (wa wb)

-0.3010 two three two 


Ngram to FSG conversion: FSG

2 (7)

3 (4)

unk (2)

 Yellow ellipse is start node

 Pink ellipse is “no gram” node

2 (7)

3 (8)

3 (4)

unk (2)

 Pink ellipse is no gram  node

 Blue ellipses are unigram nodes

 Gray ellipses are bigram nodes

( )

1 (9)

 (1)

&lt;s&gt;(0)

2 (5)

3 (10)

1 (6)

 red text represents

words

 Green (parenthesized)

numbers are node numbers

2 (11)

&lt;/s&gt;(3)

S

f

h

t

t

th f

d

t

/

i

b

d

numbers are node numbers

1 (12)

o Score of shortest path from any node to &lt;/s&gt; is subsumed

into the termination score for that node.

o The explicit probability link into &lt;/s&gt; can then be removed

- Yellow star represents termination score


A Problem: Paths are Duplicated

2 (7)

3 (4)

unk (2)

Explicit trigram path for trigram “three two one”

2 (7)

3 (8)

3 (4)

unk (2)

( )

1 (9)

 (1)

&lt;s&gt;(0)

2 (5)

3 (10)

1 (6)

2 (11)

&lt;/s&gt;(3)

E

li it t i

th

l

h

b

k d

ff

lt

ti

1 (12)

o Explicit trigram paths also have backed off alternatives


Backoff paths exist for explicit Ngrams

2 (7)

3 (4)

unk (2)

Backoff trigram path for trigram “three two one”

2 (7)

3 (8)

3 (4)

unk (2)

( )

1 (9)

 (1)

&lt;s&gt;(0)

2 (5)

3 (10)

1 (6)

2 (11)

&lt;/s&gt;(3)

E

li it t i

th

l

h

b

k d

ff

lt

ti

1 (12)

o Explicit trigram paths also have backed off alternatives


Delete “losing” edges

2 (7)

3 (4)

unk (2)

Deleted trigram link

2 (7)

3 (8)

3 (4)

unk (2)

( )

1 (9)

 (1)

&lt;s&gt;(0)

2 (5)

3 (10)

1 (6)

2 (11)

&lt;/s&gt;(3)

Wh

th

b

t b

k d

ff t i

th

hi h

th

th

1 (12)

o When the best backed off trigram path scores higher than the

explicit trigram path, the explicit trigram link can be removed

o Renormalization of backoff scores will be required to ensure sum(prob)=1


Delete “Losing” Edges

2 (7)

3 (4)

unk (2)

Deleted bigram link

2 (7)

3 (8)

3 (4)

unk (2)

( )

1 (9)

 (1)

&lt;s&gt;(0)

2 (5)

3 (10)

1 (6)

2 (11)

&lt;/s&gt;(3)

E

li it bi

li k

l

b

i

il

l

d if

1 (12)

o Explicit bigram links can also be similarly removed if

backed off score is higher than explicit link score

o Backoff scores (yellow link scores) will have to be renormalized

for probabilities to add to 1.


 Train HMMs for the acoustic model

Overall procedure for recognition with an Ngram language model

 Train HMMs for the acoustic model

 Train N-gram LM with backoff from training data

 Construct the Language graph, and from it the language HMM

g

g g p ,

g

g

 Represent the Ngram language model structure as a compacted N-gram 

graph, as shown earlier

 The graph must be dynamically constructed during recognition – it is

 The graph must be dynamically constructed during recognition it is 

usually too large to build statically

 Probabilities on demand: Cannot explicitly store all K^N probabilities 

in the graph and must be computed on the fly

in the graph, and must be computed on the fly

 K is the vocabulary size

 Other, more compact structures, such as FSAs can also be used to 

represent the lanauge graph

represent the lanauge graph

 later in the course

 Recognize

