
27

EM 

The Expectation-Maximization 

Algorithm


-3

-2

-1

0

1

2

µ ± !

μ

1

Last lecture: 

How to estimate μ given data

28

X          X  XX    X  XXX               X

Observed Data

For this problem, we got a nice, closed 

form, solution, allowing calculation of the 

μ, σ that maximize the likelihood of the 

observed data.

We’re not always so lucky...


This?

Or this?

(A modeling decision, not a math problem..., 

but if later, what math?)

29

More Complex Example




A Real Example:

CpG content of human gene promoters

“A genome-wide analysis of CpG dinucleotides in the human genome distinguishes two 

distinct classes of promoters”  Saxonov, Berg, and Brutlag, PNAS 2006;103:1412-1417

©2006 by National Academy of Sciences

30


31

No 

closed-

form

max

Parameters θ

means

µ1

µ2

variances

σ2

1

σ2

2

mixing parameters

τ1

τ2 = 1 − τ1

P.D.F.

f(x|µ1, σ2

1)

f(x|µ2, σ2

2)

Likelihood

L(x1, x2, . . . , xn|µ1, µ2, σ2

1, σ2

2, τ1, τ2)

= ￿n

i=1

￿2

j=1 τjf(xi|µj, σ2

j )

Gaussian Mixture Models / Model-based Clustering


32

-20

-10

0

10

20

-20

-10

0

10

20

0

0.05

0.1

0.15

-20

-10

0

10

20

Likelihood Surface

μ1

μ2


33

-20

-10

0

10

20

-20

-10

0

10

20

0

0.05

0.1

0.15

-20

-10

0

10

20

σ2 = 1.0

τ1 =

.5

τ2 =

.5

xi =

−10.2, −10, −9.8

−0.2,

0,

0.2

11.8,

12, 12.2

μ1

μ2


34

-20

-10

0

10

20

-20

-10

0

10

20

0

0.05

0.1

0.15

-20

-10

0

10

20

σ2 = 1.0

τ1 =

.5

τ2 =

.5

xi =

−10.2, −10, −9.8

−0.2,

0,

0.2

11.8,

12, 12.2

(-5,12)

(-10,6)

(6,-10)

(12,-5)

μ1

μ2


35

Messy: no closed form solution known for 

ﬁnding θ maximizing L

But what if we 

knew the 

hidden data?

A What-If Puzzle


36

EM as Egg vs Chicken

IF zij known, could estimate parameters θ

E.g., only points in cluster 2 inﬂuence µ2, σ2  

IF parameters θ known, could estimate zij

E.g., if |xi - µ1|/σ1 &lt;&lt; |xi - µ2|/σ2, then zi1 &gt;&gt; zi2

But we know neither; (optimistically) iterate:

E: calculate expected zij, given parameters

M: calc “MLE” of parameters, given E(zij)

Overall, a clever “hill-climbing” strategy 


37

Simple Version: 

“Classiﬁcation EM”

If zij &lt; .5, pretend it’s 0;  zij &gt; .5, pretend it’s 1

I.e., classify points as component 0 or 1

Now recalc θ, assuming that partition

Then recalc zij , assuming that θ

Then re-recalc θ, assuming new zij,  etc., etc.  

“Full EM” is a bit more involved, but this is the crux.


38

Full EM


39

The E-step:  

Find E(Zij), i.e. P(Zij=1)

Assume θ known &amp; ﬁxed

A (B): the event that xi was drawn from f1 (f2)

D: the observed datum xi

Expected value of zi1 is P(A|D)

Repeat 

for 

each 

xi}

E = 0 · P(0) + 1 · P(1)


40

Complete Data 

Likelihood

(Better):


41

M-step:

Find θ maximizing E(log(Likelihood))


42

2 Component Mixture

σ1 = σ2 = 1;  τ = 0.5



Essentially converged in 2 iterations

(Excel spreadsheet on course web)


Applications

43

Clustering is a remarkably successful exploratory data 

analysis tool

Web-search, information retrieval, gene-expression, ...

Model-based approach above is one of the leading ways to do it

Gaussian mixture models widely used

With many components, empirically match arbitrary distribution

Often well-justiﬁed, due to “hidden parameters” driving the 

visible data

EM is extremely widely used for “hidden-data” problems

Hidden Markov Models


44

EM Summary

Fundamentally a maximum likelihood parameter 

estimation problem

Useful if hidden data, and if analysis is more 

tractable when 0/1 hidden data z known

Iterate: 

E-step: estimate E(z) for each z, given θ

M-step: estimate θ maximizing E(log likelihood) 

given E(z) [where “E(logL)” is wrt random z ~ E(z) = p(z=1)]


45

EM Issues

Under mild assumptions, EM is guaranteed to 

increase likelihood with every E-M iteration, 

hence will converge.

But it may converge to a local, not global, max. 

(Recall the 4-bump surface...)

Issue is intrinsic (probably), since EM is often 

applied to problems (including clustering, 

above) that are NP-hard (next 3 weeks!)

Nevertheless, widely used, often effective

