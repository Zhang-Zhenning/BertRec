
The expectation-maximization algorithm - Part 1

Jul 3, 2021 | Reading time: 20 mins

machine learning Mathematica mathematics optimization

Contents

Introduction

What is EM about?

Maximum likelihood estimation (MLE)

… in the presence of hidden variables

What are the basic steps of EM?

A 1-dimensional example

Setting up the problem

Writing down the likelihood function

Brute forcing one parameter at a time

Reformulating the problem as a latent variable problem

EM algorithm

References

Introduction

What is EM about?

Maximum likelihood estimation (MLE)

The expectation-maximization (EM) algorithm is an iterative method to ﬁnd the local maximum likelihood of parameters in

statistical models. So what is the maximum likelihood? It’s the maximum value of the likelihood function! And what is a

what is a

likelihood function?

likelihood function? It’s a function of the model’s parameters treating the observed data as ﬁxed points, i.e., we write 

L(θ � x)

meaning that we vary the parameters θ

while taking the x

’s as given. If L(θ1 � x) &gt; L(θ2 � x)

then the sample we observed is more likely to have occurred if θ = θ1

rather than if θ = θ2

. So, given the data that we have observed, the likelihood function points to a model’s most plausible parameterization that

might have generated the observed data.

Here is an elementary example. Suppose that we have some data and want to fit a model of the form y = ax

. In this case, θ

is essentially the coefficient a

, but usually, there will be many unknown parameters. In the left image, there’s the likelihood function for several values of the

parameter a

(actually, it’s the logarithm of the likelihood function, but we will talk about this later). In the right image, we plot 

y = ax, a = − 3, …7

with a step size of 0.5, superimposed with the observed data. As you can see, a = 2

maximizes the log-likelihood and ﬁts the data better than any other line. So, ﬁtting data to models can be done via

ﬁtting data to models can be done via

maximum likelihood estimation

maximum likelihood estimation.

By the way, in a previous blog post we have proven that by maximizing the likelihood in the linear regression case,

maximizing the likelihood in the linear regression case,

this is equivalent to minimizing the mean squared error

this is equivalent to minimizing the mean squared error.

… in the presence of hidden variables



Let's talk about science!




The EM algorithm is particularly useful when there are missing data in the data set or when the model depends on hidden

hidden or

so-called latent variables

latent variables. These are variables that affect our observed data but in ways that we can’t know directly. So what’s

so special about latent parameters? Typically, if we know all the parameters, we can take the derivatives of the likelihood

function with respect to them, solve the system of equations and find the values that maximize the likelihood. Like:

∂L

∂θ1 = 0,

∂L

∂θ2 = 0, …

This is precisely what we did when we wanted to ﬁt some data to a normal distribution. However, in statistical models with

latent variables, this typically results in a set of equations where the solutions to the parameters mandate the values of the

latent variables and vice versa. By substituting one set of equations into the other, an unsolvable equation is produced. That’s

why we need the expectation-maximization algorithm. Concretely, EM can be used in any of the following scenarios:

Estimating parameters of (usually Gaussian) mixture models

Estimating parameters of Hidden Markov Models

Unsupervised learning of clusters

Filling missing data in samples

What are the basic steps of EM?

EM takes its name from the alternation between two algorithmic steps. The ﬁrst step is the expectation step

expectation step, where we form

a function for the expectation of the log-likelihood, using the current best estimates of the model’s parameters. Whereas, in

the maximization step

maximization step, we calculate the new parameters’ values by maximizing the expected log-likelihood. These new

estimates of the parameters are then used to determine the distribution of the latent variables in the next expectation step.

Don’t worry if it doesn’t make sense now; we will show an example in a minute, and we will also delve into it in subsequent

blog posts.

A 1-dimensional example

Setting up the problem

Let us consider some observed 1-dimensional data points, xi

. We assume they are generated by two normal distributions N(μ1, σ2

1)

and N(μ2, σ2

2)

, with probabilities π

and 1 − π

, respectively. In this setup, we have 5 unknown parameters: the mixing probability π

, the mean and standard deviation of the first distribution, and the mean and standard deviation of the second distribution. Let

us gather all these under a vector called θ = [π, μ1, σ1, μ2, σ2]

.

Writing down the likelihood function

Suppose that we observed a datapoint with value xi

. What is the probability of xi

occuring? Assuming φ1(x)

is the probability density function of the 1st distribution, and φ2(x)

of the second, the probability of observing xi

is:

p(xi) = πφ1(xi) + (1 − π)φ2(xi)

To be more pedantic we would write:

p(xi � θ) = πφ1(xi � μ1, σ21) + (1 − π)φ2(xi � μ2, σ22)

Which means that the PDF’s are paremeterized by μ1, σ2

1

{

}






and μ2, σ2

2

, respectively. Ok, but this is just for a single observation xi

. What if we have a bunch of xi

’s, say for i = 1, …, N

? To find the joint probability of N

independent events (which by the way is the likelihood function!) we just multiply the individual probabilities:

L(θ � x) =

N

∏

i=1p(xi � θ)

But since it’s easier to work with sums rather than products, we take the logarirthm of the likelihood, ℓ(θ � x)

:

ℓ(θ � x) = log

N

∏

i=1p(xi � θ) =

N

∑

i=1logp(xi � θ)

=

N

∑

i=1log πφ1(xi � μ1, σ2

1) + (1 − π)φ2(xi|μ2, σ2

2)

So, our objective is to maximize likelihood L(θ � x)

, which is equivalent to maximizing the log-likelihood ℓ(θ � x)

, with respect to the model’s parameters θ = [π, μ1, σ1, μ2, σ2]

, given the data points {xi}

.

Brute forcing one parameter at a time

In the following examples, we will generate some synthetic observed data from a mixture distribution with known parameters 

μ1, σ1, μ2, σ2

and mixing probability π

. We will then calculate ℓ(θ � x)

for various parameter values while keeping the rest of the parameters fixed. Every time we will do that, we will see how ℓ(θ � x)

is maximized when a model’s parameter becomes equal to its ground-truth value.

Let’s create a mixture distribution of two Gaussian distributions with known parameters μ1, σ1, μ2, σ2

and known mixing probability π = 0.3

. Normally, we won’t know the values of these parameters, and as a matter of fact, ﬁnding them will be the very objective

ﬁnding them will be the very objective

of the EM algorithm

of the EM algorithm. But for now, let’s pretend we don’t know them.

ClearAll["Global`*"];

{m1, s1} = {1, 2};

{m2, s2} = {9, 3};

npts = 5000;

dist[m_, s_] := NormalDistribution[m, s];

mixdist[p_] :=

 MixtureDistribution[{p, 1 - p}, {dist[m1, s1], dist[m2, s2]}]

data = RandomVariate[mixdist[0.3], npts];

Histogram[data]

Let’s plot the probability density functions of the mixture distribution for various mixing probabilities π

. We notice how for π → 0

the mixture distribution approaches the 1st distribution, and for π → 1

, the 2nd distribution. For in-between values, it’s a mixture! ;)

Style[Grid[{

 Table[

 Plot[PDF[mixdist[p], x], {x, -10, 20}, 

 PlotLabel -&gt; "p=" &lt;&gt; ToString@p,

 FrameLabel -&gt; {"x", "PDF(x)"}, 

 Frame -&gt; {True, True, False, False},

 AxesOrigin -&gt; {-10, 0}, Filling -&gt; Axis],

 {p, 0, 1, 0.3}]

 }],

 ImageSizeMultipliers -&gt; 0.7]

[

]






Let us now define the log-likelihood function:

logLikelihood[data_, p_, m1_, s1_, m2_, s2_] :=

 Module[{},

 Sum[

 Log[

 p PDF[dist[m1, s1], x] + (1 - p) PDF[dist[m2, s2], x] /. 

 x -&gt; data[[i]]

 ],

 {i, 1, Length@data}]

 ]

 

Ok, we are ready to go. We will first vary the mixing probability π

, keeping the rest of the model’s parameters fixed. In some sense, we are brute-forcing π

, to find π

:

llvalues = 

 Table[{p, logLikelihood[data, p, m1, s1, m2, s2]}, {p, 0, 1, 0.1}];

{pmax, llmax} = 

 llvalues[[Ordering[llvalues[[All, 2]], -1][[1]]]]

(* {0.3, -14437.1} *)

plot1 =

 Show[

 ListPlot[llvalues, Joined -&gt; True, 

 FrameLabel -&gt; {"Probability p", "Log-Likelihood"}, 

 Frame -&gt; {True, True, False, False}, 

 GridLines -&gt; {{pmax}, {llmax}}, GridLinesStyle -&gt; Dashed],

 ListPlot[llvalues, PlotStyle -&gt; {Red, AbsolutePointSize[5]}]

 ]

Do you see how ℓ(θ � x)

is maximized at π = 0.3

? By the same token, we can try other model parameters, but we will always come to the same conclusion: the log-

the log-

likelihood, therefore the likelihood, is maximized when our guesses become equal to the ground-truth values

likelihood, therefore the likelihood, is maximized when our guesses become equal to the ground-truth values

for the model’s parameters

for the model’s parameters.

Reformulating the problem as a latent variable problem

Previously, we varied one parameter at a time, keeping the rest at their ground-truth values. We will now get serious and seek

to estimate the values of 

estimate the values of all

all parameters simultaneously

 parameters simultaneously. If we attempt to directly maximize ℓ(θ|x)

, it will be tough due to the sum of terms inside the logarithm. For those of you who doubt it, just calculate the partial

derivatives of ℓ(θ|x)

with respect to π, μ1, σ1, μ2, σ2

and contemplate solving the system where all these derivatives are required to become zero. Good luck with that! :P

There’s another way to go, though. We will reformulate our problem as a problem of maximum likelihood estimation with

maximum likelihood estimation with

latent variables

latent variables. For this, we will introduce a set of latent variables called Δi � {0, 1}

. If Δi = 0

then xi

was sampled from the 1st distribution, and if Δi = 1

, then it came from the 2nd distribution. In this case, the log-likelihood ℓ(θ � x, Δ)

is given by:

ℓ(θ � x, Δ) =

N

∑

i=1 (1 − Δi)logφ1(xi) + Δilogφ2(xi) +

N

∑

i=1 (1 − Δi)logπ + Δilog(1 − π)

When we write φ1(xi)

in reality we mean φ1(xi � μ1, σ21)

[

]

[

]


, and similarly for φ2(xi)

we mean φ2(xi � μ2, σ22)

. The reason we omited it, is for keeping the log-likelihood expression easily readable. Feel free to check that the above

formula is equal to the previous expression of ℓ(θ � x)

, by first letting Δi = 0

and then Δi = 1

.

But, we don’t actually know the values Δi

. After all, these are the latent variables that we introduced into the problem! If you feel that we ain’t making any progress, hold

on. Here’s where the EM algorithm kicks in. Even though we don’t know the exact values Δi

, we will use their expected values given our current best estimates for the model’s parameters! This is the expectation step

This is the expectation step

of the EM algorithm

of the EM algorithm. So, instead of Δi

, we will use γi

defined as:

γi(θ) = E(Δi � θ, x) = Pr(Δi = 1 � θ, x)

Once we have γi

calculated, we know which distribution xi

belongs to. Therefore, we can update the model’s parameters using the weighted maximum-likelihood ﬁts. For Gaussian

distributions, this is just the mean and standard deviation of the xi

. This is the maximization step!

This is the maximization step! Actually, γi

doesn’t take discrete values like the Δi

. Instead, it lies in the interval [0, 1]

and, therefore, the EM algorithm does a soft membership assignment. I.e., for every xi

, it assigns a probability that it comes from the 1st or the 2nd distribution. That’s why, when we calculate the Gaussians’

parameters, we use a γi

-weighted average.

EM algorithm

So, here’s the EM algorithm for our particular problem:

Initialize unknown parameters (e.g., ˆπ = 0.5,

^

μ1 = random xi, σi = ∑N

i=1(xi − ˉx)2/N, …

Expectation step

Expectation step:

^

γi =

(1 − π)φ2(xi)

πφ1(xi) + (1 − π)φ2(xi)

Maximization step

Maximization step:

^

μ1 =

∑N

i=1(1 −

^

γi)xi

∑Ni=1(1 −

^

γi)

^

μ2 =

∑N

i=1

^

γixi

∑Ni=1

^

γi

^

σ1 =

∑N

i=1(1 −

^

γi)(xi −

^

μ1)2

∑N

i=1(1 −

^

γi)

^

σ2 =

∑N

i=1

^

γi(xi −

^

μ2)2

∑N

i=1

^

γi

ˆπ =

N

∑

i=1(1 −

^

γi)/N

Repeat until convergence or maximum number of iterations reached.

Here is a sample code that implements the EM algorithm for our particular problem. The code doesn’t look pretty without

Mathematica’s syntax color highlighting and the Notebook’s format, but anyway.

√

√






em[data_, p_, m1_, s1_, m2_, s2_] :=

 Module[{newp, newm1, news1, newm2, news2, g, npts},

 npts = Length@data;

 g = Table[((1 - p) PDF[dist[m2, s2], data[[i]]]) / (p PDF[dist[m1, s1], data[[i]]] + (1 - p) PDF[dist[m2, s2], data[[i]]]), {i, 1, npts}];

 newm1 = Sum[(1 - g[[i]])*data[[i]], {i, 1, npts}] / Sum[1 - g[[i]], {i, 1, npts}];

 newm2 = Sum[g[[i]]*data[[i]], {i, 1, npts}] / Sum[g[[i]], {i, 1, npts}];

 news1 = Sqrt[Sum[(1 - g[[i]])*(data[[i]] - m1)^2, {i, 1, npts}] / Sum[1 - g[[i]], {i, 1, npts}]];

 news2 = Sqrt[Sum[g[[i]]*(data[[i]] - m2)^2, {i, 1, npts}] / Sum[g[[i]], {i, 1, npts}]];

 newp = Sum[(1 - g[[i]])/npts, {i, 1, npts}];

 {newp, newm1, news1, newm2, news2, g}

 ]

doEM[data_] :=

 Module[{p, m1, s1, m2, s2, g},

 {p, m1, s1, m2, s2} = {0.5, RandomChoice[data], StandardDeviation[data], RandomChoice[data], StandardDeviation[data]};

 Print[{p, m1, s1, m2, s2}];

 For [i = 1, i &lt; 40, i++,

 {p, m1, s1, m2, s2, g} = em[data, p, m1, s1, m2, s2];

 If[Mod[i, 4] == 0, Print[{p, m1, s1, m2, s2}]]

 ];

 {p, m1, s1, m2, s2, g}

 ]

This is a short test run, where we conﬁrm that the algorithm converges to the ground-truth values (the red lines are the

ground-truth values). As we mentioned in the introduction, EM is a local algorithm, meaning it can get stuck at a local

maximum. Therefore, sometimes we may need to repeat the algorithm a few times to ensure a near-global optimal solution.

In the following plot, we see how the γi

’s vary as the observed data transition from the 1st to the 2nd distribution. E.g., when we look at observed data around x=1 (or

less), the γi

’s are equal to zero. This means that the EM algorithm doesn’t cast any doubt on the source of these values. They were

sampled from the 1st distribution. When we look at observed data around x=9 (or more), EM is conﬁdent that these values

originate from the second distribution (γi = 1

). However, when we are in between, γi

’s assume intermediate values around 0.5, conveying the uncertainty regarding which distribution each xi

belongs to. So, by applying the EM algorithm, we discovered the membership of each observed value (with some

we discovered the membership of each observed value (with some

uncertainty), 

uncertainty), and

and we estimated the model’s unknown parameters!

 we estimated the model’s unknown parameters! Neat?

References

1. The Elements of Statistical Learning, Data Mining, Inference, and Prediction by Trevor Hastie, Robert Tibshirani, and

Jerome Friedman.

SUBSCRIBE

Get new posts to your inbox

Get new posts to your inbox

name@example.com

 

SUBSCRIBE

SUBSCRIBE

A blog on things I’m interested in such as mathematics, physics, programming, machine learning, data science, and radiation oncology.

