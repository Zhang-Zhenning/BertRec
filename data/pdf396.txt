
A Developer Diary

{about:"code learn and share"}





February 17, 2019 By Abhisek Jana — 10 Comments

Forward and Backward Algorithm in

Hidden Markov Model



Introduction to Hidden Markov Model article provided basic understanding of the

Hidden Markov Model. We also went through the introduction of the three main

problems of HMM ( Evaluation, Learning and Decoding). In this Understanding

Understanding

Forward and Backward Algorithm in Hidden Markov Model 

Forward and Backward Algorithm in Hidden Markov Model article we will dive

deep into the Evaluation Problem

Evaluation Problem. We will go through the mathematical

understanding &amp; then will use Python and R to build the algorithms by ourself.

Quick Recap:

Hidden Markov Model is a Markov Chain

Markov Chain which is mainly used in problems with

temporal sequence 

temporal sequence of data. Markov Model explains that the next step depends

only on the previous step in a temporal sequence. In Hidden Markov Model the

state of the system is hidden (invisible), however each state emits a symbol at

every time step. HMM works with both discrete and continuous sequences of data.

(Here we will only see the example of discrete data)

Basic Structure of HMM:

As we have discussed earlier, Hidden Markov Model (θ) has with following

parameters :




Set of M Hidden States (SM)

A Transaction Probability Matrix (A)

A sequence of T observations (VT)

A Emission Probability Matrix (Also known as Observation Likelihood) (B)

An Initial Probability Distribution (π)

In case you are not sure of any of above terminology, please refer my previous

article on Introduction to Hidden Markov Model:

Introduction to Hidden Markov Model

Evaluation Problem:

As we have seen earlier, the Evaluation Problem can be stated as following,

Given θ,VT → Estimate p(VT|θ)Where θ → s,v,aij,bjk

Solution:

First we need to find all possible sequences of the state SM where M is the

number of Hidden States.

Then from all those sequences of SM, find the probability of which sequence

generated the visible sequence of symbols VT

Mathematically, p(VT|θ) can be estimated as,

p(VT|θ) = ∑R

r=1p(VT|ST

r )p(ST

r )where ST

r = {s1(1),s2(2)…sr(T)}

and RR=Maximum Number of possible sequences of the hidden state

So, if there are M number of hidden state, then we can define R as :

R = MT

In order to compute the probability of the model generated by the particular

particular

sequence of T visible symbols VT, we should take each conceivable sequence of

hidden state, calculate the probability that they have produced VT and then add

up these probabilities.

Question :

Question :

Question you might be having is how to proof that the above equation is valid?

Let’s try to understand this in a different way.

Remember our example? So here is the diagram of a specific sequence of 3 states.

The transition between the Hidden Layers have been grayed out intentionally, we

will come back to that in a moment.

“




In case in the above example we already know the sequence of the Hidden states

(i.e sun, sun, cloud) which generated the 3 visible symbols happy, sad &amp; happy,

then it will be very easy to calculate the probability of the visible symbols/states

given the hidden state. So we can write probability of VT given ST as:

p(happy, sad, happy | sun, sun, rain ) = p(happy|sun) x p(sad|sun) x p(happy|rain)

Mathematically,

p(VT|ST

r ) = ∏T

t=1p(v(t)|s(t))

Unfortunately we really do not know the specific sequence of hidden states

do not know the specific sequence of hidden states which

generated the visible symbols happy, sad &amp; happy.Hence we need to compute the

probability of mood changes happy, sad &amp; happy by summing over all possible

summing over all possible

weather sequences

weather sequences, weighted by their probability (transition probability).

We now have the same state diagram, however now the transition probabilities

have been given here.



We can calculate the joint probability

joint probability of the sequence of visible symbol VT

generated by a specific sequences of hidden state ST as:

p(happy,sad,happy,sun,sun,rain) = p(sun|initial state) x p(sun|sun) x p(rain|sun)

p(happy,sad,happy,sun,sun,rain) = p(sun|initial state) x p(sun|sun) x p(rain|sun)

x p(happy|sun) x 

x p(happy|sun) x x p(sad|sun) x p(happy|rain)

x p(sad|sun) x p(happy|rain)

Mathematically,


p(VT,ST) = p(VT|ST)p(ST)

Since we are using First-Order Markov model

First-Order Markov model, we can say that the probability of a

sequence of T hidden states is the multiplication of the probability of each

transition.

p(ST) = ∏T

t=1p(s(t)|s(t − 1))

Write the joint probability as following,

p(VT,ST) = p(VT|ST)p(ST)

=

T

∏

t=1p(v(t)|s(t))

T

∏

t=1p(s(t)|s(t − 1))

As you can see, we are slowly getting close to our original equation. Just one more

step is left now. The above equation is for a specific sequence

specific sequence of hidden state

that we thought might have generated the visible sequence of symbols/states. We

can now compute the probably of all

all the different possible sequences of hidden

states by summing over all the joint probabilities

summing over all the joint probabilities of VT and ST.

In our example, we have a sequence of 3 visible symbols/states, we also have 2

different states to represent. So there can be 23 = 8 possible sequences. We can

write them as:

p(happy,sad,happy|model) = p(happy,sad,happy,sun,sun,sun) +

p(happy,sad,happy|model) = p(happy,sad,happy,sun,sun,sun) +

p(happy,sad,happy,sun,sun,rain) + p(happy,sad,happy,sun,rain,rain)+ . . .

p(happy,sad,happy,sun,sun,rain) + p(happy,sad,happy,sun,rain,rain)+ . . .

We can write the generalized equation as:

p(VT|θ) =

∑

All Seq of Sp(VT,ST)

=

∑

All Seq of Sp(VT|ST)p(ST)

=

R

∑

r=1

T

∏

t=1p(v(t)|s(t))

T

∏

t=1p(s(t)|s(t − 1))

=

R

∑

r=1

T

∏

t=1p(v(t)|s(t))p(s(t)|s(t − 1))

Again, R=Maximum Number of possible sequences of the hidden state.

The above solution is simple, however the computation complexity

computation complexity is O(NT.T),

which is very high for practical scenarios. So even if we have derived the solution

to the Evaluation Problem, we need to find an alternative which should be easy to

compute.

We will a recursive dynamic programming

recursive dynamic programming approach to overcome the exponential

exponential

computation

computation we had with the solution above. There are two such algorithms,

Forward Algorithm and Backward Algorithm.

Forward Algorithm:

In Forward Algorithm (as the name suggested), we will use the computed

probability on current time step

current time step to derive the probability of the next time step

next time step.

Hence the it is computationally more efficient O(N2.T).

We need to find the answer of the following question to make the algorithm

recursive:


Given a a sequence of Visible state VT , what will be the probability that the Hidden

Markov Model will be in a particular hidden state s at a particular time step t.

If we write the above question mathematically it might be more easier to

understand.

αj(t) = p(v(1)…v(t),s(t) = j)

First, we will derive the equation using just probability &amp; then will solve again

using trellis diagram. So don’t worry if you are not able to fully understand the

next section, just read along and come back after going through the trellis

diagram.

Solution using Probabilities:

When 

When t = 1 : :

Rewrite the above equation when t=1

αj(1) = p(vk(1),s(1) = j)

= p(vk(1)|s(1) = j)p(s(1) = j)

= πjp(vk(1)|s(1) = j)

= πjbjk

where π =  initial distribution, 

bjkv(1) =  Emission Probability at t = 1

When 

When t = 2 : :

So we have the solution when t=1. Now lets rewrite the same when t=2. Our

objective here will be to come up with an equation where αj(1) is part of it, so that

we can use recursion.

αj(2) = p vk(1),vk(2),s(2) = j

=

M

∑

i=1p vk(1),vk(2),s(1) = i,s(2) = j

=

M

∑

i=1p vk(2)|s(2) = j,vk(1),s(1) = i p vk(1),s(2),s(1) = i

=

M

∑

i=1p vk(2)|s(2) = j,vk(1),s(1) = i p s(2)|vk(1),s(1) = i p vk(1),s(1) = i

=

M

∑

i=1p vk(2)|s(2) = j p s(2)|s(1) = i p vk(1),s(1) = i

= p vk(2)|s(2) = j

M

∑

i=1p s(2)|s(1) = i p vk(1),s(1) = i

= bjkv(2)

M

∑

i=1ai2αi(1)

where ai2 =  Transition Probability 

bjkv(2) =  Emission Probability at t = 2

αi(1) =  Forward probability at t = 1

Let me try to explain some part of it. We have just used the Joint Probability

Joint Probability Rule

and have broken the equation in different parts.

In Line 2 we have added s(1) = i for which we have added the summation since

there are M different hidden states. The red highlighted section in Line 4 can be

removed. Finally line 6 has 3 parts which are highlighted in colors. Since 

p(vk(2)|s(2) = j) does not depend on i, we can move it outside of the summation.

The final equation consists of αi(1) which we have already calculated when t=1.

(

)

(

)

(

) (

)

(

) (

) (

)

(

) (

) (

)

(

)

(

) (

)


Generalized Equation :

Generalized Equation :

Let’s generalize the equation now for any time step t+1:

αj(t + 1) = p vk(1)…vk(t + 1),s(t + 1) = j

=

M

∑

i=1p vk(1)…vk(t + 1),s(t) = i,s(t + 1) = j

=

M

∑

i=1p vk(t + 1)|s(t + 1) = j,vk(1)…vk(t),s(t) = i

p vk(1)…vk(t),s(t + 1),s(t) = i

=

M

∑

i=1p vk(t + 1)|s(t + 1) = j,vk(1)…vk(t),s(t) = i

p s(t + 1)|vk(1)…vk(t),s(t) = i p vk(t),s(t) = i

=

M

∑

i=1p vk(t + 1)|s(t + 1) = j p s(t + 1)|s(t) = i p vk(t),s(t) = i

= p vk(t + 1)|s(t + 1) = j

M

∑

i=1p s(t + 1)|s(t) = i p vk(t),s(t) = i

= bjkv(t+1)

M

∑

i=1aijαi(t)

The above equation follows the same derivation as we did for t=2. This equation

will be really easy to implement using any programming language. We won’t use

recursion function, just use the pre-calculated values in a loop (More on this later).

Intuition using Trellis:

We will use Trellis Diagram to get the intuition behind the Forward Algorithm. I

case you have not understood the derivation using joint probability rule, this

section will definitely help you to understand the equation.

I am repeating the same question again here:

Given a a sequence of Visible state VT , what will be the probability that the Hidden

Markov Model will be in a particular hidden state s at a particular time step t.

Step by Step Derivation:

Step by Step Derivation:

Please refer the below Trellis diagram and assume the probability that the

system/machine is at hidden state s1 at time (t − 1) is α1(t − 1). The probability of

transition to hidden state s2 at time step t can be now written as,

α1(t − 1)a12

(

)

(

)

(

)

(

)

(

)

(

) (

)

(

) (

) (

)

(

)

(

) (

)




Likewise, if we sum all the probabilities where the machine transition to state s2 at

time t from any state at time (t − 1), it gives the total probability that there will

a transition from any hidden state at (t − 1) to s2 at time step t.

Mathematically,

∑M

i=1αi(t − 1)ai2

Finally, we can say the probability that the machine is at hidden state s2 at time t,

after emitting first t number of visible symbol from sequence VT is given but the

following, (We simply multiply the emission probability to the above equation)

b2k∑M

i=1αi(t − 1)ai2

Now we can extend this to a recursive algorithm to find the probability that

sequence VT was generated by HMM θ. Here is the generalized version of the

equation.

αj(t) =

πjbjk

 when t = 1

bjk∑M

i=1αi(t − 1)aij

 when t greater than 1

Here αj(t) is the probability that the machine will be at hidden state sj at time step

t, after emitting first t visible sequence of symbols.

Implementation of Forward Algorithm:

Now lets work on the implementation. We will use both Python and R for this.

Data:

Data:

In our example we have 2 Hidden States (A,B) and 3 Visible States (0,1,2) ( in R

file, it will be (1,2,3) ). Assume that we already know our a and b.

A = 0.54 0.46

0.49 0.51

{

[

]


B = 0.16 0.26 0.58

0.25 0.28 0.47

The data_python.csv &amp; data_r.csv has two columns named, Hidden and

Visible. The only difference between the Python and R is only the starting index

of the Visible column. Python file has 0,1,2 where as R has 1,2,3.

Python:

Python:

First Load the data.

Then set the values for transition probability, emission probabilities and initial

distribution.

In python the index starts from 0, hence our t will start from 0 to T-1.

Next, we will have the forward function. Here we will store and return all the 

α0(0),α1(0)…α0(T − 1),α1(T − 1)

First we will create the alpha matrix with 2 Columns and T Rows.

As per our equation multiply initial_distribution with the bjkv(0) to calculate 

α0(0),α1(0). This will be a simple vector multiplication since both

initial_distribution and bkv(0) are of same size.

We will loop through the time steps now, starting from 1 ( remember python

index starts from 0 ).

Another loop for each hidden step j.

Use the same formula for calculating the α values.

Return all of the alpha values.

[

]

11

22

33

44

55

66

import

import pandas 

pandas as

as pd

pd

import

import numpy 

numpy as

as np

np

 

data

data  ==  pd

pd..read_csv

read_csv(('data_python.csv'

'data_python.csv'))

 

VV  ==  data

data[['Visible'

'Visible']]..values

values

11

22

33

44

55

66

77

88

# Transition Probabilities

# Transition Probabilities

aa  ==  np

np..array

array((((((0.54

0.54,,  0.46

0.46)),,  ((0.49

0.49,,  0.51

0.51))))))

 

# Emission Probabilities

# Emission Probabilities

bb  ==  np

np..array

array((((((0.16

0.16,,  0.26

0.26,,  0.58

0.58)),,  ((0.25

0.25,,  0.28

0.28,,  0.47

0.47))))))

 

# Equal Probabilities for the initial distribution

# Equal Probabilities for the initial distribution

initial_distribution

initial_distribution  ==  np

np..array

array((((0.5

0.5,,  0.5

0.5))))



11

22

33

44

55

66

77

88

99

10

10

11

11

12

12

13

13

14

14

15

15

def

def forward

forward((VV,,  aa,,  bb,,  initial_distribution

initial_distribution))::

 

    alpha

alpha  ==  np

np..zeros

zeros((((VV..shape

shape[[00]],,  aa..shape

shape[[00]]))))

 

    alpha

alpha[[00,,  ::]]  ==  initial_distribution

initial_distribution  **  bb[[::,,  VV[[00]]]]

 

 

 for

for  tt  in

in range

range((11,,  VV..shape

shape[[00]]))::

 

 for

for  jj  in

in range

range((aa..shape

shape[[00]]))::

 

 # Matrix Computation Steps

# Matrix Computation Steps

 

 #                  ((1x2) . (1x2))      *     (1)

#                  ((1x2) . (1x2))      *     (1)

 

 #                        (1)            *     (1)

#                        (1)            *     (1)

 

            alpha

alpha[[tt,,  jj]]  ==  alpha

alpha[[tt  --  11]]..dot

dot((aa[[::,,  jj]]))  **  bb[[jj,,  VV[[

 

 

 return

return alpha

alpha

 

alpha

alpha  ==  forward

forward((VV,,  aa,,  bb,,  initial_distribution

initial_distribution))

print

print((alpha

alpha))


























Output:

Output:

R Code:

R Code:

Here is the same Forward Algorithm implemented in R. If you notice, we have

removed the 2nd for loop in R code. You can do the same in python too.

Backward Algorithm:

Backward Algorithm is the time-reversed version of the Forward Algorithm. In

Backward Algorithm we need to find the probability that the machine will be in

hidden state si at time step t and will generate the remaining part of the

sequence of the visible symbol VT.

Derivation of Backward Algorithm:

Please find the Derivation of the Backward Algorithm using Probability Theory.

The concepts are same as the forward algorithm.

11

22

33

44

55

66

77

88

99

10

10

11

11

[[[[8.00000000e

8.00000000e--002

002  1.25000000e

1.25000000e--001

001]]

[[[[8.00000000e

8.00000000e--002

002  1.25000000e

1.25000000e--001

001]]

  [[2.71570000e

2.71570000e--002

002  2.81540000e

2.81540000e--002

002]]

  [[1.65069392e

1.65069392e--002

002  1.26198572e

1.26198572e--002

002]]

  [[8.75653677e

8.75653677e--003

003  6.59378003e

6.59378003e--003

003]]

……

……

  [[8.25847348e

8.25847348e--221

221  6.30684489e

6.30684489e--221

221]]

  [[4.37895921e

4.37895921e--221

221  3.29723269e

3.29723269e--221

221]]

  [[1.03487332e

1.03487332e--221

221  1.03485477e

1.03485477e--221

221]]

  [[6.18228050e

6.18228050e--222

222  4.71794300e

4.71794300e--222

222]]]]



11

22

33

44

55

66

77

88

99

10

10

11

11

12

12

13

13

14

14

15

15

16

16

17

17

18

18

19

19

20

20

21

21

22

22

data

data  ==  read

read..csv

csv(("data_r.csv"

"data_r.csv"))

 

aa  ==  matrix

matrix((cc((0.54

0.54,,  0.49

0.49,,  0.46

0.46,,  0.51

0.51)),,nrow

nrow  ==  22,,ncol

ncol  ==  22))

bb  ==  matrix

matrix((cc((0.16

0.16,,  0.25

0.25,,  0.26

0.26,,  0.28

0.28,,  0.58

0.58,,  0.47

0.47)),,nrow

nrow  ==  22,,ncol

ncol

initial_distribution

initial_distribution  ==  cc((11//22,,  11//22))

 

forward

forward  ==  function

function((vv,,  aa,,  bb,,  initial_distribution

initial_distribution)){{

 

 

 

 TT  ==  length

length((vv))

 

  mm  ==  nrow

nrow((aa))

 

  alpha

alpha  ==  matrix

matrix((00,,  TT,,  mm))

 

 

 

  alpha

alpha[[11,,  ]]  ==  initial_distribution

initial_distribution**bb[[,,  vv[[11]]]]

 

 

 

 for

for((tt in

in  22::TT)){{

 

    tmp

tmp  ==  alpha

alpha[[tt--11,,  ]]  %%**%%  aa

 

    alpha

alpha[[tt,,  ]]  ==  tmp

tmp  **  bb[[,,  vv[[tt]]]]

 

  }}

 

 return

return((alpha

alpha))

}}

 

forward

forward((data

data$$Visible

Visible,,aa,,bb,,initial_distribution

initial_distribution))


















βi(t) = p vk(t + 1)….vk(T)|s(t) = i

=

M

∑

j=0p vk(t + 1)….vk(T),s(t + 1) = j|s(t) = i

=

M

∑

j=0p vk(t + 2)….vk(T)|vk(t + 1),s(t + 1) = j,s(t) = i

p vk(t + 1),s(t + 1) = j|s(t) = i

=

M

∑

j=0p vk(t + 2)….vk(T)|vk(t + 1),s(t + 1) = j,s(t) = i

p vk(t + 1)|s(t + 1) = j,s(t) = i p s(t + 1) = j|s(t) = i

=

M

∑

j=0p vk(t + 2)….vk(T)|s(t + 1) = j p vk(t + 1)|s(t + 1) = j

p s(t + 1) = j|s(t) = i

=

M

∑

j=0βj(t + 1)bjkv(t+1)aij

where ai2 =  Transition Probability 

bjkv(t+1) =  Emission Probability at t = t + 1

βi(t + 1) =  Backward probability at t = t + 1

Intuition using Trellis:

Here is the Trellis diagram of the Backward Algorithm. Mathematically, the

algorithm can be written in following way:

βi(t) =

1

 when t = T

∑M

j=0aijbjkv(t+1)βj(t + 1)

 when t less than T



Implementation of Backward Algorithm:

We will use the same data file and parameters as defined for Forward Algorithm.

Python Code :

Python Code :

(

)

(

)

(

)

(

)

(

)

(

) (

)

(

) (

)

(

)

{










R Code :

R Code :

Output :

Output :

Conclusion:

In our next article we will use both the forward and backward algorithm to solve

the learning problem. Here I have provided a very detailed overview of the

Forward and Backward Algorithm. The output of the program may not make lot of



11

22

33

44

55

66

77

88

99

10

10

11

11

12

12

13

13

14

14

15

15

16

16

17

17

18

18

19

19

20

20

21

21

22

22

23

23

24

24

25

25

26

26

27

27

28

28

29

29

30

30

31

31

import

import pandas 

pandas as

as pd

pd

import

import numpy 

numpy as

as np

np

 

data

data  ==  pd

pd..read_csv

read_csv(('data_python.csv'

'data_python.csv'))

 

VV  ==  data

data[['Visible'

'Visible']]..values

values

 

# Transition Probabilities

# Transition Probabilities

aa  ==  np

np..array

array((((((0.54

0.54,,  0.46

0.46)),,  ((0.49

0.49,,  0.51

0.51))))))

 

# Emission Probabilities

# Emission Probabilities

bb  ==  np

np..array

array((((((0.16

0.16,,  0.26

0.26,,  0.58

0.58)),,  ((0.25

0.25,,  0.28

0.28,,  0.47

0.47))))))

 

 

def

def backward

backward((VV,,  aa,,  bb))::

 

    beta

beta  ==  np

np..zeros

zeros((((VV..shape

shape[[00]],,  aa..shape

shape[[00]]))))

 

 

 # setting beta(T) = 1

# setting beta(T) = 1

 

    beta

beta[[VV..shape

shape[[00]]  --  11]]  ==  np

np..ones

ones((((aa..shape

shape[[00]]))))

 

 

 # Loop in backward way from T-1 to

# Loop in backward way from T-1 to

 

 # Due to python indexing the actual loop will be T-2 to 0

# Due to python indexing the actual loop will be T-2 to 0

 

 for

for  tt  in

in range

range((VV..shape

shape[[00]]  --  22,,  --11,,  --11))::

 

 for

for  jj  in

in range

range((aa..shape

shape[[00]]))::

 

            beta

beta[[tt,,  jj]]  ==  ((beta

beta[[tt  ++  11]]  **  bb[[::,,  VV[[tt  ++  11]]]]))..dot

dot((aa

 

 

 return

return beta

beta

 

 

beta

beta  ==  backward

backward((VV,,  aa,,  bb))

print

print((beta

beta))



11

22

33

44

55

66

77

88

99

10

10

11

11

12

12

13

13

14

14

15

15

16

16

17

17

18

18

data

data  ==  read

read..csv

csv(("data_r.csv"

"data_r.csv"))

 

aa  ==  matrix

matrix((cc((0.54

0.54,,  0.49

0.49,,  0.46

0.46,,  0.51

0.51)),,nrow

nrow  ==  22,,ncol

ncol  ==  22))

bb  ==  matrix

matrix((cc((0.16

0.16,,  0.25

0.25,,  0.26

0.26,,  0.28

0.28,,  0.58

0.58,,  0.47

0.47)),,nrow

nrow  ==  22,,ncol

ncol

 

backward

backward  ==  function

function((VV,,  AA,,  BB)){{

 

 TT  ==  length

length((VV))

 

  mm  ==  nrow

nrow((AA))

 

  beta

beta  ==  matrix

matrix((11,,  TT,,  mm))

 

 

 

 for

for((tt in

in  ((TT--11))::11)){{

 

    tmp

tmp  ==  as

as..matrix

matrix((beta

beta[[tt++11,,  ]]  **  BB[[,,  VV[[tt++11]]]]))

 

    beta

beta[[tt,,  ]]  ==  tt((AA  %%**%%  tmp

tmp))

 

  }}

 

 return

return((beta

beta))

}}

 

backward

backward((data

data$$Visible

Visible,,aa,,bb))

11

22

33

44

55

66

77

88

99

10

10

[[[[5.30694627e

5.30694627e--221

221  5.32373319e

5.32373319e--221

221]]

  [[1.98173335e

1.98173335e--220

220  1.96008747e

1.96008747e--220

220]]

  [[3.76013005e

3.76013005e--220

220  3.71905927e

3.71905927e--220

220]]

  [[7.13445025e

7.13445025e--220

220  7.05652279e

7.05652279e--220

220]]

......

......

  [[7.51699476e

7.51699476e--002

002  7.44006456e

7.44006456e--002

002]]

  [[1.41806080e

1.41806080e--001

001  1.42258480e

1.42258480e--001

001]]

  [[5.29400000e

5.29400000e--001

001  5.23900000e

5.23900000e--001

001]]

  [[1.00000000e

1.00000000e++000

000  1.00000000e

1.00000000e++000

000]]]]


















sense now, however next article will provide more insight.

Here is the link to the code and data file in github.

Code

Also, here are the list of all the articles in this series:

1. Introduction to Hidden Markov Model

2. Forward and Backward Algorithm in Hidden Markov Model

3. Derivation and implementation of Baum Welch Algorithm for Hidden Markov

Model

4. Implement Viterbi Algorithm in Hidden Markov Model using Python and R

Feel free to post any question you may have.

Filed Under: Machine Learning

Tagged With: Backward Algorithm, Evaluation Problem, Forward Algorithm, Hidden Markov

Related

Related



Introduction to Hidden Markov Model



Implement Viterbi Algorithm in Hidden Markov Model using Python and R



Derivation and implementation of Baum Welch Algorithm for Hidden Markov

Model










Model, HMM, Machine Learning, Python, R, Unsupervised Learning

Comments

Comments

terrence chen says

April 21, 2020 at 8:36 am

this is the best article introducing hmm i have ever seen, but the

backward algo is a little buggy which can be easy to know by comparing

the result with that from forward algo

beta[0, i] = P(v2, v3, …, vT | s1 = i) after the for loop finished in your code

you missed the v1

so when the for loop is over, it is necessary to calculate P(v1, v2, v3, …,

vT and s1 = i) = πi * B[i, V[0]] * beta[0, i]

here is my code:

def backward(O, a, b, pi):

T = O.shape[0]

N = pi.shape[0]

beta = np.zeros((T, N)) # size: T * N

# initial state

beta[T – 1, :] = 1

# dynamic programming

for t in reversed(range(0, T – 1)):

for i in range(N):

beta[t, i] = (a[i, :] * b[:, O[t + 1]]).dot(beta[t + 1, :])

res = np.zeros(N)

for i in range(N):

res[i] = pi[i] * b[i, O[0]] * beta[0, i]

Subscribe to stay in loop

Subscribe to stay in loop

 indicates required

 

Subscribe

*



Email Address *








beta = np.insert(beta, 0, res, 0)

return beta

Reply

Abhisek Jana says

April 22, 2020 at 1:19 pm

Hi Terrence,

First of all thanks for going through the article and providing your

feedback !!!

There are many minor versions of HMM with different

implementations, hence I highly recommend to compare your

prediction result with one of the existing library (like I did with R

Library). The solution I provided in the article was reviewed by my

professor as one of the solution. I will go through the changes you

suggested and will provide an update.

Reply

anteater says

May 25, 2020 at 4:25 am

hi, i think there is a minor mistake in the equation

p(happy,sad,happy,sun,sun,rain) = p(sun|initial state) x p(sun|sun) x

p(cloud|sun) x p(happy|sun) x x p(sad|sun) x p(happy|rain)

it should say p(rain|sun) instead of p(cloud|sun).

also there is an x x instead of x

Reply

Abhisek Jana says

June 1, 2020 at 1:56 am

Hi,

Thanks a lot for reading the post and letting me know about the

typo. It has been corrected.

Regards,

Abhisek Jana

Reply










vij says

July 28, 2020 at 8:19 am

In case in the above example we already know the sequence of the

Hidden states (i.e sun, sun, cloud)

Should this be

sun, sun, rain.

The diagram that you refer doesn’t have cloud

Reply

mia ng says

February 8, 2021 at 6:53 pm

Hi, I do not see how the data looks like for V. Can you upload it or just

print out in the code?

Reply

Sev says

June 8, 2021 at 3:59 pm

Hi Abhisek Jana,

Can you clarify what the subscript k is?

Reply

Zhouhan Lin says

October 9, 2021 at 10:55 am

Have you noticed that the result of forward and backward algorithms are

not predicting the same probability for P(V)? I am having

P_{forward}(V) = 0.007799725285440002, and

P_{backward}(V) = 0.017362729933200004

by following your functions. every number of the calculated alpha and

beta are exactly the same as those in your post.

Something must be wrong with the algorithm.

Reply






amir says

July 20, 2022 at 11:59 pm

where is file csv data_python.csv?

Reply

Abhisek Jana says

July 21, 2022 at 2:33 pm

Its in the github project. Please find the link below ( the same given

at the end of the post )

https://github.com/adeveloperdiary/HiddenMarkovModel/tree/master/part2

Reply

Leave a Reply

Leave a Reply

Your email address will not be published. Required fields are marked *

Comment

Name *

Email *

Website

Save my name, email, and website in this browser for the next time I comment.

Post Comment


This site uses Akismet to reduce spam. Learn how your comment data is processed.

Recent Top Posts


Forward and Backward Algorithm in Hidden Markov Model

How to implement Sobel edge detection using Python from scratch

Applying Gaussian Smoothing to an Image using Python from scratch

Implement Viterbi Algorithm in Hidden Markov Model using Python and

R










How to visualize Gradient Descent using Contour plot in Python

Support Vector Machines for Beginners – Duality Problem

Understanding and implementing Neural Network with SoftMax in

Python from scratch

Support Vector Machines for Beginners – Linear SVM










Machine Translation using Attention with PyTorch

How to prepare Imagenet dataset for Image Classification

Search in this website

Search this website

Top Posts

How to Create Spring Boot Application Step by Step

215.5k views | 9 comments

How to easily encrypt and decrypt text in Java

96.3k views | 8 comments






How to implement Sobel edge detection using Python from scratch

90.9k views | 4 comments

How to deploy Spring Boot application in IBM Liberty and WAS 8.5

83k views | 8 comments

How to integrate React and D3 – The right way

77.6k views | 30 comments

How to create RESTFul Webservices using Spring Boot

71.4k views | 24 comments

Applying Gaussian Smoothing to an Image using Python from scratch

65.1k views | 6 comments

How to convert XML to JSON in Java

56.1k views | 5 comments

Implement Viterbi Algorithm in Hidden Markov Model using Python and R

54.6k views | 11 comments

Forward and Backward Algorithm in Hidden Markov Model

54.5k views | 10 comments

Tags

Angular 1.x Angular 2.x Angular JS BPM Cache Computer Vision D3.js

DataBase Data Science Deep Learning Java JavaScript


jBPM Machine Learning Microservice NLP React JS REST SAAJ Server

Spring Boot Tips Tools Visualization WebService XML Yeoman

Recent Top Posts

Forward and Backward Algorithm in Hidden Markov Model

How to implement Sobel edge detection using Python from scratch

Applying Gaussian Smoothing to an Image using Python from scratch

Implement Viterbi Algorithm in Hidden Markov Model using Python and R

Derivation and implementation of Baum Welch Algorithm for Hidden Markov Model

Support Vector Machines for Beginners - Duality Problem

Understanding and implementing Neural Network with SoftMax in Python from scratch

How to visualize Gradient Descent using Contour plot in Python

An Introduction to Spring Boot

Support Vector Machines for Beginners - Linear SVM


Recent Posts

Machine Translation using Attention with PyTorch

Machine Translation using Recurrent Neural Network and PyTorch

Support Vector Machines for Beginners – Training Algorithms

Support Vector Machines for Beginners – Kernel SVM

Support Vector Machines for Beginners – Duality Problem



Copyright © 2023 A Developer Diary�

�

Processing math: 100%

