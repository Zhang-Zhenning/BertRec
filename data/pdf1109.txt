


Published in

Towards Data Science



Jan 5, 2018

·

14 min read

Save

Probability concepts explained: Bayesian inference

for parameter estimation.

Pixabay

Introduction

Bayes’ Theorem

Mathematical definition








Mathematical definition

Example

How does Bayes’ Theorem allow us to incorporate prior beliefs?

prior. 

Caution: 


Bayesian Inference

Definition

inference 

Bayesian inference 

Using Bayes’ theorem with distributions

prior distribution

2 distributions that represent our prior probability of selling ice on any given day. The peak value of both the blue

and gold curves occur around the value of 0.3 which, as we said above, is our best guess of our prior probability of

selling ice cream. The fact that f(x) is non-zero of other values of x shows that we’re not completely certain that 0.3

is the true value of selling ice cream. The blue curve shows that it’s likely to be anywhere between 0 and 0.5,

whereas the gold curve shows that it’s likely to be anywhere between 0 and 1. The fact that the gold curve is more

spread out and has a smaller peak than the blue curve means that a prior probability expressed by the gold curve

is “less certain” about the true value than the blue curve.

Model form of Bayes’ Theorem

y = 


posterior distribution. 

likelihood distribution

.

Why did I completely disregard P(data)?

evidence

very hard 

Bayesian inference example


I’ve included this image because I think it looks nice, helps to break up the dense text and is kind of related to the

example that we’re going to go through. Don’t worry, you don’t need to understand the figure to understand what

we’re about to go through on Bayesian inference. In case you’re wondering, I made the figure with Inkscape.

Our prior probability for the length of a hydrogen bond. This is represented by a Gaussian distribution with mean μ

= 3.6Å and standard deviation σ = 0.2Å.

Prior probability for the distance of a hydrogen bond in blue and the likelihood distribution in gold derived from the 5

gold data points.

The posterior distribution in pink generated by multiplying the blue and gold distributions.

Maximum a posteriori probability estimate

MAP

Concluding remarks

Why am I always using Gaussians?


Why am I always using Gaussians?

conjugate distributions.

conjugate prior.

What happens when we get new data?

Using priors as regularisers


When is the MAP estimate equal to the maximum likelihood estimate?

Uniform distribution

Data Science

Probability

Bayesian Statistics

Machine Learning

Towards Data Science


51



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





