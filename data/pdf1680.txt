


The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up











Poisson Mixture Models

Brandon Malone

Much of this material is adapted from Bilmes 1998 and Tomasi 2004.

Many of the images were taken from the Internet

February 20, 2014

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Poisson Mixture Models



Suppose we have a dataset D which consists of DNA sequences observed

from a mixture of k bacteria. We do not know which sequence belongs

to which species.

Sequence

Species

Count

CAGAGGAT

?

5

TCAGTGTC

?

13

CTCTGTGA

?

2

AACTGTCG

?

7

CGCGTGGA

?

15

GGATGAGA

?

1

Which DNA sequences belong to the same species?

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Poisson Mixture Models



Suppose we have a dataset D which consists of DNA sequences observed

from a mixture of k bacteria. We do not know which sequence belongs

to which species.

Sequence

Species

Count

CAGAGGAT

?

5

TCAGTGTC

?

13

CTCTGTGA

?

2

AACTGTCG

?

7

CGCGTGGA

?

15

GGATGAGA

?

1

⇒

λk

Dl

zl

P

M

K

Which DNA sequences belong to the same species?

This can be described by a Poisson mixture model.

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



1

The Poisson Distribution



2

Mixture Models



3

Expectation-Maximization



4

Wrap-up

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Multiple Bernoulli trials



Suppose we have a Bernoulli-distributed variable (a weighted coin

ﬂip with parameter θ).

If we ﬂip two coins, what is our probability of seeing exactly one H?

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Multiple Bernoulli trials



Suppose we have a Bernoulli-distributed variable (a weighted coin

ﬂip with parameter θ).

If we ﬂip two coins, what is our probability of seeing exactly one H?

C1

C2

P(C1, C2)

H

H

θ · θ

H

T

θ · (1 − θ)

T

H

(1 − θ) · θ

T

T

(1 − θ) · (1 − θ)

So, P(exactly one H) = 2 · θ · (1 − θ).

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Multiple Bernoulli trials



Suppose we have a Bernoulli-distributed variable (a weighted coin

ﬂip with parameter θ).

If we ﬂip two coins, what is our probability of seeing exactly one H?

C1

C2

P(C1, C2)

H

H

θ · θ

H

T

θ · (1 − θ)

T

H

(1 − θ) · θ

T

T

(1 − θ) · (1 − θ)

So, P(exactly one H) = 2 · θ · (1 − θ).

In general, P(exactly m successes in n trials) =

�n

m

�

· θm · (1 − θ)n−m.

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Take it, to the limit, one more time



What if we have an inﬁnite number of trials and expect to see λ

successes?

lim

n→∞ P(exactly m successes in n trials) = λm

m! exp {−λ}

This is called the Poisson distribution.

We will write g(m : λ) to mean P(exactly m successes given λ).

(See the videos for a detailed derivation.)

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Mixtures of distributions



Suppose we have K Poisson distributions (components) with

parameters λ1 . . . λK mixed together with proportions p1 . . . pK.

We often write P = {p1 . . . pK} and θ = {λ1 . . . λK, P}.

procedure GenerateDataset(Poisson parameters λ1 . . . λk , mixing proportions

p1 . . . pk , samples N)

D ← ∅

for l = 1 to N do

component zl ←sample(Mult(p1 . . . pK))

observation Dl ←sample(Poisson(λzl))

D ← D ∪ Dl

end for

return D

end procedure

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Mixtures of distributions



Suppose we have K Poisson distributions (components) with

parameters λ1 . . . λK mixed together with proportions p1 . . . pK.

We often write P = {p1 . . . pK} and θ = {λ1 . . . λK, P}.

λk

Dl

zl

P

M

K

Figure: Generative model for a Poisson mixture model (PMM)

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Likelihood of data



We can write the (log) probability of any mixture model as follows.

P(D : θ) =

K

�

k

pkg(D : λk)

P(D : θ) =

N

�

l

K

�

k

pkg(Dl : λk)

ℓ(D : θ) = log

N

�

l

K

�

k

pkg(Dl : λk)

ℓ(D : θ) =

N

�

l

log

K

�

k

pkg(Dl : λk)

The learning problem can be formulated as follows.

θ∗ = arg max

θ

ℓ(D : θ)

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Membership probabilities



Notation

q(k, l) ..= pkg(Dl : λk)

joint probability of Dl and component k

P(k|l) ..= P(zl = k|Dl)

conditional probability of component k given Dl

The probability that Dl came from comonent k is expressed as

follows.

P(k|l) =

q(k, l)

�K

m q(m, l)

Also, we know each observation came from some component.

�

k

P(k|l) = 1

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Jensen’s Inequality



Recall the likelihood of the mixture model.

ℓ(D : θ) =

N

�

l

log

K

�

k

q(k, l)

Jensen’s inequality shows the following.

log

K

�

k

πkαk ≥

K

�

k

πk log αk

when π is a distribution

We can make this work for any values.

log

K

�

k

ck = log

K

�

k

ck

πk

πk

= log

K

�

k

πk

ck

πk

≥

K

�

k

πk log ck

πk

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Expectation-Maximization (EM)



Our learning problem is formulated as follows.

θ∗ = arg max

θ

ℓ(D : θ)

EM begins with a (bad) set of estimates for θ.



1 Use Jensen’s inequality to estimate a bound b on ℓ

called the expectation of ℓ



2 Find values of θ which maximize b

EM is guaranteed to ﬁnd θs which do not decrease b.

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Expectation and the Q function



Recall the deﬁnition of ℓ and Jensen’s inequality.

ℓ(D : θ) =

N

�

l

log

K

�

k

q(k, l)

≥

N

�

l

K

�

k

P(k|l) log q(k, l)

P(k|l)

This gives the expectation of ℓ with our current parameters θ.

Based on this equation, we deﬁne Q(θ) which we want to maximize.

Q(θ) =

N

�

l

K

�

k

P(k|n) log q(k, l)

(See the handout for a detailed derivation of Q.)

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Maximization and the Q function



We use the following process to maximize Q for a particular

parameter θi.



1 Diﬀerentiate Q w.r.t θi



2 Set the derivative equal to 0



3 Solve for θi

(See the handout for detailed derivations.)

λk =

�N

l P(k|l)Dl

Z(k)

pk = Z(k)

N

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



The EM algorithm for PMMs



procedure pmmEM(data D, inital p1 . . . pK , λ1 . . . λK , convergence criteria C)

while C has not been met do

▷ Update the expectations

q(k, l) ← pk · g(Dl, λk)

P(k|l) ←

q(k,l)

�K

m q(m,l)

▷ Maximize the parameters

λk ←

�N

l P(k|l)Dl

Z(k)

pk ← Z(k)

N

end while

end procedure

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Grouping the DNA sequences into clusters



After running EM, we have several useful pieces of information

about our metagenomics sample.



P(k|l). The distribution over species for each sequence.



pk. The relative genome sizes of the species.



λk. The abundance of the species.

Other questions...



Do we really know how many species there are?



Can we diﬀerentiate species with similar abundances?



How do we pick “good” initial parameters?



When have we converged?

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



More on EM



EM is a general framework that is useful whenever data is missing.



If used to estimate class probabilities in naive Bayes models, it

is called Bayesian clustering



If used in HMMs, it is called the Baum-Welch algorithm



Can be used in general Bayesian networks to calculate

parameters when some data is missing



If used with structure learning algorthms, it is called

Structural EM



Many, many others...

We maximize likelihood with EM. What if we want MAP

parameters?

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Recap



During this part of the course, we have discussed:



Mixture models as a probabilistic clustering method



Expectation-maximization as a framework for estimating

parameters when variables are hidden

Brandon Malone

Poisson Mixture Models




The Poisson Distribution

Mixture Models

Expectation-Maximization

Wrap-up



Next in probabilistic models



We will see a Bayesian version of EM.



Estimating parameters in topic models

Brandon Malone

Poisson Mixture Models

