


Published in

Towards Data Science



Jan 3, 2018

·

8 min read

Save

Probability concepts explained: Maximum likelihood

estimation

Introduction

What are parameters?








Three linear models with different parameter values.

Intuitive explanation of maximum likelihood estimation

The 10 (hypothetical) data points that we have observed

The 10 data points and possible Gaussian distributions from which the data were drawn. f1 is normally distributed

with mean 10 and variance 2.25 (variance is equal to the square of the standard deviation), this is also denoted f1

� N (10, 2.25). f2 � N (10, 9), f3 � N (10, 0.25) and f4 � N (8, 2.25). The goal of maximum likelihood is to find the

parameter values that give the distribution that maximise the probability of observing the data.

Calculating the Maximum Likelihood Estimates


The log likelihood

Monotonic behaviour of the original function, y = x on the left and the (natural) logarithm function y = ln(x). These

functions are both monotonic because as you go from left to right on the x-axis the y value always increases.

Example of a non-monotonic function because as you go from left to right on the graph the value of f(x) goes up,

then goes down and then goes back up again.


Concluding remarks

Can maximum likelihood estimation always be solved in an exact manner?

So why maximum likelihood and not maximum probability?

When is least squares minimisation the same as maximum likelihood estimation?


Regression line showing data points with random Gaussian noise

117



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science

Machine Learning

Probability

Maximum Likelihood

Towards Data Science






