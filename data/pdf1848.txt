
Hierarchical Topic Mining via Joint Spherical Tree and Text

Embedding

Yu Meng1∗, Yunyi Zhang1∗, Jiaxin Huang1, Yu Zhang1, Chao Zhang2, Jiawei Han1

1Department of Computer Science, University of Illinois at Urbana-Champaign, IL, USA

2College of Computing, Georgia Institute of Technology, GA, USA

1{yumeng5, yzhan238, jiaxinh3, yuz9, hanj}@illinois.edu

2chaozhang@gatech.edu

ABSTRACT

Mining a set of meaningful topics organized into a hierarchy is intu-

itively appealing since topic correlations are ubiquitous in massive

text corpora. To account for potential hierarchical topic structures,

hierarchical topic models generalize flat topic models by incorporat-

ing latent topic hierarchies into their generative modeling process.

However, due to their purely unsupervised nature, the learned topic

hierarchy often deviates from users’ particular needs or interests.

To guide the hierarchical topic discovery process with minimal user

supervision, we propose a new task, Hierarchical Topic Mining,

which takes a category tree described by category names only, and

aims to mine a set of representative terms for each category from

a text corpus to help a user comprehend his/her interested topics.

We develop a novel joint tree and text embedding method along

with a principled optimization procedure that allows simultaneous

modeling of the category tree structure and the corpus generative

process in the spherical space for effective category-representative

term discovery. Our comprehensive experiments show that our

model, named JoSH, mines a high-quality set of hierarchical topics

with high efficiency and benefits weakly-supervised hierarchical

text classification tasks1.

CCS CONCEPTS

• Information systems → Data mining; Document topic mod-

els; Clustering and classification; • Computing methodologies →

Natural language processing;

KEYWORDS

Topic Mining; Topic Hierarchy; Text Embedding; Tree Embedding

ACM Reference Format:

Yu Meng1∗, Yunyi Zhang1∗, Jiaxin Huang1, Yu Zhang1, Chao Zhang2, Jiawei

Han1. 2020. Hierarchical Topic Mining via Joint Spherical Tree and Text Em-

bedding. In Proceedings of the 26th ACM SIGKDD Conference on Knowledge

Discovery and Data Mining (KDD ’20), August 23–27, 2020, Virtual Event, CA,

1Source code can be found at https://github.com/yumeng5/JoSH.

∗Equal Contribution.

Permission to make digital or hard copies of all or part of this work for personal or

classroom use is granted without fee provided that copies are not made or distributed

for profit or commercial advantage and that copies bear this notice and the full citation

on the first page. Copyrights for components of this work owned by others than ACM

must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,

to post on servers or to redistribute to lists, requires prior specific permission and/or a

fee. Request permissions from permissions@acm.org.

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

© 2020 Association for Computing Machinery.

ACM ISBN 978-1-4503-7998-4/20/08...$15.00

https://doi.org/10.1145/3394486.3403242

music

folk music

jazz

choral

concert

singers

ROOT

baseball

soccer music

dance

arts

sports

ROOT

Text Corpus

dance

tango

dancers

choreographer

ballet

troupe

soccer

ﬁfa

striker

midﬁelder

goalkeeper

world cup

baseball

ballplayers

pitching

outﬁelder

baseman

catcher

sports

tennis

soccer

basketball

volleyball

football

arts

theater

opera

artist

arts festival

performing arts

___________

___________

___________

___________

___________

___________

___________

___________

___________

___________

___________

___________

Figure 1: An example of Hierarchical Topic Mining. We aim

to retrieve a set of representative terms from a given corpus

for each category in a user-provided hierarchy.

USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3394486.

3403242

1

INTRODUCTION

Topic models [6, 11], which uncover hidden semantic structure

in a text corpus via generative modeling, have proven successful

on automatic topic discovery. Hierarchical topic models [4, 29]

extend the classical ones by considering a latent topic hierarchy

during the corpus generative process, motivated by the fact that

topics are naturally correlated (e.g., “sports” is a super-topic of

“soccer”). Due to their effectiveness of discovering organized topic

structures automatically without human supervision, hierarchical

topic models have been applied to a wide range of applications

including political text analysis [10], entity disambiguation [15]

and relation extraction [1].

Despite being able to learn latent topic hierarchies from text

corpora, the applicability of hierarchical topic models to learn a user-

interested topic structure is limited seriously by their unsupervised

nature: Unsupervised generative models maximize the likelihood

of the observed data, tending to discover the most general and

prominent topics from a text collection, which may not fit a user’s

particular interest, or provide a superficial summarization of the

corpus. Furthermore, the inference algorithms of topic models yield

local optimum solutions, resulting in instability and inconsistency

across different runs. This issue even worsens in the hierarchical

setting where a larger number of topics and their correlations need

to be modeled.

In many cases, a user is interested in a specific topic structure, or

has prior knowledge about the potential topics in a corpus. These

topics, based on a user’s interest or prior knowledge, may be easily

described via a set of category names with a hierarchical structure.

Such a user-provided category hierarchy will facilitate a more stable

arXiv:2007.09536v1  [cs.CL]  18 Jul 2020


topic discovery process, yielding more desirable and consistent

results that better cater to a user’s needs. Therefore, we propose a

new task, Hierarchical Topic Mining, which takes only a topic

hierarchy described by category names as user guidance, and aims

to retrieve a set of coherent and representative terms under each

category to help users comprehend his/her interested topics. For

example, as shown in Figure 1, a user may provide a hierarchy of

interested concepts along with a corpus and rely on hierarchical

topic mining to retrieve a set of representative terms from a text

corpus (e.g., different music and dance genres, terminologies for

different sports, as well as general descriptions for internal nodes)

that provide a clear interpretation of the categories.

Several previous studies also focus on guiding topic discovery

with word-level supervision. Seed-guided topic modeling [2, 14]

incorporates user-provided seed words to bias the generative pro-

cess towards seed-related topics. A recent study CatE [24] learns

discriminative text embeddings guided by category names for rep-

resentative term retrieval. However, none of the above methods

handle hierarchical topic structures. Under the hierarchical setting,

there are supervised [33] and semi-supervised [22] models that

leverage category labels of documents to regularize the generative

process. However, they rely on a large amount of annotated docu-

ments which may be costly to obtain. Under our setting, only a set

of easy-to-provide category names that form a topic hierarchy is

needed to guide the hierarchical topic discovery process.

In this paper, we propose JoSH, a novel Joint Spherical tree and

text embedding model for Hierarchical Topic Mining. The user-

provided category tree structure and text corpus statistics are simul-

taneously modeled via directional similarity in the spherical space,

which facilitates effective estimation of category-word semantic

correlations for representative term discovery. To train our model

in the spherical space, we develop a principled EM optimization

procedure based on Riemannian optimization.

Our contributions can be summarized as follows.

(1) We propose a new task for hierarchical topic discovery, Hi-

erarchical Topic Mining, which requires a category hierarchy

described by category names as the only supervision to retrieve

a set of representative terms per category for effective topic

understanding.

(2) We develop a joint embedding framework for hierarchical topic

mining by simultaneously modeling the user-provided category

tree structure and the text generation process. The model is

defined in the spherical space, where directional similarity is

employed to characterize semantic correlations among words,

documents, and categories for accurate category representative

term retrieval.

(3) We develop an EM algorithm to optimize our model in the spher-

ical space that iterates between estimating the latent category

of words and maximizing corpus generative likelihood while

optimizing the category tree structure in the embedding space.

(4) We conduct a comprehensive set of experiments on two public

corpora from different domains on Hierarchical Topic Mining.

Our model enjoys high efficiency and mines high-quality topics.

The embeddings trained by our model can be directly used for

weakly-supervised hierarchical text classification.

2

PROBLEM FORMULATION

Definition 1 (Hierarchical Topic Mining). Given a text corpus

D and a tree-structured hierarchy T where each node ci ∈ T

is represented by the name of the category, Hierarchical Topic

Mining aims to retrieve a set of terms Ci = {w1, . . . ,wm} from D

for each category ci ∈ T such that Ci provides a clear description

of the category ci based on D.

Connection and difference between Hierarchical Topic Mod-

els. Similar to Hierarchical Topic Modeling [4], we also aim to cap-

ture the hierarchical correlations among topics during topic discov-

ery. However, Hierarchical Topic Mining is weakly-supervised

as it requires the user to provide the names of the hierarchy cat-

egories which serve as the minimal supervision and focuses on

retrieving representative terms only for the provided categories.

3

SPHERICAL TEXT AND TREE EMBEDDING

In this section, we introduce our model JoSH which jointly learns

text embeddings and tree embeddings in the spherical space, where

directional similarity is used to effectively characterize semantic

correlations among words, documents and categories.

3.1

Motivation

Mining representative terms relevant to a given category relies on

accurate estimation of semantic similarity, on which directional sim-

ilarity of text embeddings has proven most effective. For example,

cosine similarity is empirically shown [18] to better characterize

word semantic similarity and dissimilarity. Motivated by the effec-

tiveness of directional similarity for text analysis, several recent

studies employ the spherical space for topic modeling [3], text em-

bedding learning [25] and text sequence generation [16]. To learn

text embeddings tailored for the given category tree, we propose

to jointly embed the tree structure into the spherical space where

each category is surrounded by its representative terms.

Different from recent hyperbolic tree embedding models, such

as Poincaré embedding [30], Lorentz model [31] and hyperbolic

cones [9], we do not preserve the absolute tree distance in the em-

bedding space, but rather the relative category relationship reflected

in the tree structure. For example, in the category hierarchy given

in Figure 1, although the tree distance between “sports” and “arts”

and that between “baseball” and “soccer” are both 2, the latter pair

of categories should be embedded closer than the former pair due

to higher semantic similarity. Therefore, the tree distance in the

category hierarchy should not be preserved in an absolute manner,

but treated as a relative metric, e.g., for category “soccer”, its tree

distance to “sports” is smaller than that to “baseball”, so “soccer”

should be embedded closer to “sports” than to “baseball”.

3.2

Spherical Tree Embedding

We propose a novel tree embedding method that preserves the

relative category hierarchical structure in the spherical embedding

space, meanwhile encouraging inter-category distinctiveness for

clear topic interpretation.

3.2.1

The Flat Case. We start with the simplest case where all

categories are parallel and do not exhibit hierarchical structures.

We aim to jointly embed categories and their representative terms


✓intra

&lt;latexit sha1_base64="ZJq7n5K

3tdgqkv1bmqw8H5GFE0="&gt;AB/nicbVBNS8NAEN3Ur1q/ouLJS7AInk

oigh4LXjxWsK3QhrDZTtqlm03YnYglBPwrXjwo4tXf4c1/47bNQVsfD

zem2FmXpgKrtF1v63Kyura+kZ1s7a1vbO7Z+8fdHSKQZtlohE3YdUg+A

S2shRwH2qgMahgG4vp763QdQmifyDicp+DEdSh5xRtFIgX3UxEgDfI+

wiPmXKiRHYdbfhzuAsE68kdVKiFdhf/UHCshgkMkG17nluin5OFXIm

oKj1Mw0pZWM6hJ6hksag/Xx2fuGcGmXgRIkyJdGZqb8nchprPYlD0xlTH

OlFbyr+5/UyjK581OaIUg2XxRlwsHEmWbhDLgChmJiCGWKm1sdNqKMj

SJ1UwI3uLy6Rz3vBMrcX9aZbxlElx+SEnBGPXJImuSEt0iaM5OSZvJ

I368l6sd6tj3lrxSpnDskfWJ8/gvyWag=&lt;/latexit&gt;

&lt;latexit sha1_base64="ZJq7n5K

3tdgqkv1bmqw8H5GFE0="&gt;AB/nicbVBNS8NAEN3Ur1q/ouLJS7AInk

oigh4LXjxWsK3QhrDZTtqlm03YnYglBPwrXjwo4tXf4c1/47bNQVsfD

zem2FmXpgKrtF1v63Kyura+kZ1s7a1vbO7Z+8fdHSKQZtlohE3YdUg+A

S2shRwH2qgMahgG4vp763QdQmifyDicp+DEdSh5xRtFIgX3UxEgDfI+

wiPmXKiRHYdbfhzuAsE68kdVKiFdhf/UHCshgkMkG17nluin5OFXIm

oKj1Mw0pZWM6hJ6hksag/Xx2fuGcGmXgRIkyJdGZqb8nchprPYlD0xlTH

OlFbyr+5/UyjK581OaIUg2XxRlwsHEmWbhDLgChmJiCGWKm1sdNqKMj

SJ1UwI3uLy6Rz3vBMrcX9aZbxlElx+SEnBGPXJImuSEt0iaM5OSZvJ

I368l6sd6tj3lrxSpnDskfWJ8/gvyWag=&lt;/latexit&gt;

&lt;latexit sha1_base64="ZJq7n5K

3tdgqkv1bmqw8H5GFE0="&gt;AB/nicbVBNS8NAEN3Ur1q/ouLJS7AInk

oigh4LXjxWsK3QhrDZTtqlm03YnYglBPwrXjwo4tXf4c1/47bNQVsfD

zem2FmXpgKrtF1v63Kyura+kZ1s7a1vbO7Z+8fdHSKQZtlohE3YdUg+A

S2shRwH2qgMahgG4vp763QdQmifyDicp+DEdSh5xRtFIgX3UxEgDfI+

wiPmXKiRHYdbfhzuAsE68kdVKiFdhf/UHCshgkMkG17nluin5OFXIm

oKj1Mw0pZWM6hJ6hksag/Xx2fuGcGmXgRIkyJdGZqb8nchprPYlD0xlTH

OlFbyr+5/UyjK581OaIUg2XxRlwsHEmWbhDLgChmJiCGWKm1sdNqKMj

SJ1UwI3uLy6Rz3vBMrcX9aZbxlElx+SEnBGPXJImuSEt0iaM5OSZvJ

I368l6sd6tj3lrxSpnDskfWJ8/gvyWag=&lt;/latexit&gt;

&lt;latexit sha1_base64="ZJq7n5K

3tdgqkv1bmqw8H5GFE0="&gt;AB/nicbVBNS8NAEN3Ur1q/ouLJS7AInk

oigh4LXjxWsK3QhrDZTtqlm03YnYglBPwrXjwo4tXf4c1/47bNQVsfD

zem2FmXpgKrtF1v63Kyura+kZ1s7a1vbO7Z+8fdHSKQZtlohE3YdUg+A

S2shRwH2qgMahgG4vp763QdQmifyDicp+DEdSh5xRtFIgX3UxEgDfI+

wiPmXKiRHYdbfhzuAsE68kdVKiFdhf/UHCshgkMkG17nluin5OFXIm

oKj1Mw0pZWM6hJ6hksag/Xx2fuGcGmXgRIkyJdGZqb8nchprPYlD0xlTH

OlFbyr+5/UyjK581OaIUg2XxRlwsHEmWbhDLgChmJiCGWKm1sdNqKMj

SJ1UwI3uLy6Rz3vBMrcX9aZbxlElx+SEnBGPXJImuSEt0iaM5OSZvJ

I368l6sd6tj3lrxSpnDskfWJ8/gvyWag=&lt;/latexit&gt;

✓inter

&lt;latexit sha1_base64="1PZRdiq

Cb4IH3jneZfh1xgGxoM="&gt;AB/nicbVBNS8NAEN3Ur1q/ouLJS7AInk

oigh4LXjxWsK3QhrDZTtqlm03YnYglBPwrXjwo4tXf4c1/47bNQVsfD

zem2FmXpgKrtF1v63Kyura+kZ1s7a1vbO7Z+8fdHSKQZtlohE3YdUg+A

S2shRwH2qgMahgG4vp763QdQmifyDicp+DEdSh5xRtFIgX3UxEgDfI+

wiPmXCKogjsutwZ3CWiVeSOinRCuyv/iBhWQwSmaBa9zw3RT+nCjkT

UNT6mYaUsjEdQs9QSWPQfj47v3BOjTJwokSZkujM1N8TOY21nsSh6Ywpj

vSiNxX/83oZRle+SnNECSbL4oy4WDiTLNwBlwBQzExhDLFza0OG1FmQ

lB10wI3uLy6Rz3vBMrcX9aZbxlElx+SEnBGPXJImuSEt0iaM5OSZvJ

I368l6sd6tj3lrxSpnDskfWJ8/iQeWbg=&lt;/latexit&gt;

&lt;latexit sha1_base64="1PZRdiq

Cb4IH3jneZfh1xgGxoM="&gt;AB/nicbVBNS8NAEN3Ur1q/ouLJS7AInk

oigh4LXjxWsK3QhrDZTtqlm03YnYglBPwrXjwo4tXf4c1/47bNQVsfD

zem2FmXpgKrtF1v63Kyura+kZ1s7a1vbO7Z+8fdHSKQZtlohE3YdUg+A

S2shRwH2qgMahgG4vp763QdQmifyDicp+DEdSh5xRtFIgX3UxEgDfI+

wiPmXCKogjsutwZ3CWiVeSOinRCuyv/iBhWQwSmaBa9zw3RT+nCjkT

UNT6mYaUsjEdQs9QSWPQfj47v3BOjTJwokSZkujM1N8TOY21nsSh6Ywpj

vSiNxX/83oZRle+SnNECSbL4oy4WDiTLNwBlwBQzExhDLFza0OG1FmQ

lB10wI3uLy6Rz3vBMrcX9aZbxlElx+SEnBGPXJImuSEt0iaM5OSZvJ

I368l6sd6tj3lrxSpnDskfWJ8/iQeWbg=&lt;/latexit&gt;

&lt;latexit sha1_base64="1PZRdiq

Cb4IH3jneZfh1xgGxoM="&gt;AB/nicbVBNS8NAEN3Ur1q/ouLJS7AInk

oigh4LXjxWsK3QhrDZTtqlm03YnYglBPwrXjwo4tXf4c1/47bNQVsfD

zem2FmXpgKrtF1v63Kyura+kZ1s7a1vbO7Z+8fdHSKQZtlohE3YdUg+A

S2shRwH2qgMahgG4vp763QdQmifyDicp+DEdSh5xRtFIgX3UxEgDfI+

wiPmXCKogjsutwZ3CWiVeSOinRCuyv/iBhWQwSmaBa9zw3RT+nCjkT

UNT6mYaUsjEdQs9QSWPQfj47v3BOjTJwokSZkujM1N8TOY21nsSh6Ywpj

vSiNxX/83oZRle+SnNECSbL4oy4WDiTLNwBlwBQzExhDLFza0OG1FmQ

lB10wI3uLy6Rz3vBMrcX9aZbxlElx+SEnBGPXJImuSEt0iaM5OSZvJ

I368l6sd6tj3lrxSpnDskfWJ8/iQeWbg=&lt;/latexit&gt;

&lt;latexit sha1_base64="1PZRdiq

Cb4IH3jneZfh1xgGxoM="&gt;AB/nicbVBNS8NAEN3Ur1q/ouLJS7AInk

oigh4LXjxWsK3QhrDZTtqlm03YnYglBPwrXjwo4tXf4c1/47bNQVsfD

zem2FmXpgKrtF1v63Kyura+kZ1s7a1vbO7Z+8fdHSKQZtlohE3YdUg+A

S2shRwH2qgMahgG4vp763QdQmifyDicp+DEdSh5xRtFIgX3UxEgDfI+

wiPmXCKogjsutwZ3CWiVeSOinRCuyv/iBhWQwSmaBa9zw3RT+nCjkT

UNT6mYaUsjEdQs9QSWPQfj47v3BOjTJwokSZkujM1N8TOY21nsSh6Ywpj

vSiNxX/83oZRle+SnNECSbL4oy4WDiTLNwBlwBQzExhDLFza0OG1FmQ

lB10wI3uLy6Rz3vBMrcX9aZbxlElx+SEnBGPXJImuSEt0iaM5OSZvJ

I368l6sd6tj3lrxSpnDskfWJ8/iQeWbg=&lt;/latexit&gt;

✓intra  arccos(mintra)

&lt;latexit sha1_b

ase64="v+a8TtbEA+tN7KZHK5Yha

D05zc8="&gt;ACGnicbVDLSgNBEJz

1GeNr1aOXxSDES9gVQY8BLx4jmAdk

Q+idJIhsw9mesWw5Du8+CtePCji

Tbz4N06SPWhiwUBR1U1PVZBIocl1

v62V1bX1jc3CVnF7Z3dv3z4bOg4V

RzrPJaxagWgUYoI6yRIYitRCGEgs

RmMrqd+8x6VFnF0R+MEOyEMItEXH

MhIXdvzaYgE3cwnfKBMRKRgMvEl+q

A4j3U5XLDOunbJrbgzOMvEy0mJ5a

h17U+/F/M0xIi4BK3bnptQJwNFgk

ucFP1UYwJ8BANsGxpBiLqTzaJNnFO

j9Jx+rMyLyJmpvzcyCLUeh4GZDIG

GetGbiv957ZT6Vx0TKkJIz4/1E+

lQ7Ez7cnpCYWc5NgQ4EqYvzp8CAo4

mTaLpgRvMfIyaZxXPNPM7UWp6uZ1

FNgxO2Fl5rFLVmU3rMbqjLNH9sxe2

Zv1ZL1Y79bHfHTFyneO2B9YXz9QA

qLS&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="v+a8TtbEA+tN7KZHK5Yha

D05zc8="&gt;ACGnicbVDLSgNBEJz

1GeNr1aOXxSDES9gVQY8BLx4jmAdk

Q+idJIhsw9mesWw5Du8+CtePCji

Tbz4N06SPWhiwUBR1U1PVZBIocl1

v62V1bX1jc3CVnF7Z3dv3z4bOg4V

RzrPJaxagWgUYoI6yRIYitRCGEgs

RmMrqd+8x6VFnF0R+MEOyEMItEXH

MhIXdvzaYgE3cwnfKBMRKRgMvEl+q

A4j3U5XLDOunbJrbgzOMvEy0mJ5a

h17U+/F/M0xIi4BK3bnptQJwNFgk

ucFP1UYwJ8BANsGxpBiLqTzaJNnFO

j9Jx+rMyLyJmpvzcyCLUeh4GZDIG

GetGbiv957ZT6Vx0TKkJIz4/1E+

lQ7Ez7cnpCYWc5NgQ4EqYvzp8CAo4

mTaLpgRvMfIyaZxXPNPM7UWp6uZ1

FNgxO2Fl5rFLVmU3rMbqjLNH9sxe2

Zv1ZL1Y79bHfHTFyneO2B9YXz9QA

qLS&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="v+a8TtbEA+tN7KZHK5Yha

D05zc8="&gt;ACGnicbVDLSgNBEJz

1GeNr1aOXxSDES9gVQY8BLx4jmAdk

Q+idJIhsw9mesWw5Du8+CtePCji

Tbz4N06SPWhiwUBR1U1PVZBIocl1

v62V1bX1jc3CVnF7Z3dv3z4bOg4V

RzrPJaxagWgUYoI6yRIYitRCGEgs

RmMrqd+8x6VFnF0R+MEOyEMItEXH

MhIXdvzaYgE3cwnfKBMRKRgMvEl+q

A4j3U5XLDOunbJrbgzOMvEy0mJ5a

h17U+/F/M0xIi4BK3bnptQJwNFgk

ucFP1UYwJ8BANsGxpBiLqTzaJNnFO

j9Jx+rMyLyJmpvzcyCLUeh4GZDIG

GetGbiv957ZT6Vx0TKkJIz4/1E+

lQ7Ez7cnpCYWc5NgQ4EqYvzp8CAo4

mTaLpgRvMfIyaZxXPNPM7UWp6uZ1

FNgxO2Fl5rFLVmU3rMbqjLNH9sxe2

Zv1ZL1Y79bHfHTFyneO2B9YXz9QA

qLS&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="v+a8TtbEA+tN7KZHK5Yha

D05zc8="&gt;ACGnicbVDLSgNBEJz

1GeNr1aOXxSDES9gVQY8BLx4jmAdk

Q+idJIhsw9mesWw5Du8+CtePCji

Tbz4N06SPWhiwUBR1U1PVZBIocl1

v62V1bX1jc3CVnF7Z3dv3z4bOg4V

RzrPJaxagWgUYoI6yRIYitRCGEgs

RmMrqd+8x6VFnF0R+MEOyEMItEXH

MhIXdvzaYgE3cwnfKBMRKRgMvEl+q

A4j3U5XLDOunbJrbgzOMvEy0mJ5a

h17U+/F/M0xIi4BK3bnptQJwNFgk

ucFP1UYwJ8BANsGxpBiLqTzaJNnFO

j9Jx+rMyLyJmpvzcyCLUeh4GZDIG

GetGbiv957ZT6Vx0TKkJIz4/1E+

lQ7Ez7cnpCYWc5NgQ4EqYvzp8CAo4

mTaLpgRvMfIyaZxXPNPM7UWp6uZ1

FNgxO2Fl5rFLVmU3rMbqjLNH9sxe2

Zv1ZL1Y79bHfHTFyneO2B9YXz9QA

qLS&lt;/latexit&gt;

✓inter � arccos(1 � minter)

&lt;latexit sha1_b

ase64="mlCvMLt+/AX5HaG+YvKbT

ZYCez4="&gt;ACHicbVDLSsNAFJ3

4rPUVdekmWIS6sCQq6LgxmUF+4Am

hMn0ph06eTBzI5bQD3Hjr7hxoYgb

F4J/47TNQlsPDBzOuZc75wSp4Apt

+9tYWl5ZXVsvbZQ3t7Z3ds29/ZKM

smgyRKRyE5AFQgeQxM5CuikEmgUC

GgHw+uJ374HqXgS3+EoBS+i/ZiHn

FHUkm+euzgApH7uIjxgzmMEOR67fX

CpZCxRVec0mjNPfLNi1+wprEXiFK

RCjR89PtJSyLIEYmqFJdx07Ry6

lEzgSMy26mIKVsSPvQ1TSmESgvn4Y

bW8da6VlhIvWL0ZqvzdyGik1igI

9GVEcqHlvIv7ndTMrzwdKs0QYjY

7FGbCwsSaNGX1uASGYqQJZLrv1ps

QCVlugV1iU485EXSeus5uhmbi8q

dbuo0QOyRGpEodckjq5IQ3SJIw8k

mfySt6MJ+PFeDc+ZqNLRrFzQP7A+

PoBToqjRw=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="mlCvMLt+/AX5HaG+YvKbT

ZYCez4="&gt;ACHicbVDLSsNAFJ3

4rPUVdekmWIS6sCQq6LgxmUF+4Am

hMn0ph06eTBzI5bQD3Hjr7hxoYgb

F4J/47TNQlsPDBzOuZc75wSp4Apt

+9tYWl5ZXVsvbZQ3t7Z3ds29/ZKM

smgyRKRyE5AFQgeQxM5CuikEmgUC

GgHw+uJ374HqXgS3+EoBS+i/ZiHn

FHUkm+euzgApH7uIjxgzmMEOR67fX

CpZCxRVec0mjNPfLNi1+wprEXiFK

RCjR89PtJSyLIEYmqFJdx07Ry6

lEzgSMy26mIKVsSPvQ1TSmESgvn4Y

bW8da6VlhIvWL0ZqvzdyGik1igI

9GVEcqHlvIv7ndTMrzwdKs0QYjY

7FGbCwsSaNGX1uASGYqQJZLrv1ps

QCVlugV1iU485EXSeus5uhmbi8q

dbuo0QOyRGpEodckjq5IQ3SJIw8k

mfySt6MJ+PFeDc+ZqNLRrFzQP7A+

PoBToqjRw=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="mlCvMLt+/AX5HaG+YvKbT

ZYCez4="&gt;ACHicbVDLSsNAFJ3

4rPUVdekmWIS6sCQq6LgxmUF+4Am

hMn0ph06eTBzI5bQD3Hjr7hxoYgb

F4J/47TNQlsPDBzOuZc75wSp4Apt

+9tYWl5ZXVsvbZQ3t7Z3ds29/ZKM

smgyRKRyE5AFQgeQxM5CuikEmgUC

GgHw+uJ374HqXgS3+EoBS+i/ZiHn

FHUkm+euzgApH7uIjxgzmMEOR67fX

CpZCxRVec0mjNPfLNi1+wprEXiFK

RCjR89PtJSyLIEYmqFJdx07Ry6

lEzgSMy26mIKVsSPvQ1TSmESgvn4Y

bW8da6VlhIvWL0ZqvzdyGik1igI

9GVEcqHlvIv7ndTMrzwdKs0QYjY

7FGbCwsSaNGX1uASGYqQJZLrv1ps

QCVlugV1iU485EXSeus5uhmbi8q

dbuo0QOyRGpEodckjq5IQ3SJIw8k

mfySt6MJ+PFeDc+ZqNLRrFzQP7A+

PoBToqjRw=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="mlCvMLt+/AX5HaG+YvKbT

ZYCez4="&gt;ACHicbVDLSsNAFJ3

4rPUVdekmWIS6sCQq6LgxmUF+4Am

hMn0ph06eTBzI5bQD3Hjr7hxoYgb

F4J/47TNQlsPDBzOuZc75wSp4Apt

+9tYWl5ZXVsvbZQ3t7Z3ds29/ZKM

smgyRKRyE5AFQgeQxM5CuikEmgUC

GgHw+uJ374HqXgS3+EoBS+i/ZiHn

FHUkm+euzgApH7uIjxgzmMEOR67fX

CpZCxRVec0mjNPfLNi1+wprEXiFK

RCjR89PtJSyLIEYmqFJdx07Ry6

lEzgSMy26mIKVsSPvQ1TSmESgvn4Y

bW8da6VlhIvWL0ZqvzdyGik1igI

9GVEcqHlvIv7ndTMrzwdKs0QYjY

7FGbCwsSaNGX1uASGYqQJZLrv1ps

QCVlugV1iU485EXSeus5uhmbi8q

dbuo0QOyRGpEodckjq5IQ3SJIw8k

mfySt6MJ+PFeDc+ZqNLRrFzQP7A+

PoBToqjRw=&lt;/latexit&gt;

ci

&lt;latexit sha1_b

ase64="pyj0Y0qU/oqnF+tWmAsN

/COovo="&gt;AB+XicbVDLSgMxFL3

js9bXqEs3wSK4KjMi6LgxmUF+4B2

GDKZTBuaSYkUyhD/8SNC0Xc+ifu

/Bsz7Sy09UDI4Zx7ycmJMs608bxv

Z2Nza3tnt7ZX3z84PDp2T067WuaK0

A6RXKp+hDXlTNCOYbTfqYoTiNOe

9HkvR7U6o0k+LJzDIapHgkWMIN

lYKXcYSR7rWqvgsxDFroNr+ktgN

aJX5EGVGiH7tcwliRPqTCEY60Hvp

eZoMDKMLpvD7MNc0wmeARHVgqcE

p1UCySz9GlVWKUSGWPMGih/t4ocKr

LcHYyxWasV71S/M8b5Ca5Cwomstx

QZYPJTlHRqKyBhQzRYnhM0swUcx

mRWSMFSbGlW3JfirX14n3eumb5t5

vGm0vKqOGpzDBVyBD7fQgdoQwcI

TOEZXuHNKZwX5935WI5uONXOGfyB8

/kDKvKT7g=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="pyj0Y0qU/oqnF+tWmAsN

/COovo="&gt;AB+XicbVDLSgMxFL3

js9bXqEs3wSK4KjMi6LgxmUF+4B2

GDKZTBuaSYkUyhD/8SNC0Xc+ifu

/Bsz7Sy09UDI4Zx7ycmJMs608bxv

Z2Nza3tnt7ZX3z84PDp2T067WuaK0

A6RXKp+hDXlTNCOYbTfqYoTiNOe

9HkvR7U6o0k+LJzDIapHgkWMIN

lYKXcYSR7rWqvgsxDFroNr+ktgN

aJX5EGVGiH7tcwliRPqTCEY60Hvp

eZoMDKMLpvD7MNc0wmeARHVgqcE

p1UCySz9GlVWKUSGWPMGih/t4ocKr

LcHYyxWasV71S/M8b5Ca5Cwomstx

QZYPJTlHRqKyBhQzRYnhM0swUcx

mRWSMFSbGlW3JfirX14n3eumb5t5

vGm0vKqOGpzDBVyBD7fQgdoQwcI

TOEZXuHNKZwX5935WI5uONXOGfyB8

/kDKvKT7g=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="pyj0Y0qU/oqnF+tWmAsN

/COovo="&gt;AB+XicbVDLSgMxFL3

js9bXqEs3wSK4KjMi6LgxmUF+4B2

GDKZTBuaSYkUyhD/8SNC0Xc+ifu

/Bsz7Sy09UDI4Zx7ycmJMs608bxv

Z2Nza3tnt7ZX3z84PDp2T067WuaK0

A6RXKp+hDXlTNCOYbTfqYoTiNOe

9HkvR7U6o0k+LJzDIapHgkWMIN

lYKXcYSR7rWqvgsxDFroNr+ktgN

aJX5EGVGiH7tcwliRPqTCEY60Hvp

eZoMDKMLpvD7MNc0wmeARHVgqcE

p1UCySz9GlVWKUSGWPMGih/t4ocKr

LcHYyxWasV71S/M8b5Ca5Cwomstx

QZYPJTlHRqKyBhQzRYnhM0swUcx

mRWSMFSbGlW3JfirX14n3eumb5t5

vGm0vKqOGpzDBVyBD7fQgdoQwcI

TOEZXuHNKZwX5935WI5uONXOGfyB8

/kDKvKT7g=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="FOtiNeMaPXkcY5+lUd9TX

KZ5hmU="&gt;AB2XicbZDNSgMxFIX

v1L86Vq1rN8EiuCozbnQpuHFZwbZC

O5RM5k4bmskMyR2hDH0BF25EfC93

vo3pz0JbDwQ+zknIvSculLQUBN9e

bWd3b/+gfugfNfzjk9Nmo2fz0gjsi

lzl5jnmFpXU2CVJCp8LgzyLFfbj6

f0i7+gsTLXTzQrMr4WMtUCk7O6

oyaraAdLMW2IVxDC9YaNb+GS7KD

UJxa0dhEFBUcUNSaFw7g9LiwUXUz

7GgUPNM7RtRxzi6dk7A0N+5oYk

v394uKZ9bOstjdzDhN7Ga2MP/LBiW

lt1EldVESarH6KC0Vo5wtdmaJNCh

IzRxwYaSblYkJN1yQa8Z3HYSbG29

D7odumIeA6jDOVzAFYRwA3fwAB3o

goAEXuHdm3hv3seq5q3Lu0M/sj7

/AE4f4o3&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="TSEWJQTeZPTJ08go5pg

pk+Lj8="&gt;AB7nicbZDLSgMxGIX

/qbdaq45u3QSL4KrMuNGl4MZlBXuB

dhgymUwbmsuQZApl6Ju4caGIj+PO

tzHTdqGtP4QczknIyZfknBkbBN9e

bWd3b/+gftg4ah6fnPpnzZ5RhSa0S

xRXepBgQzmTtGuZ5XSQa4pFwmk/m

T5UeX9GtWFKPt5TiOBx5JljGDr

Nj3R4niqZkLt5VkEbPYbwXtYDloW4

Rr0YL1dGL/a5QqUgqLeHYmGEY5D

YqsbaMcLpojApDc0ymeEyHTkosqI

nKZfMFunJOijKl3ZIWLd3fN0osTFX

OnRTYTsxmVpn/ZcPCZndRyWReWCr

J6qGs4MgqVGFAKdOUWD53AhPNXFd

EJlhjYh2shoMQbn5W/Ru2qEj8xRA

HS7gEq4hFu4h0foQBcIzOAF3uDd

K71X72OFq+atuZ3Dn/E+fwDRiZKc&lt;

/latexit&gt;

&lt;latexit sha1_b

ase64="TSEWJQTeZPTJ08go5pg

pk+Lj8="&gt;AB7nicbZDLSgMxGIX

/qbdaq45u3QSL4KrMuNGl4MZlBXuB

dhgymUwbmsuQZApl6Ju4caGIj+PO

tzHTdqGtP4QczknIyZfknBkbBN9e

bWd3b/+gftg4ah6fnPpnzZ5RhSa0S

xRXepBgQzmTtGuZ5XSQa4pFwmk/m

T5UeX9GtWFKPt5TiOBx5JljGDr

Nj3R4niqZkLt5VkEbPYbwXtYDloW4

Rr0YL1dGL/a5QqUgqLeHYmGEY5D

YqsbaMcLpojApDc0ymeEyHTkosqI

nKZfMFunJOijKl3ZIWLd3fN0osTFX

OnRTYTsxmVpn/ZcPCZndRyWReWCr

J6qGs4MgqVGFAKdOUWD53AhPNXFd

EJlhjYh2shoMQbn5W/Ru2qEj8xRA

HS7gEq4hFu4h0foQBcIzOAF3uDd

K71X72OFq+atuZ3Dn/E+fwDRiZKc&lt;

/latexit&gt;

&lt;latexit sha1_b

ase64="Cifnh6yUGDmV1KuSrS7X

HOySDE="&gt;AB+XicbVC7TsMwFL0

pr1JeAUYWiwqJqUpYKzEwlgk+pDa

KHIcp7Xq2JHtVKqi/gkLAwix8ids

/A1OmwFajmT56Jx75eMTZxp43nf

Tm1re2d3r7fODg8Oj5xT896WuaK0

C6RXKpBhDXlTNCuYbTQaYoTiNO+

9H0vT7M6o0k+LJzDMapHgsWMIN

lYKXcUSR7reWqvgixCFrpNr+UtgT

aJX5EmVOiE7tcoliRPqTCEY62Hvp

eZoMDKMLpojHKNc0wmeIxHVoqcE

p1UCyTL9CVWKUSGWPMGip/t4ocKr

LcHYyxWai171S/M8b5ia5Cwomstx

QVYPJTlHRqKyBhQzRYnhc0swUcx

mRWSCFSbGltWwJfjrX94kvZuWb5t5

9Jptr6qjDhdwCdfgwy204QE60AUC

M3iGV3hzCufFeXc+VqM1p9o5hz9wP

n8AKbKT6g=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="pyj0Y0qU/oqnF+tWmAsN

/COovo="&gt;AB+XicbVDLSgMxFL3

js9bXqEs3wSK4KjMi6LgxmUF+4B2

GDKZTBuaSYkUyhD/8SNC0Xc+ifu

/Bsz7Sy09UDI4Zx7ycmJMs608bxv

Z2Nza3tnt7ZX3z84PDp2T067WuaK0

A6RXKp+hDXlTNCOYbTfqYoTiNOe

9HkvR7U6o0k+LJzDIapHgkWMIN

lYKXcYSR7rWqvgsxDFroNr+ktgN

aJX5EGVGiH7tcwliRPqTCEY60Hvp

eZoMDKMLpvD7MNc0wmeARHVgqcE

p1UCySz9GlVWKUSGWPMGih/t4ocKr

LcHYyxWasV71S/M8b5Ca5Cwomstx

QZYPJTlHRqKyBhQzRYnhM0swUcx

mRWSMFSbGlW3JfirX14n3eumb5t5

vGm0vKqOGpzDBVyBD7fQgdoQwcI

TOEZXuHNKZwX5935WI5uONXOGfyB8

/kDKvKT7g=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="pyj0Y0qU/oqnF+tWmAsN

/COovo="&gt;AB+XicbVDLSgMxFL3

js9bXqEs3wSK4KjMi6LgxmUF+4B2

GDKZTBuaSYkUyhD/8SNC0Xc+ifu

/Bsz7Sy09UDI4Zx7ycmJMs608bxv

Z2Nza3tnt7ZX3z84PDp2T067WuaK0

A6RXKp+hDXlTNCOYbTfqYoTiNOe

9HkvR7U6o0k+LJzDIapHgkWMIN

lYKXcYSR7rWqvgsxDFroNr+ktgN

aJX5EGVGiH7tcwliRPqTCEY60Hvp

eZoMDKMLpvD7MNc0wmeARHVgqcE

p1UCySz9GlVWKUSGWPMGih/t4ocKr

LcHYyxWasV71S/M8b5Ca5Cwomstx

QZYPJTlHRqKyBhQzRYnhM0swUcx

mRWSMFSbGlW3JfirX14n3eumb5t5

vGm0vKqOGpzDBVyBD7fQgdoQwcI

TOEZXuHNKZwX5935WI5uONXOGfyB8

/kDKvKT7g=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="pyj0Y0qU/oqnF+tWmAsN

/COovo="&gt;AB+XicbVDLSgMxFL3

js9bXqEs3wSK4KjMi6LgxmUF+4B2

GDKZTBuaSYkUyhD/8SNC0Xc+ifu

/Bsz7Sy09UDI4Zx7ycmJMs608bxv

Z2Nza3tnt7ZX3z84PDp2T067WuaK0

A6RXKp+hDXlTNCOYbTfqYoTiNOe

9HkvR7U6o0k+LJzDIapHgkWMIN

lYKXcYSR7rWqvgsxDFroNr+ktgN

aJX5EGVGiH7tcwliRPqTCEY60Hvp

eZoMDKMLpvD7MNc0wmeARHVgqcE

p1UCySz9GlVWKUSGWPMGih/t4ocKr

LcHYyxWasV71S/M8b5Ca5Cwomstx

QZYPJTlHRqKyBhQzRYnhM0swUcx

mRWSMFSbGlW3JfirX14n3eumb5t5

vGm0vKqOGpzDBVyBD7fQgdoQwcI

TOEZXuHNKZwX5935WI5uONXOGfyB8

/kDKvKT7g=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="pyj0Y0qU/oqnF+tWmAsN

/COovo="&gt;AB+XicbVDLSgMxFL3

js9bXqEs3wSK4KjMi6LgxmUF+4B2

GDKZTBuaSYkUyhD/8SNC0Xc+ifu

/Bsz7Sy09UDI4Zx7ycmJMs608bxv

Z2Nza3tnt7ZX3z84PDp2T067WuaK0

A6RXKp+hDXlTNCOYbTfqYoTiNOe

9HkvR7U6o0k+LJzDIapHgkWMIN

lYKXcYSR7rWqvgsxDFroNr+ktgN

aJX5EGVGiH7tcwliRPqTCEY60Hvp

eZoMDKMLpvD7MNc0wmeARHVgqcE

p1UCySz9GlVWKUSGWPMGih/t4ocKr

LcHYyxWasV71S/M8b5Ca5Cwomstx

QZYPJTlHRqKyBhQzRYnhM0swUcx

mRWSMFSbGlW3JfirX14n3eumb5t5

vGm0vKqOGpzDBVyBD7fQgdoQwcI

TOEZXuHNKZwX5935WI5uONXOGfyB8

/kDKvKT7g=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="pyj0Y0qU/oqnF+tWmAsN

/COovo="&gt;AB+XicbVDLSgMxFL3

js9bXqEs3wSK4KjMi6LgxmUF+4B2

GDKZTBuaSYkUyhD/8SNC0Xc+ifu

/Bsz7Sy09UDI4Zx7ycmJMs608bxv

Z2Nza3tnt7ZX3z84PDp2T067WuaK0

A6RXKp+hDXlTNCOYbTfqYoTiNOe

9HkvR7U6o0k+LJzDIapHgkWMIN

lYKXcYSR7rWqvgsxDFroNr+ktgN

aJX5EGVGiH7tcwliRPqTCEY60Hvp

eZoMDKMLpvD7MNc0wmeARHVgqcE

p1UCySz9GlVWKUSGWPMGih/t4ocKr

LcHYyxWasV71S/M8b5Ca5Cwomstx

QZYPJTlHRqKyBhQzRYnhM0swUcx

mRWSMFSbGlW3JfirX14n3eumb5t5

vGm0vKqOGpzDBVyBD7fQgdoQwcI

TOEZXuHNKZwX5935WI5uONXOGfyB8

/kDKvKT7g=&lt;/latexit&gt;

&lt;latexit sha1_b

ase64="pyj0Y0qU/oqnF+tWmAsN

/COovo="&gt;AB+XicbVDLSgMxFL3

js9bXqEs3wSK4KjMi6LgxmUF+4B2

GDKZTBuaSYkUyhD/8SNC0Xc+ifu

/Bsz7Sy09UDI4Zx7ycmJMs608bxv

Z2Nza3tnt7ZX3z84PDp2T067WuaK0

A6RXKp+hDXlTNCOYbTfqYoTiNOe

9HkvR7U6o0k+LJzDIapHgkWMIN

lYKXcYSR7rWqvgsxDFroNr+ktgN

aJX5EGVGiH7tcwliRPqTCEY60Hvp

eZoMDKMLpvD7MNc0wmeARHVgqcE

p1UCySz9GlVWKUSGWPMGih/t4ocKr

LcHYyxWasV71S/M8b5Ca5Cwomstx

QZYPJTlHRqKyBhQzRYnhM0swUcx

mRWSMFSbGlW3JfirX14n3eumb5t5

vGm0vKqOGpzDBVyBD7fQgdoQwcI

TOEZXuHNKZwX5935WI5uONXOGfyB8

/kDKvKT7g=&lt;/latexit&gt;

cj

&lt;latexit sha1_base64="XRubuHj

W+5LG8jHUmga7olNhNc="&gt;AB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZ

kRQZcFNy4r2Ae0w5DJpG1sJhmSTKEM/RM3LhRx65+482/MtLPQ1gMh3

PuJScnSjnTxvO+ncrG5tb2TnW3trd/cHjkHp90tMwUoW0iuVS9CGvKmaB

twynvVRnEScdqPJXeF3p1RpJsWjmaU0SPBIsCEj2FgpdN1BJHmsZ4m9

cjIPn0K37jW8BdA68UtShxKt0P0axJkCRWGcKx13/dSE+RYGUY4ndcG

maYpJhM8on1LBU6oDvJF8jm6sEqMhlLZIwxaqL83cpzoIpydTLAZ61WvE

P/z+pkZ3gY5E2lmqCDLh4YZR0aiogYUM0WJ4TNLMFHMZkVkjBUmxpZVsy

X4q19eJ52rhm+bebiuN72yjiqcwTlcg830IR7aEbCEzhGV7hzcmdF+

fd+ViOVpxy5xT+wPn8ASx2k+8=&lt;/latexit&gt;

&lt;latexit sha1_base64="XRubuHj

W+5LG8jHUmga7olNhNc="&gt;AB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZ

kRQZcFNy4r2Ae0w5DJpG1sJhmSTKEM/RM3LhRx65+482/MtLPQ1gMh3

PuJScnSjnTxvO+ncrG5tb2TnW3trd/cHjkHp90tMwUoW0iuVS9CGvKmaB

twynvVRnEScdqPJXeF3p1RpJsWjmaU0SPBIsCEj2FgpdN1BJHmsZ4m9

cjIPn0K37jW8BdA68UtShxKt0P0axJkCRWGcKx13/dSE+RYGUY4ndcG

maYpJhM8on1LBU6oDvJF8jm6sEqMhlLZIwxaqL83cpzoIpydTLAZ61WvE

P/z+pkZ3gY5E2lmqCDLh4YZR0aiogYUM0WJ4TNLMFHMZkVkjBUmxpZVsy

X4q19eJ52rhm+bebiuN72yjiqcwTlcg830IR7aEbCEzhGV7hzcmdF+

fd+ViOVpxy5xT+wPn8ASx2k+8=&lt;/latexit&gt;

&lt;latexit sha1_base64="XRubuHj

W+5LG8jHUmga7olNhNc="&gt;AB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZ

kRQZcFNy4r2Ae0w5DJpG1sJhmSTKEM/RM3LhRx65+482/MtLPQ1gMh3

PuJScnSjnTxvO+ncrG5tb2TnW3trd/cHjkHp90tMwUoW0iuVS9CGvKmaB

twynvVRnEScdqPJXeF3p1RpJsWjmaU0SPBIsCEj2FgpdN1BJHmsZ4m9

cjIPn0K37jW8BdA68UtShxKt0P0axJkCRWGcKx13/dSE+RYGUY4ndcG

maYpJhM8on1LBU6oDvJF8jm6sEqMhlLZIwxaqL83cpzoIpydTLAZ61WvE

P/z+pkZ3gY5E2lmqCDLh4YZR0aiogYUM0WJ4TNLMFHMZkVkjBUmxpZVsy

X4q19eJ52rhm+bebiuN72yjiqcwTlcg830IR7aEbCEzhGV7hzcmdF+

fd+ViOVpxy5xT+wPn8ASx2k+8=&lt;/latexit&gt;

&lt;latexit sha1_base64="FOtiNeM

aPXkcY5+lUd9TXKZ5hmU="&gt;AB2XicbZDNSgMxFIXv1L86Vq1rN8EiuC

ozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zk

nIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CV

JCp8LgzyLFfbj6f0i7+gsTLXTzQrMr4WMtUCk7O6oyaraAdLMW2IVxD

C9YaNb+GS7KDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RtRxz

zi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6K

C0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D7odumIeA6jDOV

zAFYRwA3fwAB3ogoAEXuHdm3hv3seq5q3Lu0M/sj7/AE4f4o3&lt;/late

xit&gt;

&lt;latexit sha1_base64="ws2Db8o

iG3yPZwNw1Bpet5aweoU="&gt;AB7nicbZDLSgMxGIX/8Vpr1dGtm2ARXJ

UZN7oU3LisYC/QDkMmk2ljcxmSTKEMfRM3LhTxcdz5NmbaLrT1h5DOQ

k5+ZKcM2OD4Nvb2t7Z3duvHdQPG0fHJ/5po2tUoQntEMWV7ifYUM4k7Vh

mOe3nmKRcNpLJvdV3ptSbZiST3aW0jgkWQZI9g6K/b9YaJ4ambCbSWZ

x8+x3wxawWLQpghXogmracf+1zBVpBUWsKxMYMwyG1UYm0Z4XReHxaG

5phM8IgOnJRYUBOVi+ZzdOmcFGVKuyUtWri/b5RYmKqcOymwHZv1rDL/y

waFzW6jksm8sFS5UNZwZFVqMKAUqYpsXzmBCaua6IjLHGxDpYdQchXP

/ypuhet0JH5jGAGpzDBVxBCDdwBw/Qhg4QmMILvMG7V3qv3scS15a34n

YGf8b7/AHTApKd&lt;/latexit&gt;

&lt;latexit sha1_base64="ws2Db8o

iG3yPZwNw1Bpet5aweoU="&gt;AB7nicbZDLSgMxGIX/8Vpr1dGtm2ARXJ

UZN7oU3LisYC/QDkMmk2ljcxmSTKEMfRM3LhTxcdz5NmbaLrT1h5DOQ

k5+ZKcM2OD4Nvb2t7Z3duvHdQPG0fHJ/5po2tUoQntEMWV7ifYUM4k7Vh

mOe3nmKRcNpLJvdV3ptSbZiST3aW0jgkWQZI9g6K/b9YaJ4ambCbSWZ

x8+x3wxawWLQpghXogmracf+1zBVpBUWsKxMYMwyG1UYm0Z4XReHxaG

5phM8IgOnJRYUBOVi+ZzdOmcFGVKuyUtWri/b5RYmKqcOymwHZv1rDL/y

waFzW6jksm8sFS5UNZwZFVqMKAUqYpsXzmBCaua6IjLHGxDpYdQchXP

/ypuhet0JH5jGAGpzDBVxBCDdwBw/Qhg4QmMILvMG7V3qv3scS15a34n

YGf8b7/AHTApKd&lt;/latexit&gt;

&lt;latexit sha1_base64="OtvC3q

DmgciT589vua4w2bB3Vc="&gt;AB+XicbVC7TsMwFL3hWcorwMhiUSExVQ

kLjJVYGItEH1IbRY7jtKaOHdlOpSrqn7AwgBArf8LG3+C0GaDlSJaPzr

lXPj5Rxpk2nvftbGxube/s1vbq+weHR8fuyWlXy1wR2iGS9WPsKacCdo

xzHDazxTFacRpL5rclX5vSpVmUjyaWUaDFI8ESxjBxkqh6w4jyWM9S+1V

kHn4FLoNr+ktgNaJX5EGVGiH7tcwliRPqTCEY60HvpeZoMDKMLpvD7M

Nc0wmeARHVgqcEp1UCySz9GlVWKUSGWPMGih/t4ocKrLcHYyxWasV71S/

M8b5Ca5DQomstxQZYPJTlHRqKyBhQzRYnhM0swUcxmRWSMFSbGlW3Jf

irX14n3eumb5t58Botr6qjBudwAVfgw204B7a0AECU3iGV3hzCufFeX

c+lqMbTrVzBn/gfP4AKzaT6w=&lt;/latexit&gt;

&lt;latexit sha1_base64="XRubuHj

W+5LG8jHUmga7olNhNc="&gt;AB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZ

kRQZcFNy4r2Ae0w5DJpG1sJhmSTKEM/RM3LhRx65+482/MtLPQ1gMh3

PuJScnSjnTxvO+ncrG5tb2TnW3trd/cHjkHp90tMwUoW0iuVS9CGvKmaB

twynvVRnEScdqPJXeF3p1RpJsWjmaU0SPBIsCEj2FgpdN1BJHmsZ4m9

cjIPn0K37jW8BdA68UtShxKt0P0axJkCRWGcKx13/dSE+RYGUY4ndcG

maYpJhM8on1LBU6oDvJF8jm6sEqMhlLZIwxaqL83cpzoIpydTLAZ61WvE

P/z+pkZ3gY5E2lmqCDLh4YZR0aiogYUM0WJ4TNLMFHMZkVkjBUmxpZVsy

X4q19eJ52rhm+bebiuN72yjiqcwTlcg830IR7aEbCEzhGV7hzcmdF+

fd+ViOVpxy5xT+wPn8ASx2k+8=&lt;/latexit&gt;

&lt;latexit sha1_base64="XRubuHj

W+5LG8jHUmga7olNhNc="&gt;AB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZ

kRQZcFNy4r2Ae0w5DJpG1sJhmSTKEM/RM3LhRx65+482/MtLPQ1gMh3

PuJScnSjnTxvO+ncrG5tb2TnW3trd/cHjkHp90tMwUoW0iuVS9CGvKmaB

twynvVRnEScdqPJXeF3p1RpJsWjmaU0SPBIsCEj2FgpdN1BJHmsZ4m9

cjIPn0K37jW8BdA68UtShxKt0P0axJkCRWGcKx13/dSE+RYGUY4ndcG

maYpJhM8on1LBU6oDvJF8jm6sEqMhlLZIwxaqL83cpzoIpydTLAZ61WvE

P/z+pkZ3gY5E2lmqCDLh4YZR0aiogYUM0WJ4TNLMFHMZkVkjBUmxpZVsy

X4q19eJ52rhm+bebiuN72yjiqcwTlcg830IR7aEbCEzhGV7hzcmdF+

fd+ViOVpxy5xT+wPn8ASx2k+8=&lt;/latexit&gt;

&lt;latexit sha1_base64="XRubuHj

W+5LG8jHUmga7olNhNc="&gt;AB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZ

kRQZcFNy4r2Ae0w5DJpG1sJhmSTKEM/RM3LhRx65+482/MtLPQ1gMh3

PuJScnSjnTxvO+ncrG5tb2TnW3trd/cHjkHp90tMwUoW0iuVS9CGvKmaB

twynvVRnEScdqPJXeF3p1RpJsWjmaU0SPBIsCEj2FgpdN1BJHmsZ4m9

cjIPn0K37jW8BdA68UtShxKt0P0axJkCRWGcKx13/dSE+RYGUY4ndcG

maYpJhM8on1LBU6oDvJF8jm6sEqMhlLZIwxaqL83cpzoIpydTLAZ61WvE

P/z+pkZ3gY5E2lmqCDLh4YZR0aiogYUM0WJ4TNLMFHMZkVkjBUmxpZVsy

X4q19eJ52rhm+bebiuN72yjiqcwTlcg830IR7aEbCEzhGV7hzcmdF+

fd+ViOVpxy5xT+wPn8ASx2k+8=&lt;/latexit&gt;

&lt;latexit sha1_base64="XRubuHj

W+5LG8jHUmga7olNhNc="&gt;AB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZ

kRQZcFNy4r2Ae0w5DJpG1sJhmSTKEM/RM3LhRx65+482/MtLPQ1gMh3

PuJScnSjnTxvO+ncrG5tb2TnW3trd/cHjkHp90tMwUoW0iuVS9CGvKmaB

twynvVRnEScdqPJXeF3p1RpJsWjmaU0SPBIsCEj2FgpdN1BJHmsZ4m9

cjIPn0K37jW8BdA68UtShxKt0P0axJkCRWGcKx13/dSE+RYGUY4ndcG

maYpJhM8on1LBU6oDvJF8jm6sEqMhlLZIwxaqL83cpzoIpydTLAZ61WvE

P/z+pkZ3gY5E2lmqCDLh4YZR0aiogYUM0WJ4TNLMFHMZkVkjBUmxpZVsy

X4q19eJ52rhm+bebiuN72yjiqcwTlcg830IR7aEbCEzhGV7hzcmdF+

fd+ViOVpxy5xT+wPn8ASx2k+8=&lt;/latexit&gt;

&lt;latexit sha1_base64="XRubuHj

W+5LG8jHUmga7olNhNc="&gt;AB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZ

kRQZcFNy4r2Ae0w5DJpG1sJhmSTKEM/RM3LhRx65+482/MtLPQ1gMh3

PuJScnSjnTxvO+ncrG5tb2TnW3trd/cHjkHp90tMwUoW0iuVS9CGvKmaB

twynvVRnEScdqPJXeF3p1RpJsWjmaU0SPBIsCEj2FgpdN1BJHmsZ4m9

cjIPn0K37jW8BdA68UtShxKt0P0axJkCRWGcKx13/dSE+RYGUY4ndcG

maYpJhM8on1LBU6oDvJF8jm6sEqMhlLZIwxaqL83cpzoIpydTLAZ61WvE

P/z+pkZ3gY5E2lmqCDLh4YZR0aiogYUM0WJ4TNLMFHMZkVkjBUmxpZVsy

X4q19eJ52rhm+bebiuN72yjiqcwTlcg830IR7aEbCEzhGV7hzcmdF+

fd+ViOVpxy5xT+wPn8ASx2k+8=&lt;/latexit&gt;

&lt;latexit sha1_base64="XRubuHj

W+5LG8jHUmga7olNhNc="&gt;AB+XicbVDLSgMxFL1TX7W+Rl26CRbBVZ

kRQZcFNy4r2Ae0w5DJpG1sJhmSTKEM/RM3LhRx65+482/MtLPQ1gMh3

PuJScnSjnTxvO+ncrG5tb2TnW3trd/cHjkHp90tMwUoW0iuVS9CGvKmaB

twynvVRnEScdqPJXeF3p1RpJsWjmaU0SPBIsCEj2FgpdN1BJHmsZ4m9

cjIPn0K37jW8BdA68UtShxKt0P0axJkCRWGcKx13/dSE+RYGUY4ndcG

maYpJhM8on1LBU6oDvJF8jm6sEqMhlLZIwxaqL83cpzoIpydTLAZ61WvE

P/z+pkZ3gY5E2lmqCDLh4YZR0aiogYUM0WJ4TNLMFHMZkVkjBUmxpZVsy

X4q19eJ52rhm+bebiuN72yjiqcwTlcg830IR7aEbCEzhGV7hzcmdF+

fd+ViOVpxy5xT+wPn8ASx2k+8=&lt;/latexit&gt;

O

&lt;latexit sha1_b

ase64="udb+OiNsS0FUNKEdbmjU

SxGdVM="&gt;AB6HicbVBNS8NAEJ3

Ur1q/qh69LBbBU0mKoMeCF2+2YD+g

DWznbRrN5uwuxFK6C/w4kERr/4k

b/4bt20O2vpg4PHeDPzgkRwbVz3

2ylsbG5t7xR3S3v7B4dH5eOTto5Tx

bDFYhGrbkA1Ci6xZbgR2E0U0igQ2

Akmt3O/84RK81g+mGmCfkRHkoecU

WOl5v2gXHGr7gJknXg5qUCOxqD81R

/GLI1QGiao1j3PTYyfUWU4Ezgr9V

ONCWUTOsKepZJGqP1sceiMXFhlSM

JY2ZKGLNTfExmNtJ5Gge2MqBnrVW8

u/uf1UhPe+BmXSWpQsuWiMBXExGT

+NRlyhcyIqSWUKW5vJWxMFWXGZlO

yIXirL6+Tdq3q2WSaV5V6LY+jCGdw

DpfgwTXU4Q4a0AIGCM/wCm/Oo/Pi

vDsfy9aCk8+cwh84nz+i84zC&lt;/lat

exit&gt;

&lt;latexit sha1_b

ase64="udb+OiNsS0FUNKEdbmjU

SxGdVM="&gt;AB6HicbVBNS8NAEJ3

Ur1q/qh69LBbBU0mKoMeCF2+2YD+g

DWznbRrN5uwuxFK6C/w4kERr/4k

b/4bt20O2vpg4PHeDPzgkRwbVz3

2ylsbG5t7xR3S3v7B4dH5eOTto5Tx

bDFYhGrbkA1Ci6xZbgR2E0U0igQ2

Akmt3O/84RK81g+mGmCfkRHkoecU

WOl5v2gXHGr7gJknXg5qUCOxqD81R

/GLI1QGiao1j3PTYyfUWU4Ezgr9V

ONCWUTOsKepZJGqP1sceiMXFhlSM

JY2ZKGLNTfExmNtJ5Gge2MqBnrVW8

u/uf1UhPe+BmXSWpQsuWiMBXExGT

+NRlyhcyIqSWUKW5vJWxMFWXGZlO

yIXirL6+Tdq3q2WSaV5V6LY+jCGdw

DpfgwTXU4Q4a0AIGCM/wCm/Oo/Pi

vDsfy9aCk8+cwh84nz+i84zC&lt;/lat

exit&gt;

&lt;latexit sha1_b

ase64="udb+OiNsS0FUNKEdbmjU

SxGdVM="&gt;AB6HicbVBNS8NAEJ3

Ur1q/qh69LBbBU0mKoMeCF2+2YD+g

DWznbRrN5uwuxFK6C/w4kERr/4k

b/4bt20O2vpg4PHeDPzgkRwbVz3

2ylsbG5t7xR3S3v7B4dH5eOTto5Tx

bDFYhGrbkA1Ci6xZbgR2E0U0igQ2

Akmt3O/84RK81g+mGmCfkRHkoecU

WOl5v2gXHGr7gJknXg5qUCOxqD81R

/GLI1QGiao1j3PTYyfUWU4Ezgr9V

ONCWUTOsKepZJGqP1sceiMXFhlSM

JY2ZKGLNTfExmNtJ5Gge2MqBnrVW8

u/uf1UhPe+BmXSWpQsuWiMBXExGT

+NRlyhcyIqSWUKW5vJWxMFWXGZlO

yIXirL6+Tdq3q2WSaV5V6LY+jCGdw

DpfgwTXU4Q4a0AIGCM/wCm/Oo/Pi

vDsfy9aCk8+cwh84nz+i84zC&lt;/lat

exit&gt;

&lt;latexit sha1_b

ase64="udb+OiNsS0FUNKEdbmjU

SxGdVM="&gt;AB6HicbVBNS8NAEJ3

Ur1q/qh69LBbBU0mKoMeCF2+2YD+g

DWznbRrN5uwuxFK6C/w4kERr/4k

b/4bt20O2vpg4PHeDPzgkRwbVz3

2ylsbG5t7xR3S3v7B4dH5eOTto5Tx

bDFYhGrbkA1Ci6xZbgR2E0U0igQ2

Akmt3O/84RK81g+mGmCfkRHkoecU

WOl5v2gXHGr7gJknXg5qUCOxqD81R

/GLI1QGiao1j3PTYyfUWU4Ezgr9V

ONCWUTOsKepZJGqP1sceiMXFhlSM

JY2ZKGLNTfExmNtJ5Gge2MqBnrVW8

u/uf1UhPe+BmXSWpQsuWiMBXExGT

+NRlyhcyIqSWUKW5vJWxMFWXGZlO

yIXirL6+Tdq3q2WSaV5V6LY+jCGdw

DpfgwTXU4Q4a0AIGCM/wCm/Oo/Pi

vDsfy9aCk8+cwh84nz+i84zC&lt;/lat

exit&gt;

(a) Intra- &amp; Inter-Category Configuration.

baseball soccer

music

dance

arts

sports

ROOT

tennis

science

ﬁlm biology

chemistry

physics

ROOT

arts

sports

science

O

&lt;latexit sha1_base64="udb+OiNsS0FUNKEdbmjUSxGdVM="&gt;AB6H

icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoMeCF2+2YD+gDWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDPzgkRwbVz32ylsbG5t7xR3S3v7

B4dH5eOTto5TxbDFYhGrbkA1Ci6xZbgR2E0U0igQ2Akmt3O/84RK81g+mGmCfkRHkoecUWOl5v2gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTY

yfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5v

JWxMFWXGZlOyIXirL6+Tdq3q2WSaV5V6LY+jCGdwDpfgwTXU4Q4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz+i84zC&lt;/latexit&gt;

&lt;latexit sha1_base64="udb+OiNsS0FUNKEdbmjUSxGdVM="&gt;AB6H

icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoMeCF2+2YD+gDWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDPzgkRwbVz32ylsbG5t7xR3S3v7

B4dH5eOTto5TxbDFYhGrbkA1Ci6xZbgR2E0U0igQ2Akmt3O/84RK81g+mGmCfkRHkoecUWOl5v2gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTY

yfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5v

JWxMFWXGZlOyIXirL6+Tdq3q2WSaV5V6LY+jCGdwDpfgwTXU4Q4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz+i84zC&lt;/latexit&gt;

&lt;latexit sha1_base64="udb+OiNsS0FUNKEdbmjUSxGdVM="&gt;AB6H

icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoMeCF2+2YD+gDWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDPzgkRwbVz32ylsbG5t7xR3S3v7

B4dH5eOTto5TxbDFYhGrbkA1Ci6xZbgR2E0U0igQ2Akmt3O/84RK81g+mGmCfkRHkoecUWOl5v2gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTY

yfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5v

JWxMFWXGZlOyIXirL6+Tdq3q2WSaV5V6LY+jCGdwDpfgwTXU4Q4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz+i84zC&lt;/latexit&gt;

&lt;latexit sha1_base64="udb+OiNsS0FUNKEdbmjUSxGdVM="&gt;AB6H

icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoMeCF2+2YD+gDWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDPzgkRwbVz32ylsbG5t7xR3S3v7

B4dH5eOTto5TxbDFYhGrbkA1Ci6xZbgR2E0U0igQ2Akmt3O/84RK81g+mGmCfkRHkoecUWOl5v2gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTY

yfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5v

JWxMFWXGZlOyIXirL6+Tdq3q2WSaV5V6LY+jCGdwDpfgwTXU4Q4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz+i84zC&lt;/latexit&gt;

(b) Embed First-Level Local Tree.

baseball soccer

music

dance

arts

sports

ROOT

tennis

science

ﬁlm biology

chemistry

physics

ROOT

arts

sports

science

dance

ﬁlm

music

baseball

soccer

tennis

biology

physics

chemistry

O

&lt;latexit sha1_base64="udb+OiNsS0FUNKEdbmjUSxGdVM="&gt;AB6H

icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoMeCF2+2YD+gDWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDPzgkRwbVz32ylsbG5t7xR3S3v7

B4dH5eOTto5TxbDFYhGrbkA1Ci6xZbgR2E0U0igQ2Akmt3O/84RK81g+mGmCfkRHkoecUWOl5v2gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTY

yfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5v

JWxMFWXGZlOyIXirL6+Tdq3q2WSaV5V6LY+jCGdwDpfgwTXU4Q4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz+i84zC&lt;/latexit&gt;

&lt;latexit sha1_base64="udb+OiNsS0FUNKEdbmjUSxGdVM="&gt;AB6H

icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoMeCF2+2YD+gDWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDPzgkRwbVz32ylsbG5t7xR3S3v7

B4dH5eOTto5TxbDFYhGrbkA1Ci6xZbgR2E0U0igQ2Akmt3O/84RK81g+mGmCfkRHkoecUWOl5v2gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTY

yfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5v

JWxMFWXGZlOyIXirL6+Tdq3q2WSaV5V6LY+jCGdwDpfgwTXU4Q4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz+i84zC&lt;/latexit&gt;

&lt;latexit sha1_base64="udb+OiNsS0FUNKEdbmjUSxGdVM="&gt;AB6H

icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoMeCF2+2YD+gDWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDPzgkRwbVz32ylsbG5t7xR3S3v7

B4dH5eOTto5TxbDFYhGrbkA1Ci6xZbgR2E0U0igQ2Akmt3O/84RK81g+mGmCfkRHkoecUWOl5v2gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTY

yfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5v

JWxMFWXGZlOyIXirL6+Tdq3q2WSaV5V6LY+jCGdwDpfgwTXU4Q4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz+i84zC&lt;/latexit&gt;

&lt;latexit sha1_base64="udb+OiNsS0FUNKEdbmjUSxGdVM="&gt;AB6H

icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoMeCF2+2YD+gDWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDPzgkRwbVz32ylsbG5t7xR3S3v7

B4dH5eOTto5TxbDFYhGrbkA1Ci6xZbgR2E0U0igQ2Akmt3O/84RK81g+mGmCfkRHkoecUWOl5v2gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTY

yfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5v

JWxMFWXGZlOyIXirL6+Tdq3q2WSaV5V6LY+jCGdwDpfgwTXU4Q4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz+i84zC&lt;/latexit&gt;

(c) Embed Second-Level Local Trees.

Figure 2: Spherical tree embeddings. All category center vectors reside on the unit sphere. (a) Representative terms are pushed

into a spherical sector centered around the category center vector. Directional distance is enforced between categories. (b) &amp;

(c) Local trees are recursively embedded onto the sphere.

such that (1) the representative terms selected for each category2

are semantically coherent and (2) the categories are distinctive from

each other, which allows clear category interpretation. For example,

in Figure 1, one can clearly recognize and understand “baseball”

and “soccer” thanks to the discriminative terms that are exclusively

relevant to the corresponding category.

Intra-Category Coherence. The representative terms of each cat-

egory should be highly semantically relevant to each other, reflected

by high directional similarity in the spherical space. To achieve this,

we require the embeddings of representative terms to be placed near

the category center direction within a local region by maximizing

Lintra =

�

ci ∈T

�

wj ∈Ci

min(0,u⊤

wjci − mintra),

(1)

where uw is the word embedding of w; ci is the category center

vector of ci. Note that u⊤wjci = cos(uwj ,ci) since the vectors reside

on the unit sphere Sp−1 ⊂ Rp. We set mintra = 0.9 which works

well in general since it requires high cosine similarity between

representative words and the category center.

When Lintra is maximized (i.e., ∀wj ∈ Ci,u⊤wjci ≥ mintra), the

representative word embeddings of the corresponding category

reside in a spherical sector centered around the category center

vector.

Inter-Category Distinctiveness. We would like to encourage dis-

tinctiveness across different categories to avoid semantic overlaps

so that the retrieved terms provide a clear and distinctive descrip-

tion of the category. To accomplish this, we enforce inter-category

directional dissimilarity by requiring the cosine distance between

2We will discuss how to select representative terms in Section 4.

any two categories to be larger than minter, i.e.,

∀ci,cj(ci � cj), 1 − c⊤

i cj &gt; minter.

Therefore, we maximize the following objective:

Linter =

�

ci ∈T

�

cj ∈T\{ci }

min(0, 1 − c⊤

i cj − minter).

(2)

We will introduce how to set minter in Section 3.2.2.

Figure 2(a) shows the configuration of category center vectors

upon enforcing intra-category coherence and inter-category dis-

tinctiveness.

3.2.2

Recursive Local Tree Embedding. We generalize the ideas

in the flat case to the hierarchical case and recursively embed lo-

cal structures of the category tree such that the relative category

relationship is preserved.

We first define the local tree structure that we work with at each

recursive step:

Definition 2 (Local Tree). A local tree Tr rooted at node cr ∈ T

consists of node cr and all its direct children nodes.

Preserving Relative Tree Distance Within Local Trees. With-

out a hierarchical structure, pairwise category distance is enforced

by Eq. (2). With a local tree structure, the category distance in the

embedding space should reflect the tree distance in a compara-

tive way. Specifically, since the tree distance between two children

nodes is larger than that between a children node and the parent

node, a category should be closer to its parent category than to its

sibling categories in the embedding space. To achieve this property,

we employ the following objective for categories in a local tree Tr :

Linter =

�

ci ∈Tr \{cr }

�

cj ∈Tr \{cr,ci }

min(0,c⊤

i cr −c⊤

i cj −minter), (3)


which generalizes Eq. (2) by forcing the directional similarity be-

tween a children category center vector and its parent category

center vector to be higher than that between two sibling categories

by minter.

Maximizing Linter results in two favorable tree embedding prop-

erties: (1) The children categories are placed near the parent cat-

egory (by requiring higher value of c⊤

i cr ), which reflects the se-

mantic correlation between a sub-category and a super-category;

(2) Any two sibling categories are well-separated (by requiring

lower value of c⊤

i cj), which encourages distinction between sibling

categories (e.g., “baseball” vs. “soccer”).

Recursively Embed Local Trees. We apply the idea of local tree

embedding recursively to embed the entire category tree structure

in a top-down manner: We first embed the local tree rooted at the

ROOT node, and then proceed to the next level to embed the local

trees of every node at the current level. We repeat this process until

we reach the leaf nodes. Figures 2(b) and 2(c) illustrate the recursive

embedding procedure, which can be realized via the following

holistic objective which combines the objectives of every local tree:

Ltree =

�

cr ∈T

�

ci ∈Tr \{cr }

�

cj ∈Tr \{cr,ci }

min(0,c⊤

i cr − c⊤

i cj −minter).

We note that minter needs to be set differently for different levels:

As Figure 2(c) shows, the sibling categories are embedded in more

localized regions as we proceed to the lower levels of the hierarchy

to reflect their intrinsic semantic similarity. As a result, for each

level L of T, we set minter(L) to be the average difference between

children-parent and inter-sibling embedding similarity across level

L, i.e.,

minter(L) =

1

NL

�

cr ∈L

�

ci ∈Tr \{cr }

�

cj ∈Tr \{cr,ci }

c⊤

i cr − c⊤

i cj,

where NL is the total number of sibling pairs within each local tree

in level L. For the simplicity of notations, we omit the argument

of minter in the rest of the paper, but it should be kept in mind that

minter is level-dependent.

Finally, after embedding the category tree, we use the same ob-

jective as Eq. (1) to encourage intra-category coherence of retrieved

terms so that the category embedding configuration can effectively

guide the text embeddings to fit the tree structure.

3.3

Spherical Text Embedding via Modeling

Conditional Corpus Generation

We introduce how to learn text embeddings tailored for the given

category hierarchy T in the spherical space by modeling the corpus

generation process conditioned on the categories. Specifically, we

assume the corpus D is generated following a three-step process:

(1) First, each document di ∈ D is generated conditioned on one

of the categories in the category hierarchy T. Since a category can

cover a wide range of semantics, it is natural to model a category as

a distribution in the embedding space instead of as a single vector.

Therefore, we extend the previous representation of a category ci

from a single center vector ci to a spherical distribution centered

around ci, i.e., a von Mises-Fisher (vMF) distribution. Specifically,

the vMF distribution of a category is parameterized by a mean vector

ci and a concentration parameterκci . The probability density closer

to ci is greater and the spread is controlled by κci . Formally, a unit

random vector x ∈ Sp−1 ⊂ Rp has the p-variate vMF distribution

vMFp(x;ci,κci ) if its probability density function is

f (x;ci,κci ) = np(κci ) exp �κci · cos(x,ci)� ,

where ∥ci ∥ = 1 is the center direction, κci ≥ 0 is the concentration

parameter, and the normalization constant np(κci ) is given by

np(κci ) =

κp/2−1

ci

(2π)p/2Ip/2−1(κci )

,

where Ir (·) represents the modified Bessel function of the first kind

at order r.

We define the generative probability of each document di condi-

tioned on its corresponding true category ci to be:

p(di | ci) = vMF(di;ci,κci ) = np(κci ) exp �κci · cos(di,ci)� , (4)

where di is the document embedding of di.

However, modeling category distribution via Eq. (4) is not di-

rectly helpful for our task, since our goal is to discover representa-

tive terms rather than documents for each category. For this reason,

we further decompose p(di | ci) into category-word distribution:

p(di | ci) ∝

�

wj ∈di

p(wj | ci) ∝

�

wj ∈di

vMF(uwj ;ci,κci ),

(5)

where each word is assumed to be generated independently based

on the document category. Eq. (5) allows direct modeling of p(wj |

ci), from which category representative terms will be derived.

(2) Second, each wordwj is generated based on the semantics of the

documentdi. Intuitively, higher directional similarity implies higher

semantic coherence, thus higher probability of co-occurrence. We

assume the probability of wj appearing in document di to be:

p(wj | di) ∝ exp(cos(uwj ,di)).

(6)

(3) Third, surrounding words wj+k in the local context window

(−h ≤ k ≤ h,k � 0, h is the local context window size) of wj

are generated conditioned on the semantics of the center word wj.

Similar to (2), we assume the probability of wj+k appearing in the

local context window of wj to be:

p(wj+k | wj) ∝ exp(cos(vwj+k ,uwj )),

(7)

where vw is the context word representation of w.

We summarize how the above three steps jointly model the

text generation process by capturing both global and local textual

contexts, conditioned on the given categories: Step (1) draws a

connection between each document and one of the categories in

T (i.e., topic assignment). Step (2) models the semantic coherence

between a word and the document it appears in (i.e., global contexts).

Step (3) models the semantic correlations of co-occurring words

within a local context window (i.e., local contexts). We note that

all three steps use directional similarity to model the correlations

among categories, documents, and words.

4

OPTIMIZATION

In this section, we introduce the optimization procedure for learning

embedding in the spherical space via our model defined in the

previous section.


4.1

Overview

We first summarize the objectives of our optimization problem as

follows (the derivation is based on maximum likelihood estimation;

details can be found at Appendix B):

L = Ltree + Ltext,

Ltree =

�

cr ∈T

�

ci ∈Tr \{cr }

�

cj ∈Tr \{cr,ci }

min(0,c⊤

i cr − c⊤

i cj −minter).

(8)

Ltext =

�

di ∈D

�

wj ∈di

�

wj+k ∈di

−h≤k ≤h,k�0

min

�

0,v⊤

wj+kuwj + u⊤

wjdi

− v⊤

wj+kuw′

j − u⊤

w′

jdi − m

�

+

�

ci ∈T

�

wj ∈Ci

�

log �np(κci )� + κciu⊤

wjci

�

1(u⊤

wjci &lt; mintra).

(9)

s.t.

∀w,d,c,

∥uw ∥ = ∥vw ∥ = ∥d∥ = ∥c∥ = 1,κc ≥ 0,

where 1(·) is the indicator function; we set m = 0.25.

We note that our objective contains latent variables, i.e., the

second term in Eq. (9) requires knowledge about the latent category

of words. At the beginning, we only know that the category name

provided by the user belongs to the corresponding category (e.g.,

wsports ∈ Csports). The goal of Hierarchical Topic Mining is to

discover the latent category assignment of more words such that

they form a clear description of the category.

To solve the optimization problem involving latent variables, we

develop an EM algorithm that iterates between the estimation of the

latent category assignment of words (i.e., E-Step) and maximization

of the embedding training objectives (i.e., M-Step). We detail the

design of the EM algorithm below:

E-Step. We update the estimation of words assigned to each cate-

gory by

C(t)

i

← Topt

�

{w};u(t)

w ,c(t)

i ,κ(t)

ci

�

,

(10)

where Topt ({w};uw,ci,κci ) denotes the set of terms ranked at the

top t positions according to vMF(uw;ci,κci ) (i.e., we assign the t

terms to ci that are most likely generated from its current estimated

category distribution). In practice, we find that gradually increasing

t (i.e., set t = 1 at the first iteration where C(1)

i

contains only the

category name wci by initializing c(1)

i

= u(1)

wci ; increment t by 1 for

the following iterations) works well. Therefore, here t also denotes

the iteration index.

Note here that we only update the estimation of category as-

signment for the top t words per category, which will become the

representative terms retrieved. The reason is that most of the terms

in the vocabulary are not representative for any of the categories;

assigning them to one of the category will have negative impact

on accurate estimation of the category distribution.

M-Step. We update the text embeddings and category embeddings

by maximizing Ltree and Ltext:

Θ(t+1) ← arg max

�

Ltext

�

Θ(t)�

+ Ltree

�

Θ(t)��

,

(11)

where Θ(t) =

�

u(t)

w ,v(t)

w ,d(t),c(t)�

.

Eq. (11) requires non-Euclidean stochastic optimization methods,

which will be introduced in the next subsection.

4.2

Riemannian Optimization

Embedding learning is usually based on stochastic optimization

techniques, but Euclidean optimization methods like SGD cannot be

directly applied to our case, because the Euclidean gradient provides

update directions in a non-curvature space, while the embeddings

in our model must be updated on the spherical surface Sp−1 with

constant positive curvature.

For the above reason, we apply the Riemannian optimization

method in the spherical space as described in [25] to train text

and tree embeddings. Specifically, the Riemannian gradient of a

parameter θ is computed as

grad L(θ) � �I − θθ⊤� ∇L(θ),

where ∇L(θ) is the Euclidean gradient of θ.

For example, the Riemannian gradient of uw is computed as

grad L(uwj ) =

�

I − uwju⊤

wj

� � �

c ∈T

1(wj ∈ C)κcc+

�

di,wj+k

1(posdi,wj,wj+k − neg &lt; m)

�

vwj+k + di

� �

,

where 1(wj ∈ C) is the indicator function of whether wj belongs

to category c; 1(posdi,wj,wj+k − neg &lt; m) is the indicator function

of whether the margin of the positive tuple over the negative one

is achieved.

The Riemannian gradient of the other embeddings can be derived

similarly. Since we aim to maximize our objective, we update the

parameters following the Riemannian gradient direction:

θ(t+1) ← Rθ (t)

�

α · grad L

�

θ(t)��

,

where α is the learning rate; Rx (z) is a first-order approximation of

the exponential mapping at x which maps the updated parameters

back to the sphere. We follow the definition in [25]:

Rx (z) �

x + z

∥x + z∥ .

4.3

Overall Algorithm

We summarize the overall algorithm of Hierarchical Topic Mining

in Algorithm 1.

Complexity. We analyze the computation cost of our algorithm

with respect to the tree size n. The tree embedding objective (Eq. (8))

loops over every local tree Tr ∈ T and every pair of sibling nodes

in Tr . Since the number of local trees is upper bounded by the

number of total tree nodes, the complexity is O(nB2) where B is

the maximum branching factor in T. The text embedding objective

(Eq. (9)) pushes each representative term into the spherical sector

centered around the category center vector, whose complexity is

O(nK). Overall, our algorithm scales linearly with the tree size.

5

EXPERIMENTS

In this section, we conduct empirical evaluations to demonstrate

the effectiveness of our model. We also carry out case studies to


Algorithm 1: Hierarchical Topic Mining.

Input: A text corpus D; a category tree T = {ci }|n

i=1;

number of terms K to retrieve per category .

Output: Hierarchical Topic Mining results Ci |n

i=1.

uw,vw,d,c ← random initialization on Sp−1;

t ← 1;

C(1)

i

← wci |n

i=1

▷ initialize with category names;

while t &lt; K + 1 do

t ← t + 1;

// Representative term retrieval;

C(t)

i

|n

i=1 ← Eq. (10)

▷ E-Step;

// Embedding training;

uw,vw,d,c ← Eq. (11)

▷ M-Step;

for i ← 1 to n do

C(t)

i

← C(t)

i

\ {wci }

▷ exclude category names;

Return C(t)

i

|n

i=1;

Table 1: Dataset statistics.

Corpus

# super-categories

# sub-categories

# documents

NYT

8

12

89,768

arXiv

3

29

230,105

show how the joint embedding space effectively models category

tree structure and textual semantics.

5.1

Experiment Setup

Datasets. We use two datasets from different domains with ground-

truth category hierarchy: (1) The New York Times annotated corpus

(NYT) [34]; (2) arXiv paper abstracts (arXiv)3. For both datasets, we

first select the major categories (with more than 1, 000 documents)

and then collect documents with exactly one ground truth category

label. The dataset statistics can be found at Table 1.

Implementation Details and Parameters. We pre-process the

corpora by discarding infrequent words that appear less than 5

times. We use AutoPhrase [35] to extract quality phrases, which

are treated as single words during embedding training. For fair

comparisons with baselines, we set hyperparameters as below for

all methods: Embedding dimension p = 100; local context window

size h = 5; number of representative terms to retrieve per category

K = 5; learning rate α is set to be 0.025 initially with linear decay.

Other parameters (if any) are set to be the default values of the

corresponding algorithm.

5.2

Hierarchical Topic Mining

Compared Methods. We compare our model with the following

baselines including unsupervised/seed-guided hierarchical topic

models and unsupervised/seed-guided text embedding models. For

baseline methods that require the number of topics nL at each level

L as input, we vary nL in [nL, 2nL, . . . , 5nL] where nL is the actual

3Data crawled from https://arxiv.org/.

Table 2: Quantitative evaluation: Hierarchical Topic Mining.

Models

NYT

arXiv

TC

MACC

TC

MACC

hLDA

-0.0070

0.1636

-0.0124

0.1471

hPAM

0.0074

0.3091

0.0037

0.1824

JoSE

0.0140

0.6818

0.0051

0.7412

Poincaré GloVe

0.0092

0.6182

-0.0050

0.5588

Anchored CorEx

0.0117

0.3909

0.0060

0.4941

CatE

0.0149

0.9000

0.0066

0.8176

JoSH

0.0166

0.9091

0.0074

0.8324

number of categories at level L and report the best performance of

the method.

• hLDA [4]: hLDA is a non-parametric hierarchical topic model. It

assumes that documents are generated from the word distribution

of a path of topics induced by the nested Chinese restaurant process.

Since hLDA is unsupervised and cannot take given category names

as supervision, we manually match the most relevant topics to the

provided category hierarchy.

• hPAM [29]: hPAM generalizes the Pachinko Allocation Model [19]

by sampling topic paths from the Dirichlet-multinomial distribu-

tions of internal nodes. We perform manual matching of topics as

we do for hLDA.

• JoSE [25]: JoSE trains spherical text embeddings with Riemann-

ian optimization. It outperforms Euclidean embedding models on

textual similarity measurement. We retrieve the nearest-neighbor

words of the category name in the spherical space as category

representative words.

• Poincaré GloVe [36]: Poincaré GloVe learns hyperbolic word em-

beddings based on the Euclidean GloVe model. It naturally encodes

the latent hierarchical word semantic correlations (e.g., hypernym-

hyponym). We retrieve the nearest-neighbor words of the category

name in the Poincaré space as category representative words.

• Anchored CorEx [8]: CorEx discovers informative topics via total

correlation maximization and can naturally model topic hierarchy

via latent factor dependencies. Its anchored version incorporates

user-provided seed words by balancing between compressing the

original corpus and preserving anchor words related information.

We provide the category names as seed words.

• CatE [24]: CatE takes category names as input and learns discrim-

inative text embeddings by enforcing distinctiveness among cate-

gories. We recursively run CatE on local trees since CatE assumes

that the provided categories are mutually-exclusive semantically.

Quantitative Evaluation. We apply two metrics on the top-K

(K = 5 in our experiments) words/phrases retrieved under each

category to evaluate all methods: Topic coherence (TC) and Mean

accuracy (MACC) as defined in [24]. The accuracy metric is obtained

from the averaged results given by five graduate students who

independently label whether each retrieved term is highly relevant

to the corresponding category. The quantitative results are reported

in Table 2.

Qualitative Results. We demonstrate the qualitative results of

NYT in Figure 3 and arXiv in Figure 5 in Appendix A. Words in


Table 3: Run time (in minutes) on NYT. Models are run on a

machine with 20 cores of Intel(R) Xeon(R) CPU E5-2680 v2

@ 2.80 GHz.

hLDA

hPAM

JoSE

Poincaré GloVe

Anchored CorEx

CatE

JoSH

53

22

5

16

61

52

6

blue boxes are input category names; words in white boxes are

retrieved representative terms of the corresponding category.

Run Time. Since topic discovery is usually performed on large-

scale text corpus, algorithm efficiency is of great importance. There-

fore, we report the run time of all methods in Table 3. JoSH takes

only slightly longer to train than JoSE, which only learns text em-

beddings.

Discussions. The two unsupervised baselines (hLDA and hPAM)

do not perform well. Despite running with different parameters for

multiple times, they still fail to generate high quality topics similar

to the ground-truth category hierarchy, showing the limitations

of unsupervised approaches. For the two unsupervised embedding

baselines, JoSE outperforms Poincaré GloVe by a large margin,

demonstrating that the spherical space is more suitable than the

hyperbolic space on capturing textual semantic correlations for

category representative term retrieval. CatE has strong performance

on the two datasets, but it has to be run recursively on each set of

sibling nodes since it requires all the input categories to be mutually

exclusive. Therefore, run time will become a potential bottleneck of

applying CatE to large-scale hierarchies. JoSH not only outperforms

all models on Hierarchical Topic Mining quality, but also enjoys

high efficiency via efficient joint modeling of category tree structure

and text corpus statistics.

5.3

Weakly-Supervised Hierarchical Text

Classification

Hierarchical Topic Mining is also closely related to the task of

text classification. Intuitively, having a good understanding of top-

ics should lead to better categorization of documents. Similar to

Hierarchical Topic Mining, the input to weakly-supervised hierar-

chical classification is also a word-described category tree. Since

weakly-supervised classification [26, 27, 37] does not require train-

ing documents, it is especially favorable when manual annotation

is expensive.

Compared Methods. We compare the following weakly-supervised

hierarchical models on their classification performance, evaluated

on the two datasets.

• WeSHClass [27]: WeSHClass leverages the provided keywords

of each category to generate a set of pseudo documents for pre-

training a hierarchical deep classifier, and self-trains the ensembled

local classifiers on unlabeled data. It uses Word2Vec [28] as word

representation.

• JoSH: Since our model makes explicit generative assumption

between topics and documents (Eq. (4)), we are able to build a

generative classifier by assigning the document to the category

with the highest probability that it gets generated from, i.e.,

yd = arg max

c

vMF(d;c,κc),

where yd is the predicted category label for document d.

Table 4: Quantitative evaluation: Weakly-supervised hierar-

chical classification.

Models

NYT

arXiv

Macro-F1

Micro-F1

Macro-F1

Micro-F1

WeSHClass

0.425

0.581

0.320

0.542

JoSH

0.429

0.600

0.367

0.610

WeSHClass + CatE

0.503

0.679

0.401

0.622

WeSHClass + JoSH

0.582

0.703

0.412

0.673

• WeSHClass + CatE [24]: It is shown in [24] that the learned dis-

criminative text embedding can be used as input feature to benefit

classification model. We replace the Word2Vec embedding used in

WeSHClass with CatE embeddings.

• WeSHClass + JoSH: We replace the Word2Vec embedding used

in WeSHClass with word embeddings learned by JoSH. Since JoSH

effectively leverages the category tree structure to guide text em-

bedding configuration, it is expected to benefit hierarchical classifi-

cation model as input features.

Quantitative Evaluation. We use two metrics for classification

evaluation, Macro-F1 and Micro-F1, which are commonly used in

multi-class classification evaluations. The results are reported in

Table 4.

Discussions. We demonstrate two potential usage of JoSH in weakly-

supervised hierarchical text classification: (1) Directly build a gener-

ative classifier based on the model assumption; (2) Use the learned

embedding as input features to existing classification models. JoSH

alone as a generative classifier even outperforms the WeSHClass

model; when used as features to WeSHClass, JoSH significantly

boosts the classification performance, proved to be more effective

than CatE which does not model the category hierarchy.

5.4

Joint Embedding Space Visualization

To understand how categories and words are distributed in the

joint embedding space and how the category tree structure is mod-

eled, we apply t-SNE [21] to visualize the embedding space in

Figure 4. Representative terms surround their category centers;

sub-categories surround their super-categories which form a cat-

egory tree structure. An interesting observation is that some sub-

categories under different super-categories are embedded closer,

e.g., in Figure 4(b), “optimization” under “math” and “algorithm”

under “computer science”. Indeed, these two sub-categories are

somewhat cross-domain—“optimization” and “algorithm” are rele-

vant to both mathematics and computer science. This shows that

JoSH not only models the given category tree structure, but also

captures semantic correlation among categories via jointly training

tree and text embedding.

6

RELATED WORK

6.1

Hierarchical Topic Modeling

Hierarchical topic models extend their flat counterparts by captur-

ing the correlations among topics and generate topic hierarchies.

hLDA [4] generalizes LDA [6] with a non-parametric probabilistic

model, the nested Chinese restaurant process, which induces a path

from the root topic to a leaf topic. The documents are assumed to

be generated by sampling words from the topics along this path.


music

songs

tunes

guitar

melody

jazz

ROOT

dance

modern-dance

dancers

choreographer

ballet

troupe

soccer

soccer federation

striker

cup

ﬁnals

champion

golf

golf club

nine-hole

tiger woods

golf courses

sawgrass

sports

tournament

championship

team

ﬁnals

basketball

arts

theater

artist

contemporary

classics

studio

small business

small businesses

self-employed

low-wage

low-income

minimum-wage

markets

stocks

currency

trading

investors

traders

business

corporations

employees

jobs

industries

wholesaling

media

television

columnists

newspapers

broadcast

radio

baseball

dodgers

pitching

yankees

outﬁelder

ballplayers

movies

ﬁlms

hollywood

comedies

ﬁlm maker

blockbusters

design

architects

building

designers

modernist

sculptor

theater

playhouse

shubert

broadway

mccarter

lortel

hockey

n.h.l

canucks

lindros

mogilny

defenseman

education

curriculum

school-based

educational

elementary

instruction

technology

software

chip

electronics

technologies

computer

health

aids

health-care

mental health

patients

pediatric

science

physics

biology

chemistry

scientist

astronomy

politics

ideology

partisan

political

conservatism

liberal

Figure 3: Hierarchical Topic Mining results on NYT.

−15

−10

−5

0

5

10

−10

−5

0

5

10

ROOT

education

technology

politics

sports

science

business

arts

golf

soccer

baseball

hockey

 health

media

markets

design

music

dance

movies

theater

small business 

(a) NYT joint embedding space.

−15

−10

−5

0

5

10

15

−20

−15

−10

−5

0

5

10

15

20

ROOT

math

physics

numerical analysis

geometry

complex variables

pde

optimization

statistics

probability

combinatorics

algebra

representation theory

ode

dynamical systems

optics

ﬂuid dynamics

atomic

accelerator

plasma

game theory

information theory

artiﬁcial intelligence

natural language processing

networking

software engineering

computational complexity

cryptography

algorithm

programming languages

databases

 pattern recognition

computer science

(b) arXiv joint embedding space.

Figure 4: Joint embedding space visualization. Category center vectors are denoted as stars; representative words are denoted

as dots in the same color with corresponding category.

Another famous hierarchical topic model, hPAM [29] is built on

the Pachinko Allocation Model [19] which models documents as a

mixture of distributions over a set of topics; the co-occurrences of

topics are further represented via a directed acyclic graph. hPAM

represents the topic hierarchical structure through the Dirichlet-

multinomial parameters of the internal node distributions. There

are also supervised hierarchical topic models. HSLDA [33] extends

sLDA [5] by incorporating a breadth first traversal in the label space

during document generation. SSHLDA [22] is a semi-supervised

hierarchical topic model that not only explores new latent topics in

the label space, but also makes use of the information from the hier-

archy of observed labels. A seed-guided topic modeling framework,

CorEx [8], learns informative topics that maximize total correla-

tion. It is similar to our setting as it incorporates seed words by

preserving seed relevant information. CorEx is able to generate

topic hierarchy via latent factor dependencies. Different from the

previous unsupervised and supervised topic models, our framework

takes as guidance only a category hierarchy described by category

names, and models category-word semantic correlation via joint

spherical text and tree embedding.

6.2

Text Embedding and Tree Embedding

Text embeddings [17, 23, 25, 28, 32] effectively capture textual se-

mantic similarity via distributed representation learning of words,

phrases, sentences, etc. Several topic modeling frameworks, such

as [3, 7, 20] leverage text embeddings to model contextualized

semantic similarity of words, making up the bag-of-words gen-

erative assumption in classical topic models. Poincaré GloVe [36]

adapts the original GloVe model by training word embedding in

the Poincaré space where the latent hierarchical semantic relations

between words are naturally captured. A recent text embedding

model CatE [24] proposes to learn discriminative text embeddings

for category representative term retrieval given a set of category

names as user guidance, which is similar to our setting. CatE makes

mutual exclusive assumption on category semantics, which does

not hold when categories exhibit a hierarchical structure. None of

the previous text embedding framework is able to model a given

hierarchical category structure in the embedding space to guide

text embedding learning.

With the recent advances in hyperbolic embedding space, sev-

eral frameworks have been developed to model tree structures.

Poincaré embedding [30] learns to model hierarchical structure

in the Poincaré ball. Since the embedding distance directly corre-

sponds to tree distance, Poincaré embedding can be used to infer

lexical entailment relationship by embedding the tree structure

of WordNet or perform link prediction by embedding networks.

Later, Lorentz model [31] brings a more principled optimization

approach in the hyperbolic space to learn tree structures; hyper-

bolic cones [9] are proposed to model hierarchical relations and


admit an optimal shape with a closed form expression. These hy-

perbolic tree embedding methods, however, are not suitable for

embedding category trees in a joint space with words. The reason

is that hyperbolic embeddings preserve the absolute tree distance,

i.e., similar embedding distances imply similar tree distances. In

a category tree, lower-level sibling categories are generally more

semantically similar than higher-level ones despite the same tree

distance. Therefore, category embedding distances should not be

solely determined by tree distances. In our model, text and cate-

gory tree are jointly embedded, allowing the tree structure to better

reflect the textual semantics of the categories.

7

CONCLUSIONS AND FUTURE WORK

In this paper, we propose a new task for hierarchical topic discovery

guided by a user-provided category tree described with category

names only. To effectively model the category tree structure while

capturing text corpus statistics, we propose a joint spherical space

embedding model JoSH that uses directional similarity to character-

ize semantic correlations among words, documents, and categories.

We develop an EM algorithm based on Riemannian optimization for

training the model in the spherical space. JoSH mines high-quality

topics and enjoys high efficiency. We also show that JoSH can be

applied to the task of weakly-supervised hierarchical classification,

serving as either a generative classifier on its own, or input features

to existing classification models.

In the future, we aim to extend JoSH to not only focus on a user-

given category structure, but also be able to discover other latent

topics from a text corpus, probably by relaxing the assumption that a

document is generated from one of the given topics or collaborating

with other taxonomy construction algorithms [12, 13]. Also, the

promising results of our joint spherical space embedding model may

shed light on future studies of embedding tree or graph structures

along with textual data in the spherical space for mining structured

knowledge from text corpora.

ACKNOWLEDGMENTS

Research was sponsored in part by US DARPA KAIROS Program

No. FA8750-19-2-1004 and SocialSim Program No. W911NF-17-C-

0099, National Science Foundation IIS 16-18481, IIS 17-04532, and

IIS 17-41317, and DTRA HDTRA11810026. Any opinions, findings,

and conclusions or recommendations expressed herein are those of

the authors and should not be interpreted as necessarily represent-

ing the views, either expressed or implied, of DARPA or the U.S.

Government. The U.S. Government is authorized to reproduce and

distribute reprints for government purposes notwithstanding any

copyright annotation hereon. We thank anonymous reviewers for

valuable and insightful feedback.

REFERENCES

[1] Enrique Alfonseca, Katja Filippova, Jean-Yves Delort, and Guillermo Garrido.

2012. Pattern Learning for Relation Extraction with a Hierarchical Topic Model.

In ACL.

[2] David Andrzejewski and Xiaojin Zhu. 2009. Latent Dirichlet Allocation with

Topic-in-Set Knowledge. In HLT-NAACL.

[3] Kayhan Batmanghelich, Ardavan Saeedi, Karthik Narasimhan, and Sam Gersh-

man. 2016. Nonparametric spherical topic modeling with word embeddings. In

ACL. 537.

[4] David M. Blei, Thomas L. Griffiths, Michael I. Jordan, and Joshua B. Tenenbaum.

2003. Hierarchical Topic Models and the Nested Chinese Restaurant Process. In

NIPS.

[5] David M Blei and Jon D Mcauliffe. 2008. Supervised topic models. In NIPS.

121–128.

[6] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet

Allocation. In NIPS.

[7] Adji B. Dieng, Francisco J. R. Ruiz, and David M. Blei. 2019. Topic Modeling in

Embedding Spaces. ArXiv abs/1907.04907 (2019).

[8] Ryan J. Gallagher, Kyle Reing, David C. Kale, and Greg Ver Steeg. 2017. Anchored

Correlation Explanation: Topic Modeling with Minimal Domain Knowledge.

TACL (2017).

[9] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. 2018. Hyperbolic

Entailment Cones for Learning Hierarchical Embeddings. In ICML.

[10] Justin Grimmer. 2010. A Bayesian hierarchical topic model for political texts:

Measuring expressed agendas in Senate press releases. Political Analysis 18, 1

(2010), 1–35.

[11] Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In SIGIR.

[12] Jiaxin Huang, Yiqing Xie, Yu Meng, Jiaming Shen, Yunyi Zhang, and Jiawei Han.

2020. Guiding Corpus-based Set Expansion by Auxiliary Sets Generation and

Co-Expansion. In WWW.

[13] Jiaxin Huang, Yiqing Xie, Yu Meng, Yunyi Zhang, and Jiawei Han. 2020. CoRel:

Seed-Guided Topical Taxonomy Construction by Concept Learning and Relation

Transferring. In KDD.

[14] Jagadeesh Jagarlamudi, Hal Daumé, and Raghavendra Udupa. 2012. Incorporating

Lexical Priors into Topic Models. In EACL.

[15] Saurabh S Kataria, Krishnan S Kumar, Rajeev R Rastogi, Prithviraj Sen, and

Srinivasan H Sengamedu. 2011. Entity disambiguation with hierarchical topic

models. In KDD.

[16] Sachin Kumar and Yulia Tsvetkov. 2019. Von Mises-Fisher Loss for Training

Sequence to Sequence Models with Continuous Outputs. In ICLR.

[17] Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences

and Documents. In ICML.

[18] Omer Levy and Yoav Goldberg. 2014. Linguistic Regularities in Sparse and

Explicit Word Representations. In CoNLL.

[19] Wei Li and Andrew McCallum. 2006. Pachinko allocation: DAG-structured

mixture models of topic correlations. In ICML. 577–584.

[20] Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. 2015. Topical Word

Embeddings. In AAAI.

[21] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.

Journal of machine learning research 9, Nov (2008), 2579–2605.

[22] Xianling Mao, Zhaoyan Ming, Tat-Seng Chua, Si Kan Li, Hongfei Yan, and Xi-

aoming Li. 2012. SSHLDA: A Semi-Supervised Hierarchical Topic Model. In

EMNLP-CoNLL.

[23] Yu Meng, Jiaxin Huang, Guangyuan Wang, Zihan Wang, Chao Zhang, and Jiawei

Han. 2020. Unsupervised Word Embedding Learning by Incorporating Local and

Global Contexts. Frontiers in Big Data (2020).

[24] Yu Meng, Jiaxin Huang, Guangyuan Wang, Zihan Wang, Chao Zhang, Yu Zhang,

and Jiawei Han. 2020. Discriminative Topic Mining via Category-Name Guided

Text Embedding. In WWW.

[25] Yu Meng, Jiaxin Huang, Guangyuan Wang, Chao Zhang, Honglei Zhuang, Lance

Kaplan, and Jiawei Han. 2019. Spherical Text Embedding. In NeurIPS.

[26] Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han. 2018. Weakly-Supervised

Neural Text Classification. In CIKM.

[27] Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han. 2019. Weakly-Supervised

Hierarchical Text Classification. In AAAI.

[28] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.

2013. Distributed Representations of Words and Phrases and their Composition-

ality. In NIPS.

[29] David M. Mimno, Wei Li, and Andrew McCallum. 2007. Mixtures of hierarchical

topics with Pachinko allocation. In ICML ’07.

[30] Maximilian Nickel and Douwe Kiela. 2017. Poincaré Embeddings for Learning

Hierarchical Representations. In NIPS.

[31] Maximilian Nickel and Douwe Kiela. 2018. Learning Continuous Hierarchies in

the Lorentz Model of Hyperbolic Geometry. In ICML.

[32] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove:

Global Vectors for Word Representation. In EMNLP.

[33] Adler J. Perotte, Frank D. Wood, Noémie Elhadad, and Nicholas Bartlett. 2011.

Hierarchically Supervised Latent Dirichlet Allocation. In NIPS.

[34] Evan Sandhaus. 2008. The New York Times Annotated Corpus.

[35] Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R. Voss, and Jiawei Han.

2018. Automated Phrase Mining from Massive Text Corpora. IEEE Transactions

on Knowledge and Data Engineering 30 (2018), 1825–1837.

[36] Alexandru Tifrea, Gary Bécigneul, and Octavian-Eugen Ganea. 2019. Poincaré

Glove: Hyperbolic Word Embeddings. In ICLR.

[37] Yu Zhang, Frank F Xu, Sha Li, Yu Meng, Xuan Wang, Qi Li, and Jiawei Han. 2019.

HiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories.

In ICDM.


A

HIERARCHICAL TOPIC MINING RESULTS

ON ARXIV

Figure 5 shows part of the Hierarchical Topic Mining results on

arXiv.

numerical analysis

random matrix

spectral analysis

fourier analysis

asymptotic analysis

convex analysis

math

numbers

polynomials

scalars

mathematicians

planes

geometry

spheres

hypersurfaces

geodesics

foliations

submanifolds

probability

random variable

markov chain

conditional

random walk

marginal

pde

integral equations

hamilton-jacobi

fully nonlinear

fokker-planck

heat equation

optimization

minimization

maximization

stochastic optimization

non-convex

objective function

(a) “Math” subtree.

plasma

electrostatic

tokamak

heating

magnetized

space charge

accelerator

lhc

fermilab

collider

linac

storage ring

physics

particle physics

black holes

dark matter

neutrino

photons

ﬂuid dynamics

aps-dfd

liquid

ﬂuid motion

droplet

supersonic

atomic

excited states

molecule

helium

ionization

metastable

optics

invisibility

cloaks

lenses

metamaterials

electromagnetic

(b) “Physics” subtree.

natural language processing

machine translation

parsing

question answering

information extraction

summarization

networking

cloud computing

p2p

iot

sdn

virtualization

computer science

computer

machine learning

artiﬁcial intelligence

data mining

robotics

programming languages

libraries

python

java

c++

compiler

game theory

decision problems

inﬂuence diagrams

two-player

incomplete information

nash equilibria

pattern recognition

image processing

computer vision

image segmentation

object recognition

vision tasks

(c) “Computer Science” subtree.

Figure 5: Results of Hierarchical Topic Mining on arXiv:

Only 5 sub-categories per super-category are shown here.

B

DERIVATION OF OBJECTIVE

The derivation of Eqs. (8) and (9) is provided as follows.

The conditional likelihood of the corpus given the category hi-

erarchy is obtained by combining the assumptions described in

Eqs. (5), (6) and (7):

P(D | T) =

�

di ∈D

p(di | ci)

�

wj ∈di

p(wj | di)

�

wj+k ∈d

−h ≤k ≤h,k�0

p(wj+k | wj)

∝

�

di ∈D

�

wj ∈di

p(wj | ci)p(wj | di)

�

wj+k ∈di

−h ≤k ≤h,k�0

p(wj+k | wj),

(12)

where ci is the latent true category of di.

To make the learning of text embedding and category distribu-

tion explicit, we re-write Eq. (12) by re-arranging the product of

p(w | c) over categories:

P(D | T) ∝

�

ci ∈T

�

wj ∈ci

p(wj | ci)

·

�

di ∈D

�

wj ∈di

p(wj | di)

�

wj+k ∈di

−h ≤k ≤h,k�0

p(wj+k | wj),

Taking the log-likelihood as our objective to maximize, we have

L =

�

ci ∈T

�

wj ∈ci

logp(wj | ci)

+

�

di ∈D

�

wj ∈di

logp(wj | di)

+

�

di ∈D

�

wj ∈di

�

wj+k ∈di

−h≤k ≤h,k�0

logp(wj+k | wj)

+ constant.

(13)

We omit the constant term and split Eq. (13) into category distri-

bution modeling and corpus-based embedding learning objectives,

plugging in the definition of the probability expressions given by

Eqs. (5), (6) and (7). For category distribution modeling, we have:

Lcat =

�

ci ∈T

�

wj ∈ci

logp(wj | ci)

=

�

ci ∈T

�

wj ∈ci

log �np(κci )� + κci · cos(uwj ,ci).

(14)

Eq. (14) achieves the same effect as Eq. (1) on encouraging word

representative terms to have high directional similarity with the

category center vector, except that Eq. (14) does not incorporate an

intra-category margin. Thus we extend Eq. (14) into the following:

L∗

cat =

�

ci ∈T

�

wj ∈Ci

�

log �np(κci )� + κciu⊤

wjci

�

1(u⊤

wjci &lt; mintra),

(15)

where 1(·) is the indicator function.

For corpus-based embedding learning, we have:

Lcorpus =

�

di ∈D

�

wj ∈di

�����

�

logp(wj | di) +

�

wj+k ∈di

−h≤k ≤h,k�0

logp(wj+k | wj)

�����

�

=

�

di ∈D

�

wj ∈di

�����

�

cos(uwj ,di) +

�

wj+k ∈di

−h ≤k ≤h,k�0

cos(vwj+k ,uwj )

�����

�

.

Directly maximizing the above objective results in trivial solution

that all text embedding vectors are converged to the same point (so

that the cosine similarity term is always maximized). To tackle this

issue, we employ the same technique used in [25] where the log-

likelihood of a positive co-occurring tuple (wj,wj+k,di) is pushed

over that of a negative tuple (w′

j,wj+k,di) by a margin m, where

w′

j is a randomly sampled word from the vocabulary.

Lcorpus =

�

di ∈D

�

wj ∈di

�

wj+k ∈di

−h≤k ≤h,k�0

min

�

0, −m + cos(vwj+k ,uwj )+

cos(uwj ,di)) − cos(vwj+k ,uw′

j ) − cos(uw′

j ,di)

�

.

Finally, Ltext = L∗

cat + Lcorpus.

