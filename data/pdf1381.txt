


Published in

Towards Data Science



Jul 29, 2021

·

8 min read

·

Save

Word2Vec Explained

Explaining the Intuition of Word2Vec &amp; Implementing it in Python

This image was taken by Raphael Schaller from Unsplash

Table of Contents








Introduction

What is a Word Embedding?

Word2Vec Architecture


You can see that the words King and Queen are close to each other in position. (Image provided by the author)

CBOW (Continuous Bag of Words)

"Have a great day"

CBOW Architecture. Image taken from Efficient Estimation of Word Representation in Vector Space

Continuous Skip-Gram Model

Example of generating training data for skip-gram model. Window size is 3. Image provided by author

Skip-Gram Model architecture (Image provided by author)


Implementation

Data

Requirements

Note: 

Import Data


Note:

PATH

Preprocess Data


Stopword Filtering Note

Embed


Words in the Shakespeare data which is most similar to thou (Image provided by the Author)

PCA on Embeddings






Words similar to each other would be placed closer together to one another. Image provided by author

Embedding projector - visualization of high-dimensional data

Concluding Remarks

Node2Vec Explained

Resources










Bayesian A/B Testing Explained

Recommendation Systems Explained

Monte Carlo Method Explained

Markov Chain Explained

Data Science


7



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





Python

Artificial Intelligence

Machine Learning

NLP

