
A Mixture Model for Contextual Text Mining

Qiaozhu Mei

Department of Computer Science

University of Illinois at Urbana-Champaign

Urbana,IL 61801

qmei2@uiuc.edu

ChengXiang Zhai

Department of Computer Science

University of Illinois at Urbana-Champaign

Urbana,IL 61801

czhai@cs.uiuc.edu

ABSTRACT

Contextual text mining is concerned with extracting topical

themes from a text collection with context information (e.g.,

time and location) and comparing/analyzing the variations

of themes over diﬀerent contexts. Since the topics covered

in a document are usually related to the context of the doc-

ument, analyzing topical themes within context can poten-

tially reveal many interesting theme patterns. In this paper,

we propose a new general probabilistic model for contextual

text mining that can cover several existing models as special

cases. Speciﬁcally, we extend the probabilistic latent seman-

tic analysis (PLSA) model by introducing context variables

to model the context of a document. The proposed mixture

model, called contextual probabilistic latent semantic anal-

ysis (CPLSA) model, can be applied to many interesting

mining tasks, such as temporal text mining, spatiotempo-

ral text mining, author-topic analysis, and cross-collection

comparative analysis. Empirical experiments show that the

proposed mixture model can discover themes and their con-

textual variations eﬀectively.

Categories and Subject Descriptors: H.3.3 [Informa-

tion Search and Retrieval]: Text Mining

General Terms: Algorithms

Keywords: Contextual text mining, context, mixture model,

EM algorithm, theme pattern, clustering

1.

INTRODUCTION

A text document is often associated with various kinds

of context information, such as the time and location at

which the document was produced, the author(s) who wrote

the document, and its publisher. The contents of text doc-

uments with the same or similar context are often corre-

lated in some way. For example, news articles written in

the period of some major event all tend to be inﬂuenced

by the event in some way, and papers written by the same

researcher tend to share similar topics. In order to reveal in-

teresting content patterns in such contextualized text data,

it is necessary to consider context information when ana-

Permission to make digital or hard copies of all or part of this work for

personal or classroom use is granted without fee provided that copies are

not made or distributed for proﬁt or commercial advantage and that copies

bear this notice and the full citation on the ﬁrst page. To copy otherwise, to

republish, to post on servers or to redistribute to lists, requires prior speciﬁc

permission and/or a fee.

KDD’06, August 20–23, 2006, Philadelphia, Pennsylvania, USA.

Copyright 2006 ACM 1-59593-339-5/06/0008 ...$5.00.

lyzing the topics covered in such data. Indeed, there have

been several recent studies in this direction. For example,

the time stamps of text documents have been considered

in some recent work on temporal text mining [9, 16, 14,

4].

Also, author-topic analysis is studied in [17], and cross-

collection comparative text mining is studied in [18].

All

these studies consider some kinds of context information,

i.e., time, authorship, and subcollection.

Time, authorship, and subcollection are by no means the

only possible context information of a document. In fact,

any metadata entry of a document can indicate a context

and all documents with the same value of this metadata en-

try can be considered as in the same context. For example,

the source of a news article, the author’s age group, occu-

pation, and location of a weblog article, and the citation

frequency of a research paper, are all reasonable context

information. Moreover, a document may belong to multi-

ple contexts, and any combination of its metadata entries

makes a “complex” context. By analyzing the variations of

topics over these contexts, a lot of interesting text mining

tasks can be addressed, such as spatiotemporal text mining,

author-topic evolutionary analysis over time, and opinion

comparison over diﬀerent age groups and occupations.

However, existing techniques are usually tuned for some

speciﬁc tasks, and are not applicable to consider other kinds

of contexts. For example, one cannot directly use the tem-

poral text mining techniques to model the occupation of

authors. This indicates a serious limitation of existing con-

textual analysis of themes: every time when a new combina-

tion of context information is to be considered, people have

to seek for solutions in an ad hoc way.

Therefore, it is highly desirable to introduce a general text

mining problem, contextual text mining, which is abstracted

from a family of text mining tasks with various types of

contextual analysis. It is desirable to derive a model that is

highly general to conduct the common tasks of these speciﬁc

contextual text mining problems, and easy to be applied to

each of them with appropriate regularization.

In this work, we deﬁne the general problem of Contex-

tual Text Mining (CtxTM) and its common tasks, which

is abstracted from a family of speciﬁc text mining problems.

We extend the probabilistic latent semantic analysis (PLSA)

model to incorporate context information, and develop a

contextual probabilistic latent semantic analysis (CPLSA)

model to facilitate contextual text mining in a general way.

By ﬁtting the model to the text data to mine, we can (1)

discover the global salient themes from the collection of doc-

uments; (2) analyze the content variation of the themes in

649

Research Track Poster


any given view of context; and (3) analyze the coverage of

themes associated with any given context.

These tasks are general and can be easily applied to diﬀer-

ent speciﬁc contextual text mining problems. In this paper,

we show that many existing contextual theme analysis prob-

lems can be deﬁned as special cases of CtxTM, and can be

solved with regularized versions of the mixture model we

proposed, corresponding to the context information and the

mining tasks it involves. Although it may not be the only

possible model for contextual text mining, the model is quite

ﬂexible to adapt diﬀerent assumptions.

2.

CONTEXTUAL TEXT MINING

Given a collection of documents with context information,

we assume that there is a set of topics, or themes in the col-

lection which vary over diﬀerent contexts. Our goal is gen-

erally to conduct context-sensitive analysis of these themes.

As stressed in previous work [14], a theme in a contextual-

ized text collection D is a probabilistic distribution of words

that characterizes a semantically coherent topic or subtopic.

Without loss of generality, we will assume that there are al-

together k major themes in our collection, Θ = {θ1,..., θk}.

To model the context of a document, we introduce a con-

cept called context feature, which is deﬁned as any meta-data

of a document (e.g., the time stamp in temporal text mining

or authorship in author-topic analysis). The context that a

document belongs to can be indicated by the context fea-

tures of this document, which is formally deﬁned as follows:

Deﬁnition 1 (Context) Let F = {f1, f2, ..., f|F |} be

a set of context features.

A Context c in a document

collection is decided by any combination of context fea-

tures in F, formally c ⊆ 2|F|.

The whole set of possi-

ble contexts is denoted as C = {c1, ..., cn}. Suppose D =

{(D1, C1), ..., (D|D|, C|D|)} is a collection of documents, each

document Di is a sequence of words from a vocabulary set

V = {w1, ..., w|V |}, and Ci ⊆ F is a set of context features

which are associated with the document Di. A document

Di belongs to a context c iﬀ. Ci ⊆ c. This tells us that a

document can belong to multiple contexts. In another word,

the contexts are possible to overlap.

In contextual text mining, our goal is to analyze the top-

ics/subtopics in such a text collection in a context-sensitive

way. Speciﬁcally, we would like to model the k major themes

and how they vary according to diﬀerent contexts, and would

also like to model the coverage of diﬀerent themes in a doc-

ument or documents that share certain kinds of context.

To accommodate context-sensitive theme analysis, we con-

sider variations of these k themes over diﬀerent contexts. For

example, if the context we are interested in is time, we will

assume that there is a potentially distinct “version” of the

k themes in each diﬀerent time period; diﬀerent such “ver-

sions” model the variations of themes across time stamps.

We formally deﬁne such a variation as a View of themes.

Deﬁnition 2 (View) A view of themes in a contextu-

alized text collection D is a sequence of themes θi1, ..., θik,

where θil is the variation of theme θl according to view vi.

We will assume that there are n views in our collection,

v1, ..., vn, each corresponds to a context ci. Therefore, a doc-

ument is assumed to potentially have multiple views; pre-

cisely which views are taken depends on the document and

its context. Each view vi is assumed to be taken in any doc-

uments in the context ci, which can also be overlapping.

Deﬁnition 3(Context Support) The support of a con-

text ci, s(ci) is the set of documents in context ci, i.e.,

s(ci) = {Dj|Cj ⊆ ci}. Since each context is associated with

a view, we also call s(ci) as the support of the view vi.

To analyze the strength of themes, we further model the

variable coverage of diﬀerent themes in a document. For ex-

ample, some documents would favor some particular themes

and thus would have a larger coverage of them.

Deﬁnition 4 (Coverage) A coverage of themes in a

document (κj) is a distribution over the themes p(l|κi).

Clearly, �k

l=1 p(l|κj) = 1.

We will assume that there are m distinct theme coverages

in our collection, κ1, ..., κm. For example, if we assume that

each document has a potentially distinct theme coverage,

then m = |D|. In general, however, a document can cover

themes according to multiple coverages. For example, if we

are interested in modeling theme coverage associated with

time stamps, we may assume that the actual theme coverage

in a document would be a mixture of the document-speciﬁc

theme coverage and another theme coverage associated with

the time context of the document.

We use c(κj) to denote the contexts where the coverage

κj is applicable, and we also deﬁne the support of a coverage

in the same way as we deﬁne that of a context.

Deﬁnition 5 (Coverage Support) The support of a

coverage κj, s(κj) is the set of documents in which the cov-

erage κj is taken, i.e., s(κj) = {Di|∃c ∈ c(κj) s.t. Ci ⊆ c}.

The latent structure of themes, views and coverages in a

contextualized document collection is illustrated in Figure 1.

 

View0:   θ01,     θ02,   ….   ,  θ0k  

View1:   θ11,     θ12,   ….   ,  θ1k  

Viewn:   θn1,     θn2,   ….   ,  θnk  

…. 

Theme:    1,       2,     ….   , k 

Coverage0: 

Coverage1: 

Coveragem: 

…. 

…. 

…. 

…. 

c0 

c1 









































































































































































































































































































































































































































































































































cn 

c2 

Collection 

Document 

Contexts 

Figure 1: The theme-view-coverage structure in a

text collection

With these deﬁnitions, the task of Contextual Text

Mining (CtxTM) can be deﬁned as to recover the n views,

vi = (θi1, ..., θik), i = 1, ..., n, and the m theme coverages

κ1, ..., κm from the collection D, and to analyze them in a

context-sensitive way. There are many diﬀerent ways to an-

alyze the views and theme coverages. Below we discuss a

few interesting cases.

1.

Theme extraction:

We may extract the global

salient themes. Although each theme θl varies in diﬀerent

contexts, it is also beneﬁcial to have an explicit model for

θl in a global view. Basically, this will give us the common

information that is shared by all the variations of θl in all dif-

ferent contexts. In practice, we always include a global view

v0, which corresponds to a global context c0 = F. Clearly,

all documents Di ∈ D belong to c0 since Ci ⊆ c0.

2.

View comparison: We may compare the n views.

The comparison of a theme θl from diﬀerent views usually

represents the content variation of θl corresponding to dif-

650

Research Track Poster


ferent contexts. By comparing θil for each view vi which

corresponds to context ci, we can analyze the inﬂuence of

the context ci on the contents of θl.

3. Coverage comparison: We may compare the m cov-

erages. The variations of p(l|κj) can tell us how likely θl is

covered by the documents in the coverage support s(κj). By

associating p(l|κj) within contexts c(κj) that κj is applica-

ble, we can analyze how closely a theme is associated to a

context, or how context-sensitive a theme is.

4.

Others: With contextual text mining, we can also

analyze other problems such as the inﬂuence of an individ-

ual context feature on the theme coverage, e.g., the theme-

location distribution in spatiotemporal theme analysis.

Among these cases, 2 and 3 are the most important, which

distinguish contextual text mining from the traditional theme

extraction work, and the application of them facilitate other

types of analysis.

With the deﬁnition of the general problem of contextual

text mining (CtxTM), we can show that some speciﬁc con-

textual text mining problems are special cases of CtxTM.

For example, in temporal text mining, each context feature

is a time stamp. Therefore, a context is either a time stamp

or a set of consecutive time stamps, or time period. A view

of themes is taken in all the documents in the corresponding

time. The goal of temporal text mining is mainly to compare

the coverage variation over diﬀerent contexts (e.g., theme life

cycles in [14]), and sometimes also the content variation of

themes over diﬀerent views, (e.g., evolutionary theme pat-

tern in [14]). In author-topic analysis, each context feature

is an author, and each context is either an author or a set

of authors. Each view is then taken in the document with

the same author or authors. We are interested in comparing

the content variations over diﬀerent views (authors) [17].

3.

A CONTEXTUAL MIXTURE MODEL

In this section, we propose an extension of the Probabilis-

tic Latent Semantic Analysis (PLSA) model [7, 8], called

Contextual Probabilistic Latent Semantic Analysis (CPLSA)

model, for contextual text mining. Our main idea is to al-

low a document to be generated using multiple views and

multiple coverages. The views and coverages actually used

in a document usually depend on its context, which could

be the time or location where the document is written, the

source from which the document comes, or any other meta-

data. We ﬁrst propose the general CPLSA model, and then

introduce two simpliﬁed versions of this model that are es-

pecially suitable for two representative tasks of contextual

text mining.

3.1

The CPLSA Model

In CPLSA, we assume that document D (with context

C) is generated by generating each word in it as follows:

(1) Choose a view vi according to the view distribution

p(vi|D, C). (2) Choose a coverage κj according to the cov-

erage distribution p(κj|D, C).

(3) Generate a word using

θil.

Formally, the log-likelihood of the whole collection is

log p(D)

=

�

(D,C)∈D

�

w∈V

c(w, D) log(

n

�

i=1

p(vi|D, C)

×

m

�

j=1

p(κj|D, C)

k

�

l=1

p(l|κj)p(w|θil))

The parameters are the view selection probability p(vi|D, C),

the coverage distribution selection probability p(κj|D, C),

the coverage distribution p(l|κj), and the theme distribu-

tion p(w|θil).

As a mixture model, we have a total of n × k multino-

mial distribution component models. Each set of k multino-

mial distributions, θi1, ..., θik, represents a potentially dis-

tinct view of the topics that we are interested in.

How-

ever, while we can potentially use all the views to generate

a document, often the generation of a particular document

D in a particular context C only involves a subset of these

views.

This is because in any interesting context mining

scenario, diﬀerent views generally have diﬀerent supporting

documents, though it is also common for the views to over-

lap in some supporting documents.

More speciﬁcally, the view selection distribution p(vi|D, C)

determines which views will actually be used when generat-

ing words in document D. This distribution would assign

zero probabilities to those views that are not selected. For

example, if the views that we are to model correspond to

the temporal context of a document and we have one global

view spanning in the entire time period, then a document at

time point ti would be generated using two diﬀerent views –

the view corresponding to time point ti and the global view,

which is applied to all the documents.

Orthogonal to the choice of views, we also assume that

we have choices of theme coverage distributions. The diﬀer-

ent coverage distributions are to reﬂect the uneven coverage

of topics in diﬀerent context and to capture the common

coverage patterns. For example, if we suspect that the cov-

erage may vary depending on the location of the authors,

we can associate a particular coverage distribution to each

location, which will be shared by all the documents in the

location. After we learn such coverage distributions, we can

then compare them across diﬀerent locations. Once again,

exactly which coverage distributions to use would depend

on the context of the document to be generated.

The mixture model can be ﬁt to a contextualized collec-

tion D using a maximum likelihood estimator. The EM al-

gorithm [5] can be used in a straightforward way to estimate

the parameters; the updating formulas are as follows:

p(zw,i,j,l = 1) =

p(t)(vi|D,C)p(t)(κj |D,C)p(t)(l|κj)p(t)(w|θil)



� n

i′=1 p(t)(vi′ |D,C) � m

j′=1 p(t)(κj′ |D,C) � k

l′=1 p(t)(l′|κj′ )p(t)(w|θi′l′ )

p(t+1)(vi|D, C) =

�

w∈V c(w,D) � m

j=1

� k

l=1 p(zw,i,j,l=1)



� n

i′=1

�

w∈V c(w,D) � m

j=1

� k

l=1 p(zw,i′,j,l=1)

p(t+1)(κj|D, C) =

�

w∈V c(w,D) � n

i=1

� k

l=1 p(zw,i,j,l=1)



� m

j′=1

�

w∈V c(w,D) � n

i=1

� k

l=1 p(zw,i,j′,l=1)

p(t+1)(l|κj) =

�

(D,C)∈D

�

w∈V c(w,D) � n

i=1 p(zw,i,j,l=1)



� k

l′=1

�

(D,C)∈D

�

w∈V c(w,D) � n

i=1 p(zw,i,j,l′ =1)

p(t+1)(w|θil) =

�

(D,C)∈D c(w,D) � m

j=1 p(zw,i,j,l=1)



�

w′∈V

�

(D,C)∈D c(w′,D) � m

j=1 p(zw′,i,j,l=1)

However, since the model has many parameters and has

a high-degree of freedom, ﬁtting it with a maximum like-

lihood estimator, in general, would face a serious problem

of multiple local maxima. Fortunately, in contextual text

mining, we almost always associate them with appropriate

partitions of context. As a result, the model is often highly

constrained. For example, if all we are interested in is to

compare non-overlapping views across diﬀerent time, then

p(κj|D, C) becomes a delta function, i.e., p(κj|D, C) = 1

if and only if κj is the coverage distribution for the time

context of D, and p(κj|D, C) = 0 for all other κj’s.

Unfortunately, even with such constraints, the model may

651

Research Track Poster


still have many free parameters to estimate. One possibil-

ity is to add some parametric constraint such as assuming

all coverage distributions are from the same Dirichlet distri-

bution as done in LDA [2], which would clearly reduce the

number of free parameters; indeed, we can easily generalize

our model in the same way as LDA generalizes PLSA [8].

However, one concern with such a strategy is that the para-

metric constraint is artiﬁcial and may restrict the capacity

of the model to extract discriminative themes, which is our

goal in contextual text mining. Another approach is to fur-

ther regularize the estimation of the model by heuristically

searching for a good initial point in EM; speciﬁc heuristics

would depend on the particular contextual text mining task.

This approach is adopted in our experiments and will be fur-

ther discussed in Section 3.2.

In order to model the noise (e.g., common English words)

in the text, we could designate the ﬁrst theme as modeling

such noise. That is, all θ1j’s will be set to model the noise.

We may further tie all of them so that we have just one

common background unigram language model θ1. This can

also be regarded as applying an inﬁnitely strong prior on the

ﬁrst theme in all views.

3.2

Special Versions of CPLSA

Considering that the most important tasks of contextual

text mining are view comparison and theme coverage com-

parison across contexts, as discussed in Section 2, we in-

troduce two special cases of CPLSA, which are particularly

useful to do these two tasks.

We ﬁrst introduce the special version of CPLSA to facili-

tate view comparison. In some cases, we are only interested

to model the content variation of themes across contexts,

e.g., when we are analyzing the theme evolutions over time

[14], or comparing the common themes and corresponding

speciﬁc themes across subcollections [18]. In these cases, we

can fairly assume that the theme coverage over contexts is

ﬁxed, thus does not depend on the contexts that a docu-

ment is in. Under this assumption, the �m

j=1 p(κj|D, C) in

the model will be simpliﬁed as �m

j=1 p(κj|D). If we further

assume that there is only one coverage κ applicable to each

document, the log-likelihood function can be written as

�

(D,C)∈D

�

w∈V

c(w, D) log(

n

�

i=1

p(vi|D, C)

k

�

l=1

p(l|κD)p(w|θil))

where κD is the coverage associated with the document D.

We call this simpliﬁed version of model as ﬁxed-coverage

contextual mixture model (FC-CPLSA). If we have

three views, where one is the global view and the other two

correspond to subcollections, it will allow us to compare the

common themes and speciﬁc themes in the two views, as

discussed in [18]. If each view corresponds to a time stamp,

this model will allow us to analyze the content evolutions of

themes over time, as discussed in [14].

In some other cases, we are only interested to model the

variation of theme coverage over contexts, e.g., when we

are analyzing the life cycles (i.e., strength variations over

time) of themes.

In these cases, we are not interested in

the content variation of local themes, and thus make the

assumption that diﬀerent views of themes are stable. With

this assumption, we can simplify the model likelihood as

�

(D,C)∈D

�

w∈V

c(w, D) log(

m

�

j=1

p(κj|D, C)

k

�

l=1

p(l|κj)p(w|θl))

where p(w|θl) is the global word distribution of theme l,

which does not vary across contexts. We call this simpliﬁed

model as ﬁxed-view contextual mixture model (FV-

CPLSA). If the only context feature is time, we have two

types of coverage distributions κD and κT , where κD is the

coverage distribution corresponding to each document and

κT is the theme coverage for each time period. This will

allow us to model the theme life cycles, as introduced in [14].

If we have two context features, time and location, and each

context is a combination of time stamp and location, we also

have two groups of theme coverage distributions, κD and

κT L. This will allow us to analyze the spatiotemporal theme

distributions in a spatiotemporal text mining framework.

With these two special simpliﬁed versions, the CPLSA

model can be applied to solve a broad family of text mining

problems with contextual analysis.

4.

EXPERIMENTS

We apply the general CPLSA model presented in Section 3

to three diﬀerent datasets and text mining tasks. Empirical

results show that this model can model the themes and their

variations across diﬀerent contexts eﬀectively.

4.1

Temporal-Author-Topic analysis

In this experiment, we evaluate the performance of the

CPLSA models on author-topic comparative analysis.

If

two authors have similar research interest, we assume that

there is a set of common themes which can be found in their

publications. Since diﬀerent author has diﬀerent preferences

and focuses, the content of these themes will also vary cor-

responding to each author. Previous work on author-topic

analysis only consider the authorship of documents as the

context [17].

Intuitively, however, the topics that an au-

thor favors also evolute over time. We add another type of

context information, i.e., publication time, to test the eﬀec-

tiveness of our model on handling multiple types of contexts.

We collect the abstracts of 282 papers published by two

famous Data Mining researchers from ACM Digital library.

We split the whole time line into three spans: before the

year 1993, from 1993 to 1999, and after the year 1999. This

will give us 12 possible views as in Table 1. Since we are

not interested in analyzing the coverage variations across

contexts (i.e. time and authors), we assume the coverage of

themes only depends on documents but not on the contexts.





#Context



Views





Features



(A and B are two authors)







0



Global View







1



A; B; &lt; 92; 93 ∼ 99; 00 ∼ 05









A, &lt; 92; A, 93 ∼ 99; A, 00 ∼ 05





2



B, &lt; 92; B, 93 ∼ 99; B, 00 ∼ 05





Table 1: Possible Views in Author-Topic Analysis

Therefore, we use the FC-CPLSA model presented in Sec-

tion 3.2 to model the themes and their views corresponding

to diﬀerent contexts. Our goal is thus to estimate all the

parameters in the regularized model, and compare p(w|θjl)

over diﬀerent view vj.

To avoid the EM algorithm being trapped in suboptimal

local maximums, we need to make associations between each

θjl to its corresponding global view θl. We achieve this by

selecting a good starting point for the EM algorithm. Specif-

ically, we begin with a prior of a large p(v0|D, C) to view 0,

which is the global view. This ensures us to get the strong

signal of global themes instead of local biased themes. In

the following iterations, we gradually decay this prior and

652

Research Track Poster






Vews:



Global



Author A



Author B



Author A: 2000∼



1993∼1999



2000∼











pattern 0.110689



project 0.0444375



research 0.0550772



close 0.0805878



rule 0.0616733



index 0.0430914









frequent 0.040613



itemset 0.0432976



next 0.0308254



pattern 0.072078



distribute 0.0567852



graph 0.0343051









frequent-pattern 0.0393



intertransaction 0.03072



transition 0.0308254



sequential 0.0462879



researcher 0.0324659



web 0.0306886







Author



sequential 0.0359059



support 0.0264818



panel 0.0275384



min



support 0.03526



algorithm 0.0217309



gspan 0.0273849







Topic



method 0.0214187



associate 0.0258175



technical 0.0275384



length 0.0315721



over 0.0162951



substructure 0.02005







Analysis



pattern-growth 0.02035



frequent 0.0181942



technology 0.0258949



threshold 0.0296533



fdm 0.0227141



gindex 0.016431









condense 0.0184008



closet 0.0176081



article 0.0154127



frequent 0.0196054



study 0.0116576



bide 0.016431









increment 0.0138457



apriori 0.0170468



revolution 0.0154127



top-k 0.0176324



scable 0011357.



magnitude 0.0151909









constraint 0.0130636



prefixspan 0.0130272



tremendous 0.0154127



without 0.0175662



pass 0.011357



size 0.0114699









push 0.0103159



pseudo 0.0109016



innovate 0.0154127



fp-tree 0.0102471



disclose 0.011357



xml 0.010954





Table 2: Comparison of the content of theme “Frequent Pattern Mining” over diﬀerent views

terminate the EM algorithm early when the average view

distribution for view 0 (i.e., �

D∈D p(v0|D, C)/|D|) drops

under a threshold, say 0.1. This gives us a good starting

point for the EM algorithm.

Then, we do this procedure

again for multiple trials and select the best start point (i.e.,

the one with the highest likelihood).

Finally, we run the

EM algorithm beginning with this selected start point until

it converges.

The results for this experiment are selectively

presented in the following table.

In Table 2, we see that the content of this selected theme

varies over diﬀerent views. From the global view, in which

all documents are included, we can tell that this theme is

talking about frequent pattern mining. From the view of Au-

thor A, we see speciﬁc frequent pattern mining techniques

such as database projection, apriori, preﬁxspan, and closet.

From the view of Author B, we see that he is not as deep into

techniques of mining frequent patterns, but rather more as-

sociated with introductional and innovated work of frequent

pattern mining.

From the view of the years before 1993,

the corresponding theme barely has any connection to fre-

quent pattern mining. This is reasonable however, since the

ﬁrst and most inﬂuential paper of frequent pattern mining

was published in 1993. From the view of year 1993 to 1999,

we see that this theme evolutes to talk about association

rules, which is perhaps the most important application of

frequent pattern mining at that time. Speciﬁc techniques,

such as fdm (Fast Distributed Mining of associate rules) ap-

pears high in the word distribution. From the view of the

years after 1999, it is interesting to see the appearance of

more new applications of frequent pattern mining, such as

graphs and web. The terms corresponding to speciﬁc tech-

niques of mining graph patterns and sequential patterns,

e.g., gspan and bide, are with high probabilities in the theme

word distribution. In the view corresponding to a combined

context (Author A and after 1999), the top terms include

“close”, “top-k”, and “fp-tree”, which well reveal the pref-

erences of author A in frequent pattern mining. The view

speciﬁc theme for the combined context “Author B after

1999” is not well associated with the global theme again,

which is consistent to the fact that Author B is not activate

in frequent pattern mining any more after 2000.

This experiment shows that the CPLSA model can ex-

tract and compare the theme variations over diﬀerent views

eﬀectively.

4.2

Spatiotemporal theme analysis

In this experiments, we show the eﬀectiveness of CPLSA

models on spatiotemporal analysis of themes. The context

features we consider in this experiment is time stamps and

location information of documents. The tasks of this spe-

ciﬁc contextual text mining problem are: (1) extract global

themes from the collection, which are shared by diﬀerent

time and locations;

(2) for each time stamp, compute the

distribution of theme and locations, from which we can draw

the theme distribution snapshots over locations; and (3)

compare the views of themes across contexts.

It is interesting to see that the second task is not a com-

mon task of CtxTM. Let a context C be denoted as (t, l)

where t and l refer to time and location, the task is to esti-

mate p(κ|D, (t, l)) for each κ, and P(θ, l|t) for any t:

p(θ, l|t) =

�

κ:(t,l)∈c(κ)

p(θ|κ)p(κ|t, l)p(l|t)

We collect 9377 MSN Space documents with a time-bounded

query submitted to Google blogsearch, with the keywords

“Hurricane Katrina”.

In this dataset, 7118 documents pro-

vide explicit location information, and the locations of oth-

ers are tagged as “unknown”. We segment the time stamps

into six weeks, extract and compare the common themes

over diﬀerent locations in United States.

Each combination of the 50 States and six week consists a

unique “context”, which gives us 50∗6 = 300 contexts. Since

there are many contexts, it is diﬃcult to estimate all the

views precisely. Since we are only interested in the strength

variations of global themes over all the contexts, it is reason-

able to simplify the model by assuming that the content of

the global themes does not vary over contexts. Therefore,

we use the FV-CPLSA model presented in Section 3.2 to

model the global themes and their coverage variations over

time and locations. We further assume that p(κC|D, C) is a

constant that controls the impact of the context on selecting

the coverage of themes. By estimating the free parameters,

our goal is to compute the theme-location coverage:

p(θ, l|t) =

p(θ, l, t)



�

θ′

�

l′ p(θ′, l′, t) =

p(θ|κt,l)p(t, l)



�

θ′

�

l′ P(θ′|κt,l′)P(t, l′)

where p(κt,l|t, l) = 1, p(t, l) can be computed from the word

count in time period t at location l divided by the total word

count in the collection.

With p(θ, l|t) computed, we can visualize the theme-location

coverage by fulﬁll p(θ, l|t) in a snapshot map. In Figure 2, we

show one of the 10 global themes we extracted from the blog

dataset and its theme-location coverage at diﬀerent time.

From the top terms in this theme, we can infer that this

theme is talking about aid and donations that were made

to the hurricane aﬀected areas. Figure 2 well demonstrates

the evolution of theme-location coverage over diﬀerent time

periods. A detailed description of theme variation over time

and location can be found in [13].

The next task is similar to the experiment in Section 4.1,

which is to compare the views of themes across contexts.

Speciﬁcally, we partition the states into four groups: Af-

fected States; Peripheral States; Coast States; and Inland

States. We partition the time line into spans with the length

of two weeks. Then we use the FC-CPLSA model to com-

pare the views of themes corresponding to diﬀerent contexts.

The results are selectively shown in Table 3.

It is easy to see that from the view of “Periphery States”,

the content of the theme “donation” is quite similar to the

common theme extracted in Figure 2. People tend to talk

about donations and supplies with food. However, from the

view of “Aﬀected Areas”, which corresponds to the hur-

653

Research Track Poster








(a) Week1: 08/23-08/29

(b) Week Three:09/06-09/12

(c) Week Five: 09/20-09/26

Figure 2: Selected snapshots for theme “Aid and Donation” of Hurricane Katrina.





Affected States



Peripheral States



Week1-2



Week5-6









medical 0.0192



donate 0.0238



donate 0.0351



their 0.0142







comfort 0.0141



relief 0.0204



help 0.0296



help 0.0120







health 0.0137



red 0.0132



relief 0.0181



family 0.0091







ship 0.0133



cross 0.0105



red 0.0151



rebuild 0.0088







volunteer 0.0129



link 0.0086



please 0.0143



school 0.0080







hospital 0.0090



food 0.0078



cross 0.0142



children 0.0068







team 0.0081



medical 0.0074



need 0.0134



need 0.0061







assist 0.0081



supply 0.0069



volunteer 0.0120



health 0.0059







care 0.0072



charity 0.0067



victim 0.0084



evacuee 0.0057







service 0.0053



volunteer 0.0060



blood 0.0057



parish 0.0051





Table 3: Comparison of the content of the theme

“Aid and Donation” over diﬀerent views

ricane aﬀected states such as Louisiana, people care more

about medical aid and hospital cares. In the ﬁrst two weeks,

the view of this theme is still quite similar to the common

theme. However in the last two weeks, we can notice that

the “helps” become more about rebuilding and helping the

returning evacuees.

This group of experiments show that our general model is

eﬀective to analyze spatiotemporal theme patterns.

4.3

Event Impact Analysis

In many scenarios, a collection of documents are usually

associated with a series of events.

For example, weblogs

usually reﬂect people’s opinions about the events happen-

ing. The research topics covered by scientiﬁc literatures are

also likely to be aﬀected by the inﬂuential related events,

such as the invention of WWW, and the proposing of a new

research direction. The impact of such event can usually be

analyzed by comparing the themes in the documents pub-

lished before versus after the event.

In this experiment,

we apply CPLSA on the problem of event impact analysis.

Since each event gives a possible segmentation of the time

line, this analysis also provides an evaluation of CPLSA on

modeling overlapping views that are not orthogonal to each

other. Although the experiments in previous sections also

covers some overlapping views (e.g., a view corresponding

to a location and a view corresponding to a time stamp),

these overlaps are caused by diﬀerent types of, or orthog-

onal context features (e.g., time and location).

In reality

however, the overlapping views with the same type of con-

text feature is desirable. For example, a business analyzer

may need to analyze and compare the customers’ opinions

in the ﬁrst week, in the ﬁrst month, in the ﬁrst season, or in

the ﬁrst year after a new product is released. One strength

of our model is that we allow the analysis views that overlap

with each other. In this experiment, we evaluate our model

on event impact analysis and overlapping view analysis.

We collect the abstracts of 1472 papers published in 28

years’ SIGIR conferences from ACM Digital Library.

We

select two inﬂuential events to the Information Retrieval

community in the 90s. One is the beginning of Text RE-

trieval Conferences (TREC) in 1992, which provide large-

scale standard text datasets and judgements for many re-

trieval problems. The other is the introduction of language

model into Information Retrieval in 1998, which began a

genre of research and led to a lot of publications. Our goal

is to use the CPLSA model to reveal the impact of these

two events in IR research, i.e., how the content of research

topics change after the two events.

To achieve this, we assign the abstracts in SIGIR proceed-

ings into four contexts, each corresponds to a time span. The

ﬁrst context includes all the documents were published be-

fore 1993, in which is the ﬁrst SIGIR conference after the

start of TREC. The second context contains documents pub-

lished on or after that. The third context includes abstracts

before the year 1998, in which the ﬁrst paper of language

model in information retrieval was published. The fourth

context contains all abstracts published on or after 1998.

It is clear that there are overlaps between these contexts.

We also include a global view, which corresponds to all the

abstracts in SIGIR proceedings.

We use the same strategy as presented in Section 4.1 to

avoid the EM algorithm to be trapped in unexpected local

maximums. We extract 10 salient global themes from this

collection and present the most interesting one.

From the global view in Table 4, we see that this theme

is talking about retrieval models, especially term weight-

ing and relevance feedback.

The content of this common

theme varies from diﬀerent views. From the Pre-Trec view,

which corresponds to the time before 1993, we see that vec-

tor space model dominates, and boolean queries are men-

tioned frequently. In the Post-Trec view, however, we no-

tice that XML retrieval model has been paid more attention

to.

Also, we see speciﬁc types of data (email) and other

terms related to the nature of TREC (e.g., collect, judge-

ment, rank).

It is more interesting when comparing the

view “Pre-Language Model” and “Post-Language Model”.

We see that before 1998, the retrieval models are dominated

by probabilistic models. After 1998, however, it is very clear

that language model dominates the theme. The top ranked

terms have changed to indicate language models, parame-

ter estimations, likelihood and probability distributions, and

language model smoothing. This is consistent with our prior

knowledge. The overlapping views, for example Pre-LM and

Pre-Trec, do share some content but clearly with diﬀerent

focuses. Pre-Trec, which is more faraway, emphasizes vector

space model while Pre-LM emphasizes probabilistic models.

This experiment shows that our method is eﬀective to ana-

lyze event impact and model the overlapping views.

5.

RELATED WORK

The most relevant work is the Probabilistic Latent Se-

mantic Analysis model (PLSA) proposed by Hofmann [7,

8], which models a document as a mixture of aspects, where

each aspect is represented by a multinomial distribution over

the whole vocabulary. Our CPLSA model is a natural ex-

tension of PLSA to incorporate context. To avoid overﬁtting

in PLSA, Blei and co-authors proposed a generative aspect

model called Latent Dirichlet Allocation (LDA), which could

654

Research Track Poster






Views:



Global



Pre-Trec



Post-Trec



Pre-Language Model



Post-Language Model











term 0.159983



vector 0.0514067



xml 0.0677684



probabilist 0.0777954



model 0.16867









relevance 0.0751814



concept 0.0297583



element 0.0212121



model 0.0431573



language 0.0752643









weight 0.0659849



extend 0.0297405



email 0.0197383



logic 0.0403557



estimate 0.0520434









feedback 0.0372254



model 0.0291697



collect 0.0191258



ir 0.0337741



parameter 0.0281169







SIGIR



independence 0.031063



space 0.0236088



locate 0.0187425



boolean 0.028073



distribution 0.0268227









model 0.0309212



boolean 0.0151455



judgment 0.0140086



fuzzy 0.0201544



probable 0.0205655









frequent 0.0233021



function 0.0123171



rank 0.010205



algebra 0.0199632



smooth 0.0197662









probabilist 0.018762



u 0.00898533



overlap 0.00975133



probable 0.0124902



score 0.0166799









document 0.0173198



feedback 0.00860945



contextual 0.00936265



estimate 0.0119202



retrieval 0.0137085









assume 0.0172082



specify 0.0083182



solution 0.00913



weight 0.0111257



markov 0.0118979









dependency 0.0157547



correlate 0.00779721



subtopic 0.00791172



rank 0.0107045



likelihood 0.00585364





Table 4: Comparison of theme content over diﬀerent views in SIGIR collection

also extract a set of themes from a document collection [2].

LDA, however, does not model context either. Although we

have not explored it, one can also make LDA contextualized

in the same way as we have done to PLSA in this paper. Re-

cently, some extensions of this work have considered some

speciﬁc types of context. For example, temporal context is

considered in [6, 16, 4, 14].

Multi-collection context is an-

alyzed in [18].

Author-topic analysis is proposed in [17].

Li et al. proposed a probabilistic model to detect retrospec-

tive news events by explaining the generation of “four Ws1”

from each news article [11]. Our work is a generalization of

these studies of speciﬁc context and provides a general prob-

abilistic model which can be applied to all kinds of context.

Temporal context is also addressed in Kleinberg’s work on

discovering bursty and hierarchical structures in streams [9]

and some work on topic/event/trend detection and tracking

(e.g., [1, 3, 12, 10, 15]). However, most of this work assumes

one document only belongs to one topic and cannot be easily

generalized to analyze other contexts.

6.

CONCLUSIONS

In this paper, we present a study of the general problem of

contextual text mining. We formally deﬁned the basic tasks

of contextual theme analysis, and proposed a novel prob-

abilistic mixture model to extract themes and model their

content and coverage variations over diﬀerent, possibly over-

lapping contexts. The problem deﬁnition and the proposed

model are quite general and cover a family of speciﬁc contex-

tual theme analysis problems and methods as special cases.

Empirical experiments on three diﬀerent datasets show that

the proposed model is eﬀective for extracting the themes and

comparing the views and coverages of themes across quite

diﬀerent contexts.

Our work is an initial step toward a general model for con-

textual text mining. An important future research direction

is to further study how to better estimate the proposed mix-

ture model as discussed in Section 3.1. Another important

future research direction is to create evaluation criteria and

judgements so that we can quantitatively evaluate diﬀerent

contextual text mining approaches.

7.

ACKNOWLEDGMENTS

This work was in part supported by the National Science

Foundation under award numbers 0425852, 0347933, and

0428472.

8.

REFERENCES

[1] J. Allan, J. Carbonell, G. Doddington, J. Yamron,

and Y. Yang. Topic detection and tracking pilot

study: Final report. In Proceedings of DARPA

Broadcast News Transcription and Understanding

Workshop, 1998.



1who, when, where and what (keywords)

[2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent

dirichlet allocation. J. Mach. Learn. Res., 3:993–1022,

2003.

[3] S. Boykin and A. Merlino. Machine learning of event

segmentation for news on demand. Commun. ACM,

43(2):35–41, 2000.

[4] C. C. Chen, M. C. Chen, and M.-S. Chen. Liped:

Hmm-based life proﬁles for adaptive event detection.

In Proceeding of KDD ’05, pages 556–561, 2005.

[5] A. P. Dempster, N. M. Laird, and D. B. Rubin.

Maximum likelihood from incomplete data via the EM

algorithm. Journal of Royal Statist. Soc. B, 39:1–38,

1977.

[6] T. L. Griﬃths and M. Steyvers. Fiding scientiﬁc

topics. Proceedings of the National Academy of

Sciences, 101(suppl.1):5228–5235, 2004.

[7] T. Hofmann. Probabilistic latent semantic analysis. In

Proceedings of UAI’99.

[8] T. Hofmann. Probabilistic latent semantic indexing.

In Proceedings of ACM SIGIR’99.

[9] J. Kleinberg. Bursty and hierarchical structure in

streams. In Proceedings of KDD ’02, pages 91–101.

[10] A. Kontostathis, L. Galitsky, W. M. Pottenger,

S. Roy, and D. J. Phelps. A survey of emerging trend

detection in textual data mining. Survey of Text

Mining, pages 185–224, 2003.

[11] Z. Li, B. Wang, M. Li, and W.-Y. Ma. A probabilistic

model for retrospective news event detection. In

Proceedings of SIGIR’05, pages 106–113, 2005.

[12] J. Ma and S. Perkins. Online novelty detection on

temporal sequences. In Proceedings of KDD’03, pages

613–618, 2003.

[13] Q. Mei, C. Liu, H. Su, and C. Zhai. A probabilistic

approach to spatiotemporal theme pattern mining on

weblogs. In Proceedings of WWW ’06, pages 533–542,

2006.

[14] Q. Mei and C. Zhai. Discovering evolutionary theme

patterns from text: an exploration of temporal text

mining. In Proceeding of KDD’05, pages 198–207,

2005.

[15] R. Nallapati, A. Feng, F. Peng, and J. Allan. Event

threading within news topics. In Proceedings of

CIKM’04, pages 446–453, 2004.

[16] J. Perkio, W. Buntine, and S. Perttu. Exploring

independent trends in a topic-based search engine. In

Proceedings of WI ’04, pages 664–668, 2004.

[17] M. Steyvers, P. Smyth, M. Rosen-Zvi, and

T. Griﬃths. Probabilistic author-topic models for

information discovery. In Proceedings of KDD’04,

pages 306–315, 2004.

[18] C. Zhai, A. Velivelli, and B. Yu. A cross-collection

mixture model for comparative text mining. In

Proceedings of KDD’04, pages 743–748, 2004.

655

Research Track Poster

