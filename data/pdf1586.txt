
Generalized Algorithms for Constructing Statistical Language Models

Cyril Allauzen, Mehryar Mohri, Brian Roark

AT&amp;T Labs â€“ Research

180 Park Avenue

Florham Park, NJ 07932, USA

ï¿½ allauzen,mohri,roark

ï¿½ @research.att.com

Abstract

Recent text and speech processing applications such as

speech mining raise new and more general problems re-

lated to the construction of language models. We present

and describe in detail several new and efï¬cient algorithms

to address these more general problems and report ex-

perimental results demonstrating their usefulness.

We

give an algorithm for computing efï¬ciently the expected

counts of any sequence in a word lattice output by a

speech recognizer or any arbitrary weighted automaton;

describe a new technique for creating exact representa-

tions of

ï¿½ -gram language models by weighted automata

whose size is practical for ofï¬‚ine use even for a vocab-

ulary size of about 500,000 words and an

ï¿½ -gram order

ï¿½ï¿½ï¿½ ; and present a simple and more general technique

for constructing class-based language models that allows

each class to represent an arbitrary weighted automaton.

An efï¬cient implementation of our algorithms and tech-

niques has been incorporated in a general software library

for language modeling, the GRM Library, that includes

many other text and grammar processing functionalities.

1

Motivation

Statistical language models are crucial components of

many modern natural language processing systems such

as speech recognition, information extraction, machine

translation, or document classiï¬cation.

In all cases, a

language model is used in combination with other in-

formation sources to rank alternative hypotheses by as-

signing them some probabilities.

There are classical

techniques for constructing language models such as

ï¿½ -

gram models with various smoothing techniques (see

Chen and Goodman (1998) and the references therein for

a survey and comparison of these techniques).

In some recent text and speech processing applications,

several new and more general problems arise that are re-

lated to the construction of language models. We present

new and efï¬cient algorithms to address these more gen-

eral problems.

Counting. Classical language models are constructed

by deriving statistics from large input texts. In speech

mining applications or for adaptation purposes, one often

needs to construct a language model based on the out-

put of a speech recognition system. But, the output of a

recognition system is not just text. Indeed, the word er-

ror rate of conversational speech recognition systems is

still too high in many tasks to rely only on the one-best

output of the recognizer. Thus, the word lattice output

by speech recognition systems is used instead because it

contains the correct transcription in most cases.

A word lattice is a weighted ï¬nite automaton (WFA)

output by the recognizer for a particular utterance.

It

contains typically a very large set of alternative transcrip-

tion sentences for that utterance with the corresponding

weights or probabilities. A necessary step for construct-

ing a language model based on a word lattice is to derive

the statistics for any given sequence from the lattices or

WFAs output by the recognizer. This cannot be done by

simply enumerating each path of the lattice and counting

the number of occurrences of the sequence considered in

each path since the number of paths of even a small au-

tomaton may be more than four billion. We present a

simple and efï¬cient algorithm for computing the expected

count of any given sequence in a WFA and report experi-

mental results demonstrating its efï¬ciency.

Representation of language models by WFAs. Clas-

sical

ï¿½ -gram language models admit a natural representa-

tion by WFAs in which each state encodes a left context

of width less than

ï¿½ . However, the size of that represen-

tation makes it impractical for ofï¬‚ine optimizations such

as those used in large-vocabulary speech recognition or

general information extraction systems. Most ofï¬‚ine rep-

resentations of these models are based instead on an ap-

proximation to limit their size. We describe a new tech-

nique for creating an exact representation of

ï¿½ -gram lan-

guage models by WFAs whose size is practical for ofï¬‚ine

use even in tasks with a vocabulary size of about 500,000

words and for

ï¿½ï¿½ï¿½ .

Class-based models. In many applications, it is nat-

ural and convenient to construct class-based language

models, that is models based on classes of words (Brown

et al., 1992). Such models are also often more robust

since they may include words that belong to a class but

that were not found in the corpus. Classical class-based

models are based on simple classes such as a list of

words. But new clustering algorithms allow one to create

more general and more complex classes that may be reg-

ular languages. Very large and complex classes can also

be deï¬ned using regular expressions. We present a simple

and more general approach to class-based language mod-

els based on general weighted context-dependent rules


(Kaplan and Kay, 1994; Mohri and Sproat, 1996). Our

approach allows us to deal efï¬ciently with more complex

classes such as weighted regular languages.

We have fully implemented the algorithms just men-

tioned and incorporated them in a general software li-

brary for language modeling, the GRM Library, that in-

cludes many other text and grammar processing function-

alities (Allauzen et al., 2003). In the following, we will

present in detail these algorithms and brieï¬‚y describe the

corresponding GRM utilities.

2

Preliminaries

Deï¬nition 1 A system











is a semiring

(Kuich and Salomaa, 1986) if:





 is a commuta-

tive monoid with identity element



 ;





 is a monoid

with identity element



 ;



distributes over

 ; and



 is an

annihilator for

 : for all





"ï¿½



#$ï¿½



 .

Thus, a semiring is a ring that may lack negation. Two

semirings often used in speech processing are: the log

semiring

%&amp;ï¿½'ï¿½(*),+.-0/12

3 456-0 1 (Mohri, 2002)

which is isomorphic to the familiar real or probability

semiring

ï¿½(87965:; &lt;= via a

&gt;@?1A

morphism with, for

all

B CD(E)F+.-0/ :

;

2

3G4

CHï¿½ï¿½ID&gt;@?1AJï¿½LKNMPOQï¿½RISPQ6,KTMPOQï¿½UICTU

and the convention that:

KTMPOQï¿½UI-#

ï¿½



and

ID&gt;@?1Aï¿½1Vï¿½W-

, and the tropical semiring

XYï¿½Yï¿½Z([7\)

+.-0/1G]!^@_`6-ï¿½G1 which can be derived from the log

semiring using the Viterbi approximation.

Deï¬nition 2 A weighted ï¬nite-state transducer

a

over a

semiring



is an 8-tuple

abï¿½cï¿½edfg!GhGiH jk lQUmP

where:

d

is the ï¬nite input alphabet of the transducer;

f

is the ï¬nite output alphabet;

g

is a ï¬nite set of states;

h0nog

the set of initial states;

ipnqg

the set of ï¬nal

states;



a ï¬nite

set of transitions;



the initial weight function;

and



the ï¬nal weight function mapping

i

to

 .

A Weighted automaton

Â'ï¿½â€šï¿½edg!GhGiH jk lQUmP is de-

ï¬ned in a similar way by simply omitting the output la-

bels. We denote by

Æ’ï¿½Ââ€nï¿½dâ€¦ the set of strings accepted

by an automaton

Â

and similarly by

Æ’ï¿½Zâ€ â€¡ the strings de-

scribed by a regular expression

â€ 

.

Given a transition

Ë†Fï¿½j , we denote by

â€°Å 

Ë†=â€¹ its input

label,

Å’xÅ 

Ë†=â€¹ its origin or previous state and

ï¿½ÂÅ 

Ë†=â€¹ its desti-

nation state or next state,

Å½"Å 

Ë†Nâ€¹ its weight,

ÂPÅ 

Ë†=â€¹ its output

label (transducer case). Given a state

ÂDâ€˜g , we denote

by

jVÅ 

Ââ€¹ the set of transitions leaving

Â .

A path

â€™â€œï¿½'Ë†1â€xâ€¢=â€¢Nâ€¢UË†.â€“

is an element of

jkâ€¦ with con-

secutive transitions:

ï¿½ÂÅ 

Ë†Ëœâ€”Lâ„¢â€ â€¹Sï¿½Å¡Å’xÅ 

Ë†wâ€”Zâ€¹ ,

â€°â€ºï¿½ÂÅ“P=Å¾NÅ¾NÅ¾= Å¸ . We

extend

ï¿½

and

Å’

to paths by setting:

ï¿½ÂÅ 

â€™Â â€¹Vï¿½â€šï¿½ÂÅ 

Ë†

â€“

â€¹ and

Å’Â¡Å 

â€™Â â€¹Fï¿½Â¢Å’Â¡Å 

Ë†.â€â€¹ .

A cycle

â€™

is a path whose origin and

destination states coincide:

ï¿½ÂÅ 

â€™Â â€¹;ï¿½ï¿½Å’Â¡Å 

â€™Â â€¹ . We denote by

Â£

ï¿½ZÂÂ¤ Â.Â¥Â¦ the set of paths from

Â to

ÂÂ¤Â¥ and by

Â£

ï¿½ZÂÂ¤UÂ§Â¡GÂ1Â¥Â¨

and

Â£

ï¿½ZÂÂ¤UÂ§Â¡UÂ©Â GÂ5Â¥Â¦ the set of paths from

Â

to

ÂÂ¥ with in-

put label

Â§&amp;&amp;d

â€¦ and output label

Â©

(transducer case).

These deï¬nitions can be extended to subsets

Âª$ ÂªkÂ¥Â¡nÂ«g ,

by:

Â£

ï¿½ÂªGÂ§Q ÂªÂ¬Â¥Â­â€¡ï¿½Â®)8Â¯Â°5Â±xÂ²tÂ¯UÂ³LÂ°5Â±`Â³

Â£

ï¿½ZÂÂ¤UÂ§Â¡GÂ.Â¥Â­ . The label-

ing functions

â€° (and similarly

Â ) and the weight func-

tion

Å½

can also be extended to paths by deï¬ning the la-

bel of a path as the concatenation of the labels of its

constituent transitions, and the weight of a path as the

 -product of the weights of its constituent transitions:

â€°Å 

â€™Â â€¹Sï¿½â€°Å 

Ë†

â€

â€¹â€¢Nâ€¢Nâ€¢Uâ€°Å 

Ë†

â€“

â€¹ ,

Å½"Å 

â€™Â â€¹ï¿½Â´Å½Å 

Ë†

â€

â€¹Â Âµâ€¢=â€¢Nâ€¢0Å½"Å 

Ë†

â€“

â€¹ . We

also extend

Å½

to any ï¬nite set of paths

Â¶

by setting:

Å½"Å 

Â¶;â€¹9ï¿½Â¸Â·Å¡Â¹

Â°ËœÂº

Å½"Å 

â€™Â â€¹ . The output weight associated by

Â

to each input string

Â§yï¿½dâ€¦ is:

Å 

Å 

ÂSâ€¹

â€¹tï¿½ZÂ§B[ï¿½

Â»

Â¹

Â°5Â¼{Â½@Â¾=Â²

Â¿5Â²

Ã€Ã

lQï¿½Ã‚Å’Â¡Å 

â€™Â â€¹Â¨xÃƒÅ½Å 

â€™Â â€¹&lt;ÃƒmÂ ï¿½Lï¿½ÂÅ 

â€™Â â€¹Â¨

Å 

Å 

ÂSâ€¹

â€¹tï¿½ZÂ§B is deï¬ned to be



 when

Â£

ï¿½ZhUÂ§Q i9kï¿½'Ã„ . Simi-

larly, the output weight associated by a transducer

a

to a

pair of input-output string

ï¿½LÂ§Â¡UÂ©&lt; is:

Å 

Å 

a;â€¹

â€¹Ã…ï¿½LÂ§QGÂ©&lt;Âï¿½

Â»

Â¹

Â°5Â¼8Â½@Â¾=Â²

Â¿ËœÂ²

Ã†wÂ²

Ã€QÃ

lÂ¡ï¿½Ã‡Å’xÅ 

â€™Â â€¹LÂ¡,Å½"Å 

â€™Â â€¹P,mBï¿½Zï¿½ÂÅ 

â€™Â â€¹L

Å 

Å 

aSâ€¹

â€¹Ã…ï¿½LÂ§Â¡UÂ©&lt;ï¿½





when

Â£

ï¿½ZhUÂ§Â¡UÂ©Â Gi9ï¿½Â¢Ã„ . A successful

path in a weighted automaton or transducer

Ãˆ

is a path

from an initial state to a ï¬nal state.

Ãˆ

is unambiguous if

for any string

Â§ï¿½ydâ€¦ there is at most one successful path

labeled with

Â§ . Thus, an unambiguous transducer deï¬nes

a function.

For any transducer

a , denote by

Â¶!Ã‰1ï¿½Za the automaton

obtained by projecting

a

on its output, that is by omitting

its input labels.

Note that the second operation of the tropical semiring

and the log semiring as well as their identity elements are

identical. Thus the weight of a path in an automaton

Â

over the tropical semiring does not change if

Â

is viewed

as a weighted automaton over the log semiring or vice-

versa.

3

Counting

This section describes a counting algorithm based on

general weighted automata algorithms.

Let

Â

ï¿½

ï¿½gkGhGiHd ÃŠ.GÃ‹Â lGmÂ¤ be an arbitrary weighted automa-

ton over the probability semiring and let

â€ 

be a regular

expression deï¬ned over the alphabet

d . We are interested

in counting the occurrences of the sequences

Â§##Æ’ï¿½Lâ€ â€¡

in

Â

while taking into account the weight of the paths

where they appear.

3.1

Deï¬nition

When

Â

is deterministic and pushed, or stochastic, it can

be viewed as a probability distribution

Â£

over all strings


0

a:Îµ/1

b:Îµ/1

1/1

X:X/1

a:Îµ/1

b:Îµ/1

Figure 1: Counting weighted transducer

a

with

dÃŒï¿½

+BC/ . The transition weights and the ï¬nal weight at state

 are all equal to

 .

d;â€¦ .1 The weight

Å 

Å 

ÂSâ€¹

â€¹tï¿½ZÂ§B associated by

Â

to each string

Â§

is then

Â£

ï¿½ZÂ§B . Thus, we deï¬ne the count of the sequence

Â§ in

Â ,

Ã

ï¿½LÂ§Â  , as:

Ã

ï¿½LÂ§B[ï¿½ÃŒÃ

Ã

Â°5ÃBÃ‘

Ã’

Ã“{Ã’

Â¿

Å 

Å 

ÂSâ€¹

â€¹Ã…ï¿½LÂ§Â 

where

Ã’

Ã“{Ã’

Â¿ denotes the number of occurrences of

Â§ in the

string

Ã“ , i.e., the expected number of occurrences of

Â§

given

Â . More generally, we will deï¬ne the count of

Â§ as

above regardless of whether

Â

is stochastic or not.

In most speech processing applications,

Â

may be an

acyclic automaton called a phone or a word lattice out-

put by a speech recognition system. But our algorithm is

general and does not assume

Â

to be acyclic.

3.2

Algorithm

We describe our algorithm for computing the expected

counts of the sequences

Â§EFÆ’ï¿½Lâ€ â€¡ and give the proof of

its correctness.

Let

Ã”

be the formal power series (Kuich and Salomaa,

1986)

Ã”

over the probability semiring deï¬ned by

Ã”

ï¿½

Ã•

â€¦Â¬:Â§y:

Ã•

â€¦ , where

Â§yDÆ’ï¿½Lâ€ â€¡ .

Lemma 1 For all

Ã–

ï¿½dSâ€¦ ,

ï¿½

Ã”

 

Ã–

Âï¿½

Ã’

Ã–

Ã’

Â¿ .

Proof.

By deï¬nition of the multiplication of power se-

ries in the probability semiring:

ï¿½

Ã”

 

Ã–

Ã—ï¿½

Ã

Ã

Â¿Â Ã˜Ã™`Ãš

ï¿½

Ã•

â€¦

 

Ã“

â€:Eï¿½LÂ§QGÂ§BS:ï¿½

Ã•

â€¦

GÃ›Â¤

ï¿½



Ã

Â¿Â Ã˜Ã™`ÃšDï¿½

Ã’

Ã–

Ã’

Â¿

This proves the lemma.









Ã”

is a rational power series as a product and closure of

the polynomial power series

Ã•

and

Â§

(Salomaa and Soit-

tola, 1978; Berstel and Reutenauer, 1988).

Similarly,

since

â€ 

is regular, the weighted transduction deï¬ned by

ï¿½ed\:â€¡+=v=/w â€¦Ëœï¿½Zâ€ W:â€ â€¡Nï¿½ed\:F+vN/.Gâ€¦ is rational. Thus, by the

theorem of SchÂ¨utzenberger (SchÂ¨utzenberger, 1961), there

exists a weighted transducer

a

deï¬ned over the alphabet

d

and the probability semiring realizing that transduc-

tion. Figure 1 shows the transducer

a

in the particular

case of

d*ï¿½Âµ+B C/ .



1There exist a general weighted determinization and weight

pushing algorithms that can be used to create a deterministic and

pushed automaton equivalent to an input word or phone lattice

(Mohri, 1997).

Proposition 1 Let

Â

be a weighted automaton over the

probability semiring, then:

Å 

Å 

Â¶

Ã‰

ï¿½Â*Ãœ[aÃ…â€¹

â€¹Ã…ï¿½LÂ§Â Âï¿½

Ã

ï¿½LÂ§B

Proof.

By deï¬nition of

a , for any

Ã–

ï¿½d9â€¦ ,

Å 

Å 

a;â€¹

â€¹Ã…ï¿½

Ã–

UÂ§Â Âï¿½

ï¿½

Ã”

UÂ§B , and by lemma 1,

Å 

Å 

a;â€¹

â€¹Ã…ï¿½

Ã–

GÂ§Bâ€¡ï¿½

Ã’

Ã–

Ã’

Â¿ . Thus, by

deï¬nition of composition:

Å 

Å 

Â¶

Ã‰

ï¿½ZÂâ€˜Ãœ[atâ€¹

â€¹Ã…ï¿½LÂ§BÃï¿½

Ã

Â¹

Â°5Â¼{Â½Â¦Â¾NÂ²

Ã€QÃtÂ²ZÃšPÃ™â€”ZÃ

Â¹.ÃŸ

Å 

Å 

Â;â€¹

â€¹Ã…ï¿½

Ã–

H:

Ã’

Ã–

Ã’

Â¿

ï¿½

Ã

ÃšBÂ°5ÃBÃ‘

Ã’

Ã–

Ã’

Â¿

Å 

Å 

Â;â€¹

â€¹tï¿½

Ã–

Ã ï¿½

Ã

ï¿½LÂ§Â 

This ends the proof of the proposition.









The proposition gives a simple algorithm for computing

the expected counts of

â€ 

in a weighted automaton

Â

based on two general algorithms: composition (Mohri et

al., 1996) and projection of weighted transducers. It is

also based on the transducer

a

which is easy to construct.

The size of

a

is in

Ã¡

ï¿½

Ã’

d

Ã’

6

Ã’

ÂÃ¢

Ã’

 , where

ÂÃ¢

is a ï¬nite

automaton accepting

â€ 

. With a lazy implementation of

a , only one transition can be used instead of

Ã’

d

Ã’ , thereby

reducing the size of the representation of

a

to

Ã¡

ï¿½

Ã’

Â

Ã¢

Ã’

 .

The weighted automaton

Ã£

ï¿½â€œÂ¶

Ã‰

ï¿½Âï¿½Ãœ;a contains

v -

transitions. A general

v -removal algorithm can be used

to compute an equivalent weighted automaton with no

v -

transition. The computation of

Å 

Å 

Ã£

â€¹

â€¹Ã…ï¿½LÂ§B for a given

Â§

is

done by composing

Ã£

with an automaton representing

Â§

and by using a simple shortest-distance algorithm (Mohri,

2002) to compute the sum of the weights of all the paths

of the result.

For numerical stability, implementations often replace

probabilities with

ID&gt;Â¦?5A probabilities. The algorithm just

described applies in a similar way by taking

ID&gt;@?1A of the

weights of

a

(thus all the weights of

a

will be zero in

that case) and by using the log semiring version of com-

position and

v -removal.

3.3

GRM Utility and Experimental Results

An efï¬cient implementation of the counting algorithm

was incorporated in the GRM library (Allauzen et al.,

2003). The GRM utility grmcount can be used in par-

ticular to generate a compact representation of the ex-

pected counts of the

ï¿½ -gram sequences appearing in a

word lattice (of which a string encoded as an automaton

is a special case), whose order is less or equal to a given

integer. As an example, the following command line:

grmcount -n3 foo.fsm &gt; count.fsm

creates an encoded representation count.fsm of the

ï¿½ -

gram sequences,

ï¿½ï¿½Ã¤â€˜Ã¥ , which can be used to construct a

trigram model. The encoded representation itself is also

given as an automaton that we do not describe here.

The counting utility of the GRM library is used in a va-

riety of language modeling and training adaptation tasks.


Our experiments show that grmcount is quite efï¬cient.

We tested this utility with 41,000 weighted automata out-

puts of our speech recognition system for the same num-

ber of speech utterances. The total number of transitions

of these automata was

=Ã¦JÅ¾

Ã¦ M. It took about 1h52m, in-

cluding I/O, to compute the accumulated expected counts

of all

ï¿½ -gram,

ï¿½Ã§Ã¤Ã¨Ã¥ , appearing in all these automata

on a single processor of a 1GHz Intel Pentium processor

Linux cluster with 2GB of memory and 256 KB cache.

The time to compute these counts represents just

â€



Ã©UÃª th of

the total duration of the 41,000 speech utterances used in

our experiment.

4

Representation of

Ã« -gram Language

Models with WFAs

Standard smoothed

ï¿½ -gram models, including backoff

(Katz, 1987) and interpolated (Jelinek and Mercer, 1980)

models, admit a natural representation by WFAs in which

each state encodes a conditioning history of length less

than

ï¿½ .

The size of that representation is often pro-

hibitive. Indeed, the corresponding automaton may have

Ã’

d

Ã’

Ã¬

â„¢â€ states and

Ã’

d

Ã’

Ã¬ transitions. Thus, even if the vo-

cabulary size is just 1,000, the representation of a classi-

cal trigram model may require in the worst case up to one

billion transitions. Clearly, this representation is even less

adequate for realistic natural language processing appli-

cations where the vocabulary size is in the order of several

hundred thousand words.

In the past, two methods have been used to deal with

this problem. One consists of expanding that WFA on-

demand. Thus, in some speech recognition systems, the

states and transitions of the language model automaton

are constructed as needed based on the particular input

speech utterances. The disadvantage of that method is

that it cannot beneï¬t from ofï¬‚ine optimization techniques

that can substantially improve the efï¬ciency of a rec-

ognizer (Mohri et al., 1998).

A similar drawback af-

fects other systems where several information sources are

combined such as a complex information extraction sys-

tem. An alternative method commonly used in many ap-

plications consists of constructing instead an approxima-

tion of that weighted automaton whose size is practical

for ofï¬‚ine optimizations. This method is used in many

large-vocabulary speech recognition systems.

In this section, we present a new method for creat-

ing an exact representation of

ï¿½ -gram language models

with WFAs whose size is practical even for very large-

vocabulary tasks and for relatively high

ï¿½ -gram orders.

Thus, our representation does not suffer from the disad-

vantages just pointed out for the two classical methods.

We ï¬rst brieï¬‚y present the classical deï¬nitions of

ï¿½ -

gram language models and several smoothing techniques

commonly used. We then describe a natural representa-

tion of

ï¿½ -gram language models using failure transitions.

This is equivalent to the on-demand construction referred

to above but it helps us introduce both the approximate

solution commonly used and our solution for an exact of-

ï¬‚ine representation.

4.1

Classical Deï¬nitions

In an

ï¿½ -gram model, the joint probability of a string

Å½

Ãª

Å¾=Å¾NÅ¾RÅ½Sâ€“

is given as the product of conditional proba-

bilities:

Ã­Ã Ã®

ï¿½ZÅ½

Ãª

Å¾NÅ¾=Å¾RÅ½

â€“

Ã—ï¿½

â€“

Ã¯

â€”@Ã™

Ãª

Ã­ÂÃ®

ï¿½LÅ½

â€”

Ã’

Ã°

â€”



(1)

where the conditioning history

Ã°

â€” consists of zero or more

words immediately preceding

Å½

â€” and is dictated by the

order of the

ï¿½ -gram model.

Let

Ã

ï¿½

Ã°

Å½

denote the count of

ï¿½ -gram

Ã°

Å½

and let

Ã±

Ã­ÂÃ®

ï¿½LÅ½

Ã’

Ã°

 be the maximum likelihood probability of

Å½

given

Ã° , estimated from counts.

Ã±

Ã­ÂÃ®

is often adjusted

to reserve some probability mass for unseen

ï¿½ -gram se-

quences.

Denote by

Ã²

Ã­ÂÃ®

ï¿½ZÅ½

Ã’

Ã°

 the adjusted conditional

probability. Katz or absolute discounting both lead to an

adjusted probability

Ã²

Ã­ÂÃ® .

For all

ï¿½ -grams

Ã°

ï¿½Â«Å½

Ã°

Â¥ where

Ã°

â€¡d

â€“ for some

Å¸Ã³

 , we refer to

Ã°

Â¥ as the backoff

ï¿½ -gram of

Ã° . Conditional

probabilities in a backoff model are of the form:

Ã´ÃµTÃ¶Â¨Ã·Ã¸

Ã¹PÃºcÃ»

Ã¼rÃ½

Ã´ÃµTÃ¶Â¨Ã·Ã¸

Ã¹PÃº

Ã¾

Ã¿

ï¿½

Ã¶LÃ¹Ã·â€Ãºï¿½ï¿½

ï¿½ï¿½

Ã´`ÃµTÃ¶Â¨Ã·Ã¸

Ã¹Â¨Ãº ÃµÂÃ¾

(2)

where

ï¿½

is a factor that ensures a normalized model.

Conditional probabilities in a deleted interpolation model

are of the form:

Ã´`ÃµTÃ¶Â¨Ã·Ã¸

Ã¹PÃºQÃ»ï¿½Ã¼

Ã¶

ï¿½ï¿½

ÃºÃ´`ÃµTÃ¶Â¨Ã·Ã¸

Ã¹Â¤Ãº 

ï¿½ï¿½

Ã´`ÃµNÃ¶Â¨Ã·Ã¸

Ã¹ï¿½Â­Ãº#Ã¾

Ã¿

ï¿½

Ã¶LÃ¹Ã·â€Ãº!ï¿½"ï¿½

ï¿½ï¿½

Ã´`ÃµNÃ¶Â¨Ã·Ã¸

Ã¹ï¿½Â­Ãº

 ÃµÂÃ¾

(3)

where

ï¿½ is the mixing parameter between zero and one.

In practice, as mentioned before, for numerical sta-

bility,

ID&gt;Â¦?5A

probabilities are used.

Furthermore, due

the Viterbi approximation used in most speech process-

ing applications, the weight associated to a string

Â§

by a

weighted automaton representing the model is the mini-

mum weight of a path labeled with

Â§ . Thus, an

ï¿½ -gram

language model is represented by a WFA over the tropical

semiring.

4.2

Representation with Failure Transitions

Both backoff and interpolated models can be naturally

represented using default or failure transitions. A fail-

ure transition is labeled with a distinct symbol

# . It is the

default transition taken at state

Â when

Â does not admit

an outgoing transition labeled with the word considered.

Thus, failure transitions have the semantics of otherwise.


w  w

i-2     i-1

w   w

i-1     i

wi

wi-1

Ï†

wi

Ï†

wi

Îµ

Ï†

wi

Figure 2: Representation of a trigram model with failure

transitions.

The set of states of the WFA representing a backoff or

interpolated model is deï¬ned by associating a state

Â$ to

each sequence of length less than

ï¿½

found in the corpus:

gÂµï¿½Âµ+Â



|

Ã’

Ã°xÃ’&amp;%

ï¿½('Ëœ_*)

Ã

ï¿½

Ã°

,+*&lt;/

Its transition set

j

is deï¬ned as the union of the following

set of failure transitions:

+Â¤ï¿½ZÂ-ï¿½Â³Ã….#Â¡NID&gt;@?1Aï¿½/0Â¤ Â1Â³ZH|1Â-ï¿½Â³xDg$/

and the following set of regular transitions:

+1ï¿½ZÂ2JGÅ½9NID&gt;Â¦?5ABï¿½

Ã­ÂÃ®

ï¿½LÅ½

Ã’

Ã°

UGï¿½32-Ââ€|5Â1VDg!

Ã

ï¿½

Ã°

Å½,+â€˜P/

where

ï¿½

2-

is deï¬ned by:

4 ï¿½65

Ã»

Ã¼87

ï¿½65

Ã¾Ã‡Ã¿9ï¿½;:*Ã¸

Ã¹1Ã·Ã¸&lt;:

4

7

ï¿½

Â³

5

Ã¾Ã‡Ã¿{Ã¸

Ã¹Ã·Ã¸Ã»

4

= ÃµxÃ¹Ã»EÃ·&gt;ï¿½Â­Ã¹ï¿½

(4)

Figure 2 illustrates this construction for a trigram model.

Treating

v -transitions as regular symbols, this is a

deterministic automaton.

Figure 3 shows a complete

Katz backoff bigram model built from counts taken from

the following toy corpus and using failure transitions:

? s

@

b a a a a

? /s

@

? s

@

b a a a a

? /s

@

? s

@

a

? /s

@

where

? s

@ denotes the start symbol and

? /s

@ the end sym-

bol for each sentence. Note that the start symbol

? s

@ does

not label any transition, it encodes the history

? s

@ . All

transitions labeled with the end symbol

? /s

@ lead to the

single ï¬nal state of the automaton.

4.3

Approximate Ofï¬‚ine Representation

The common method used for an ofï¬‚ine representation of

an

ï¿½ -gram language model can be easily derived from the

representation using failure transitions by simply replac-

ing each

# -transition by an

v -transition. Thus, a transition

that could only be taken in the absence of any other alter-

native in the exact representation can now be taken re-

gardless of whether there exists an alternative transition.

Thus the approximate representation may contain paths

whose weight does not correspond to the exact probabil-

ity of the string labeling that path according to the model.

&lt;/s&gt;

a

&lt;/s&gt;/1.101

a/0.405

Ï†/4.856

&lt;/s&gt;/1.540

a/0.441

b

b/1.945

a/0.287

Ï†/0.356

&lt;s&gt;

a/1.108

Ï†/0.231

b/0.693

Figure 3: Example of representation of a bigram model

with failure transitions.

Consider for example the start state in ï¬gure 3, labeled

with

? s

@ . In a failure transition model, there exists only

one path from the start state to the state labeled

 , with a

cost of 1.108, since the

#

transition cannot be traversed

with an input of

 . If the

#

transition is replaced by an

v -transition, there is a second path to the state labeled



â€“ taking the

v -transition to the history-less state, then the

 transition out of the history-less state. This path is not

part of the probabilistic model â€“ we shall refer to it as an

invalid path. In this case, there is a problem, because the

cost of the invalid path to the state â€“ the sum of the two

transition costs (0.672) â€“ is lower than the cost of the true

path. Hence the WFA with

v -transitions gives a lower

cost (higher probability) to all strings beginning with the

symbol

 . Note that the invalid path from the state labeled

? s

@ to the state labeled

C has a higher cost than the correct

path, which is not a problem in the tropical semiring.

4.4

Exact Ofï¬‚ine Representation

This section presents a method for constructing an ex-

act ofï¬‚ine representation of an

ï¿½ -gram language model

whose size remains practical for large-vocabulary tasks.

The main idea behind our new construction is to mod-

ify the topology of the WFA to remove any path contain-

ing

v -transitions whose cost is lower than the correct cost

associated by the model to the string labeling that path.

Since, as a result, the low cost path for each string will

have the correct cost, this will guarantee the correctness

of the representation in the tropical semiring.

Our construction admits two parts: the detection of the

invalid paths of the WFA, and the modiï¬cation of the

topology by splitting states to remove the invalid paths.

To detect invalid paths, we determine ï¬rst their initial

non- v transitions. Let

jBA denote the set of

v -transitions

of the original automaton. Let

Â£

Â¯ be the set of all paths

â€™ï¿½ï¿½Ë†Ëœâ€xÅ¾=Å¾NÅ¾UË†.â€“ï¿½jï¿½I$j

A



â€“ ,

Å¸(+z , leading to state

Â such

that for all

â€° ,

â€°ï¿½Ã§ÂÅ¾NÅ¾=Å¾GÅ¸ ,

Å’Â¡Å 

Ë†

â€”

â€¹ is the destination state of

some

v -transition.

Lemma 2 For an

ï¿½ -gram language model, the number

of paths in

Â£

Â¯ is less than the

ï¿½ -gram order:

Ã’

Â£

Â¯

Ã’&amp;%

ï¿½ .

Proof.

For all

â€™`â€”!

Â£

Â¯ , let

â€™â€”"ï¿½qâ€™Â¥

â€”

Ë†wâ€” . By deï¬nition,

there is some

Ë†ËœÂ¥

â€”

â€¡j

A such that

ï¿½ÂÅ 

Ë†.Â¥

â€”

â€¹xï¿½zÅ’xÅ 

Ë†wâ€”Â¨â€¹xï¿½ï¿½Â

2C . By

deï¬nition of

v -transitions in the model,

Ã’

Ã°

â€”

Ã’9%

ï¿½yI0 for

all

â€° . It follows from the deï¬nition of regular transitions

that

ï¿½ÂÅ 

Ë†wâ€”Lâ€¹[ï¿½

Â

2CD-

ï¿½

Â . Hence,

Ã°

â€”Hï¿½

Ã°&amp;E

ï¿½

Ã° , i.e.

Ë†wâ€”;ï¿½


qâ€™

râ€™

Ï€â€™

q

e

r

eâ€™

Ï€

Figure 4: The path

Ë†â€™

is invalid if

â€°Å 

Ë†=â€¹Qï¿½ï¿½v ,

â€°Å 

â€™Â â€¹xï¿½Âµâ€°Å 

â€™

Â¥

â€¹ ,

â€™ï¿½

Â£0F , and either (i)

G1Â¥Ã ï¿½HG and

Å½"Å 

Ë†â€™Â â€¹

%

Å½"Å 

â€™QÂ¥@â€¹ or (ii)

â€°Å 

Ë†wÂ¥Ã‚â€¹Â ï¿½0v and

Å½"Å 

Ë†â€™Â â€¹

%

Å½"Å 

â€™Â¡Â¥Â­Ë†Â¥Ã‚â€¹ .

Ë†

E

ï¿½ï¿½Ë† , for all

â€™Â â€”GUâ€™

E



Â£

Â¯ . Then,

Â£

Â¯Sï¿½Âµ+=â€™Ë†9|5â€™â€¡

Â£

Â¯IP/5)

+Ë†1/ . The history-less state has no incoming non- v paths,

therefore, by recursion,

Ã’

Â£

Â¯

Ã’

ï¿½

Ã’

Â£

Â¯I

Ã’

60ï¿½

Ã’

Ã°

Å½

Ã’J%

ï¿½ .









We now deï¬ne transition sets

K

Â¯UÂ¯

Â³ (originally empty)

following this procedure: for all states

GÂµg

and all

â€™sï¿½oË†Ëœâ€Â¡Å¾=Å¾NÅ¾GË†.â€“E

Â£LF , if there exists another path

â€™ÂÂ¥ and

transition

Ë†,Å¡j;A such that

ï¿½ÂÅ 

Ë†=â€¹Â¬ï¿½Å’xÅ 

â€™Â â€¹ ,

Å’Â¡Å 

â€™Â¡Â¥@â€¹Â¬ï¿½Å’Â¡Å 

Ë†Nâ€¹ ,

and

â€°Å 

â€™Â¡Â¥Ã‡â€¹Â ï¿½ï¿½â€°Å 

â€™Â â€¹ , and either (i)

ï¿½ÂÅ 

â€™{Â¥Ã‚â€¹`ï¿½ï¿½ÂÅ 

â€™Â â€¹ and

Å½"Å 

Ë†â€™Â â€¹

%

Å½"Å 

â€™Â¥@â€¹ or (ii) there exists

Ë†ËœÂ¥[,j

A such that

Å’Â¡Å 

Ë†.Â¥Ã‚â€¹8ï¿½â€œï¿½ÂÅ 

â€™Â¥Ã‚â€¹

and

ï¿½ÂÅ 

Ë†.Â¥Ã‚â€¹Â ï¿½ï¿½ÂÅ 

â€™Â â€¹ and

Å½"Å 

Ë†=â€™Â â€¹

%

Å½"Å 

â€™QÂ¥Â¨Ë†wÂ¥Ã‚â€¹ , then we add

Ë†

â€ to

the set:

KNM

Ã

Â¹wÃŸ

M

Ã

Â¹

Â³

ÃŸï¿½O

KNM

Ã

Â¹wÃŸ

M

Ã

Â¹

Â³

ÃŸ

)ï¿½+Ë†.â€w/ . See ï¬gure 4 for

an illustration of this condition. Using this procedure, we

can determine the set:

P

juÅ 

Ââ€¹`ï¿½s+Ë†"jÅ 

Ââ€¹Q|QPÂ.Â¥eGË†9RKkÂ¯UÂ¯UÂ³Ã…/ .

This set provides the ï¬rst non- v transition of each invalid

path. Thus, we can use these transitions to eliminate in-

valid paths.

Proposition 2 The cost of the construction of

P

jVÅ 

Ââ€¹ for all

Âyg

is

ï¿½

Ã‰

Ã’

d

Ã’@Ã’

g

Ã’ , where

ï¿½

is the n-gram order.

Proof.

For each

Â,\g

and each

â€™Å¡

Â£

Â¯ , there are at

most

Ã’

d

Ã’ possible states

ÂËœÂ¥ such that for some

Ë†#ï¿½j

A ,

Å’Â¡Å 

Ë†=â€¹Â¡ï¿½ÂµÂ.Â¥ and

ï¿½ÂÅ 

Ë†Nâ€¹Â¡ï¿½ÂµÂ . It is trivial to see from the proof

of lemma 2 that the maximum length of

â€™

is

ï¿½ . Hence,

the cost of ï¬nding all

â€™{Â¥ for a given

â€™

is

ï¿½

Ã’

d

Ã’ . Therefore,

the total cost is

ï¿½

Ã‰

Ã’

d

Ã’Â¦Ã’

g

Ã’ .









For all non-empty

P

jVÅ 

Ââ€¹ , we create a new state

P

Â and

for all

Ë†D

P

jÅ 

Ââ€¹ we set

Å’Â¡Å 

Ë†=â€¹Âï¿½

P

Â . We create a transition

ï¿½

P

ÂÂ¤ vG&lt; ÂËœ , and for all

Ë†â€¡Â«j

Iâ€˜jSA such that

ï¿½ÂÅ 

Ë†=â€¹Sï¿½Ã§Â ,

we set

ï¿½ÂÅ 

Ë†=â€¹Ã ï¿½

P

Â . For all

Ë†#j

A such that

ï¿½ÂÅ 

Ë†=â€¹Âï¿½rÂ and

Ã’

K

Â¯

M

Ã

T

ÃŸ

Ã’

ï¿½r , we set

ï¿½ÂÅ 

Ë†=â€¹Âï¿½

P

Â . For all

Ë†u*j

A such that

ï¿½ÂÅ 

Ë†Nâ€¹Â¡ï¿½ï¿½Â and

Ã’

K

Â¯

M

Ã

T

ÃŸ

Ã’

+Â« , we create a new intermediate

backoff state

U

Â and set

ï¿½ÂÅ 

Ë†=â€¹`ï¿½VU

Â ; then for all

Ë†Â¤Â¥yjVÅ 

P

Ââ€¹ , if

Ë†wÂ¥!W

RK

Â¯

M

Ã

T

ÃŸ , we add a transition

X

Ë†ï¿½Å¡ï¿½U

ÂPUâ€°Å 

Ë†5Â¥Ã‡â€¹eGÅ½"Å 

Ë†wÂ¥@â€¹tUï¿½ÂÅ 

Ë†Â¥Ã‚â€¹L

to

j .

Proposition 3 The WFA over the tropical semiring mod-

iï¬ed following the procedure just outlined is equivalent to

the exact online representation with failure transitions.

Proof.

Assume that there exists a string

Y for which the

WFA returns a weight

P

Å½ï¿½ZYw less than the correct weight

Å½"ï¿½ZY that would have been assigned to

Y

by the exact

online representation with failure transitions.

We will

call an

v -transition

Ë†

â€” within a path

â€™Â¸ï¿½ÃŒË†

â€

Å¾NÅ¾NÅ¾ Ë†

â€“

in-

valid if the next non- v transition

Ë†

E ,

[\+oâ€° , has the la-

bel

Å½ , and there is a transition

Ë† with

Å’Â¡Å 

Ë†=â€¹ï¿½

Å’Â¡Å 

Ë†â€”Lâ€¹ and

b

Îµ/0.356

a

a/0.287a/0.441

Îµ/0

Îµ/4.856

a/0.405

&lt;/s&gt;

&lt;/s&gt;/1.101

&lt;s&gt; b/0.693

a/1.108

Îµ/0.231b/1.945

&lt;/s&gt;/1.540

Figure 5:

Bigram model encoded exactly with

v -

transitions.

â€°Å 

Ë†=â€¹9ï¿½qÅ½ . Let

â€™

be a path through the WFA such that

â€°Å 

â€™Â â€¹;ï¿½VY and

Å½"Å 

â€™Â â€¹;ï¿½

P

Å½ï¿½ZYw , and

â€™

has the least number

of invalid

v -transitions of all paths labeled with

Y

with

weight

P

Å½ï¿½ZY . Let

Ë†Ëœâ€” be the last invalid

v -transition taken

in path

â€™ . Let

â€™xÂ¥ be the valid path leaving

Å’xÅ 

Ë†Â¤â€”Â¨â€¹ such that

â€°Å 

â€™Â¥Ã‚â€¹!ï¿½Wâ€°Å 

Ë†

â€”@7Â¡â€

Å¾NÅ¾NÅ¾ Ë†

â€“

â€¹ .

Å½"Å 

â€™QÂ¥Ã‚â€¹(+Â¸Å½"Å 

Ë†

â€”

Å¾NÅ¾=Å¾GË†

â€“

â€¹ , otherwise

there would be a path with fewer invalid

v -transitions with

weight

P

Å½ï¿½ZYw . Let

G be the ï¬rst state where paths

â€™

Â¥ and

Ë†

â€”@7xâ€

Å¾NÅ¾=Å¾GË†

â€“ intersect. Then

G"ï¿½Â«ï¿½ÂÅ 

Ë†

E

â€¹ for some

[(+0â€° . By

deï¬nition,

Ë†Ëœâ€”Â¦7Â¡â€xÅ¾=Å¾NÅ¾GË†

E



Â£LF , since intersection will occur

before any

v -transitions are traversed in

â€™ . Then it must

be the case that

Ë†Ëœâ€”Â¦7Â¡â€V]K

Ã¬

Ã

T

C

ÃŸ

M

Ã

T

C

ÃŸ , requiring the path to

be removed from the WFA. This is a contradiction.









4.5

GRM Utility and Experimental Results

Note that some of the new intermediate backoff states (

U

Â )

can be fully or partially merged, to reduce the space re-

quirements of the model. Finding the optimal conï¬gu-

ration of these states, however, is an NP-hard problem.

For our experiments, we used a simple greedy approach

to sharing structure, which helped reduce space dramati-

cally.

Figure 5 shows our example bigram model, after ap-

plication of the algorithm. Notice that there are now two

history-less states, which correspond to

Â and

P

Â in the al-

gorithm (no

U

Â was required). The start state backs off to

Â , which does not include a transition to the state labeled

 , thus eliminating the invalid path.

Table 1 gives the sizes of three models in terms of

transitions and states, for both the failure transition and

v -transition encoding of the model. The DARPA North

American Business News (NAB) corpus contains 250

million words, with a vocabulary of 463,331 words. The

Switchboard training corpus has 3.1 million words, and a

vocabulary of 45,643. The number of transitions needed

for the exact ofï¬‚ine representation in each case was be-

tween 2 and 3 times the number of transitions used in the

representation with failure transitions, and the number of

states was less than twice the original number of states.

This shows that our technique is practical even for very

large tasks.

Efï¬cient implementations of model building algo-

rithms have been incorporated into the GRM library.

The GRM utility grmmake produces basic backoff

models, using Katz or Absolute discounting (Ney et

al., 1994) methods, in the topology shown in ï¬g-




Model





^ -representation







exact ofï¬‚ine

Corpus



order





arcs



states





arcs



states















NAB



3-gram



102752



16838





303686



19033















SWBD



3-gram



2416



475





5499



573















SWBD



6-gram



15430



6295





54002



12374



Table 1: Size of models (in thousands) built from the

NAB and Switchboard corpora, with failure transitions

#

versus the exact ofï¬‚ine representation.

ure 3, with

v -transitions in the place of failure tran-

sitions.

The utility grmshrink removes transitions

from the model according to the shrinking methods of

Seymore and Rosenfeld (1996) or Stolcke (1998).

The

utility grmconvert takes a backoff model produced by

grmmake or grmshrink and converts it into an exact

model using either failure transitions or the algorithm just

described. It also converts the model to an interpolated

model for use in the tropical semiring. As an example,

the following command line:

grmmake -n3 counts.fsm &gt; model.fsm

creates a basic Katz backoff trigram model from the

counts produced by the command line example in the ear-

lier section. The command:

grmshrink -c1 model.fsm &gt; m.s1.fsm

shrinks the trigram model using the weighted difference

method (Seymore and Rosenfeld, 1996) with a threshold

of 1. Finally, the command:

grmconvert -tfail m.s1.fsm &gt; f.s1.fsm

outputs the model represented with failure transitions.

5

General class-based language modeling

Standard class-based or phrase-based language models

are based on simple classes often reduced to a short list

of words or expressions. New spoken-dialog applications

require the use of more sophisticated classes either de-

rived from a series of regular expressions or using general

clustering algorithms. Regular expressions can be used to

deï¬ne classes with an inï¬nite number of elements. Such

classes can naturally arise, e.g., dates form an inï¬nite set

since the year ï¬eld is unbounded, but they can be eas-

ily represented or approximated by a regular expression.

Also, representing a class by an automaton can be much

more compact than specifying them as a list, especially

when dealing with classes representing phone numbers

or a list of names or addresses.

This section describes a simple and efï¬cient method

for constructing class-based language models where each

class may represent an arbitrary (weighted) regular lan-

guage.

Let

Ã

â€.

Ã

Ã‰5NÅ¾NÅ¾=Å¾N

Ã

Ã¬

be a set of

ï¿½

classes and assume

that each class

Ã

â€” corresponds to a stochastic weighted

automaton

Â

â€” deï¬ned over the log semiring. Thus, the

weight

Å 

Å 

Ââ€”Zâ€¹

â€¹tï¿½ZÅ½ associated by

Ââ€” to a string

Å½

can be in-

terpreted as

ID&gt;Â¦?5A of the conditional probability

Â£

ï¿½LÅ½

Ã’

Ã

â€”e .

Each class

Ã

â€” deï¬nes a weighted transduction:

Â

â€”

IB}

Ã

â€”

This can be viewed as a speciï¬c obligatory weighted

context-dependent rewrite rule where the left and right

contexts are not restricted (Kaplan and Kay, 1994; Mohri

and Sproat, 1996). Thus, the transduction corresponding

to the class

Ã

â€” can be viewed as the application of the fol-

lowing obligatory weighted rewrite rule:

Â

â€”

}

Ã

â€”_

v



v

The direction of application of the rule, left-to-right or

right-to-left, can be chosen depending on the task 2. Thus,

these

ï¿½

classes can be viewed as a set of batch rewrite

rules (Kaplan and Kay, 1994) which can be compiled into

weighted transducers. The utilities of the GRM Library

can be used to compile such a batch set of rewrite rules

efï¬ciently (Mohri and Sproat, 1996).

Let

a

be the weighted transducer obtained by compil-

ing the rules corresponding to the classes. The corpus can

be represented as a ï¬nite automaton

â€ 

. To apply the rules

deï¬ning the classes to the input corpus, we just need to

compose the automaton

â€ 

with

a

and project the result

on the output:

X

â€ qï¿½\Â¶Ã‰ï¿½Lâ€ Ãœ[a

X

â€ 

can be made stochastic using a pushing algorithm

(Mohri, 1997).

In general, the transducer

a

may not

be unambiguous. Thus, the result of the application of

the class rules to the corpus may not be a single text but

an automaton representing a set of alternative sequences.

However, this is not an issue since we can use the gen-

eral counting algorithm previously described to construct

a language model based on a weighted automaton. When

Æ’sï¿½r)

Ã¬

â€”Â¦Ã™Â¡â€

Æ’ï¿½ZÂâ€”e , the language deï¬ned by the classes, is

a code, the transducer

a

is unambiguous.

Denote now by

X

`

the language model constructed

from the new corpus

X

â€ 

. To construct our ï¬nal class-

based language model

` , we simply have to compose

X

`

with

a

â„¢`â€ and project the result on the output side:

`

ï¿½\Â¶Ã‰1ï¿½

X

`

Ãœ[a

â„¢â€



A more general approach would be to have two trans-

ducers

aÂ¡â€ and

aÃ‰ , the ï¬rst one to be applied to the corpus

and the second one to the language model. In a proba-

bilistic interpretation,

a8â€ should represent the probability

distribution

Â£

ï¿½

Ã

â€”

Ã’

Å½ and

a

Ã‰ the probability distribution

Â£

ï¿½LÅ½

Ã’

Ã

â€”

 . By using

a

â€

ï¿½za

and

a

Ã‰

ï¿½ï¿½a

â„¢`â€ , we are in fact

making the assumptions that the classes are equally prob-

able and thus that

Â£

ï¿½

Ã

â€”

Ã’

Å½Ãƒï¿½

Â£

ï¿½ZÅ½

Ã’

Ã

â€”



_

d

Ã¬

E

Ã™Â¡â€

Â£

ï¿½LÅ½

Ã’

Ã

E

 .

More generally, the weights of

aÃ â€ and

aÃ‰ could be the re-

sults of an iterative learning process. Note however that



2The simultaneous case is equivalent to the left-to-right one

here.


0/0

returns:returns/0

batman:&lt;movie&gt;/0.510

1

         batman:&lt;movie&gt;/0.916

returns:Îµ/0

Figure 6: Weighted transducer

a

obtained from the com-

pilation of context-dependent rewrite rules.

0

1

batman

2

returns

0

1

&lt;movie&gt;/0.510

3

&lt;movie&gt;/0.916

2/0

returns/0

Îµ/0

Figure 7: Corpora

â€ 

and

X

â€ 

.

we are not limited to this probabilistic interpretation and

that our approach can still be used if

a[â€ and

aÃ‰ do not

represent probability distributions, since we can always

push

X

â€ 

and normalize

` .

Example.

We illustrate this construction in the simple

case of the following class containing movie titles:

% movie

+ï¿½s+ï¿½ batman

GJÅ¾

ï¿½ï¿½ batman returns

 &lt;Å¾

a/

The compilation of the rewrite rule deï¬ned by this class

and applied left to right leads to the weighted transducer

a

given by ï¬gure 6. Our corpus simply consists of the

sentence â€œbatman returnsâ€ and is represented by the au-

tomaton

â€ 

given by ï¬gure 7. The corpus

X

â€ 

obtained by

composing

â€ 

with

a

is given by ï¬gure 7.

6

Conclusion

We presented several new and efï¬cient algorithms to

deal with more general problems related to the construc-

tion of language models found in new language process-

ing applications and reported experimental results show-

ing their practicality for constructing very large models.

These algorithms and many others related to the construc-

tion of weighted grammars have been fully implemented

and incorporated in a general grammar software library,

the GRM Library (Allauzen et al., 2003).

Acknowledgments

We thank Michael Riley for discussions and for having

implemented an earlier version of the counting utility.

References

Cyril

Allauzen,

Mehryar

Mohri,

and

Brian

Roark.

2003.

GRM

Library-Grammar

Library.

http://www.research.att.com/sw/tools/grm,

AT&amp;T

Labs

- Research.

Jean Berstel and Christophe Reutenauer. 1988. Rational Series

and Their Languages. Springer-Verlag: Berlin-New York.

Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jen-

nifer C. Lai, and Robert L. Mercer. 1992. Class-based n-

gram models of natural language. Computational Linguis-

tics, 18(4):467â€“479.

Stanley Chen and Joshua Goodman. 1998. An empirical study

of smoothing techniques for language modeling. Technical

Report, TR-10-98, Harvard University.

Frederick Jelinek and Robert L. Mercer. 1980. Interpolated

estimation of markov source parameters from sparse data.

In Proceedings of the Workshop on Pattern Recognition in

Practice, pages 381â€“397.

Ronald M. Kaplan and Martin Kay.

1994.

Regular models

of phonological rule systems.

Computational Linguistics,

20(3).

Slava M. Katz. 1987. Estimation of probabilities from sparse

data for the language model component of a speech recog-

niser. IEEE Transactions on Acoustic, Speech, and Signal

Processing, 35(3):400â€“401.

Werner Kuich and Arto Salomaa. 1986. Semirings, Automata,

Languages. Number 5 in EATCS Monographs on Theoreti-

cal Computer Science. Springer-Verlag, Berlin, Germany.

Mehryar Mohri and Richard Sproat. 1996. An Efï¬cient Com-

piler for Weighted Rewrite Rules. In

bc th Meeting of the

Association for Computational Linguistics (ACL â€™96), Pro-

ceedings of the Conference, Santa Cruz, California. ACL.

Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley.

1996. Weighted Automata in Text and Speech Processing.

In Proceedings of the 12th biennial European Conference on

Artiï¬cial Intelligence (ECAI-96), Workshop on Extended ï¬-

nite state models of language, Budapest, Hungary. ECAI.

Mehryar Mohri, Michael Riley, Don Hindle, Andrej Ljolje, and

Fernando C. N. Pereira. 1998. Full expansion of context-

dependent networks in large vocabulary speech recognition.

In Proceedings of the International Conference on Acoustics,

Speech, and Signal Processing (ICASSP).

Mehryar Mohri. 1997. Finite-State Transducers in Language

and Speech Processing. Computational Linguistics, 23:2.

Mehryar Mohri. 2002. Semiring Frameworks and Algorithms

for Shortest-Distance Problems. Journal of Automata, Lan-

guages and Combinatorics, 7(3):321â€“350.

Hermann Ney, Ute Essen, and Reinhard Kneser.

1994.

On

structuring probabilistic dependences in stochastic language

modeling. Computer Speech and Language, 8:1â€“38.

Arto Salomaa and Matti Soittola. 1978. Automata-Theoretic

Aspects of Formal Power Series.

Springer-Verlag: New

York.

Marcel Paul SchÂ¨utzenberger. 1961. On the deï¬nition of a fam-

ily of automata. Information and Control, 4.

Kristie Seymore and Ronald Rosenfeld. 1996. Scalable backoff

language models. In Proceedings of the International Con-

ference on Spoken Language Processing (ICSLP).

Andreas Stolcke. 1998. Entropy-based pruning of backoff lan-

guage models. In Proc. DARPA Broadcast News Transcrip-

tion and Understanding Workshop, pages 270â€“274.

