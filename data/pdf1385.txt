
NLP Language Models

1



• Introduction to Language Models 

• Noisy Channel model

• Simple Markov Models

• Smoothing

Statistical Language Models


NLP Language Models

2



Introduction

 

 

●  Statistical models of words of sentences – 

language models

●  Probability of all possible sequences of 

words

●  Inspired in speech recognition techniques 

●  Probability of next word based on previous

 


NLP Language Models

3



Probability Theory (I)



X be uncertain outcome of some event.  

 Represented as a random variable



V(X)  finite number of possible outcome (not a real number)



P(X=x), probability of the particular outcome x (x belongs V(X)) 

• X desease of your patient, V(X) all possible diseases        


NLP Language Models

4



Conditional probability of the outcome of an event based upon 

the outcome of a second event

We pick two words randomly from a book. We know first word 

is the, we want to know probability second word is dog

   P(W2 = dog|W1 = the) = | W1 = the,W2= dog|/ |W1 = the|

Bayes’s law: P(x|y) = P(x) P(y|x) / P(y)

Probability Theory(II)


NLP Language Models

5



Bayes’s law: P(x|y) = P(x) P(y|x) / P(y)

P(desease/symptom)= 

P(desease)P(symptom/desease)/P(symptom)

P(w1,n| speech signal) = P(w1,n)P(speech signal | w1,n)/ 

P(speech signal)

We only need to maximize the numerator

P(speech signal | w1,n) expresses how well the speech signal 

fits the sequence of words w1,n

Probability Theory (III)


NLP Language Models

6



Statistical Model of a Language

 Vocabulary (V), word  w ∈ V

 Language (L), sentence s ∈ L

  L ⊂ V*  usually  infinite

s = w1,…wN

•

Probability of s  P(s)  

•

For sequences of words of length n   assign a number to 

P(W1,n= w1,n), being w1,n a sequence of words

  


NLP Language Models

7



Ngram Model

•

Simple but durable statistical model

•

Useful to indentify words in noisy, ambigous input.

•

Speech recognition, many input speech sounds similar and 

confusable

•

Machine translation, spelling correction, handwriting 

recognition, predictive text input

•

 Other NLP tasks: part of speech tagging, NL 

generation, word similarity


NLP Language Models

8



CORPORA

•

Corpora (singular corpus) are online collections of text 

or speech.

•

Brown Corpus: 1 million word collection from 500 

written texts  from different genres (newspaper,novels, 

academic).

• Punctuation can be treated as words.

•

Switchboard corpus: 2430 Telephone conversations 

averaging 6 minutes each, 240 hour of speech and 3 

million words” 


NLP Language Models

9



Training and Test Sets

• Probabilities of N-gram model come from the 

corpus it is trained for

• Data in the corpus is divided into training set 

(or training corpus) and test set (or test 

corpus).

• Perplexity: compare statistical models


NLP Language Models

10



Ngram Model

•

How can we compute probabilities of entire sequences            

 P(w1,w2,..,wn) 

Descomposition using the chain rule of probability P(w1,w2,..,wn) 

=P(w1)P(w2|w1)P(w3|w1,w2),… P(wn|w1..,wn-1)

•

Assigns a conditional probability to possible next words 

considering the history.

•

Markov assumption : we can predict the probability of some 

future unit without looking too far into the past.

•

Bigrams only consider previous usint, trigrams, two previous 

unit, n-grams, n previous unit

 


NLP Language Models

11



Ngram Model

•

Assigns a conditional probability to possible next words.Only n-

1 previous words have effect on the probabilities of next word

•

For n = 3, Trigrams P(wn|w1..,wn-1) = P(wn|wn-2,wn-1) 

•

How we estimate these trigram or N-gram probabilities? 

    To maximize the likelihood of the training set T given 

the model M  ---    P(T/M)

•

To create the model use training text  (corpus), taking counts 

and normalizing them so they lie between 0 and 1.


NLP Language Models

12



Ngram Model

•

For n = 3, Trigrams 

 P(wn|w1..,wn-1) = P(wn|wn-2,wn-1) 

•

To create the model use training text and record pairs and 

triples of words that appear in the text and how many times

 P(wi|wi-2,wi-1)= C(wi-2,i) / C(wi-2,i-1)

P(submarine|the, yellow) = C(the,yellow, 

submarine)/C(the,yellow)

Relative frequency: observed frequency of a particular 

sequence divided by observed fequency of a prefix


NLP Language Models

13



W

X

W*

Y

encoder

decoder

Channel

 p(y|x)

message

input to 

channel

Output from

channel

Attempt to 

reconstruct 

message 

based 

on output

Noisy Channel Model

In language processing the problem is 

reduced to decode for getting the most 

likely input given the output


NLP Language Models

14



We want to retrieve X from Y 

Real Language X

Noisy channel   X-&gt;Y

Observed Language Y

Correct text

Errors

Text with 

errors


NLP Language Models

15



We want to retrieve X from Y 

Real Language X

Noisy channel   X-&gt;Y

Observed Language Y

Correct text

Space 

removing

Text 

without 

spaces


NLP Language Models

16



We want to retrieve X from Y 

Real Language X

Noisy channel   X-&gt;Y

Observed Language Y

Text

Text to 

speech 

generator

Speech


NLP Language Models

17



We want to retrieve X from Y 

Real Language X

Noisy channel   X-&gt;Y

Observed Language Y

Source 

Language

Translation

Target 

Language


NLP Language Models

18





Acoustic chain                                      word chain

Language model    Acoustic Model

Example: ASR Automatic Speech 

Recognizer

ASR

X1 ... XT

w1 ... wN


NLP Language Models

19





Target Language Model    

Translation Model

Example: Machine Translation


NLP Language Models

20



• Naive Implementation

• Enumerate s ∈ L

• Compute p(s)

• Parameters of the model |L|

• But ...

• L is usually infinite

• How to estimate the parameters?

• Simplifications

• History

• hi = { wi, … wi-1}

• Markov Models




NLP Language Models

21



• Markov Models of order n + 1

• P(wi|hi) = P(wi|wi-n+1, … wi-1)

• 0-gram

• 1-gram

• P(wi|hi) = P(wi)

• 2-gram

• P(wi|hi) = P(wi|wi-1)

• 3-gram

• P(wi|hi) = P(wi|wi-2,wi-1)




NLP Language Models

22



• n large:

• more context information (more discriminative power)

• n small:

• more cases in the training corpus (more reliable)

• Selecting n: 

• ej.  for |V| = 20.000

n

num. parameters

2 (bigrams)

400,000,000

3 

(trigrams)

8,000,000,000,000

4 (4-grams)

1.6 x 1017


NLP Language Models

23



• Parameters of an n-gram model

• |V|n

• MLE estimation

• From a training corpus

• Problem of sparseness


NLP Language Models

24



• 1-gram Model

• 2-gram Model

• 3-gram Model

P MLE w=C w

∣V∣

P MLE wi∣wi−1 ,wi−2 =

Cwi−2 wi−1 wi

C wi−2 wi−1

P MLE wi∣wi−1=

C wi−1wi

Cwi−1


NLP Language Models

25






NLP Language Models

26






NLP Language Models

27





True probability 

distribution


NLP Language Models

28





The seen cases are overestimated the 

unseen ones have a null probability


NLP Language Models

29



Save a part of the mass 

probability from seen cases 

and assign it to the unseen 

ones

SMOOTHING


NLP Language Models

30



• Some methods perform on the countings:

• Laplace, Lidstone, Jeffreys-Perks

• Some methods perform on the probabilities:

• Held-Out

• Good-Turing

• Descuento

• Some methods combine models

• Linear interpolation

• Back Off

