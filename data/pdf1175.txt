
Trending Now

DSA

Data Structures

Algorithms

Interview Preparation

Data Science

Topic-wise Practice

C

C++

Java

JavaScript

Python

CSS

Competitive Programming

Machine Learning

Aptitude

Write &amp; Earn

Web Development

Puzzles

Projects



Read

Discuss

Courses

Practice

Video

Entropy: Entropy is a way of measuring the uncertainty/randomness of a random variable X

Kullback-Leibler Divergence









pawangfg


In other words, entropy measures the amount of information in a random variable. It is normally measured in bits.

Joint Entropy: The joint Entropy of a pair of discrete random variables X, Y ~ p (x, y) is the amount of information

needed on average to specify both their values.



Conditional Entropy: The conditional entropy of a random variable Y given another X expresses how much extra

information one still needs to supply on average to communicate Y given that the other party knows X.






Example: 

Calculate the Entropy of Fair coin:






Here, the entropy of fair coin is maximum i.e 1. As the biasness of the coin increases the information/entropy

decreases. Below is the plot of Entropy vs Biasness, the curve will look as follows:



Biasness of Coin vs Entropy

Cross Entropy: Cross-entropy is a measure of the difference between two probability distributions (p and q) for a

given random variable or set of events. In other words, Cross-entropy is the average number of bits needed to

encode data from a source of distribution p when we use model q.

Cross-entropy can be defined as:




Kullback-Leibler Divergence: KL divergence is the measure of the relative difference between two probability

distributions for a given random variable or set of events. KL divergence is also known as Relative Entropy. It can

be calculated by the following formula:

The difference between Cross-Entropy and KL-divergence is that Cross-Entropy calculates the total distributions

required to represent an event from the distribution q instead of p, while KL-divergence represents the extra amount

of bit required to represent an event from the distribution q instead of p. 

Properties of KL-divergence:

D(p || q) is always greater than or equal to 0.

D(p || q) is not equal to D(q || p).  The KL-divergence is not communicative.

If p=q, then D(p || q) is 0.

Example and Implementation: 

Suppose there are two boxes that contain 4 types of balls (green, blue, red, yellow). A ball is drawn from the box

randomly having the given probabilities. Our task is to calculate the difference of distributions of two boxes i.e KL-

divergence.

Code: Python code implementation to solve this problem.

python3


Similar Reads

1.

Role of KL-divergence in Variational Autoencoders

Output:

KL-divergence(box_1 || box_2): 0.057 

KL-divergence(box_2 || box_1): 0.056 

KL-divergence(box_1 || box_1): 0.000 

Using Scipy rel_entr function

KL-divergence(box_1 || box_2): 0.057 

KL-divergence(box_2 || box_1): 0.056 

KL-divergence(box_1 || box_1): 0.000 

Applications of KL-divergence:

Entropy and KL-divergence have many useful applications particularly in data science and compression.

Entropy can be used in data preprocessing steps such as feature selections. For Example, If we want to

classify the different NLP docs based on their topics, then we can check for the randomness of the different

word appears in the doc. There is more chance of the word “computer” appears in technology-related docs but

the same cannot be said for the word “the”. 

The Entropy can also be used for text compression and quantifying the compression. The data which contains

some pattern is easier to compress than the data which is more random.

KL-divergence is also used in many NLP and computer vision models such as in Variational Auto Encoder to

compare the original image distribution and distribution of images generated from the encoded distribution.

Last Updated : 10 Jan, 2023

# box =[P(green),P(blue),P(red),P(yellow)]

box_1 = [0.25, 0.33, 0.23, 0.19]

box_2 = [0.21, 0.21, 0.32, 0.26]

 

import numpy as np

from scipy.special import rel_entr

 

def kl_divergence(a, b):

 return sum(a[i] * np.log(a[i]/b[i]) for i in range(len(a)))

  

print('KL-divergence(box_1 || box_2): %.3f ' % kl_divergence(box_1,box_2))

print('KL-divergence(box_2 || box_1): %.3f ' % kl_divergence(box_2,box_1))

 

# D( p || p) =0

print('KL-divergence(box_1 || box_1): %.3f ' % kl_divergence(box_1,box_1))

 

print("Using Scipy rel_entr function")

box_1 = np.array(box_1)

box_2 = np.array(box_2)

 

print('KL-divergence(box_1 || box_2): %.3f ' % sum(rel_entr(box_1,box_2)))

print('KL-divergence(box_2 || box_1): %.3f ' % sum(rel_entr(box_2,box_1)))

print('KL-divergence(box_1 || box_1): %.3f ' % sum(rel_entr(box_1,box_1)))






Courses

 

Related Tutorials

1.

Deep Learning Tutorial

2.

Top 101 Machine Learning Projects with Source Code

3.

Machine Learning Mathematics

4.

Natural Language Processing (NLP) Tutorial

5.

Data Science for Beginners

Previous

Next  

Article Contributed By :

pawangfg

@pawangfg

Vote for difficulty

Current difficulty : Hard

 

 

 

 





Easy



Normal



Medium



Hard



Expert

Improved By :

gydis

Article Tags :

Machine Learning

Practice Tags :

Machine Learning

Report Issue


course-img



 102k+ interested Geeks

Complete Machine Learning &amp;

Data Science Program

 Beginner to Advance

course-img



 142k+ interested Geeks

Python Programming Foundation -

Self Paced

 Beginner and Intermediate

course-img



 26k+ interested Geeks

Master JavaScript - Complete

Beginner to Advanced

 Beginner and Intermediate

course-img



 794k+ interested Geeks

Complete Interview Preparation -

Self Paced

 Beginner to Advance


 A-143, 9th Floor, Sovereign Corporate Tower,

Sector-136, Noida, Uttar Pradesh - 201305

 feedback@geeksforgeeks.org



































































































Company

About Us

Careers

In Media

Contact Us

Terms and

Conditions

Privacy Policy

Copyright Policy

Third-Party

Copyright Notices

Advertise with us

Languages

Python

Java

C++

GoLang

SQL

R Language

Android Tutorial

Data Structures

Array

String

Linked List

Stack

Queue

Tree

Graph

Algorithms

Sorting

Searching

Greedy


Dynamic

Programming

Pattern Searching

Recursion

Backtracking

Web

Development

HTML

CSS

JavaScript

Bootstrap

ReactJS

AngularJS

NodeJS

Write &amp; Earn

Write an Article

Improve an Article

Pick Topics to Write

Write Interview

Experience

Internships

Video Internship

Computer

Science

GATE CS Notes

Operating Systems

Computer Network

Database

Management

System

Software

Engineering

Digital Logic Design

Engineering Maths

Data Science &amp;

ML

Data Science With

Python

Data Science For

Beginner

Machine Learning

Tutorial


Maths For Machine

Learning

Pandas Tutorial

NumPy Tutorial

NLP Tutorial

Interview

Corner

Company

Preparation

Preparation for SDE

Company Interview

Corner

Experienced

Interview

Internship Interview

Competitive

Programming

Aptitude

Python

Python Tutorial

Python

Programming

Examples

Django Tutorial

Python Projects

Python Tkinter

OpenCV Python

Tutorial

GfG School

CBSE Notes for

Class 8

CBSE Notes for

Class 9

CBSE Notes for

Class 10

CBSE Notes for

Class 11

CBSE Notes for

Class 12

English Grammar

UPSC/SSC/BANKING

SSC CGL Syllabus

SBI PO Syllabus


IBPS PO Syllabus

UPSC Ethics Notes

UPSC Economics

Notes

UPSC History

Notes

@geeksforgeeks , Some rights reserved

