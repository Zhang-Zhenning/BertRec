


Published in

Towards Data Science



May 16, 2019

·

5 min read

·

Save

From Word Embeddings to Pretrained Language

Models — A New Age in NLP — Part 1

Traditional Context-Free Representations

Bag of Words or One Hot Encoding

BoW Representation








TF-IDF Representation

Distributional Similarity based Representations — Word Embeddings

Word2Vec

Word2Vec CBOW vs Skip-gram




GloVe

Part 2

References

An Idiot's Guide to Word2vec Natural Language Processing

Machine Learning

Deep Learning

Transfer Learning




Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





NLP

Representation Learning

