


CONTRIBUTED ARTICLES

Language Models: Past, Present, and Future

VIEW AS:









SHARE:

















Credit: Andrij Borys Associates;

Shutterstock

By Hang Li 

Communications of the ACM, July 2022, Vol. 65 No. 7, Pages 56-63

10.1145/3490443

Comments (1)

Natural language processing (NLP) has undergone revolutionary

changes in recent years. Thanks to the development and use of

pre-trained language models, remarkable achievements have

been made in many applications. Pre-trained language models

offer two major advantages. One advantage is that they can

significantly boost the accuracy of many NLP tasks. For

example, one can exploit the BERT model to achieve

performances higher than humans in language understanding.8

One can also leverage the GPT-3 model to generate texts that

resemble human writings in language generation.3 A second

advantage of pre-trained language models is that they are

universal language processing tools. To conduct a machine

learning-based task in traditional NLP, one had to label a large

amount of data to train a model. In contrast, one currently needs

only to label a small amount of data to fine-tune a pre-trained

language model because it has already acquired a significant amount of knowledge necessary for language

processing.

Back to Top

Key Insights



This article offers a brief introduction to language modeling, particularly pre-trained language modeling,

from the perspectives of historical development and future trends for general readers in computer science. It

is not a comprehensive survey but an overview, highlighting the basic concepts, intuitive explanations,

technical achievements, and fundamental challenges. While positioned as an introduction, this article also

helps knowledgeable readers to deepen their understanding and initiate brainstorming. References on pre-

trained language models for beginners are also provided.

SIGN IN

SIGN IN for Full Access

» Forgot Password?

» Create an ACM Web Account

MORE NEWS &amp; OPINIONS

All Sliders to the Right

George V. Neville-Neil

GPT-4's Successes, and GPT-

4's Failures

Gary Marcus

Home / Magazine Archive / July 2022 (Vol. 65, No. 7) / Language Models: Past, Present, and Future / Abstract

Can AI Demonstrate Creativity?

Keith Kirkpatrick

User Name

Password







ACM.org

Join ACM

About Communications

ACM Resources

Alerts &amp; Feeds









HOME



VIDEOS

SIGN IN







CURRENT ISSUE



NEWS



BLOGS



OPINION



RESEARCH



PRACTICE



CAREERS



ARCHIVE


For Authors

For Advertisers 



Privacy Policy

Help

Contact Us

Mobile Site

Copyright © 2023 by the ACM. All rights reserved.





Sign In

Sign in using your ACM Web Account username and

password to access premium content if you are an ACM

member, Communications subscriber or Digital Library

subscriber.

Username

Password

Forgot Password?

SIGN IN

Need Access?

Please select one of the options below for access to

premium content and features.

Create a Web Account

If you are already an ACM member, Communications

subscriber, or Digital Library subscriber, please set up a

web account to access premium content on this site.

CREATE A WEB ACCOUNT

Join the ACM

Become a member to take full advantage of ACM's

outstanding computing information resources, networking

opportunities, and other benefits.

BECOME A MEMBER 

Subscribe to Communications of

the ACM Magazine

Get full access to 50+ years of CACM content and receive

the print version of the magazine monthly.

GET A SUBSCRIPTION

Purchase the Article

Non-members can purchase this article or a copy of the

magazine in which it appears.

PURCHASE THIS ARTICLE

Comments

John Shahbazian

November 21, 2022 07:57

Thank you for this article! I enjoyed learning about the different NLP models and the history behind them.

Previously, I had only heard the names and occasionally played with demos, but I never dove into how they

work as I figured it would be too complex. However this article makes understanding the basic idea of them

very easy! It was timely and very well written.

Displaying 1 comment

Log in to Read the Full Article

