
N-gram smoothing models

When dealing with n-gram models, smoothing refers to the practice of adjusting empirical probability estimates to

account for insufficient data.

In the descriptions below, we use the notation wj

i, i &lt; j, to denote the (j - i)-gram (wi,wi+1,…,wj).

Laplace Smoothing

Laplace smoothing is the assumption that each n-gram in a corpus occurs exactly one more time than it actually does.

p(wi ∣ wi−1

i−n+1) =

1 + c(wii−n+1)

|V| ∑wic(wi

i−n+1)

where c(a) denotes the empirical count of the n-gram a in the corpus, and |V| corresponds to the number of unique n-

grams in the corpus.

Models

AdditiveNGram

Additive/Lidstone Smoothing

Additive/Lidstone smoothing is a generalization of Laplace smoothing, where we assume that each n-gram in a corpus

occurs k more times than it actually does (where k can be any non-negative value, but typically ranges between [0, 1]):

p(wi ∣ wi−1

i−n+1) =

k + c(wii−n+1)

k |V| ∑wic(wii−n+1)

where c(a) denotes the empirical count of the n-gram a in the corpus, and |V| corresponds to the number of unique n-

grams in the corpus.

Models

AdditiveNGram

Good-Turing Smoothing

Good-Turing smoothing is a more sophisticated technique which takes into account the identity of the particular n-gram

when deciding the amount of smoothing to apply. It proceeds by allocating a portion of the probability space occupied by

n-grams which occur with count r+1 and dividing it among the n-grams which occur with rate r.

r∗ = (r + 1)

g(r + 1)

g(r)

p(wii−n+1 ∣ c(wii−n+1) = r) =

r∗

N

where r∗ is the adjusted count for an n-gram which occurs r times, g(x) is the number of n-grams in the corpus which

occur x times, and N is the total number of n-grams in the corpus.

Models

GoodTuringNGram

References

[1] Chen &amp; Goodman (1998). “An empirical study of smoothing techniques for language modeling”. Harvard Computer

Science Group Technical Report TR-10-98.

[2] Gale &amp; Sampson (1995). “Good-Turing frequency estimation without tears”. Journal of Quantitative Linguistics, 2(3),

217-237.

Go

numpy-ml

Machine learning, in NumPy



Navigation

Hidden Markov models

Gaussian mixture models

Latent Dirichlet allocation

N-gram smoothing models

MLENGram

AdditiveNGram

GoodTuringNGram

Multi-armed bandits

Reinforcement learning

Nonparametric models

Matrix factorization

Tree-based models

Neural networks

Linear models

Preprocessing

Utilities

Quick search

Processing math: 100%

