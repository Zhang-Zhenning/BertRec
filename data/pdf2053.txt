
arXiv:math/0607812v1  [math.ST]  31 Jul 2006

The Annals of Statistics

2006, Vol. 34, No. 3, 1204–1232

DOI: 10.1214/009053606000000353

c

⃝ Institute of Mathematical Statistics, 2006

SEMIPARAMETRIC ESTIMATION OF A TWO-COMPONENT

MIXTURE MODEL

By Laurent Bordes, St´ephane Mottelet and

Pierre Vandekerkhove

Universit´e de Technologie de Compi`egne, Universit´e de Technologie de

Compi`egne and Universit´e de Marne-la-Vall´ee

Suppose that univariate data are drawn from a mixture of two

distributions that are equal up to a shift parameter. Such a model is

known to be nonidentiﬁable from a nonparametric viewpoint. How-

ever, if we assume that the unknown mixed distribution is symmetric,

we obtain the identiﬁability of this model, which is then deﬁned by

four unknown parameters: the mixing proportion, two location pa-

rameters and the cumulative distribution function of the symmetric

mixed distribution. We propose estimators for these four parame-

ters when no training data is available. Our estimators are shown to

be strongly consistent under mild regularity assumptions and their

convergence rates are studied. Their ﬁnite-sample properties are il-

lustrated by a Monte Carlo study and our method is applied to real

data.

1. Introduction.

Cumulative distribution functions (c.d.f.) of p-variate

multi-component mixture models are generally deﬁned by

G(x) =

k

�

j=1

λjFj(x),

x ∈ Rp,

(1)

where the unknown mixture proportions λj (λj ≥ 0 and �k

j=1 λj = 1) and

the unknown c.d.f. Fj are to be estimated. It is commonly assumed that the

Fj’s belong to a parametric family, which means that the space of unknown

parameters is reduced to a Euclidean set, leading to parametric inference.

There is an extensive literature on this subject, including the monographs

of Everitt and Hand [16], Titterington, Smith and Makov [40], McLachlan



Received November 2004; revised October 2005.

AMS 2000 subject classiﬁcations. Primary 62G05, 62G20; secondary 62E10.

Key words and phrases. Semiparametric, two-component mixture model, identiﬁabil-

ity, contrast estimators, consistency, rate of convergence, mixing operator.





This is an electronic reprint of the original article published by the

Institute of Mathematical Statistics in The Annals of Statistics,

2006, Vol. 34, No. 3, 1204–1232. This reprint diﬀers from the original in

pagination and typographic detail.





1


2

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

and Basford [28] and McLachlan and Peel [29]. The main types of estima-

tors that have been proposed are the following: maximum likelihood (e.g.,

[7, 24, 25, 35]), minimum chi-square (e.g., [11]), method of moments (e.g.,

[26]), Bayesian approaches (e.g., [13, 15]) and techniques based on moment

generating functions (e.g., [34]). Note that the number of components k in

model (1) may also be an unknown parameter to be estimated, leading to

various rates of convergence for maximum likelihood estimators, as discussed

by Chen [6]. In this case, the selection of a model is an important topic; see,

for example, [10, 22, 23].

The choice of a parametric family for the Fj’s may be diﬃcult when little

is known about subpopulations. However, models of type (1) are generally

nonparametrically nonidentiﬁable without additional assumptions. This is

no longer true when training data are available, that is, when some data

are of known origin with respect to the components of the mixture distribu-

tion. In this case nonparametric techniques can be applied; see, for example,

[4, 17, 21, 31, 33, 36, 39, 40]. As Hall and Zhou [18] state, “very little is

known about nonparametric inference in mixtures without training data.”

These authors looked at p-variate data drawn from a mixture of two distri-

butions, each having independent components, and proved that, under mild

regularity assumptions, their model is identiﬁable for p ≥ 3. They proposed

root-n consistent estimators of the 2p univariate marginal distributions and

the mixing proportion. In a working paper Kitamura [20] investigates iden-

tiﬁability of type (1) models with the presence of covariates.

Note that even if model (1) is not nonparametrically identiﬁable, there

exist, for p = 1 and k = 2, many real data sets in the statistical literature for

which such a model is used under parametric assumptions for the Fj’s. For

example, Azzalini and Bowman [1] provided data on the length of intervals

between eruptions and the duration of the eruption for the Old Faithful

Geyser in Yellowstone National Park. Another example deals with average

amounts of precipitation (rainfall) in inches for United States cities (from the

Statistical Abstract of the United States, 1975; see [30]). These two data sets

are available in the R statistical package. Moreover, in some studies, the only

parameters of interest are mixture proportions, in which case components

Fj in model (1) are nuisance parameters (see, e.g., [8]). In this paper we

consider the two-component identiﬁable restriction of model (1) deﬁned by

G(x) = λF(x − µ1) + (1 − λ)F(x − µ2),

x ∈ R.

(2)

Unknown parameters are the c.d.f. F of a symmetric distribution, two real

location parameters µ1 and µ2 and the mixing proportion λ. Note that this

model has also been studied by Hunter, Wang and Hettmansperger [19] in

an independent work. Model (2) above is called semiparametric inasmuch

as the unknown parameters can be separated into a functional part F and a


SEMIPARAMETRIC MIXTURE MODEL

3

Euclidean part (µ1,µ2,λ). Note that such a model should be distinguished

from the so-called semi- or nonparametric mixture models (e.g., [27]) where

G is deﬁned by

G(x) =

�

R

F(x;θ)dH(θ),

x ∈ R,

(3)

where F belongs to a parametric family and H is an unknown distribution

function on R. However, as noted by Lindsay and Lesperance [27], there is a

link between models (1) and (3) if H is discrete with k points of support. Of

course, such a link exists between models (2) and (3) by assuming that, in the

latter model, F(·;θ) = F(· − θ) with F in the c.d.f. family F of symmetric

distributions, and that H puts masses λ and 1 − λ at points µ1 and µ2,

respectively.

One of the fundamental issues with mixture models of type (1) is to pro-

vide identiﬁability results. When the Fj’s belong to certain speciﬁc paramet-

ric families (e.g., the continuous exponential family), identiﬁability results

are available; see, for example, [2, 5, 38]. More is needed when we aim to

estimate the Fj’s nonparametrically (see [18] for the two components case).

Working with model (2), we need to prove that G is deﬁned by a unique

quadruple (λ,µ1,µ2,F).

The paper is organized as follows. In the next section we give an iden-

tiﬁability result for model (2). In Section 3 we provide a methodology for

estimating unknown parameters in our two-component mixture model. Con-

sistency results and convergence rates of our estimators are given in the same

section. Our main results are proved in Section 5. In Section 4 ﬁnite-sample

properties of our estimators are illustrated by a Monte Carlo study and our

method is applied to precipitation data.

2. Identiﬁability.

First, note that if F in model (2) admits a density

function f (an even function), the mixture distribution admits a density

function g deﬁned by

g(x) = λf(x − µ1) + (1 − λ)f(x − µ2),

x ∈ R,

(4)

where θ = (λ,µ1,µ2) ∈ Θ = [0,1/2) × (R2\∆) and ∆ = {(x,x);x ∈ R}.

The aim of this section is to investigate identiﬁability, that is, the possi-

bility of having

λF(x − µ1) + (1 − λ)F(x − µ2)

= λ′F ′(x − µ′

1) + (1 − λ′)F ′(x − µ′

2)

∀x ∈ R,

(5)

for two diﬀerent quadruples (θ,F) and (θ′,F ′) in Θ×F, where θ′ = (λ′,µ′

1,µ′

2)

and F is the c.d.f. set of symmetric distributions. Note that it is suﬃcient

to consider λ ∈ [0,1/2) because the model is invariant by permutation of

(λ,µ1) and (1 − λ,µ2). Note also that what we mean by identiﬁability is


4

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

not entirely an injectivity condition, since if λ = 0, we only need to obtain

λ′ = 0, µ2 = µ′

2 and F = F ′. Clearly, identiﬁability fails if we allow λ to be

equal to 1/2. Indeed, suppose that f is itself an even mixture density func-

tion, for example, f(x) = h(x − µ)/2 + h(x + µ)/2 with h an even density

function. If g(x) = f(x − µ2) with λ = 0, then (5) is obviously satisﬁed with

λ′ = 1/2, µ′

1 = µ+µ2, µ′

2 = µ2 −µ and f ′ = h. The main identiﬁability result

is summarized in the following theorem.

Theorem 2.1.

If (λ,µ1,µ2,F) and (λ′,µ′

1,µ′

2,F ′) are two parameters

of [0,1/2) × (R2\∆) × F satisfying (5), then λ = λ′, µ2 = µ′

2 and F = F ′,

and µ1 = µ′

1 if λ &gt; 0.

Hunter, Wang and Hettmansperger [19] have established a similar result

for the parametric part (λ,µ1,µ2) of the model. Their results are slightly

diﬀerent from ours since they considered identiﬁability from the injectivity

point of view. They also gave a necessary condition for identifying a type

(2) model with three components.

A question which naturally arises concerns the possibility of extending

our identiﬁability result when scale parameters are introduced into model

(2). In fact, it is easy to show that such a model is generally not identiﬁable.

3. Methodology and theoretical results.

Let X1,...,Xn be n indepen-

dent and identically distributed random variables with common c.d.f. G

given by model (2). We shall denote by θ0 and F0 the true values of the

unknown Euclidean parameter and the unknown mixed c.d.f. The aim of

this section is to propose estimators for θ0 and F0. Asymptotic results are

given with respect to n → +∞.

The ﬁrst key idea developed in Section 3.1 is based on the possibility of

expressing F as a function of G and θ (resp. f as a function of g and θ) by

inverting the relation (2) [resp. by inverting the relation (4)]. The second key

idea, developed in Section 3.2, involves using the symmetry property of F0

in order to propose a contrast function for the Euclidean parameter θ when

G is known. Then, in Sections 3.3 and 3.4, replacing G by the corresponding

empirical c.d.f., we propose estimators of θ0 and F0 and give some asymptotic

results for these estimates. These results are obtained under two kinds of

assumptions on F0:

C1. F0 is strictly increasing and Lipschitz on R.

C2. F0 is strictly increasing, twice continuously diﬀerentiable on R and F ′′

0 ∈

L1(R).


SEMIPARAMETRIC MIXTURE MODEL

5

3.1. Inversion formula.

Assume that in the mixture model deﬁned by

(2) the Euclidean parameter θ = (λ,µ1,µ2), with µ1 ̸= µ2 and λ ∈ [0,1/2), is

known. The key idea consists in rewriting (2) as

F(x) =

1



1 − λG(x + µ2) + −λ



1 − λF(x + η)

∀x ∈ R,

(6)

where η = µ2 − µ1 ̸= 0, and hence, using (6) as a recurrence formula. Let ℓ

be a positive integer. By using (6) ℓ times, we get

F(x) =

1



1 − λ

ℓ−1

�

k=0

� −λ



1 − λ

�k

G(x + µ2 + kη)

+

� −λ



1 − λ

�ℓ

F(x + ℓη)

∀x ∈ R.

(7)

Let us show that

F(x) =

1



1 − λ

�

k≥0

� −λ



1 − λ

�k

G(x + µ2 + kη)

∀x ∈ R.

(8)

If we denote by H the right-hand side in (8), then by (7) we get, for all

ℓ ≥ 1,

∥F − H∥∞ ≤

�

λ



1 − λ

�ℓ

+

1



1 − λ

�

k≥ℓ

�

λ



1 − λ

�k

≤

�

λ



1 − λ

�ℓ�

1 +

1



1 − 2λ

�

,

where ∥ · ∥∞ denotes the supremum norm. Since the right-hand side of the

above inequality can be made arbitrarily small, it follows that F = H. Sim-

ilarly, working with densities [see (4)] and replacing the supremum norm by

the L1(R)-norm ∥ · ∥1, we get

f(x) =

1



1 − λ

�

k≥0

� −λ



1 − λ

�k

g(x + µ2 + kη)

for µ-almost all x ∈ R,

(9)

where µ is Lebesgue measure on R.

At this point it is convenient to introduce the linear bounded operators

Aθ and A−1

θ

deﬁned by

Aθ = λτµ1 + (1 − λ)τµ2

and

A−1

θ

=

1



1 − λ

�

k≥0

� −λ



1 − λ

�k

τ−µ2−kη,

(10)

where τµ (µ ∈ R) is the shift operator deﬁned by τµf = f(· − µ). With the

above deﬁnitions of Aθ and A−1

θ , formulae (2) and (4) are equivalent to G =


6

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

AθF and g = Aθf, respectively, whereas formulae (8) and (9) are equivalent

to F = A−1

θ G and f = A−1

θ g, respectively.

The interest of the operator A−1

θ

is that if θ is known, the c.d.f. F may

be recovered from a nonparametric estimate ˆG of G by considering the

reversed estimates ˆF = A−1

θ

ˆG. This also holds for the density f, deﬁning ˆf =

A−1

θ ˆg with ˆg a nonparametric estimator of g. Unfortunately, the Euclidean

parameter θ is generally unknown and thus we need to propose an estimate

of θ separately. It should be noted that the above inversion formulae do

not require the model to be identiﬁable. We saw in Section 2 that a crucial

factor in obtaining identiﬁability is using the symmetry of the unknown

mixed distribution. In the next paragraph we use the symmetry of the mixed

distribution to provide a contrast function.

3.2. A contrast function.

The second key point follows from the following

simple remark. Let Fθ = A−1

θ G = A−1

θ Aθ0F0, where θ ∈ Θ. Clearly, if θ = θ0,

we have Fθ = F0 (from Section 3.1), and it must have the invariance property

of c.d.f.’s of symmetric distributions, F0(x) = 1 − F0(−x). For simplicity, let

us introduce Sr, the symmetry operator deﬁned by SrF(·) = 1 − F(−·). The

preceding remark may be reformulated as follows: if θ = θ0, then A−1

θ G =

SrA−1

θ G or, equivalently, G = AθSrA−1

θ G, by applying Aθ on the left-hand

side of the last equality. What about the converse? The answer is given in

the following theorem, whose proof is given in Section 5.

Theorem 3.1.

Consider model (2) with F0 the c.d.f. of a symmetric

distribution and θ0 ∈ Θ. If, for θ ∈ Θ, we have G = AθSrA−1

θ G, then θ = θ0.

Assuming that G is known, we can recover the true value θ0 of θ by

minimizing a discrepancy measure between G and Gθ = AθSrA−1

θ G. Recall

that G is unknown but can be estimated, which is why we choose to consider

the discrepancy measure K, deﬁned by

K(θ) ≡ K(θ;G) =

�

R

(Gθ(x) − G(x))2 dG(x),

θ ∈ Θ.

(11)

The choice of introducing the weighted measure G in the above integral

follows from the consideration that if G is replaced by its empirical c.d.f.,

then the integral sign turns into a simple sum. As a consequence of the

preceding theorem, assuming that F is suﬃciently smooth and that G is

known, we are able to show that K is a contrast function for the unknown

Euclidean parameter θ.

Corollary 3.1.

Under assumption C1, K is a contrast function: for

all θ ∈ Θ, K(θ) ≥ 0 and K(θ) = 0 if and only if θ = θ0.


SEMIPARAMETRIC MIXTURE MODEL

7

3.3. Estimators of the Euclidean parameter θ.

The above Corollary 3.1

suggests that the unknown Euclidean parameter θ should be estimated as

follows:

ˆθn = argmin

θ∈Θ

K(θ; ˆGn),

where ˆGn is an estimator of the c.d.f. of G. It is important to note that if ˆGn

is a stepwise function, K(θ; ˆGn) is also a stepwise function with respect to

parameters µ1 and µ2, and does not have the required regularity properties

for diﬀerentiable optimization techniques to be applied in order to ﬁnd ˆθn.

This is the reason why we need to distinguish two cases: (P1) the parameters

µ1 and µ2 are known and (P2) the parameters µ1 and µ2 are unknown.

(P1) The parameters µ1 and µ2 are known, whereas λ and F are unknown.

For this problem, we suppose that the true mixing proportion λ0 belongs

to [0,1/2 − d], where d ∈ (0,1/2). In this case the parameter θ reduces to λ

and we estimate λ by

ˆλn = arg min

λ∈[0,1/2−d]

K(λ; ˆGn),

where ˆGn is the empirical c.d.f. of G deﬁned by

ˆGn(x) = 1



n

n

�

i=1

1Xi≤x

∀x ∈ R,

(12)

where 1 denotes the indicator function. Let us give an explicit formula for

ˆG(n)

λ

= AλSrA−1

λ

ˆGn involving a sum of n terms:

ˆGλ(x) = 1 + 1



n

n

�

i=1

�

λ



λ − 11x≤η+2µ1−Xi + 1 − 2λ



λ

�

λ



λ − 1

�L(i,x)�

,

(13)

where

L(i,x) = max

�

1,

�x − 2µ1 + Xi − η



η

��

,

and where ⌈x⌉ denotes the smallest integer greater than or equal to x and

η = µ2 − µ1. The following theorem, whose proof is provided in Section 5,

gives the asymptotic behavior of ˆλn.

Theorem 3.2.

Assume that the c.d.f. F0 satisﬁes assumption C1. Then

(i) ˆλn converges almost surely to λ0, and (ii) we have √



n(ˆλn −λ0) = OP (1).


8

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

Note that if F0 is assumed to admit a ﬁrst-order moment, then, using the

ﬁrst-order moment equation of g, we show that λ0 can be directly estimated

by the natural empirical estimator

¯λn = n−1 �n

i=1 Xi − µ2



µ2 − µ1

,

which obviously satisﬁes results of the above theorem.

(P2) The parameters µ1,µ2,λ and F are unknown.

For this problem, we suppose that Θ = [0,1/2−d]×X , where 0 &lt; d &lt; 1/2

and X is a compact subset of R2 such that X ∩ ∆ = ∅, and the unknown

Euclidean parameter θ is an interior point of Θ. As explained previously, we

need to change K(·; ˆGn) into the more regular version Kr(·; ˆGn) deﬁned by

Kr(θ; ˆGn) =

�

R

( ˜G(n)

θ (x) − ˜Gn(x))2 d ˆGn(x),

where ˜G(n)

θ

= AθSrA−1

θ

˜Gn and ˜Gn(x) =

� x

−∞

ˆgn(y)dy, with

ˆgn(x) = 1



bn

�

R

q

�x − y



bn

�

d ˆGn(y),

where (bn)n≥1 is a sequence of real numbers decreasing to 0. Our numerical

applications are based upon the kernel function q deﬁned by q(x) = (1 −

|x|)1|x|≤1.

As for the (P1) problem, we prove in Section 5 asymptotic results sum-

marized in the next theorem for the estimator ˆθn. From a general point of

view, ˜Gn is a smooth estimate of the c.d.f. G deﬁned, for x ∈ R, by

˜Gn(x) = 1



n

n

�

k=1

Q

�x − Xk



bn

�

,

(14)

where Q(x) =

� x

−∞ q(y)dy, with q an even density function with compact

support and second-order moment equal to 1, and (bn)n≥1 is a sequence of

nonnegative real numbers decreasing to 0 with nbn → +∞ and √



nb2

n = O(1).

The fact that q has compact support leads to an explicit formula for ˜G(n)

θ ,

involving a sum of n terms,

˜Gθ(x) = 1 +

1



(nbn)

n

�

i=1

�

λ



λ − 1Q

�−x + η + 2µ1 − Xi



bn

�

+ 1 − 2λ



λ

�

λ



λ − 1

�L2(i,x)

(15)


SEMIPARAMETRIC MIXTURE MODEL

9

+ 2λ − 1



λ(λ − 1)

L2(i,x)−1

�

k=L1(i,x)

�

λ



λ − 1

�k

× Q

�−x + (k + 1)η + 2µ1 − Xi



bn

��

,

where, for k = 1,2,

Lk(i,x) = max

�

1,

�x − 2µ1 + Xi − η + (−1)kbn



η

��

.

Theorem 3.3.

If the c.d.f. F0 satisﬁes C1, then ˆθn converges almost

surely to θ0. If, in addition, F0 satisﬁes C2, we have n1/4−α(ˆθn − θ0) =

oa.s.(1), for all α &gt; 0.

3.4. Estimators of functional parameter F.

As suggested by the inver-

sion formula (8), once we get a consistent estimator ˆθn of the unknown (or

partially unknown) Euclidean true parameter θ0, it is natural to seek to

approximate the unknown c.d.f. F0 by ˜Fn = A−1

ˆθn

ˆGn. However, since we ap-

proximate the c.d.f. of a symmetric distribution, we constrain ˜Fn to satisfy

the invariance property ˜Fn = Sr ˜Fn, leading to the ﬁnal estimator

ˆFn = 1



2(I + Sr)A−1

ˆθn

ˆGn,

(16)

where I is the identity operator. By similar arguments, the unknown density

function f0 can in turn be estimated by

ˆfn = 1



2(I + Sd)A−1

ˆθn ˆgn,

(17)

where the operator Sd is deﬁned by (Sdf)(x) = f(−x) (corresponding to

the invariance property of densities of symmetric distributions). The next

theorem gives asymptotic results for both ˆFn and ˆfn for problems (P1) and

(P2). These theorems are proved in Section 5.

Theorem 3.4.

(i) If F0 satisﬁes C1, then ∥ ˆFn −F0∥∞ converges almost

surely to 0 for problems (P1) and (P2).

(ii) Under C1, we have ∥ ˆFn − F0∥∞ = OP (n−1/2) for problem (P1). Un-

der C2, for problem (P2) we have ∥ ˆFn − F0∥∞ = oa.s.(n−1/4+α) for any

α &gt; 0.

(iii) Under C1 (resp. C2), for problem (P1) [resp. (P2)], ∥ ˆfn − f0∥1 con-

verges almost surely to 0.


10

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

Let us notice that generally ˆFn (resp. ˆfn) is not a c.d.f. function (resp. a

density function). Indeed, by the deﬁnition of a mixture, g belongs to the

range of the operator Aθ, whereas this is no longer true for its approximate

ˆgn. Since there is no possibility that ˆgn is a two-component mixture in the

sense of model (2), it follows that A−1

ˆθn ˆgn cannot be a density function,

and the same holds for ˆfn. However, from a practical point of view, we

can easily transform estimators ˆfn into density functions. Let us consider

f ∗

n = ˆfn1 ˆfn≥0. It is straightforward to show that ∥f ∗

n − f0∥1 ≤ ∥ ˆfn − f0∥1,

and then we have the almost sure convergence of ∥f ∗

n − f0∥1 to 0, given the

assumptions of Theorem 3.4(iii). Moreover, under the same assumptions and

with sn =

�

R f ∗

n(x)dx, we have

|sn − 1| =

����

�

R

(f ∗

n(x) − f0(x))dx

����

≤ ∥f ∗

n − f0∥1

≤ ∥ ˆfn − f0∥1 → 0

a.s.

Therefore, ˜fn = s−1

n f ∗

n are density functions that satisfy ∥ ˜fn − f0∥1 → 0,

almost surely.

3.5. Discussion of the three-component case.

As we discussed in Section

2, identiﬁability results exist (see [19]) for the following three-component

model:

G(x) = λ1F(x − µ1) + λ2F(x − µ2) + λ3F(x − µ3)

∀x ∈ R,

where F is the c.d.f. of a symmetric distribution, and the λi’s are nonnegative

real numbers with λ1 + λ2 + λ3 = 1. A question naturally arises concerning

the possibility of extending our estimation method to the above model.

Following the method presented in Section 3.1, we get, for all ℓ ≥ 1,

F(x) = G(x + µ3)



λ3

+

ℓ−1

�

k=1

(−1)k

�

(i1,...,ik)∈{1,2}k

λi1 ···λik



λk+1

3

G(x + µ3 + ηi1 + ··· + ηik)

+ (−1)ℓ

�

(i1,...,iℓ)∈{1,2}ℓ

λi1 ···λiℓ



λℓ

3

F(x + ηi1 + ··· + ηiℓ)

∀x ∈ R,

where we suppose that λ3 &gt; max(λ1,λ2) and we denote ηi = µ3 − µi for

i = 1,2. To prove that a type (8) formula exists, we need to show that, for


SEMIPARAMETRIC MIXTURE MODEL

11

all x ∈ R, we have

F(x) = G(x + µ3)



λ3

+

+∞

�

k=1

(−1)k

�

(i1,...,ik)∈{1,2}k

λi1 ···λik



λk+1

3

G(x + µ3 + ηi1 + ··· + ηik).

(18)

Unfortunately, taking x ≥ 1, it is easy to see that (18) is not satisﬁed by

taking, for example, λ1 = λ2 = 4/15, λ3 = 7/15, µ1 = 0, µ2 = −1, µ3 = 1 and

F the c.d.f. of the uniform distribution on (−1,1). Note, however, that if the

inversion formula (18) is valid [this is the case, e.g., for 2max(λ1,λ2) &lt; λ3],

the methodology proposed in this section for the two-component case may

be applied.

4. Numerical study.

We consider two distinct problems. The ﬁrst is to

estimate λ given that µ1 and µ2 are known. In this case we use an explicit

formula for ˆG(n)

λ . In the second case we estimate θ = (λ,µ1,µ2) and we con-

sider ˜G(n)

θ , the regularized version of ˆG(n)

θ . Explicit formulae for ˆG(n)

λ

and

˜G(n)

θ

are given in (13) and (15). Recall that the computation of ˜G(n)

θ

involves

the choice of a bandwidth bn. All the simulation results have been obtained

with bn = n−1/4. This value is not optimal to estimate the density g but it is

compatible with the assumption √



nb2

n = O(1) needed to achieve the conver-

gence rate given in Theorem 3.3. Note that in all our simulations the variance

σ2

g under g is close to 1; our choice for bn is then close to the bandwidth

that minimizes the mean integrated squared error, usually approximated by

σg(4/3n)1/5 (see, e.g., [3]). It is known to be a good approximation for nor-

mal data and a Gaussian kernel but we cannot insure that it leads to an

optimal choice for our problem. For the real example of rainfall data given

at the end of this section, we used the bandwidth (bn = 3.84) provided by

the R software.

Choice of optimization method.

Problem (P1) attempts to ﬁnd an esti-

mate ˆλn of λ when µ1 and µ2 are known,

ˆλn = arg min

λ∈[0,1/2−d]

K(λ; ˆGn).

(19)

Problem (P2) attempts to ﬁnd an estimate ˆθn of θ = (λ,µ1,µ2),

ˆθn = arg min

θ∈Θ

˜Kr(θ; ˆGn).

(20)

Both problems require the minimization of a diﬀerentiable functional. As far

as problem (19) is concerned, numerical experiments indicate that K(·; ˆGn)

is strictly convex in [0,1/2 − d] and, thus, an unconstrained minimization


12

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

Table 1

Empirical mean and standard error (in brackets) of λ

estimates, obtained from 500 simulations of i.i.d.

samples of size n, for the (P1) problem with

µ1 = −1 and µ2 = 2



n



λ

0.15

0.25

0.35



100

0.151 (0.058)

0.256 (0.060)

0.347 (0.057)

400

0.148 (0.031)

0.252 (0.032)

0.349 (0.029)



algorithm can safely be used, with a starting point in this interval. We use

the quasi-Newton BFGS (Broyden, Fletcher, Goldfarb and Shanno) method

(see, e.g., [32]). In the second case, some experiments with the same un-

constrained method show that Kr(·; ˆGn) is not convex, and that Kr(·; ˆGn)

has local minima not belonging to Θ. So we use the constrained version of

the BFGS algorithm, where bounds on the variables can be taken into ac-

count. In both cases, we provide the gradient of the functional, which can

be readily computed from the explicit formulae given in Section 3.3. All the

computations are performed with Scilab.

Numerical result of the Monte Carlo study for Gaussian mixtures.

In

this section we denote by N(µ,σ2) a Gaussian distribution with mean µ

and variance σ2. The performance of our method is evaluated, via a Monte

Carlo study, on the Gaussian mixture

λ ∗ N(µ1,1) + (1 − λ) ∗ N(µ2,1),

for the (P1) problem (see Table 1) and in the (P2) problem (see Tables

2 and 3). More precisely, Table 1 summarizes the performance of our method

for diﬀerent values of λ, that is, λ = 0.15 (weakly bumped model), λ = 0.25

(moderately bumped model) and λ = 0.35 (strongly bumped model), when

Table 2

Empirical mean and standard error of (λ,µ1,µ2) semiparametric

estimates, obtained from 200 simulations of i.i.d. samples of

size n, for the (P2) problem with bn = n−1/4



n

(λ, µ1, µ2)

Empirical means

Standard errors



100

(0.15,−1,2)

(0.161,−0.948,2.030)

(0.052,0.365,0.137)

200

(0.15,−1,2)

(0.157,−1.027,2.023)

(0.035,0.283,0.101)

100

(0.25,−1,2)

(0.249,−1.011,2.009)

(0.060,0.289,0.154)

200

(0.25,−1,2)

(0.251,−1.000,2.010)

(0.041,0.195,0.101)

100

(0.35,−1,2)

(0.347,−0.988,1.990)

(0.056,0.230,0.145)

200

(0.35,−1,2)

(0.357,−0.976,2.012)

(0.046,0.176,0.114)




SEMIPARAMETRIC MIXTURE MODEL

13

Table 3

Empirical mean and standard error of (λ,µ1,µ2) maximum

likelihood estimates, obtained from 200 simulations of i.i.d.

samples of size n, for the (P2) problem with bn = n−1/4



n

(λ, µ1, µ2)

Empirical means

Standard errors



100

(0.15,−1,2)

(0.163,−0.987,2.018)

(0.054,0.431,0.138)

200

(0.15,−1,2)

(0.152,−1.013,2.004)

(0.035,0.283,0.089)

100

(0.25,−1,2)

(0.256,−1.008,2.020)

(0.051,0.268,0.132)

200

(0.25,−1,2)

(0.247,−1.003,2.004)

(0.046,0.204,0.114)

100

(0.35,−1,2)

(0.342,−1.041,1.980)

(0.054,0.231,0.161)

200

(0.35,−1,2)

(0.345,−1.009,1.991)

(0.041,0.159,0.111)



µ1 = −1 and µ2 = 2 are known. Table 2 summarizes the performance of our

method in estimating λ = 0.15,0.25,0.35, and µ1 = −1, µ2 = 2, while Table

3 gives the performance of the standard maximum likelihood approach in

the same framework.

Comments on Tables 1–3.

The results in Table 1 show ﬁrst that em-

pirical bias amounts to less than 1% of the true values, and that standard

errors are reasonably small. In order to clarify the analysis of the results

given in Table 1 and to quantify the inﬂuence of bumps on the estimation

eﬃciency, we can normalize the empirical standard errors with respect to

the true values of the parameters (std/λ). We obtain for λ = 0.15,0.25,0.35,

normalized empirical standard errors equal to 0.386, 0.240, 0.162, respec-

tively, for n = 100, and equal to 0.206, 0.128, 0.0828, for n = 400. These

indicators show, roughly speaking, that our estimation method is around

2.4 times more precise when λ = 0.35, in comparison with the case where

λ = 0.15, and 1.6 times more precise when λ = 0.35 in comparison with the

case where λ = 0.25. This shows that the nonnegligibility of one subpopu-

lation with respect to the other subpopulation improves the quality of the

estimators.

Concerning Tables 2 and 3, it is interesting to note that, when the lo-

cation parameters are unknown, the previous remark is no longer true. In

fact, even if the previous comments on empirical standard errors are clearly

relevant, it is worth noting that the smaller empirical bias is not obtained

for the highly bumped model, but for the moderately bumped model. To

explain this phenomenon, we can remark that when λ = 0.15, there are few

data to estimate µ1, whereas when λ = 0.35, even if there are many more

data to estimate µ1, this estimation is disturbed by the left tail of the dis-

tribution centered on µ2. Finally, it is with λ = 0.25 that we obtain the best

compromise and therefore the best estimates with regard to minimum bias.

In addition, we observe that the performance of the maximum likelihood


14

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

approach (which is known to be asymptotically eﬃcient) is in the range of

those obtained by our method, which illustrates the good behavior of our

semiparametric approach with respect to the parametric approach.

A trimodal example.

We use a basic symmetric density f which is already

a mixture, that is,

f(x) = 1



8ϕ(x + 4) + 3



4ϕ(x) + 1



8ϕ(x − 4),

where ϕ is the density function of the standard Gaussian distribution. The

density of the simulated data is taken as

g(x) = 1



4f(x) + 3



4f(x − 4)

∀x ∈ R.

We performed the estimation on a simulated sample of size n = 100. The

results are given in Figure 1. Figure 1(a) shows ˜f superimposed with the

true density function f and Figure 1(b) shows the reconstructed density

function ˜g(·) = ˆλ ˜f(· − ˆµ1) + (1 − ˆλ) ˜f(· − ˆµ2), using estimated values of λ, µ1

and µ2, superimposed with the true density function g. The optimization

required 31 iterations and 45 evaluations of Kr(·; ˆGn) and its gradient.

Standard errors for Euclidean parameters are computed by the Jackknife

method (see, e.g., [14]). We observe that for a reasonably small sample size

n = 100 the reconstructed mixture density ˜g almost yields the true density

g. The main diﬀerences appear around local modes and in the tails of g.

Numerical results on real data.

We use the average amount of precipi-

tation (rainfall) in inches for each of 70 United States (and Puerto Rican)

cities (from the Statistical Abstract of the United States, 1975; see [30]).

We consider two models. The ﬁrst is model (2) in which we denote by ˆλ,

ˆµ1, ˆµ2 and ˜f the estimators of λ, µ1, µ2 and f (the density function of the

c.d.f. F). The second model is a parametric version of model (2) in which

we assume that f is the density function of a centered Gaussian distribution

with variance equal to σ2. Estimators of unknown parameters of the second

model are denoted by ˜λ, ˜µ1, ˜µ2 and ˜σ2, and calculated according to the

maximum likelihood method.

Figure 2(a) shows ˜f, the estimator of f superimposed with the density

function of N(0, ˜σ2). Figure 2(b) shows ˆg, the empirical estimate of g (ob-

tained by the kernel method) superimposed with both the reconstructed

density ˆλ ˜f(· − ˆµ1) + (1− ˆλ) ˜f(· − ˆµ2) (using estimated values ˆλ, ˆµ1 and ˆµ2 of

λ, µ1 and µ2) and the density of the parametric model where the Euclidean

parameter is replaced by its maximum likelihood estimator. The optimiza-

tion required 32 iterations and 66 evaluations of Kr(·; ˆGn) and its gradient.

We observe in Figure 2(a) that the nonparametric density estimate ˆf is

provided with two symmetric small bumps at the beginnings of its tails,


SEMIPARAMETRIC MIXTURE MODEL

15

while the best ﬁtting Gaussian density does not obviously beneﬁt from this

kind of singularity and is sharper around the origin. In Figure 2(b) we can

see that these two additional bumps make the diﬀerence in the good ﬁt-

ting behavior of the reconstructed mixing distribution, except in a small

area around [−20,0] (the area of interest being [−20,75]), where the best

ﬁtting Gaussian mixture is slightly closer to ˆg. Notice also that the smaller

bump on the left of ˆg is clearly detected by our method, while the best

ﬁtting Gaussian mixture almost misses this singularity. Again, standard er-

rors (given in brackets) for Euclidean parameters are computed using the

Jackknife method.

5. Proofs.





Fig. 1.

Estimated parameters ˆµ1 = 0.691 (0.760), ˆµ2 = 3.728 (0.153), ˆλ = 0.232 (0.079)

for n = 100 and bn = n−1/4, the results in parentheses corresponding to the empiri-

cal standard errors. (a) Graph of

˜f (solid ) and graph of f (dashed ). (b) Graph of

ˆλ ˜f(· − ˆµ1) + (1 − ˆλ) ˜f(· − ˆµ2) (solid ) and graph of g (dashed ).


16

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE





Fig. 2.

Estimated parameters for model (2): ˆµ1 = 13.107 (3.299), ˆµ2 = 39.056 (1.395),

ˆλ = 0.171 (0.078) (the bandwidth is ﬁxed at 3.84). Estimated parameters for model

λ ∗ N(µ1,σ2) + (1 − λ) ∗ N(µ2,σ2):

˜µ1 = 15.715

(2.220),

˜µ2 = 40.773

(1.297),

˜λ = 0.235 (0.060) and ˜σ = 8.504 (1.187), the results in parentheses corresponding

to the empirical standard errors. (a) Graph of the nonparametric density estima-

tor

ˆf

and graph of the density of N(0, ˜σ2) (dashed). (b) Graph of ˆg (dashed ),

graph

of

ˆλ ˜f(· − ˆµ1) + (1 − ˆλ) ˜f(· − ˆµ2)

(solid )

and

graph

of

the

density

of

˜λN(˜µ1, ˜σ2) + (1 − ˜λ)N(˜µ2, ˜σ2) (dash-dot).

5.1. Notation and preliminary results.

According to whether we are look-

ing at density function estimation or c.d.f. estimation, the operators Aθ and

A−1

θ , given in (10), are deﬁned, respectively, on spaces L1(R) or L∞(R) (en-

dowed with the usual norms ∥ · ∥1 and ∥ · ∥∞, resp.). Independently of the

space under consideration, it is straightforward to check that the norms (de-

noted ||| · |||) of operators Aθ and A−1

θ , for λ ∈ [0,1/2 − d] and d ∈ (0,1/2),


SEMIPARAMETRIC MIXTURE MODEL

17

satisfy

|||Aθ||| ≤ 1

and

|||A−1

θ ||| ≤

1



1 − 2λ ≤ 1



2d.

(21)

Let us recall some basic results on ˆGn and ˜Gn deﬁned respectively by

(12) and (14). From well-known results on empirical processes (see, e.g.,

[37]), for general distribution functions G, we have

√



n∥ ˆGn − G∥∞ = OP (1),

(22)

and the law of iterated logarithm (LIL)

∥ ˆGn − G∥∞ = Oa.s.

��



log log(n)



n

�

.

(23)

If ∥f∥∞ &lt; ∞, and if f has derivative f (1) with ∥f (1)∥∞ &lt; ∞, the same holds

for g, and by Corollary 1, page 766 in [37], if q has compact support, and if

√



nb2

n = O(1), then we have

√



n∥ ˆGn − ˜Gn∥∞ = Oa.s.(1).

(24)

Hence, the result (23) holds for ˜Gn.

In the remainder of this paper we denote by ˙L and ¨L the ﬁrst- and second-

order derivatives of a general function L with respect to λ ∈ [0,1/2 − d] for

problem (P1) and θ ∈ Θ = [0,1/2 − d] × X for problem (P2) (see Section

3.3 for assumptions on the Euclidean parameter space). In the sequel | · |2

denotes the Euclidean norm.

Lemma 5.1.

There exists c ∈ (0,+∞), such that for all θ ∈ Θ and n ≥ 1,

we have

∥ ˆG(n)

θ

− Gθ∥∞ ≤ c∥ ˆGn − G∥∞.

(25)

Proof.

Straightforward, since we have

∥ ˆG(n)

θ

− Gθ∥∞ = ∥AθSr[A−1

θ ( ˆGn − G)]∥∞

≤ |||A−1

θ ||| × ∥ ˆGn − G∥∞

≤ 1



2d∥ ˆGn − G∥∞.

□

Lemma 5.2.

Under C1, the mapping θ �→ Gθ(x) is Lipschitz on Θ uni-

formly in x ∈ R, and the contrast function K is Lipschitz on Θ.


18

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

Proof.

For all (θ,θ′) ∈ Θ2, we have |K(θ) − K(θ′)| ≤ C∥Gθ − Gθ′∥∞.

Therefore, it is suﬃcient to prove that θ �→ Gθ(·) is uniformly Lipschitz on

Θ. Simple calculations lead to

∥Gθ − Gθ′∥∞ ≤ ∥AθSrA−1

θ G − Aθ′SrA−1

θ G∥∞ + ∥A−1

θ G − A−1

θ′ G∥∞.

(26)

First we prove that the ﬁrst term on the right-hand side of (26) is Lipschitz.

Let us remark now that, for all bounded functions H, we have, for all x ∈ R,

|AθH(x) − Aθ′H(x)| ≤ 2|λ − λ′| × ∥H∥∞

+ sup

x∈R

max

i=1,2|H(x) − H(x − (µi − µ′

i))|.

(27)

On the other hand, noticing η = µ2 −µ1 (resp. η′ = µ′

2 −µ′

1), we remark that

for all θ ∈ Θ, A−1

θ G satisﬁes, for all (z,z′) ∈ R2,

|SrA−1

θ G(z) − SrA−1

θ G(z′)|

= |A−1

θ G(−z) − A−1

θ G(−z′)|

=

�����

1



1 − λ

∞

�

k=0

�

−

λ



1 − λ

�k

(G(−z + µ2 + kη) − G(−z′ + µ2 + kη))

�����

≤ 1



2d sup

y∈R

|G(y) − G(y − z + z′)|

≤ 1



2d|G|Lip|z − z′|,

because under C1 G is Lipschitz with Lipschitz constant |G|Lip. Now replac-

ing H in (27) by SrA−1

θ G, it follows from the above inequality that

|AθSrA−1

θ G(x) − Aθ′SrA−1

θ G(x)| ≤ C|θ − θ′|2.

(28)

It remains to be proved that the second term on the right-hand side of

inequality (26) is Lipschitz. We have

|A−1

θ G(x) − A−1

θ′ G(x)|

≤

�����

1



1 − λ

∞

�

k=0

�

−

λ



1 − λ

�k

(G(x + µ2 + kη) − G(x + µ′

2 + kη′))

�����

+

�����

�

1



1 − λ

∞

�

k=0

�

−

λ



1 − λ

�k

−

1



1 − λ′

∞

�

k=0

�

−

λ′



1 − λ′

�k�

× G(x + µ2 + kη)

�����.

G is supposed to be Lipschitz. We have, for all x ∈ R,

|G(x + µ2 + kη) − G(x + µ′

2 + kη′)| ≤ |G|Lip(k + 1)|θ − θ′|2;


SEMIPARAMETRIC MIXTURE MODEL

19

thus, we obtain, by the two previous inequalities,

|A−1

θ G(x) − A−1

θ′ G(x)|

≤

c1



1 − λ′

∞

�

k=0

(k + 1)

�

λ′



1 − λ′

�k

|θ − θ′|2 + c2∥G∥∞|λ − λ′|

≤ c3|θ − θ′|2,

(29)

where c1, c2 and c3 are nonnegative real constants. From inequalities (26)–

(29), it follows that the function θ �→ Gθ(x) is Lipschitz on Θ uniformly in

x ∈ R, and thus, K is Lipschitz on Θ.

□

Lemma 5.3.

For any α &gt; 0, under C1 we have

sup

θ∈Θ

|K(θ; ˆGn) − K(θ)| = oa.s.(n−1/2+α).

(30)

The same result holds replacing K(·; ˆGn) by Kr(·; ˆGn). It is an obvious

consequence of properties of ˜Gn, since by the LIL result (23) for ˆGn and

(24), we have ∥ ˜Gn − G∥∞ = Oa.s.(

�



n−1 log log n).

Proof of Lemma 5.3.

Considering the random variables Zi(θ) =

(Gθ(Xi) − G(Xi))2 and using Lemma 5.1, we show that

|K(θ; ˆGn) − K(θ)| ≤ c∥ ˆGn − G∥∞ + sup

θ∈Θ

�����

1



n

n

�

i=1

(Zi(θ) − E(Zi(θ)))

�����,

where c is a nonnegative constant. The two terms on the right-hand sides no

longer depend on θ. The ﬁrst tends to 0 with the desired rate of convergence

by the LIL result given in (23). The second term is the supremum of an

empirical process indexed by the functional class H = {h(·,θ) = (Gθ(·) −

G(·))2,θ ∈ Θ} of Lipschitz bounded functions. Indeed, we have, by Lemma

5.2,

|h(x,θ) − h(x,θ′)| ≤ |Gθ(x) + Gθ′(x) − 2Gθ(x)| × |Gθ(x) − Gθ′(x)|

≤ c|θ − θ′|2.

Let (εn)n≥1 be a sequence of real numbers decreasing to 0. It follows by a

Bernstein type theorem of van der Vaart and Wellner ([41], page 246) that

there exist nonnegative constants A and B such that

P

�

sup

θ∈Θ

�����

1



n

n

�

i=1

(Zi(θ) − EZi(θ))

����� &gt; εn

�

≤ A(√



nεn)B exp(−2nε2

n).

It follows that if εn = n−1/2+α with α &gt; 0, we get, by the Borel–Cantelli

lemma,

sup

θ∈Θ

�����

1



n

n

�

i=1

(Zi(θ) − EZi(θ))

����� = oa.s.(n−1/2+α),


20

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

which concludes the proof.

□

Lemma 5.4.

Under C1, for k = 1,2, there exists a real constant c &gt; 0

such that, for all (λ1,λ2) ∈ [0,1/2 − d]2 and L ∈ L∞(R),

����

� ∂k



∂λk AλSrA−1

λ

�

λ=λ1

L −

� ∂k



∂λk AλSrA−1

λ

�

λ=λ2

L

����

∞

≤ c|λ1 − λ2| × ∥L∥∞.

A straightforward consequence of the above lemma is that, for k = 1,2

and L ∈ L∞(R), there exists another real constant c &gt; 0 such that

����

∂k



∂λk AλSrA−1

λ L

����

∞

≤ c∥L∥∞.

(31)

Proof of Lemma 5.4.

We prove the uniform Lipschitz property only

for the case where k = 1, since the case where k = 2 uses the same technical

arguments. For all (λ1,λ2) ∈ [0,1/2−d]2, L ∈ L∞(R), and all x ∈ R, we have

����

� ∂



∂λAλSrA−1

λ

�

λ=λ1

L(x) −

� ∂



∂λAλSrA−1

λ

�

λ=λ2

L(x)

����

≤

����

1



(1 − λ1)2 −

1



(1 − λ2)2

����

×

�

k≥0

(k + 1)

�

λ1



1 − λ1

�k

(32)

× |L(−x + µ1 + µ2 + kη) − L(−x + 2µ2 + (k + 1)η)|

+

1



(1 − λ2)2

�

k≥0

(k + 1)

����

� −λ1



1 − λ1

�k

−

� −λ2



1 − λ2

�k����

× |L(−x + µ1 + µ2 + kη) − L(−x + 2µ2 + (k + 1)η)|.

By the mean value theorem, there exist ¯λ and ˜λ lying on the line segment

with extremities λ1 and λ2 such that

����

1



(1 − λ1)2 −

1



(1 − λ2)2

���� ≤

2



(1 − ¯λ)3 |λ1 − λ2|,

and for all k ≥ 0,

����

� −λ1



1 − λ1

�k

−

� −λ2



1 − λ2

�k���� ≤ k

�

˜λ



1 − ˜λ

�k−1�

1



1 − ˜λ

�2

|λ1 − λ2|.

Using the above inequalities with (32), we obtain

����

� ∂



∂λAλSrA−1

λ

�

λ=λ1

L −

� ∂



∂λAλSrA−1

λ

�

λ=λ2

L

����

∞

≤ 12∥L∥∞|λ1 − λ2|



d3

,

which concludes the proof.

□


SEMIPARAMETRIC MIXTURE MODEL

21

5.2. Proof of Theorem 2.1.

Step 1.

Let {sin(α1t),...,sin(αpt)} be a family of p functions deﬁned on

R. These functions are linearly independent if and only if we have

αi ̸= 0

for 1 ≤ i ≤ p

and

|αi| ̸= |αj|

for 1 ≤ i &lt; j ≤ p.

(33)

Indeed, suppose that for β1,...,βp in R we have

p

�

i=1

βi sin(αit) = 0

∀t ∈ R.

Then, taking the derivative of the above expression with respect to t at

orders 1,3,...,2p − 1, we get at t = 0 the system of linear equations

p

�

i=1

βiα2j+1

i

= 0

for 0 ≤ j ≤ p − 1.

The corresponding determinant is a Vandermonde type determinant diﬀer-

ent from 0 if and only if (33) is satisﬁed.

Step 2.

We denote by Φ and Φ′ the characteristic functions of F and F ′,

respectively. Calculating the characteristic function of the two sides in (5),

we get, for all t ∈ R,

(λexp(itµ1) + (1 − λ)exp(itµ2))Φ(t)

= (λ′ exp(itµ′

1) + (1 − λ′)exp(itµ′

2))Φ′(t).

(34)

Since F and F ′ are c.d.f.’s of symmetric distributions, their characteristic

functions are real continuous functions equal to 1 at t = 0. We have from

(34) that the imaginary part of

(λexp(itµ1) + (1 − λ)exp(itµ2))(λ′ exp(−itµ′

1) + (1 − λ′)exp(−itµ′

2))

is equal to 0 in a neighborhood of 0. Then we have

λλ′ sin((µ1 − µ′

1)t) + λ(1 − λ′)sin((µ1 − µ′

2)t)

+ (1 − λ)λ′ sin((µ2 − µ′

1)t) + (1 − λ)(1 − λ′)sin((µ2 − µ′

2)t) = 0

(35)

on the whole real line, by analyticity of sine functions. We shall now consider

two cases.

Case 1: λ = 0.

Then (35) reduces to

λ′ sin((µ2 − µ′

1)t) + (1 − λ′)sin((µ2 − µ′

2)t) = 0.

(36)

If λ′ &gt; 0, then we have 1 − λ′ &gt; λ′ &gt; 0, and by step 1 we need to consider

the following cases:

• µ2 = µ′

2 or µ2 = µ′

1, hence by (36) µ′

2 = µ′

1, which is not admissible.

• |µ2 = µ′

1| = |µ2 = µ′

2|, which by (36) leads to λ′ + (1 − λ′) = 0 (impossible)

or λ′ − (1 − λ′) = 0 (not admissible).

It follows that λ′ = λ = 0 and, hence, by (35) µ′

2 = µ2.


22

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

Case 2: λ &gt; 0.

From Case 1, we also have λ′ &gt; 0. Therefore, it remains

to show that if µ1 ̸= µ2, µ′

1 ̸= µ′

2, (λ,λ′) ∈ (0,1/2)2 and that, for all t ∈ R,

λλ′ sin((µ1 − µ′

1)t) + λ(1 − λ′)sin((µ1 − µ′

2)t)

+(1 − λ)λ′ sin((µ2 − µ′

1)t) + (1 − λ)(1 − λ′)sin((µ2 − µ′

2)t) = 0,

(37)

we have (λ,µ1,µ2) = (λ′,µ′

1,µ′

2). If we denote β1 = λλ′, β2 = λ(1 − λ′), β3 =

λ′(1 − λ) and β4 = (1 − λ)(1 − λ′), then (37) is equivalent to

β1 sin(αt) + β2 sin((α′ − η)t)

+ β3 sin((α + η)t) + β4 sin(α′t) = 0

∀t ∈ R,

(38)

where α = µ1 − µ′

1, α′ = µ2 − µ′

2 and η = µ2 − µ1. It is straightforward to see

that if α = α′ = 0, then λ = λ′. Then it remains to show that (α,α′) = (0,0)

is not admissible. To avoid a lengthy proof, we consider only the case α = 0

and α′ ̸= 0. The case α ̸= 0 and α′ = 0 is its symmetric counterpart and the

case α ̸= 0 and α′ ̸= 0 involves substantial calculations but is straightforward.

Hence, if we suppose that α = 0 and α′ ̸= 0, equation (38) reduces to

β2 sin((α′ − η)t) + β3 sin(ηt) + β4 sin(α′t) = 0

∀t ∈ R.

(39)

Since α′ and η are nonnull, by Step 1, we have to consider the following

cases:

• α′ = η: hence, (β3 + β4)sin(ηt) = 0 for all t ∈ R. Then β3 + β4 = 0, which

is not possible.

• |α′ − η| = |η|: hence, α′ = 2η. Then (39) reduces to

(β2 + β3)sin(ηt) + β4 sin(2ηt) = 0

∀t ∈ R,

which, again by Step 1, cannot be satisﬁed for all t ∈ R.

• Cases |α′ − η| = |α′| and |η| = |α′| lead respectively to α′ = η/2 and η =

−α′, hence, as in the previous case, the resulting equations cannot be

satisﬁed for all t ∈ R.

Step 3.

Now, since λ ∈ [0,1/2) we have |λexp(itµ1)+(1−λ)exp(itµ2)| ≥

1 − 2λ. Then Φ = Φ′ and, ﬁnally, F and F ′ are equal.

5.3. Proofs of Theorem 3.1 and Corollary 3.1.

Proof of Theorem 3.1.

Let us write ΦH for the characteristic func-

tion deﬁned by ΦH(t) =

�

R exp(itx)dH(x) for all t ∈ R. Using the deﬁnitions

of Aθ and A−1

θ

in (10), we obtain

ΦGθ(t) =

λexp(itµ1) + (1 − λ)exp(itµ2)



λexp(−itµ1) + (1 − λ)exp(−itµ2)ΦG(−t)

∀t ∈ R.

(40)


SEMIPARAMETRIC MIXTURE MODEL

23

Moreover, because ΦG(t) = (λ0 exp(itµ0

1)+(1−λ0)exp(itµ0

2))ΦF0(t) and ΦF0

is an even function, ΦGθ(t) = ΦG(t) for all t ∈ R implies that the imaginary

part of

(λexp(itµ1) + (1 − λ)exp(itµ2))(λ0 exp(−itµ0

1) + (1 − λ0)exp(−itµ0

2))

is null in a neighborhood of 0. Finally, by Step 2 of the proof of Theorem

2.1, we conclude that θ = θ0.

□

Proof of Corollary 3.1.

Given the assumptions concerning F0, we

show that Gθ is a continuous function. By Theorem 3.1, if θ ̸= θ0, there

exists x0 ∈ R such that G(x0) ̸= Gθ(x0), and there exist ε &gt; 0 and α &gt; 0

such that |G(x) − Gθ(x)| &gt; ε on [x0 − α,x0 + α]. It follows that

K(θ) ≥ ε2

� x0+α

x0−α

dG(x) = ε2(G(x0 + α) − G(x0 − α)) &gt; 0.

Otherwise, if θ = θ0 it is straightforward to check that K(θ) = 0.

□

5.4. Proof of Theorem 3.2.

Since the consistency proof for ˆλn follows

the lines of the consistency proof for ˆθn of problem (P2), it is omitted. For

the remainder of this proof, we therefore suppose that ˆλn converges almost

surely to λ0. By a ﬁrst-order Taylor expansion of ˙K(·; ˆGn) around ˆλn, we

have

¨K(λ∗

n; ˆGn)√



n(ˆλn − λ0) = −√



n ˙K(λ0; ˆGn),

(41)

where λ∗

n lies on the line segment with extremities λ0 and ˆλn. The desired

result follows by proving the two statements

√



n ˙K(λ0; ˆGn) = OP (1)

(42)

and

¨K(λ∗

n; ˆGn) a.s.

−→ 2

�

R

˙G2

λ0 dG &gt; 0.

(43)

Result (42) follows from

| ˙K(λ0; ˆGn)| =

����

�

R

2 ˙ˆG

(n)

λ0 (x)( ˆG(n)

λ0 (x) − ˆGn(x))d ˆGn(x)

����

≤ 2∥ ˆG(n)

λ0 − ˆGn∥∞ × ∥ ˙ˆG

(n)

λ0 ∥∞

≤ 2∥Aλ0SrA−1

λ0 [ ˆGn − G]∥∞ ×

����

� ∂



∂λAλSrA−1

λ

�

λ=λ0

ˆGn

����

∞

.

The above inequality with Lemma 5.1 and (31) give the existence of a non-

negative constant c such that | ˙K(λ0; ˆGn)| ≤ c∥ ˆGn − G∥∞. Thus, from result


24

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

(22), we get (42). In order to prove (43), let us write the second derivative

of K(·; ˆGn) at point λ,

¨K(λ; ˆGn) = 2

�

R

¨ˆG(n)

λ ( ˆG(n)

λ

− ˆGn)d ˆGn + 2

�

R

( ˙ˆG(n)

λ )2 d ˆGn.

We have���� ¨K(λ∗

n; ˆGn) − 2

�

R

˙G2

λ0 dG

����

≤ | ¨K(λ∗

n; ˆGn) − ¨K(λ0; ˆGn)| +

���� ¨K(λ0; ˆGn) − 2

�

R

˙G2

λ0 dG

����.

(44)

By very simple calculations, we show that

| ¨K(λ∗

n; ˆGn) − ¨K(λ0; ˆGn)| ≤ c|λ∗

n − λ0|,

where c is a nonnegative constant arising from Lemma 5.4, (31) and the

fact that ˆGn is a cumulative distribution function. By the above inequality

and the strong consistency of ˆλn, we conclude that ¨K(λ∗

n; ˆGn) − ¨K(λ0; ˆGn)

converges almost surely to 0.

Concerning the second term of the right-hand side of (44), let us write

���� ¨K(λ0; ˆGn) − 2

�

R

˙G2

λ0 dG

����

≤ 2

����

�

R

˙G2

λ0 d ˆGn −

�

R

˙G2

λ0 dG

���� + 2∥¨ˆG(n)

λ0 ∥∞ × ∥ ˆG(n)

λ0 − ˆGn∥∞

+ 2(∥ ˙ˆG(n)

λ0 ∥∞ + ∥ ˙Gλ0∥∞) × ∥ ˙ˆG(n)

λ0 − ˙Gλ0∥∞.

Let us investigate the three terms on the right-hand side of the above in-

equality. From Lemmas 5.1 and 5.4, the second term is bounded, up to a

multiplicative nonnegative constant, by

(∥ ˆG(n)

λ0 − Gλ0∥∞ + ∥ ˆGn − Gλ0∥∞) ≤ (1 + |||Aλ0SrA−1

λ0 |||) × ∥ ˆGn − G∥∞,

and then, tends to 0 almost surely, by using (21) and the LIL result given in

(23). By similar arguments, we show that the third term has the same prop-

erty. The ﬁrst term is a centered empirical mean of i.i.d. random variables

which, by Lemma 5.4, have a ﬁnite mean. Therefore, this term converges

almost surely to 0 by the strong law of large numbers. Thus, it follows that

���� ¨K(λ0; ˆGn) − 2

�

R

˙G2

λ0 dG

���� = oa.s.(1).

We conclude the proof, noticing that ¨K(λ0) &gt; 0 [the proof, under C1, is sim-

ilar to the proof of positive deﬁnitiveness of ¨K(θ0) in Section 5.5; therefore,

it is omitted], and then

¨K(λ0) = 2

�

R

˙G2

λ0 dG &gt; 0.


SEMIPARAMETRIC MIXTURE MODEL

25

5.5. Proof of Theorem 3.3.

Proof of the consistency.

Our method is based on a consistency proof

for miminum contrast estimators by Dacunha–Castelle and Duﬂo ([9], pages

94–96). Let us consider a countable dense set D in Θ. Then infθ∈Θ Kr(θ; ˆGn) =

infθ∈D Kr(θ; ˆGn) is a measurable random variable. We deﬁne, in addition,

the random variable

W(n,ξ) = sup{|Kr(θ; ˆGn) − Kr(θ′; ˆGn)|;(θ,θ′) ∈ D2,|θ − θ′|2 ≤ ξ},

and recall that K(θ0) = 0. Let us consider a nonempty open ball B0 centered

on θ0 such that K is bounded from below by a positive real number 2ε on

Θ\B0. Let us consider a sequence (ξp)p≥1 decreasing to zero, and take p

such that there exists a covering of Θ\B0 by a ﬁnite number ℓ of balls

(Bi)1≤i≤ℓ with centers θi ∈ Θ, i = 1,...,ℓ, and radius less than ξp. Following

Dacunha–Castelle and Duﬂo [9], we have

limsup

n

{ˆθn /∈ B0} ⊆ limsup

n

{W(n,ξp) &gt; ε}

∪ limsup

n

�

inf

1≤i≤ℓ(Kr(θi; ˆGn) − Kr(θ0; ˆGn)) ≤ ε

�

.

(45)

By the uniform convergence result of Lemma 5.3, we have

P

�

limsup

n

�

inf

1≤i≤ℓ(Kr(θi; ˆGn) − Kr(θ0; ˆGn)) ≤ ε

��

= 0.

(46)

Because K is Lipschitz on Θ by Lemma 5.2, we have that, for suﬃciently

large p′, |K(θ) − K(θ′)| ≤ ε/2 for all (θ,θ′) such that |θ − θ′|2 ≤ ξp′. This

implies

limsup

n

{W(n,ξp′) &gt; ε}

⊆ limsup

n

�

2sup

θ∈Θ

|Kr(θ; ˆGn) − K(θ)| + |K(θ) − K(θ′)| &gt; ε

�

⊆ limsup

n

�

2sup

θ∈Θ

|Kr(θ; ˆGn) − K(θ)| &gt; ε/2

�

,

and by Lemma 5.3 we have

P

�

limsup

n

�

2sup

θ∈Θ

|Kr(θ; ˆGn) − K(θ)| &gt; ε/2

��

= 0,

which leads to

P

�

limsup

n

{W(n,ξp′) &gt; ε}

�

= 0.

(47)

By (45)–(47), we have proved the strong consistency of the contrast estima-

tor ˆθn.


26

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

Proof for the convergence rate.

By standard Lebesgue theory, it is straight-

forward to show that, under C2, the contrast function K is twice continu-

ously diﬀerentiable on Θ. If ¨K(θ0) is positive deﬁnite, by Corollary 3.1 and

a Taylor expansion of K of order 2 at θ0, there exist η &gt; 0 and α &gt; 0 such

that, for all u satisfying |u|2 &lt; η and θ0 + u ∈

◦

Θ,

K(θ0 + u) ≥ α|u|2

2.

(48)

For a column vector v = (v1,v2,v3)T ∈ R3, we have

vT ¨K(θ0)v = 2

�

R

(vT ˙Gθ0(x))2 dG(x) ≥ 0.

(49)

By C2, we obtain that x �→ ˙Gθ0(x) is continuous and that G is continuous

and strictly increasing on R. Thus, (49) implies that x �→ vT

˙

Gθ0(x) is the

null function if vT ¨K(θ0)v = 0. Because under C2 we have f ′

0 ∈ L1(R), it is

easy to show that vT ˙gθ0(·) ∈ L1(R), where gθ = AθSfA−1

θ g. Moreover, using

the Lebesgue derivation theorem and (40), and denoting η0 = µ0

2 − µ0

1, we

obtain

ΦvT ˙Gθ0(t) = vT ˙ΦGθ0(t)

=

2ΦG(−t)



(λ0 exp(−itµ0

1) + (1 − λ0)exp(−itµ0

2))2

× [cos(η0t)(v1(1 − 2λ0) + it(v2(1 − λ0) + v3λ0))

+ it(v2λ0 + v3(1 − λ0))].

Therefore, vT ¨K(θ0)v = 0 implies that ΦvT ˙Gθ0 is the null function. Because

ΦG(−t)/(λ0 exp(−itµ0

1)+(1−λ0)exp(−itµ0

2))2 is not null in a neighborhood

of 0, we obtain that the right multiplicative term of the right-hand side of

the above equality is null in a neighborhood of 0, which in turn implies that

v = 0. Thus ¨K(θ0) is positive deﬁnite and (48) holds.

Now, let us consider B0(ηn), the open ball centered at θ0 with radius

ηn &gt; 0. Notice that, for all θ ∈ Θ \ B0(ηn), we have |θ − θ0|2 ≥ ηn. Then we

write the event inclusions

{ˆθn /∈ B0(ηn)} ⊆

�

inf

θ∈Θ\B0(ηn)Kr(θ; ˆGn) &lt; Kr(θ0; ˆGn)

�

⊆

�

inf

θ∈Θ\B0(ηn)K(θ) − sup

θ∈Θ

|Kr(θ; ˆGn) − K(θ)| &lt; Kr(θ0; ˆGn)

�

⊆

�

inf

θ∈Θ\B0(ηn)K(θ) &lt; 2sup

θ∈Θ

|Kr(θ; ˆGn) − K(θ)|

�

⊆

�

inf

θ∈Θ\B0(ηn)K(θ) &lt; γn

�

∪

�

γn ≤ 2sup

θ∈Θ

|Kr(θ; ˆGn) − K(θ)|

�


SEMIPARAMETRIC MIXTURE MODEL

27

for any arbitrary sequence γn. Thus, we have

limsup

n

{ˆθn /∈ B0(ηn)} ⊆ limsup

n

�

inf

θ∈Θ\B0(ηn)K(θ) &lt; γn

�

∪ limsup

n

�

γn ≤ 2sup

θ∈Θ

|Kr(θ; ˆGn) − K(θ)|

�

.

Choosing now γn = n−1/2+α and ηn = n−1/4+β/2, with 0 &lt; α &lt; β taken ar-

bitrarily small, it follows from (48) and the uniform almost sure rate of

convergence of Kr( ˆGn) toward K, given in Lemma 5.3, that

P

�

limsup

n

�

inf

θ∈Θ\B0(ηn)K(θ) &lt; γn

��

= 0

and

P

�

limsup

n

�

γn ≤ 2sup

θ∈Θ

|Kr(θ; ˆGn) − K(θ)|

��

= 0.

In conclusion, ˆθn converges almost surely toward θ0 at rate n−1/4+δ, with

δ &gt; 0 chosen arbitrarily small.

5.6. Proof of Theorem 3.4.

Proof of (i) and (ii).

We have

ˆFn − F0 = 1



2(I + Sr)[A−1

ˆθn

ˆGn − A−1

θ0 G].

Thus, there exists a nonnegative real constant c such that

∥ ˆFn − F0∥∞ ≤ ∥A−1

ˆθn ( ˆGn − G)∥∞ + ∥(A−1

ˆθn − A−1

θ0 )G∥∞

≤ |||A−1

ˆθn ||| × ∥ ˆGn − G∥∞ + c|ˆθn − θ0|2

≤ 1



2d∥ ˆGn − G∥∞ + c|ˆθn − θ0|2,

where the second inequality follows from (29) in the proof of Lemma 5.2

and the last inequality follows from (21), using the fact that G is Lipschitz.

Finally, the above inequality together with (22) [resp. (23)] and Theorem

3.2 (resp. Theorem 3.3) yield result (i) [resp. result (ii)].

Proof of (iii).

By the Devroye [12] L1-consistency result, we have

∥ˆgn − g∥1 =

�

R

|ˆgn(x) − g(x)|dx a.s.

−→ 0

(50)


28

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

as n → +∞, providing that bn → 0 and nbn → +∞. Then we can write

∥ ˆfn − f0∥1 = ∥A−1

ˆθn ˆgn − A−1

θ0 g∥1

≤ ∥A−1

ˆθn (ˆgn − g)∥1 + ∥(A−1

ˆθn − A−1

θ0 )g∥1

≤ 1



2d∥ˆgn − g∥1 + C|ˆθn − θ0|2,

where the last inequality comes from (29), because f ′

0 ∈ L1(R) and, thus,

the same holds for g. We conclude with Theorems 3.2 and 3.3, and (50).

REFERENCES

[1] Azzalini, A. and Bowman, A. W. (1990). A look at some data on the Old Faithful

Geyser. Appl. Statist. 39 357–365.

[2] Barndorff-Nielsen, O. (1965). Identiﬁability of mixtures of exponential families.

J. Math. Anal. Appl. 12 115–121. MR0183058

[3] Bowman, A. W. and Azzalini, A. (1997). Applied Smoothing Techniques for Data

Analysis. Oxford Univ. Press.

[4] Cerrito, P. B. (1992). Using stratiﬁcation to estimate multimodal density functions

with applications to regression. Comm. Statist. Simulation Comput. 21 1149–

1164.

[5] Chandra, S. (1977). On the mixtures of probability distributions. Scand. J. Statist.

4 105–112. MR0474583

[6] Chen, J. (1995). Optimal rate of convergence for ﬁnite mixture models. Ann. Statist.

23 221–233. MR1331665

[7] Cohen, A. C. (1967). Estimation in mixtures of two normal distributions. Techno-

metrics 9 15–28. MR0216626

[8] Cruz-Medina, I. R. and Hettmansperger, T. P. (2004). Nonparametric estima-

tion in semi-parametric univariate mixture models. J. Stat. Comput. Simul. 74

513–524. MR2073229

[9] Dacunha-Castelle, D. and Duflo, M. (1983). Probabilit´es et Statistique 2.

Probl`emes `a temps mobile. Masson, Paris. MR0732786

[10] Dacunha-Castelle, D. and Gassiat, E. (1999). Testing the order of a model us-

ing locally conic parametrization: Population mixtures and stationary ARMA

processes. Ann. Statist. 27 1178–1209. MR1740115

[11] Day, N. E. (1969). Estimating the components of a mixture of normal distributions.

Biometrika 56 463–474. MR0254956

[12] Devroye, L. (1983). The equivalence of weak, strong and complete convergence in

L1 for kernel density estimates. Ann. Statist. 11 896–904. MR0707939

[13] Diebolt, J. and Robert, C. P. (1994). Estimation of ﬁnite mixture distributions

through Bayesian sampling. J. Roy. Statist. Soc. Ser. B 56 363–375. MR1281940

[14] Efron, B. and Tibshirani, R. J. (1993). An Introduction to the Bootstrap. Chapman

and Hall, London. MR1270903

[15] Escobar, M. D. and West, M. (1995). Bayesian density estimation and inference

using mixtures. J. Amer. Statist. Assoc. 90 577–588. MR1340510

[16] Everitt, B. S. and Hand, D. J. (1981). Finite Mixture Distributions. Chapman

and Hall, London. MR0624267

[17] Hall, P. (1981). On the nonparametric estimation of mixture proportions. J. Roy.

Statist. Soc. Ser. B 43 147–156. MR0626759


SEMIPARAMETRIC MIXTURE MODEL

29

[18] Hall, P. and Zhou, X.-H. (2003). Nonparametric estimation of component distri-

butions in a multivariate mixture. Ann. Statist. 31 201–224. MR1962504

[19] Hunter, D. R., Wang, S. and Hettmansperger, T. P. (2006). Inference for mix-

tures of symmetric distributions. Ann. Statist. To appear.

[20] Kitamura, Y. (2004). Nonparametric identiﬁability of ﬁnite mixtures. Preprint.

[21] Lancaster, T. and Imbens, G. (1996). Case-control studies with contaminated

controls. J. Econometrics 71 145–160. MR1381079

[22] Lemdani, M. and Pons, O. (1999). Likelihood ratio tests in contamination models.

Bernoulli 5 705–719. MR1704563

[23] Leroux, B. G. (1992). Consistent estimation of a mixing distribution. Ann. Statist.

20 1350–1360. MR1186253

[24] Lindsay, B. G. (1983). The geometry of mixture likelihoods: A general theory. Ann.

Statist. 11 86–94. MR0684866

[25] Lindsay, B. G. (1983). The geometry of mixture likelihoods. II. The exponential

family. Ann. Statist. 11 783–792. MR0707929

[26] Lindsay, B. G. and Basak, P. (1993). Multivariate normal mixtures: A fast consis-

tent method of moments. J. Amer. Statist. Assoc. 88 468–476. MR1224371

[27] Lindsay, B. G. and Lesperance, M. L. (1995). A review of semiparametric mixture

models. J. Statist. Plann. Inference 47 29–39. MR1360957

[28] McLachlan, G. J. and Basford, K. E. (1988). Mixture Models. Inference and

Applications to Clustering. Dekker, New York. MR0926484

[29] McLachlan, G. J. and Peel, D. (2000). Finite Mixture Models. Wiley, New York.

MR1789474

[30] McNeil, D. R. (1977). Interactive Data Analysis. Wiley, New York.

[31] Murray, G. D. and Titterington, D. M. (1978). Estimation problems with data

from a mixture. Appl. Statist. 27 325–334.

[32] Nocedal, J. and Wright, S. J. (1999). Numerical Optimization. Springer, Berlin.

MR1713114

[33] Qin, J. (1999). Empirical likelihood ratio based conﬁdence intervals for mixture pro-

portions. Ann. Statist. 27 1368–1384. MR1740107

[34] Quandt, R. E. and Ramsey, J. B. (1978). Estimating mixtures of normal distribu-

tions and switching regressions. J. Amer. Statist. Assoc. 73 730–738. MR0521324

[35] Redner, R. A. and Walker, H. F. (1984). Mixture densities, maximum likelihood

and the EM algorithm. SIAM Rev. 26 195–239. MR0738930

[36] Shashahani, B. M. and Landgrebe, D. A. (1994). The eﬀect of unlabeled sam-

ples in reducing the small sample-size problem and mitigating the Hughes phe-

nomenon. IEEE Trans. Geoscience and Remote Sensing

32 1087–1095.

[37] Shorack, G. R. and Wellner, J. A. (1986). Empirical Processes with Applications

to Statistics. Wiley, New York. MR0838963

[38] Teicher, H. (1961). Identiﬁability of mixtures.

Ann. Math. Statist. 32 244–248.

MR0120677

[39] Titterington, D. M. (1983). Minimum distance non-parametric estimation of mix-

ture proportions. J. Roy. Statist. Soc. Ser. B 45 37–46. MR0701074

[40] Titterington, D. M., Smith, A. F. M. and Makov, U. E. (1985). Statistical

Analysis of Finite Mixture Distributions. Wiley, Chichester. MR0838090

[41] van der Vaart, A. and Wellner, J. (1996). Weak Convergence and Empirical

Processes. Springer, New York. MR1385671


30

L. BORDES, S. MOTTELET AND P. VANDEKERKHOVE

L. Bordes

S. Mottelet

Universit´e de Technologie de Compi`egne

B.P. 529

60205 Compi`egne cedex

France

E-mail: laurent.bordes@utc.fr

stephane.mottelet@utc.fr

P. Vandekerkhove

Universit´e de Marne-la-Vall´ee

Cit´ee Descartes-5 Boulevard Descartes

Champs-sur-Marne

77454 Marne-la-Vall´ee cedex 2

France

E-mail: pierre.vandek@univ-mlv.fr

