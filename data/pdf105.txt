
Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds

Yu Zhang1, Yu Meng1, Xuan Wang1, Sheng Wang2, Jiawei Han1

1University of Illinois at Urbana-Champaign, IL, USA

2University of Washington, Seattle, WA, USA

{yuz9,yumeng5,xwang174,hanj}@illinois.edu

swang@cs.washington.edu

Abstract

Discovering latent topics from text corpora has

been studied for decades. Many existing topic

models adopt a fully unsupervised setting, and

their discovered topics may not cater to users’

particular interests due to their inability of

leveraging user guidance. Although there exist

seed-guided topic discovery approaches that

leverage user-provided seeds to discover topic-

representative terms, they are less concerned

with two factors: (1) the existence of out-of-

vocabulary seeds and (2) the power of pre-

trained language models (PLMs). In this paper,

we generalize the task of seed-guided topic

discovery to allow out-of-vocabulary seeds.

We propose a novel framework, named SEE-

TOPIC, wherein the general knowledge of

PLMs and the local semantics learned from the

input corpus can mutually beneﬁt each other.

Experiments on three real datasets from differ-

ent domains demonstrate the effectiveness of

SEETOPIC in terms of topic coherence, accu-

racy, and diversity.1

1

Introduction

Automatically discovering informative and coher-

ent topics from massive text corpora is central to

text analysis through helping users efﬁciently di-

gest a large collection of documents (Grifﬁths and

Steyvers, 2004) and advancing downstream appli-

cations such as summarization (Wang et al., 2009,

2022), classiﬁcation (Chen et al., 2015; Meng et al.,

2020b), and generation (Liu et al., 2021).

Unsupervised topic models have been the main-

stream approach to topic discovery since the pro-

posal of pLSA (Hofmann, 1999) and LDA (Blei

et al., 2003). Despite their encouraging perfor-

mance in ﬁnding informative latent topics, these

topics may not reﬂect user preferences well, mainly

due to their unsupervised nature. For example,

given a collection of product reviews, a user may

be speciﬁcally interested in product categories

1The code and datasets are available at

https://github.com/yuzhimanhua/SeeTopic.

Table 1: Three datasets (Cohan et al., 2020; McAuley

and Leskovec, 2013; Zhang et al., 2017) from different

domains and their topic categories (i.e., seeds). Red:

Seeds never seen in the corpus (i.e., out-of-vocabulary).

In all three datasets, a large proportion of seeds are out-

of-vocabulary.

Dataset

Category Names (Seeds)

SciDocs

(Scientiﬁc

Papers)

cardiovascular diseases

chronic kidney disease

chronic respiratory diseases

diabetes mellitus

digestive diseases

hiv/aids

hepatitis a/b/c/e

mental disorders

musculoskeletal disorders

neoplasms (cancer)

neurological disorders

Amazon

(Product

Reviews)

apps for android

books

cds and vinyl

clothing, shoes and jewelry

electronics

health and personal care

home and kitchen

movies and tv

sports and outdoors

video games

Twitter

(Social

Media

Posts)

food

shop and service

travel and transport

college and university

nightlife spot

residence

outdoors and recreation

arts and entertainment

professional and other places

(e.g., “books”, “electronics”), but unsupervised

topic models may generate topics containing dif-

ferent sentiments (e.g., “good”, “bad”). To con-

sider users’ interests and needs, seed-guided topic

discovery approaches (Jagarlamudi et al., 2012;

Gallagher et al., 2017; Meng et al., 2020a) have

been proposed to ﬁnd representative terms for each

category based on user-provided seeds or category

names.2 However, there are still two less concerned

factors in these approaches.

The Existence of Out-of-Vocabulary Seeds. Pre-

vious studies (Jagarlamudi et al., 2012; Gallagher

et al., 2017; Meng et al., 2020a) assume that all

user-provided seeds must be in-vocabulary (i.e.,

appear at least once in the input corpus), so that

they can utilize the occurrence statistics or Skip-

Gram embedding methods (Mikolov et al., 2013)

to model seed semantics. However, user-interested

categories can have speciﬁc or composite descrip-

tions, which may never appear in the corpus. Table

1 shows three datasets from different domains: sci-

2In this paper, we use “seeds” and “category names” inter-

changeably.

arXiv:2205.01845v1  [cs.CL]  4 May 2022


entiﬁc papers, product reviews, and social media

posts. In each dataset, documents can belong to one

or more categories, and we list the category names

provided by the dataset collectors. These seeds

should reﬂect their particular interests. In all three

datasets, we have a large proportion of seeds (45%

in SciDocs, 60% in Amazon, and 78% in Twitter)

that never appear in the corpus. Some category

names are too speciﬁc (e.g., “chronic respiratory

diseases”, “nightlife spot”) to be exactly matched,

others are the composition of multiple entities (e.g.,

“hepatitis a/b/c/e”, “neoplasms (cancer)”, “clothing,

shoes and jewelry”).3

The Power of Pre-trained Language Models.

Techniques used in previous studies are mainly

based on LDA variants (Jagarlamudi et al., 2012) or

context-free embeddings (Meng et al., 2020a). Re-

cently, pre-trained language models (PLMs) such

as BERT (Devlin et al., 2019) have achieved signif-

icant improvement in a wide range of text mining

tasks. In topic discovery, the generic representation

power of PLMs learned from web-scale corpora

(e.g., Wikipedia or PubMed) may complement the

information a model can obtain from the input cor-

pus. Moreover, out-of-vocabulary seeds usually

have meaningful in-vocabulary components (e.g.,

“night” and “life” in “nightlife spot”, “health” and

“care” in “health and personal care”). The opti-

mized tokenization strategy of PLMs (Sennrich

et al., 2016; Wu et al., 2016) can help segment

the seeds into such meaningful components (e.g.,

“nightlife” → “night” and “##life”), and the contex-

tualization power of PLMs can help infer the cor-

rect meaning of each component (e.g., “##life” and

“care”) in the category name. Therefore, PLMs are

much needed in handling out-of-vocabulary seeds

and effectively learning their semantics.

Contributions. Being aware of these two factors,

in this paper, we study seed-guided topic discovery

in the presence of out-of-vocabulary seeds. Our

proposed SEETOPIC framework consists of two

modules: (1) The general representation module

3One possible idea to deal with composite seeds is to split

them into multiple seeds. However, there are many possible

ways to express the conjunctions (e.g., “/”, “()”, “,” and “and”

in Table 1), which may require manual tuning. Besides, simple

chunking rules will induce splits that break the semantics

of the original composition (e.g., “professional and other

places” may be split into “professional” and “other places”).

Moreover, even after the split, some seeds are still out-of-

vocabulary. Therefore, we propose to use PLMs to tackle

out-of-vocabulary seeds in a uniﬁed way. In experiments, we

will show that our model is able to tackle composite seeds.

For example, given the seed “hepatitis a/b/c/e”, we can ﬁnd

terms relevant to “hepatitis b” and “hepatitis c” (see Table 4).

uses a PLM to derive the representation of each

term (including out-of-vocabulary seeds) based on

the general linguistic knowledge acquired through

pre-training. (2) The seed-guided local representa-

tion module learns in-vocabulary term embeddings

speciﬁc to the input corpus and the given seeds.

In order to optimize the learned representations

for topic coherence, which is commonly reﬂected

by pointwise mutual information (PMI) (Newman

et al., 2010), our objective implicitly maximizes

the PMI between each word and its context, the

documents it appears, as well as the category it

belongs to. The learning of the two modules is

connected through an iterative ensemble ranking

process, in which the general knowledge of PLMs

and the term representations speciﬁcally learned

from the target corpus conditioned on the seeds can

complement each other.

To summarize, this study makes three contri-

butions.

(1) Task: we propose to study seed-

guided topic discovery in the presence of out-of-

vocabulary seeds. (2) Framework: we design a uni-

ﬁed framework that jointly models general knowl-

edge through PLMs and local corpus statistics

through embedding learning. (3) Experiment: ex-

tensive experiments on three datasets demonstrate

the effectiveness of SEETOPIC in terms of topic

coherence, accuracy, and diversity.

2

Problem Deﬁnition

As shown in Table 1, we assume a seed can be

either a single word or a phrase. Given a corpus D,

we use VD to denote the set of terms appearing in

D. In accordance with the assumption of category

names, each term can also be a single word or a

phrase. In practice, given a raw corpus, one can

use existing phrase chunking tools (Manning et al.,

2014; Shang et al., 2018) to detect phrases in it.

After phrase chunking, if a category name is still

not in VD, we deﬁne it as out-of-vocabulary.

Problem Deﬁnition.

Given a corpus D

=

{d1, ..., d|D|} and a set of category names C =

{c1, ..., c|C|} where some category names are out-

of-vocabulary, the task is to ﬁnd a set of in-

vocabulary terms Si = {w1, ..., wS} ⊆ VD for

each category ci such that each term in Si is se-

mantically close to ci and far from other categories

cj (∀j ̸= i).

3

The SEETOPIC Framework

In this section, we ﬁrst introduce how we model

general and local text semantics using a PLM mod-


ule and a seed-guided embedding learning module,

respectively. Then, we present the iterative ensem-

ble ranking process and our overall framework.

3.1

Modeling General Text Semantics using a

PLM

PLMs such as BERT (Devlin et al., 2019) aim to

learn generic language representations from web-

scale corpora (e.g., Wikipedia or PubMed) that

can be applied to a wide variety of text-related

applications. To transfer such general knowledge

to our topic discovery task, we employ a PLM to

encode each category name and each in-vocabulary

term to a vector. To be speciﬁc, given a term w ∈

C ∪VD, we input the sequence “[CLS] w [SEP]”

into the PLM. Here, w can be a phrase containing

multiple words, and each word can be out of the

PLM’s vocabulary. To deal with this, most PLMs

use a pre-trained tokenizer (Sennrich et al., 2016;

Wu et al., 2016) to segment each unseen word into

frequent subwords. Then, the contextualization

power of PLMs will help infer the correct meaning

of each word/subword, so as to provide a more

precise representation of the whole category.

After LM encoding, following (Sia et al., 2020;

Thompson and Mimno, 2020; Li et al., 2020), we

take the output of all tokens from the last layer and

average them to get the term embedding ew. In

this way, even if a seed ci is out-of-vocabulary,

we can still obtain its representation eci.

3.2

Modeling Local Text Semantics in the

Input Corpus

The motivation of topic discovery is to discover

latent topic structures from the input corpus. There-

fore, purely relying on general knowledge in the

PLM is insufﬁcient because topic discovery results

should adapt to the input corpus D. Now, we in-

troduce how we learn another set of embeddings

{uw|w ∈ VD} from D.

Previous studies on embedding learning assume

that the semantic of a term is similar to its local

context (Mikolov et al., 2013), the document it

appears (Tang et al., 2015; Xun et al., 2017a), and

the category it belongs to (Meng et al., 2020a).

Inspired by these studies, we propose the following

embedding learning objective.

J =

�

d∈D

�

wi∈d

�

wj∈C(wi,h)

p(wj|wi)

�

��

�

context

+

�

d∈D

�

w∈d

p(d|w)

�

��

�

document

+

�

ci∈C

�

w∈Si

p(ci|w)

�

��

�

category

,

(1)

where

p(z|w) =

exp(uT

wvz)

�

z′ exp(uTwvz′), (z can be wj, d, or ci). (2)

In this objective, uwi (and vwj), vd, vci are the

embedding vectors of terms, documents, and cate-

gories, respectively. C(wi, h) is the set of context

terms of wi in d. Speciﬁcally, if d = w1w2...wL,

then C(wi, h) = {wj|i − h ≤ j ≤ i + h, j ̸= i},

where h is the context window size.

Note that the last term in Eq. (1) encourages

the similarity between each category ci and its rep-

resentative terms Si. Here, we adopt an iterative

process to gradually update category-representative

terms.

Initially, Si consists of just a few in-

vocabulary terms similar to ci according to the

PLM. At each iteration, the size of Si will increase

to contain more category-discriminative terms (the

selection criterion of these terms will be introduced

in the next section), and we need to encourage their

proximity with ci in the next iteration.

Directly optimizing the full softmax in Eq. (2)

is costly. Therefore, we adopt the negative sam-

pling strategy (Mikolov et al., 2013) for efﬁcient

approximation.

Interpreting the Objective. In topic modeling

studies, pointwise mutual information (PMI) (New-

man et al., 2010) is a standard evaluation metric

for topic coherence (Lau et al., 2014; Röder et al.,

2015). Levy and Goldberg (2014) prove that the

Skip-Gram embedding model is implicitly factoriz-

ing the PMI matrix. Following their proof, we can

show that maximizing Eq. (1) is implicitly doing

the following factorization:

UT

w[Vw; Vd; Vc] = [Xww; Xwd; Xwc],

(3)

where the columns of Uw, Vw, Vd, Vc are uwi,

vwj, vd, vci, respectively (wi, wj ∈ VD, d ∈ D,

ci ∈ C); Xww, Xwd, and Xwc are PMI matrices.

Xww =

�

log

�

#D(wi, wj) · λD

#D(wi) · #D(wj) · b

��

wi,wj∈VD

,

Xwd =

�

log

� #d(w) · λD

#D(w) · λd · b

��

w∈VD, d∈D

,

Xwc =

�

xw,ci

�

w∈VD, ci∈C,

where

xw,ci =

�

log |C|

b ,

if w ∈ Si,

−∞,

if w ∈ Sj (∀j ̸= i).

(4)

Here, #D(wi, wj) denotes the number of co-

occurrences of wi and wj in a context window in

D; #D(w) denotes the number of occurrences of w


in D; λD is the total number of terms in D; #d(w)

denotes the number of times w occurs in d; λd is

the total number of terms in d; b is the number of

negative samples. (For the derivation of Eq. (3),

please refer to Appendix A.)

To summarize, the learned local representations

uw are implicitly optimized for topic coherence,

where term co-occurrences are measured in context,

document, and category levels.

3.3

Ensemble Ranking

We have obtained two sets of term embeddings

that model text semantics from different angles:

{ew|w ∈ C ∪ VD} carries the PLM’s knowledge,

while {uw|w ∈ VD} models the input corpus as

well as user-provided seeds. We now propose an

ensemble ranking method to leverage information

from both sides to grab more discriminative terms

for each category.

Given a category ci and its current term set Si,

we ﬁrst calculate the scores of each term w ∈ VD.

scoreG(w|Si) =

1

|Si|

�

w′∈Si

cos(ew, ew′),

scoreL(w|Si) =

1

|Si|

�

w′∈Si

cos(uw, uw′).

(5)

The subscript “G” here means “general”, while “L”

means “local”. Then, we sort all terms by these

two scores, respectively. Each term w will hence

get two rank positions rankG(w) and rankL(w).

We propose the following ensemble score based on

the reciprocal rank:

score(w|Si) =

�1

2

�

1

rankG(w)

�ρ

+1

2

�

1

rankL(w)

�ρ�1/ρ

.

(6)

Here, 0 &lt; ρ ≤ 1 is a constant. In practice, in-

stead of ranking all terms in the vocabulary, we

only check the top-M results in the two ranking

lists.

If a term w is not among the top-M ac-

cording to scoreG(w) (resp., scoreL(w)), we set

rankG(w) = +∞ (resp., rankL(w) = +∞). In

fact, when ρ = 1, Eq. (6) becomes the arith-

metic mean of the two reciprocal ranks

1

rankG(w)

and

1

rankL(w). This is essentially the mean recip-

rocal rank (MRR) commonly used in ensemble

ranking, where a high position in one ranking list

can largely compensate a low position in the other.

In contrast, when ρ → 0, Eq. (6) becomes the

geometric mean of the two reciprocal ranks (see

Appendix B), where two ranking lists both have

the “veto power” (i.e., a term needs to be ranked

as top-M in both ranking lists to obtain a non-zero

Algorithm 1: SEETOPIC

Input: A text corpus D = {d1, ..., d|D|}, a set of

seeds C = {c1, ..., c|C|}, and a PLM.

Output: (S1, ..., S|C|), where each Si is a set of

category-discriminative terms for ci.

1 Compute {ew|w ∈ C ∪ VD} using the PLM;

2 // Initialize Si;

3 S1, ..., S|C| ← ∅;

4 for n ← 1 to N do

5

for i ← 1 to |C| do

6

wn ←

arg max

w∈VD\(S1∪...∪S|C|)

cos(ew, eci);

7

Si ← Si ∪ {wn};

8 // Update Si for T iterations;

9 for t ← 1 to T do

10

Learn {uw|w ∈ VD} from the input corpus D

and the up-to-date representative terms

S1, ..., S|C| according to Eq. (1);

11

scoreG(w|Si) and scoreL(w|Si) ← Eq. (5);

12

score(w|Si) ← Eq. (6);

13

S1, ..., S|C| ← ∅;

14

for n ← 1 to (t + 1)N do

15

for i ← 1 to |C| do

16

Si ← Eq. (7);

17 Return (S1, ..., S|C|);

ensemble score). In experiment, we set ρ = 0.1

and show it outperforms MRR (i.e., ρ = 1) in our

topic discovery task.

After computing the ensemble score score(w|Si)

for each w, we update Si. To guarantee that each

Si is category-discriminative, we do not allow any

term to belong to more than one category. There-

fore, we gradually expand each Si by turns. At the

beginning, we reset S1 = ... = S|C| = ∅. When it

is Si’s turn, we add one term Si according to the

following criterion:

Si ← Si ∪ {

arg max

w∈VD\(S1∪...∪S|C|)

score(w|Si)}.

(7)

3.4

Overall Framework

We summarize the entire SEETOPIC framework in

Algorithm 1. To deal with out-of-vocabulary cat-

egory names, we ﬁrst utilize a PLM to ﬁnd their

nearest in-vocabulary terms as the initial category-

discriminative term set Si (Lines 1-7). After ini-

tialization, |Si| = N (∀1 ≤ i ≤ |C|). Note that

for an in-vocabulary category name ci ∈ VD, itself

will be added to the initial Si as the top-1 similar

in-vocabulary term.

After getting the initial Si, we update it by T it-

erations (Lines 8-16). At each iteration, according

to the up-to-date S1, S2, ..., S|C|, we relearn embed-

dings uw, vw, vd, and vci using Eq. (1) (Line 10).

The two set of embeddings, {ew|w ∈ C ∪ VD}

(computed at Line 1) and {uw|w ∈ VD} (up-

dated at Line 10), are then leveraged to perform

ensemble ranking (Lines 11-12). Based on the


ensemble score score(w|Si), we update Si using

Eq. (7) (Lines 13-16). After the t-th iteration,

|Si| = (t + 1)N (∀1 ≤ i ≤ |C|).

Complexity Analysis. The time complexity of

using the PLM is O((|C| + |VD|)αPLM), where

αPLM is the complexity of encoding one term via

the PLM. The total complexity of local embed-

ding is O(TλD(h+|C|)b) because in each iteration

1 ≤ t ≤ T, every w ∈ D interacts with every other

term in the context window of size h, its belong-

ing document, and each category ci ∈ C, and each

update involves b negative samples. The total com-

plexity of ensemble ranking is O(T|VD||C||Si|) as

in each iteration 1 ≤ t ≤ T, we compute scores

between each w ∈ VD and each w′ ∈ Si (∀ci ∈ C).

4

Experiments

4.1

Experimental Setup

Datasets. We conduct experiments on three pub-

lic datasets from different domains: (1) SciDocs

(Cohan et al., 2020)4 is a large collection of sci-

entiﬁc papers supporting diverse evaluation tasks.

For the MeSH classiﬁcation task (Coletti and Ble-

ich, 2001), about 23K medical papers are collected,

each of which is assigned to one of the 11 common

disease categories derived from the MeSH vocabu-

lary. We use the title and abstract of each paper as

documents and the 11 category names as seeds. (2)

Amazon (McAuley and Leskovec, 2013)5 contains

product reviews from May 1996 to July 2014. Each

Amazon review belongs to one or more product cat-

egories. We use the subset sampled by Zhang et al.

(2020, 2022), which contains 10 categories and

100K reviews. (3) Twitter (Zhang et al., 2017)6

is a crawl of geo-tagged tweets in New York City

from August 2014 to November 2014. The dataset

collectors link these tweets with Foursquare’s POI

database and assign them to 9 POI categories. We

take these category names as input seeds.

Seeds used in the three datasets are shown in

Table 1. Dataset statistics are summarized in Ta-

ble 2. For all three datasets, we use AutoPhrase

(Shang et al., 2018)7 to perform phrase chunking

in the corpus, and we remove words and phrases

occurring less than 3 times.

Previous studies (Jagarlamudi et al., 2012; Meng

et al., 2020a) have tried some other datasets (e.g.,

RCV1, 20 Newsgroups, NYT, and Yelp). However,

the category names they use in these datasets are

4https://github.com/allenai/scidocs

5http://jmcauley.ucsd.edu/data/amazon/index.html

6https://github.com/franticnerd/geoburst

7https://github.com/shangjingbo1226/AutoPhrase

Table 2: Dataset Statistics.

Dataset

SciDocs

Amazon

Twitter

#Documents

23,473

100,000

135,529

#In-vocabulary Terms

(After Phrase Chunking)

55,897

56,942

17,577

Avg Doc Length

239.8

119.0

6.7

#Seeds

11

10

9

#Out-of-vocabulary Seeds

(After Phrase Chunking)

5

6

7

all picked from in-vocabulary terms. Therefore,

we do not consider these datasets for evaluation in

our task settings.

Following (Sia et al., 2020), we adopt a 60-40

train-test split for all three datasets. The training

set is used as the input corpus D, and the testing

set is used for calculating topic coherence metrics

(see evaluation metrics for details).

Compared Methods. We compare our SEETOPIC

framework with the following methods, includ-

ing seed-guided topic modeling methods, seed-

guided embedding learning methods, and PLMs.

(1) SeededLDA (Jagarlamudi et al., 2012)8 is a

seed-guided topic modeling method. It improves

LDA by biasing topics to produce input seeds

and by biasing documents to select topics relevant

to the seeds they contain. (2) Anchored CorEx

(Gallagher et al., 2017)9 is a seed-guided topic

modeling method. It incorporates user-provided

seeds by balancing between compressing the in-

put corpus and preserving seed-related informa-

tion. (3) Labeled ETM (Dieng et al., 2020)10 is

an embedding-based topic modeling method. It in-

corporates distributed representation of each term.

Following (Meng et al., 2020a), we retrieve repre-

sentative terms according to their embedding sim-

ilarity with the category name. (4) CatE (Meng

et al., 2020a)11 is a seed-guided embedding learn-

ing method for discriminative topic discovery. It

takes category names as input and jointly learns

term embedding and speciﬁcity from the input cor-

pus. Category-discriminative terms are then se-

lected based on both embedding similarity with

the category and speciﬁcity. (5) BERT (Devlin

et al., 2019)12 is a PLM. Following Lines 1-7 in

Algorithm 1, we use BERT to encode each input

category name and each term to a vector, and then

perform similarity search to directly ﬁnd all repre-

8https://github.com/vi3k6i5/GuidedLDA

9https://github.com/gregversteeg/corex_topic

10https://github.com/adjidieng/ETM

11https://github.com/yumeng5/CatE

12https://huggingface.co/bert-base-uncased


Table 3: NPMI, LCP, MACC, and Diversity of compared algorithms on three datasets. NPMI and LCP measure

topic coherence; MACC measures term accuracy; Diversity (abbreviated to Div.) measures topic diversity. Bold:

the highest score. Underline: the second highest score. ∗: signiﬁcantly worse than SEETOPIC (p-value &lt; 0.05).

∗∗: signiﬁcantly worse than SEETOPIC (p-value &lt; 0.01).

Methods

SciDocs

Amazon

Twitter

NPMI

LCP

MACC

Div.

NPMI

LCP

MACC

Div.

NPMI

LCP

MACC

Div.

SeededLDA

0.056∗∗

-0.616

0.156∗∗

0.451∗∗

0.070∗∗

-0.753

0.147∗∗

0.393∗∗

0.013∗∗

-2.254∗∗

0.195∗∗

0.696∗∗

Anchored CorEx

0.106∗∗

-1.090∗∗

0.264∗∗

1.000

0.134∗∗

-0.982∗

0.333∗∗

1.000

0.090∗∗

-2.192∗∗

0.233∗∗

1.000

Labeled ETM

0.334∗

-0.775∗∗

0.458∗∗

0.961∗

0.308∗∗

-1.051∗∗

0.585∗∗

1.000

0.305∗

-1.098∗∗

0.268∗∗

0.989

CatE

0.345∗

-0.725∗∗

0.633∗∗

1.000

0.317∗∗

-0.844∗∗

0.856∗

1.000

0.356

-0.827

0.483∗∗

1.000

BERT

0.313∗∗

-0.841∗∗

0.740∗∗

0.891∗∗

0.294∗∗

-1.093∗∗

0.832∗∗

1.000

0.313∗∗

-1.044∗∗

0.627

0.944∗∗

BioBERT

0.309∗∗

-0.852∗∗

0.938

0.982∗∗

–

–

–

–

–

–

–

–

SEETOPIC-NoIter

0.341∗∗

-0.768∗∗

0.887

1.000

0.322∗∗

-0.986∗∗

0.892

1.000

0.318

-1.004∗∗

0.618

1.000

SEETOPIC

0.358

-0.634

0.909

1.000

0.342

-0.696

0.904

1.000

0.320

-0.907

0.633

1.000

sentative terms. (6) BioBERT (Lee et al., 2020)13

is a PLM. It is used in the same way as BERT.

Since BioBERT is speciﬁcally trained for biomedi-

cal text mining tasks, we report its performance on

the SciDocs dataset only. (7) SEETOPIC-NoIter

is a variant of our SEETOPIC framework. In Algo-

rithm 1, after initialization (Lines 1-7), it executes

Lines 9-16 only once (i.e., T = 1) to ﬁnd all repre-

sentative terms.

Here, all seed-guided topic modeling and em-

bedding baselines (i.e., SeededLDA, Anchored

CorEx, CatE, and Labeled ETM) can only take

in-vocabulary seeds as input. For a fair compar-

ison, we run Lines 1-7 in Algorithm 1 to get the

initial representative in-vocabulary terms for each

category, and input these terms as seeds into the

baselines. In other words, all compared methods

use BERT/BioBERT to initialize their term sets.

Evaluation Metrics. We evaluate topic discovery

results from three different angles: topic coherence,

term accuracy, and topic diversity.

(1) NPMI (Lau et al., 2014) is a standard metric in

topic modeling to measure topic coherence. Within

each topic, it calculates the normalized pointwise

mutual information for each pair of terms in Si.

NPMI = 1

|C|

|C|

�

i=1

1

�|Si|

2

�

�

wj,wk∈Si

log

P (wj,wk)

P (wj)P (wk)

− log P(wj, wk), (8)

where P(wj, wk) is the probability that wj and wk

co-occur in a document; P(wj) is the marginal

probability of wj.14

(2) LCP (Mimno et al., 2011) is another standard

metric to measure topic coherence. It calculates the

pairwise log conditional probability of top-ranked

13https://huggingface.co/dmis-lab/biobert-v1.1

14When calculating Eqs. (8) and (9), to avoid log 0, we

use P(wj, wk) + ϵ and P(w) + ϵ to replace P(wj, wk) and

P(w), respectively, where ϵ = 1/|D|.

terms.

LCP = 1

|C|

|C|

�

i=1

1

�|Si|

2

�

�

wj,wk∈Si

j&lt;k

log P(wj, wk)

P(wj)

.

(9)

Note that PMI (Newman et al., 2010) is also a stan-

dard metric for topic coherence. We do observe

that SEETOPIC outperforms baselines in terms of

PMI in most cases. However, since our local em-

bedding step is implicitly optimizing a PMI-like

objective, we no longer use it as our evaluation

metric.

(3) MACC (Meng et al., 2020a) measures term ac-

curacy. It is deﬁned as the proportion of retrieved

terms that actually belong to the corresponding

category according to the category name.

MACC = 1

|C|

|C|

�

i=1

1

|Si|

�

wj∈Si

1(wj ∈ ci),

(10)

where 1(wj ∈ ci) is the indicator function of

whether wj is relevant to category ci. MACC re-

quires human evaluation, so we invite ﬁve anno-

tators to perform independent annotation. The re-

ported MACC score is the average MACC of the

ﬁve annotators. A high inter-annotator agreement

is observed, with Fleiss’ kappa (Fleiss, 1971) being

0.856, 0.844, and 0.771 on SciDocs, Amazon, and

Twitter, respectively.

(4) Diversity (Dieng et al., 2020) measures the

mutual exclusivity of discovered topics. It is the

percentage of unique terms in all topics, which cor-

responds to our task requirement that each retrieved

term is discriminatively close to one category and

far from the others.

Diversity = | �|C|

i=1 Si|

�|C|

i=1 |Si|

.

(11)

Experiment Settings. We use BioBERT as the


PLM on SciDocs, and BERT-base-uncased as the

PLM on Amazon and Twitter. The embedding

dimension of uw is 768 (the same as ew); the

number of negative samples b = 5. In ensem-

ble ranking, the length of the general/local ranking

list M = 100; the hyperparameter ρ in Eq. (6) is

set as 0.1; the number of iterations T = 4; after

each iteration, we increase the size of Si by N = 3.

We use the top-10 ranked terms in each topic for

ﬁnal evaluation (i.e., |Si| = 10 in Eqs. (8)-(11)).

Experiments are run on Intel Xeon E5-2680 v2 @

2.80GHz and one NVIDIA GeForce GTX 1080.

4.2

Performance Comparison

Table 3 shows the performance of all methods. We

run each experiment 3 times with the average score

reported. To show statistical signiﬁcance, we con-

duct a two-tailed unpaired t-test to compare SEE-

TOPIC and each baseline. (The performance of

BERT and BioBERT is deterministic according to

our usage. When comparing SEETOPIC with them,

we conduct a two-tailed Z-test instead.) The signif-

icance level is also marked in Table 3.

We have the following observations from Table

3. (1) Our SEETOPIC model performs consistently

well. In fact, it achieves the highest score in 8

columns and the second highest in the remaining 4

columns. (2) Classical seed-guided topic modeling

baselines (i.e., SeededLDA and Anchored CorEx)

perform not well in respect of NPMI (topic coher-

ence) and MACC (term accuracy). Embedding-

based topic discovery approaches (i.e., Labeled

ETM and CatE) make some progress, but they still

signiﬁcantly underperform the PLM-empowered

SEETOPIC model on SciDocs and Amazon. (3)

SEETOPIC consistently performs better than SEE-

TOPIC-NoIter on all three datasets, indicating the

positive contribution of the proposed iterative pro-

cess. (4) SEETOPIC guarantees the mutual exclu-

sivity of S1, ..., S|C|. In comparison, SeededLDA,

Labeled ETM, and BERT cannot guarantee such

mutual exclusivity.

In-vocabulary vs.

Out-of-vocabulary.

Figure

1 compares the MACC scores of different seed-

guided topic discovery methods on in-vocabulary

categories and out-of-vocabulary categories. We

ﬁnd that the performance improvement of SEE-

TOPIC upon baselines on out-of-vocabulary cat-

egories is larger than that on in-vocabulary ones.

For example, on Amazon, SEETOPIC underper-

forms CatE on in-vocabulary categories but outper-

forms CatE on out-of-vocabulary ones; on Twit-

ter, the gap between SEETOPIC and baselines be-

0

0.2

0.4

0.6

0.8

1

In-Vocab

Out-of-Vocab

MACC

SeededLDA

Anchored CorEx

(a) SciDocs

0

0.2

0.4

0.6

0.8

1

In-Vocab

Out-of-Vocab

MACC

Labeled ETM

CatE

SeeTopic

(b) Amazon

0

0.2

0.4

0.6

0.8

In-Vocab

Out-of-Vocab

MACC

(c) Twitter

Figure 1: MACC of seed-guided topic discovery meth-

ods on in-vocabulary categories and out-of-vocabulary

categories.

0.3

0.32

0.34

0.36

0.1

0.3

0.5

0.7

0.9

NPMI

ρ

SciDocs

Amazon

Twitter

(a) Effect of ρ on NPMI

-1.1

-1

-0.9

-0.8

-0.7

-0.6

0.1

0.3

0.5

0.7

0.9

LCP

ρ

SciDocs

Amazon

Twitter

(b) Effect of ρ on LCP

0.3

0.32

0.34

0.36

0.38

1

2

3

4

5

NPMI

T

SciDocs

Amazon

Twitter

(c) Effect of T on NPMI

-1.1

-1

-0.9

-0.8

-0.7

-0.6

1

2

3

4

5

LCP

T

SciDocs

Amazon

Twitter

(d) Effect of T on LCP

Figure 2: Parameter study of SEETOPIC measured by

topic coherence.

comes much more evident on out-of-vocabulary

categories. Note that all baselines in Figure 1 do

not utilize the power of PLMs, so this observation

validates our claim that PLMs are helpful in tack-

ling out-of-vocabulary seeds.

4.3

Parameter Study

We study the effect of two important hyperparame-

ters: ρ (the hyperparameter in ensemble ranking)

and T (the number of iterations). We vary the

value of ρ in {0.1, 0.3, 0.5, 0.7, 0.9, 1} (SEETOPIC

uses ρ = 0.1 by default) and the value of T in

{1, 2, 3, 4, 5} (SEETOPIC uses T = 4 by default,

and SEETOPIC-NoIter is the case when T = 1).

Figure 2 shows the change of model performance

measured by NPMI and LCP.


Table 4: Top-5 representative terms retrieved by different algorithms for three out-of-vocabulary categories from

SciDocs, Amazon, and Twitter. : at least 3 of the 5 annotators judge the term as relevant to the seed. : at most

2 of the 5 annotators judge the term as relevant to the seed.

Method

Top-5 Representative Terms

Dataset: SciDocs,

Category Name: hepatitis a/b/c/e

SeededLDA

patients (), treatment (), placebo (), study (), group ()

Anchored CorEx

expression (), gene (), cells (), genes (), genetic ()

Labeled ETM

hepatitis b virus hbv dna (), serum hbv dna (), serum alanine aminotransferase (),

alanine aminotransferase alt (), below detection limit ()

CatE

chronic hepatitis b virus hbv infection (), hepatitis b e antigen hbeag (), hepatitis b virus hbv dna (),

normal alanine aminotransferase (), hbeag-negative chronic hepatitis b ()

BioBERT

hepatitis b virus hbv dna (), chronic hepatitis b virus hbv infection (), hepatitis b e antigen hbeag (),

hepatitis b virus hbv infection (), chronic hepatitis c virus hcv ()

SEETOPIC-NoIter

hepatitis b virus hbv dna (), hepatitis b e antigen hbeag (), chronic hepatitis b virus hbv infection (),

hepatitis b surface antigen hbsag (), hbeag-negative chronic hepatitis b ()

SEETOPIC

chronic hepatitis b virus hbv infection (), hbeag-negative chronic hepatitis b (), hepatitis c virus hcv-infected (),

hepatitis b virus hbv dna (), chronic hepatitis c virus hcv ()

Dataset: Amazon,

Category Name: sports and outdoors

SeededLDA

use (), good (), one (), product (), like ()

Anchored CorEx

sports (), use (), size (), wear (), ﬁt ()

Labeled ETM

cars and tracks (), tracks and cars (), search options (), championships (), cool bosses ()

CatE

outdoorsmen (), outdoor activities (), cars and tracks (), foot support (), offers plenty ()

BERT

cars and tracks (), outdoor activities (), outdoorsmen (), sports (), sporting events ()

SEETOPIC-NoIter

outdoorsmen (), outdoor activities (), cars and tracks (), indoor soccer (), bike riding ()

SEETOPIC

canoeing (), picnics (), bike rides (), bike riding (), rafting ()

Dataset: Twitter,

Category Name: travel and transport

SeededLDA

nyc (), new york (), line (), high (), time square ()

Anchored CorEx

new york (), post photo (), new (), day (), today ()

Labeled ETM

tourism (), theview (), ﬁle (), morning view (), gma ()

CatE

maritime (), tourism (), natural history (), scenery (), elevate ()

BERT

maritime (), tourism (), natural history (), olive oil (), baggage claim ()

SEETOPIC-NoIter

maritime (), tourism (), natural history (), scenery (), navy ()

SEETOPIC

wildlife (), scenery (), maritime (), highlinepark (), aquarium ()

According to Figures 2(a) and 2(b), in most

cases, the performance of SEETOPIC deteriorates

as ρ increases from 0.1 to 0.9. Thus, setting ρ = 0.1

always leads to competitive NPMI and LCP scores

on the three datasets. Although ρ = 1 is better

than ρ = 0.9, its performance is still suboptimal in

comparison with ρ = 0.1. This ﬁnding indicates

that replacing the mean reciprocal rank (i.e., ρ = 1)

with our proposed Eq. (6) is reasonable. According

to Figures 2(c) and 2(d), SEETOPIC usually per-

forms better when there are more iterations. On

SciDocs and Twitter, the scores start to converge

after T = 4. Besides, more iterations will result

in longer running time. Overall, we believe setting

T = 4 strikes a good balance.

4.4

Case Study

Finally, we show the terms retrieved by different

methods as a case study. From each of the three

datasets, we select an out-of-vocabulary category

and show its topic discovery results in Table 4. We

mark a retrieved term as correct () if at least 3 of

the 5 annotators judge the term as relevant to the

seed. Otherwise, we mark the term as incorrect ().

For the category “hepatitis a/b/c/e” from Sci-

Docs, SeededLDA and Anchored CorEx can only

ﬁnd very general medical terms, which are relevant

to all seeds in SciDocs and thus inaccurate; Labeled

ETM and CatE ﬁnd terms about “alanine amino-

transferase”, whose elevation suggest not only hep-

atitis but also other diseases like diabetes and heart

failure, thus not discriminative either; BioBERT

and SEETOPIC, with the power of a PLM, can ac-

curately pick terms relevant to “hepatitis b” and

“hepatitis c”. For the category “sports and out-

doors” from Amazon, SeededLDA and Anchored

CorEx again ﬁnd very general terms, most of which

are not category-discriminative; Labeled ETM and

CatE are able to pick more speciﬁc terms such

as “cars and tracks”, but they still make mistakes;

BERT, as a PLM, can accurately ﬁnd terms that

have lexical overlap with the category name (e.g.,

“outdoorsmen”, “sporting events”), meanwhile such

terms are less diverse; SEETOPIC-NoIter starts to

discover more concrete terms than BERT (e.g., “in-

door soccer”, “bike riding”) by leveraging local

text semantics; the full SEETOPIC model, with an

iterative updating process, can ﬁnd more speciﬁc

and informative terms (e.g., “canoeing”, “picnics”,

and “rafting”). For the category “travel and trans-


port” from Twitter, both BERT and CatE make

mistakes by including the term “natural history”;

SEETOPIC-NoIter, without an iterative update pro-

cess, also includes this error; the full SEETOPIC

model ﬁnally excludes this error and achieves the

highest accuracy in the retrieved top-5 terms among

all compared methods.

5

Related Work

Seed-Guided Topic Discovery. Seed-guided topic

models aim to leverage user-provided seeds to dis-

cover underlying topics according to users’ inter-

ests. Early studies take LDA (Blei et al., 2003)

as the backbone and incorporate seeds into model

learning. For example, Andrzejewski et al. (2009)

consider must-link and cannot-link constraints

among seeds as priors. SeededLDA (Jagarlamudi

et al., 2012) encourages topics to contain more

seeds and encourages documents to select topics

relevant to the seeds they contain. Anchored CorEx

(Gallagher et al., 2017) extracts maximally informa-

tive topics by jointly compressing the corpus and

preserving seed relevant information. Recent stud-

ies start to utilize embedding techniques to learn

better word semantics. For example, CatE (Meng

et al., 2020a) explicitly encourages distinction

among retrieved topics via category-name guided

embedding learning. However, all these models

require the provided seeds to be in-vocabulary,

mainly because they focus on the input corpus only

and are not equipped with general knowledge of

PLMs.

Embedding-Based Topic Discovery. A number

of studies extend LDA to involve word embed-

ding. The common strategy is to adapt distribu-

tions in LDA to generate real-valued data (e.g.,

Gaussian LDA (Das et al., 2015), LFTM (Nguyen

et al., 2015), Spherical HDP (Batmanghelich et al.,

2016), and CGTM (Xun et al., 2017b)). Some

other studies think out of the LDA backbone. For

example, TWE (Liu et al., 2015) uses topic struc-

tures to jointly learn topic embeddings and improve

word embeddings. CLM (Xun et al., 2017a) col-

laboratively improves topic modeling and word

embedding by coordinating global and local con-

texts. ETM (Dieng et al., 2020) models word-topic

correlations via word embeddings to improve the

expressiveness of topic models. More recently, Sia

et al. (2020) show that directly clustering word em-

beddings (e.g., word2vec or BERT) also generates

good topics; Thompson and Mimno (2020) further

ﬁnd that BERT and GPT-2 discover high-quality

topics, but RoBERTa does not. These models are

unsupervised and hard to be applied to seed-guided

settings. In contrast, our SEETOPIC framework

joint leverages PLMs, word embeddings, and seed

information.

6

Conclusions and Future Work

In this paper, we study seed-guided topic discov-

ery in the presence of out-of-vocabulary seeds. To

understand and make use of in-vocabulary com-

ponents in each seed, we utilize the tokenization

and contextualization power of PLMs. We pro-

pose a seed-guided embedding learning framework

inspired by the goal of maximizing PMI in topic

modeling, and an iterative ensemble ranking pro-

cess to jointly leverage general knowledge of the

PLM and local signals learned from the input cor-

pus. Experimental results show that SEETOPIC

outperforms seed-guided topic discovery baselines

and PLMs in terms of topic coherence, term accu-

racy, and topic diversity. A parameter study and a

case study further validate some design choices in

SEETOPIC.

In the future, it would be interesting to extend

SEETOPIC to seed-guided hierarchical topic dis-

covery, where parent and child information in the

input category hierarchy may help infer the mean-

ing of out-of-vocabulary nodes.

Acknowledgments

We thank anonymous reviewers for their valu-

able and insightful feedback. Research was sup-

ported in part by US DARPA KAIROS Program

No. FA8750-19-2-1004, SocialSim Program No.

W911NF-17-C-0099, and INCAS Program No.

HR001121C0165, National Science Foundation

IIS-19-56151, IIS-17-41317, and IIS 17-04532,

and the Molecule Maker Lab Institute: An AI Re-

search Institutes program supported by NSF under

Award No. 2019897, and the Institute for Geospa-

tial Understanding through an Integrative Discov-

ery Environment (I-GUIDE) by NSF under Award

No. 2118329. Any opinions, ﬁndings, and con-

clusions or recommendations expressed herein are

those of the authors and do not necessarily rep-

resent the views, either expressed or implied, of

DARPA or the U.S. Government.

References

David Andrzejewski, Xiaojin Zhu, and Mark Craven.

2009. Incorporating domain knowledge into topic

modeling via dirichlet forest priors.

In ICML’09,

pages 25–32.


Kayhan Batmanghelich,

Ardavan Saeedi,

Karthik

Narasimhan, and Sam Gershman. 2016. Nonpara-

metric spherical topic modeling with word embed-

dings. In ACL’16, pages 537–542.

David M Blei, Andrew Y Ng, and Michael I Jordan.

2003.

Latent dirichlet allocation.

JMLR, 3:993–

1022.

Xingyuan Chen, Yunqing Xia, Peng Jin, and John Car-

roll. 2015. Dataless text classiﬁcation with descrip-

tive lda. In AAAI’15, pages 2224–2231.

Arman Cohan, Sergey Feldman, Iz Beltagy, Doug

Downey, and Daniel S Weld. 2020.

Specter:

Document-level

representation

learning

using

citation-informed transformers.

In ACL’20, pages

2270–2282.

Margaret H Coletti and Howard L Bleich. 2001. Med-

ical subject headings used to search the biomedical

literature. JAMIA, 8(4):317–323.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.

Gaussian lda for topic models with word embed-

dings. In ACL’15, pages 795–804.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

Kristina Toutanova. 2019. Bert: Pre-training of deep

bidirectional transformers for language understand-

ing. In NAACL-HLT’19, pages 4171–4186.

Adji B Dieng, Francisco JR Ruiz, and David M Blei.

2020. Topic modeling in embedding spaces. TACL,

8:439–453.

Joseph L Fleiss. 1971. Measuring nominal scale agree-

ment among many raters.

Psychological Bulletin,

76(5):378.

Ryan J Gallagher, Kyle Reing, David Kale, and Greg

Ver Steeg. 2017. Anchored correlation explanation:

Topic modeling with minimal domain knowledge.

TACL, 5:529–542.

Thomas L Grifﬁths and Mark Steyvers. 2004. Finding

scientiﬁc topics. PNAS, 101(suppl 1):5228–5235.

Thomas Hofmann. 1999. Probabilistic latent semantic

indexing. In SIGIR’99, pages 50–57.

Jagadeesh Jagarlamudi, Hal Daumé, and Raghavendra

Udupa. 2012. Incorporating lexical priors into topic

models. In EACL’12, pages 204–213.

Jey Han Lau, David Newman, and Timothy Baldwin.

2014. Machine reading tea leaves: Automatically

evaluating topic coherence and topic model quality.

In EACL’14, pages 530–539.

Jinhyuk

Lee,

Wonjin

Yoon,

Sungdong

Kim,

Donghyeon Kim, Sunkyu Kim, Chan Ho So, and

Jaewoo Kang. 2020. Biobert: a pre-trained biomed-

ical language representation model for biomedical

text mining. Bioinformatics, 36(4):1234–1240.

Omer Levy and Yoav Goldberg. 2014. Neural word em-

bedding as implicit matrix factorization. NIPS’14,

pages 2177–2185.

Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang,

Yiming Yang, and Lei Li. 2020. On the sentence

embeddings from pre-trained language models. In

EMNLP’20, pages 9119–9130.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong

Sun. 2015. Topical word embeddings. In AAAI’15,

pages 2418–2424.

Zequn Liu, Shukai Wang, Yiyang Gu, Ruiyi Zhang,

Ming Zhang, and Sheng Wang. 2021. Graphine: A

dataset for graph-aware terminology deﬁnition gen-

eration. In EMNLP’21, pages 3453–3463.

Christopher D Manning, Mihai Surdeanu, John Bauer,

Jenny Rose Finkel, Steven Bethard, and David Mc-

Closky. 2014. The stanford corenlp natural language

processing toolkit. In ACL’14, System Demonstra-

tions, pages 55–60.

Julian McAuley and Jure Leskovec. 2013. Hidden fac-

tors and hidden topics: understanding rating dimen-

sions with review text.

In RecSys’13, pages 165–

172.

Yu Meng, Jiaxin Huang, Guangyuan Wang, Zihan

Wang, Chao Zhang, Yu Zhang, and Jiawei Han.

2020a.

Discriminative topic mining via category-

name guided text embedding. In WWW’20, pages

2121–2132.

Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,

Heng Ji, Chao Zhang, and Jiawei Han. 2020b. Text

classiﬁcation using label names only: A language

model self-training approach. In EMNLP’20, pages

9006–9017.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-

rado, and Jeff Dean. 2013. Distributed representa-

tions of words and phrases and their compositional-

ity. In NIPS’13, pages 3111–3119.

David Mimno,

Hanna Wallach,

Edmund Talley,

Miriam Leenders, and Andrew McCallum. 2011.

Optimizing semantic coherence in topic models. In

EMNLP’11, pages 262–272.

David Newman, Jey Han Lau, Karl Grieser, and Tim-

othy Baldwin. 2010. Automatic evaluation of topic

coherence. In NAACL-HLT’10, pages 100–108.

Dat Quoc Nguyen, Richard Billingsley, Lan Du, and

Mark Johnson. 2015. Improving topic models with

latent feature word representations. TACL, 3:299–

313.

Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan

Wang, and Jie Tang. 2018. Network embedding as

matrix factorization: Unifying deepwalk, line, pte,

and node2vec. In WSDM’18, pages 459–467.


Michael Röder, Andreas Both, and Alexander Hinneb-

urg. 2015. Exploring the space of topic coherence

measures. In WSDM’15, pages 399–408.

Rico Sennrich, Barry Haddow, and Alexandra Birch.

2016. Neural machine translation of rare words with

subword units. In ACL’16, pages 1715–1725.

Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren,

Clare R Voss, and Jiawei Han. 2018.

Automated

phrase mining from massive text corpora.

IEEE

TKDE, 30(10):1825–1837.

Suzanna Sia, Ayush Dalmia, and Sabrina J Mielke.

2020. Tired of topic models? clusters of pretrained

word embeddings make for fast and good topics too!

In EMNLP’20, pages 1728–1736.

Jian Tang, Meng Qu, and Qiaozhu Mei. 2015. Pte: Pre-

dictive text embedding through large-scale heteroge-

neous text networks. In KDD’15, pages 1165–1174.

Laure Thompson and David Mimno. 2020. Topic mod-

eling with contextualized word representation clus-

ters. arXiv preprint arXiv:2010.12626.

Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong

Gong. 2009.

Multi-document summarization us-

ing sentence-based topic models. In ACL’09, pages

297–300.

Mu-Chun Wang, Zixuan Liu, and Sheng Wang. 2022.

Textomics: A dataset for genomics data summary

generation. In ACL’22.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V

Le,

Mohammad Norouzi,

Wolfgang Macherey,

Maxim Krikun,

Yuan Cao,

Qin Gao,

Klaus

Macherey, et al. 2016.

Google’s neural machine

translation system: Bridging the gap between hu-

man and machine translation.

arXiv preprint

arXiv:1609.08144.

Guangxu Xun, Yaliang Li, Jing Gao, and Aidong

Zhang. 2017a. Collaboratively improving topic dis-

covery and word embeddings by coordinating global

and local contexts. In KDD’17, pages 535–543.

Guangxu Xun, Yaliang Li, Wayne Xin Zhao, Jing Gao,

and Aidong Zhang. 2017b. A correlated topic model

using word embeddings. In IJCAI’17, pages 4207–

4213.

Chao Zhang, Keyang Zhang, Quan Yuan, Fangbo Tao,

Luming Zhang, Tim Hanratty, and Jiawei Han. 2017.

React: Online multimodal embedding for recency-

aware spatiotemporal activity modeling.

In SI-

GIR’17, pages 245–254.

Yu Zhang, Shweta Garg, Yu Meng, Xiusi Chen, and

Jiawei Han. 2022. Motifclass: Weakly supervised

text classiﬁcation with higher-order metadata infor-

mation. In WSDM’22, pages 1357–1367.

Yu Zhang, Yu Meng, Jiaxin Huang, Frank F Xu, Xuan

Wang, and Jiawei Han. 2020. Minimally supervised

categorization of text with metadata. In SIGIR’20,

pages 1231–1240.

A

The Embedding Learning Objective

In Section 3.2, we propose the following embed-

ding learning objective:

J =

�

d∈D

�

wi∈d

�

wj∈C(wi,h)

exp(uT

wivwj)

�

w′∈VD exp(uTwivw′)

�

��

�

Jcontext

+

�

d∈D

�

w∈d

exp(uT

wvd)

�

d′∈D exp(uTwvd′)

�

��

�

Jdocument

+

�

ci∈C

�

w∈Si

exp(uT

wvci)

�

c′∈C exp(uTwvc′)

�

��

�

Jcategory

.

(12)

Now we prove that maximizing J is implicitly

performing the factorization in Eq. (3).

Levy and Goldberg (2014) have proved that max-

imizing Jcontext is implicitly doing the following

factorization.

uT

wivwj = log

�

#D(wi, wj) · λD

#D(wi) · #D(wj) · b

�

,

i.e., UT

wVw = Xww.

(13)

We follow their approach to consider the other two

terms Jdocument and Jcategory in Eq. (12). Using

the negative sampling strategy to rewrite Jdocument,

we get

�

w∈VD

�

d∈D

#d(w)

�

log σ(uT

wvd)+bEd′�

log σ(−uT

wvd′)

��

,

(14)

where σ(·) is the sigmoid function. Following

(Levy and Goldberg, 2014; Qiu et al., 2018), we

assume the negative sampling distribution ∝ λd.15

Then, the objective becomes

�

w∈VD

�

d∈D

#d(w) log σ(uT

wvd) +

�

w∈VD

#D(w)

�

d′∈D

b · λd′

λD

log σ(−uT

wvd′).

(15)

For a speciﬁc term-document pair (w, d), we con-

sider its effect in the objective:

Jw,d = #d(w) log σ(uT

wvd)+#D(w)b · λd

λD

log σ(−uT

wvd).

(16)

Let xw,d = uT

wvd. To maximize Jw,d, we should

have

0 = ∂Jw,d

∂xw,d = #d(w)σ(−xw,d)− #D(w) · b · λd

λD

σ(xw,d).

(17)

15In practice, the negative sampling distribution ∝ λ3/4

d

,

but related studies (Levy and Goldberg, 2014; Qiu et al., 2018)

usually assume a linear relationship in their derivation.


That is,

e2xw,d−

� #d(w) · λD

#D(w) · b · λd −1

�

exw,d− #d(w) · λD

#D(w) · b · λd = 0.

(18)

Therefore, exw,d

= −1 (which is invalid) or

exw,d =

#d(w)·λD

#D(w)·b·λd . In other words,

uT

wvd = xw,d = log

� #d(w) · λD

#D(w) · b · λd

�

,

i.e., UT

wVd = Xwd.

(19)

Similarly, for Jcategory, the objective can be

rewritten as

�

w∈VD

�

ci∈C

1w∈Si log σ(uT

wvci) +

�

w∈VD

1w∈S1∪...∪S|C|

�

c′∈C

b

|C| log σ(−uT

wvc′).

(20)

Following the derivation of Jdocument, we get

uT

wvci = xw,ci = log

�

1w∈Si|C|

1w∈S1∪...∪S|C| · b

�

,

i.e., UT

wVci = Xwc.

(21)

Putting Eqs. (13), (19), and (21) together gives

us Eq. (3).

B

The Ensemble Ranking Function

In Section 3.3, we propose the following ensemble

ranking function:

score(w|Si) =

�1

2

�

1

rankG(w)

�ρ

+1

2

�

1

rankL(w)

�ρ�1/ρ

.

(22)

Now we prove this ranking function is a general-

ization of the arithmetic mean reciprocal rank (i.e.,

MRR) and the geometric mean reciprocal rank:

lim

ρ→1 score(w|Si) = 1

2

�

1

rankG(w) +

1

rankL(w)

�

;

lim

ρ→0 score(w|Si) =

�

1

rankG(w) ·

1

rankL(w)

�1/2

.

(23)

The case of ρ → 1 is trivial. When ρ → 0, we aim

to show that

lim

ρ→0 log score(w|Si) = log

�

1

rankG(w) ·

1

rankL(w)

�1/2

.

(24)

In fact, let rG =

1

rankG(w) and rL =

1

rankL(w).

lim

ρ→0 log score(w|Si) = lim

ρ→0 log

�1

2rρ

G + 1

2rρ

L

�1/ρ

= lim

ρ→0

log

� 1

2rρ

G + 1

2rρ

L

�

ρ

= lim

ρ→0

1

2 rρ

G log rG+ 1

2 rρ

L log rL

1

2 rρ

G+ 1

2 rρ

L

1

=

limρ→0

�

rρ

G log rG + rρ

L log rL

�

limρ→0

�

rρ

G + rρ

L

�

= log rG + log rL

2

= log(rG · rL)1/2.

(25)

The third line is obtained by applying L’Hopital’s

rule.

