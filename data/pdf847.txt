




Learning to Rank for

Information Retrieval

and Natural Language Processing


Synthesis Lectures on Human

Language Technology

Editor

Graeme Hirst, University of Toronto

Synthesis Lectures on Human Language Technologies is edited by Graeme Hirst of the University of

Toronto. The series consists of 50- to 150-page monographs on topics relating to natural language

processing, computational linguistics, information retrieval, and spoken language understanding.

Emphasis is on important new techniques, on new applications, and on topics that combine two or

more HLT subﬁelds.

Learning to Rank for Information Retrieval and Natural Language Processing

Hang Li

2011

Computational Modeling of Human Language Acquisition

Afra Alishahi

2010

Introduction to Arabic Natural Language Processing

Nizar Y. Habash

2010

Cross-Language Information Retrieval

Jian-Yun Nie

2010

Automated Grammatical Error Detection for Language Learners

Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault

2010

Data-Intensive Text Processing with MapReduce

Jimmy Lin and Chris Dyer

2010

Semantic Role Labeling

Martha Palmer, Daniel Gildea, and Nianwen Xue

2010


iii

Spoken Dialogue Systems

Kristiina Jokinen and Michael McTear

2009

Introduction to Chinese Natural Language Processing

Kam-Fai Wong, Wenjie Li, Ruifeng Xu, and Zheng-sheng Zhang

2009

Introduction to Linguistic Annotation and Text Analytics

Graham Wilcock

2009

Dependency Parsing

Sandra Kübler, Ryan McDonald, and Joakim Nivre

2009

Statistical Language Models for Information Retrieval

ChengXiang Zhai

2008


Copyright © 2011 by Morgan &amp; Claypool

All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in

any form or by any means—electronic, mechanical, photocopy, recording, or any other except for brief quotations in

printed reviews, without the prior permission of the publisher.

Learning to Rank for Information Retrieval and Natural Language Processing

Hang Li

www.morganclaypool.com

ISBN: 9781608457076

paperback

ISBN: 9781608457083

ebook

DOI 10.2200/S00348ED1V01Y201104HLT012

A Publication in the Morgan &amp; Claypool Publishers series

SYNTHESIS LECTURES ON HUMAN LANGUAGE TECHNOLOGY

Lecture #12

Series Editor: Graeme Hirst, University of Toronto

Series ISSN

Synthesis Lectures on Human Language Technology

Print 1947-4040

Electronic 1947-4059


Learning to Rank for

Information Retrieval

and Natural Language Processing

Hang Li

Microsoft

SYNTHESIS LECTURES ON HUMAN LANGUAGE TECHNOLOGY #12

C

M

&amp;

cLaypool

Morgan

publishers

&amp;


ABSTRACT

Learning to rank refers to machine learning techniques for training the model in a ranking task.

Learning to rank is useful for many applications in information retrieval,natural language processing,

and data mining. Intensive studies have been conducted on the problem recently and signiﬁcant

progress has been made. This lecture gives an introduction to the area including the fundamental

problems, existing approaches, theories, applications, and future work.

The author begins by showing that various ranking problems in information retrieval and

natural language processing can be formalized as two basic ranking tasks, namely ranking creation

(or simply ranking) and ranking aggregation. In ranking creation, given a request, one wants to

generate a ranking list of offerings based on the features derived from the request and the offerings.

In ranking aggregation, given a request, as well as a number of ranking lists of offerings, one wants

to generate a new ranking list of the offerings.

Ranking creation (or ranking) is the major problem in learning to rank.It is usually formalized

as a supervised learning task.The author gives detailed explanations on learning for ranking creation

and ranking aggregation, including training and testing, evaluation, feature creation, and major

approaches.Manymethodshavebeenproposedforrankingcreation.Themethodscanbecategorized

as the pointwise, pairwise, and listwise approaches according to the loss functions they employ.They

can also be categorized according to the techniques they employ, such as the SVM based, Boosting

SVM, Neural Network based approaches.

The author also introduces some popular learning to rank methods in details. These in-

clude PRank, OC SVM, Ranking SVM, IR SVM, GBRank, RankNet, LambdaRank, ListNet &amp;

ListMLE, AdaRank, SVM MAP, SoftRank, Borda Count, Markov Chain, and CRanking.

The author explains several example applications of learning to rank including web search,

collaborative ﬁltering, deﬁnition search, keyphrase extraction, query dependent summarization, and

re-ranking in machine translation.

A formulation of learning for ranking creation is given in the statistical learning framework.

Ongoing and future research directions for learning to rank are also discussed.

KEYWORDS

learning to rank, ranking, ranking creation, ranking aggregation, information retrieval,

natural language processing, supervised learning, web search, collaborative ﬁltering,

machine translation.


vii

Contents

Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi

1

Learning to Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1

1.1

Ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

1.2

Learning to Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.3

Ranking Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.4

Ranking Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

1.5

Learning for Ranking Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

1.6

Learning for Ranking Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

2

Learning for Ranking Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

2.1

Document Retrieval as Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

2.2

Learning Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.2.1

Training and Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.2.2

Training Data Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

2.2.3

Feature Construction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2.2.4

Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

2.2.5

Relations with Other Learning Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

2.3

Learning Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

2.3.1

Pointwise Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

2.3.2

Pairwise Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

2.3.3

Listwise Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

2.3.4

Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

3

Learning for Ranking Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

3.1

Learning Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

3.2

Learning Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

4

Methods of Learning to Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

4.1

PRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

4.1.1

Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

4.1.2

Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37


viii

4.2

OC SVM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

4.2.1

Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

4.2.2

Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

4.3

Ranking SVM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

4.3.1

Linear Model as Ranking Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

4.3.2

Ranking SVM Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

4.3.3

Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

4.4

IR SVM. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

4.4.1

Modiﬁed Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

4.4.2

Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

4.5

GBRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

4.5.1

Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

4.5.2

Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

4.6

RankNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

4.6.1

Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

4.6.2

Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

4.6.3

Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

4.6.4

Speed up of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

4.7

LambdaRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

4.7.1

Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

4.7.2

Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54

4.8

ListNet and ListMLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

4.8.1

Plackett-Luce model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

4.8.2

ListNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56

4.8.3

ListMLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

4.9

AdaRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

4.9.1

Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

4.9.2

Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

4.10

SVM MAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

4.10.1 Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

4.10.2 Learning Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

4.11

SoftRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64

4.11.1 Soft NDCG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64

4.11.2 Approximation of Rank Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

4.11.3 Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

4.12

Borda Count . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68


ix

4.13

Markov Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

4.14

Cranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

4.14.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

4.14.2 Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72

4.14.3 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72

5

Applications of Learning to Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75

6

Theory of Learning to Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

6.1

Statistical Learning Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

6.2

Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

6.3

Relations between Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

6.4

Theoretical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83

7

Ongoing and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89

Author’s Biography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101



Preface

This book presents a survey on learning to rank and describes methods for learning to rank

in detail. The major focus of the book is supervised learning for ranking creation.

The book targets researchers and practitioners in information retrieval, natural language pro-

cessing, machine learning, data mining, and other related ﬁelds. It assumes that the readers of the

book have basic knowledge of statistics and machine learning.

Chapter 1 gives a formal deﬁnition of learning to rank. Chapter 2 describes learning for

ranking creation, and Chapter 3 describes learning for ranking aggregation. Chapter 4 explains in

details about state-of-the-art learning to rank methods. Chapter 5 presents applications of learning

to rank. Chapter 6 introduces theory of learning to rank. Chapter 7 introduces ongoing and future

research on learning to rank.

I would like to express my sincere gratitude to my colleagues Tie-Yan Liu, Jun Xu, Tao Qin,

Yunbo Cao, and Yunhua Hu. We have been working together on learning to rank. Many thanks go

to our intern students, Zhe Cao, Ming-Feng Tsai, Xiubo Geng, Yanyan Lan, Fen Xia, Ming Li, Xin

Jiang, and Wei Chen, who also participated in the related research.

I am very grateful to Wei-Ying Ma, Hsiao-Wuen Hon, and Harry Shum for their encour-

agement and guidance.

I also thank our internal collaborators at Microsoft,Chris Burges,Stephen Robertson,Michael

Taylor, John Guiver, Dmitriy Meyerzon, Victor Poznanski, Rangan Majumder, Steven Yao, and

our external collaborators Rong Jin, Zhi-Hua Zhou, Hamed Valizadegan, Cheng Xiang Zhai, and

Thorsten Joachims.

I am deeply indebted to Jun Xu and Tao Qin who provide some materials for writing this

book.

Many thanks also go to the two anonymous reviewers, Chen Wang, Wei Wu, and Wei Chen,

who have read the draft of this book and made many valuable comments.

I would also like to thank Graeme Hirst and Michael Morgan. Without their support, this

book would not have been published.

Hang Li

April 23, 2011



1

C H A P T E R

1

Learning to Rank

1.1

RANKING

There are many tasks in information retrieval (IR) and natural language processing (NLP), for

which the central problem is ranking. These include document retrieval, entity search, question

answering, meta-search, personalized search, online advertisement, collaborative ﬁltering, document

summarization, and machine translation.

In our view, there are basically two types of ranking problems: ranking creation1 (or simply

ranking) and ranking aggregation. Ranking creation is to create a ranking list of objects using the

features of the objects, while ranking aggregation is to create a ranking list of objects using multiple

ranking lists of the objects, as will be formally described later in this chapter.

Document retrieval, collaborative ﬁltering, re-ranking in machine translation are examples of

ranking creation, and meta-search is an example of ranking aggregation.

DOCUMENT RETRIEVAL

Document retrieval includes web search, enterprise search, desktop search, etc. Although having

limitations, it is still the most practical way for people to access the enormous amount of information

existing on the web and computers. For example, according to a report by IProspect 2, 56% of the

internet users use web search every day and 88% of the internet users use web search every week.

Document retrieval can be described as the following task (cf., Fig. 1.1), in which ranking

plays a key role. The retrieval system maintains a collection of documents. Given a query from

the user, the system retrieves documents containing the query words from the collection, ranks

the documents, and presents the top ranked list of documents (say, 1,000 documents) to the user.

Ranking is performed mainly based on the relevance of the documents with respect to the query.

COLLABORATIVE FILTERING

Collaborative ﬁltering is the most fundamental model for computer systems to make recommen-

dations to the users in electronic commerce, online advertisement, etc. For example, if the users’

preferences on some of the movies in a database are known, then we can employ collaborative ﬁl-

tering to recommend to the users movies which they might have not seen and might be interested

in.

1Ranking creation is a term coined by the author of this book.

2http://www.iprospect.com/premiumPDFs/iProspectSurveyComplete.pdf


2

1. LEARNING TO RANK

documents

{

}

N

d

d

d

D

,

,

,

2

1

L

=

ranking based on 

relevance

Retrieval

System

q

n

q

q

q

d

d

d

,

2

,

1,

M

q

query

ranking of documents

Figure 1.1: Document Retrieval. Downward arrow represents ranking of documents

The data in collaborative ﬁltering is given in a matrix, in which rows correspond to users and

columns correspond to items (cf., Fig. 1.2). Some elements of the matrix are known, while the others

are not.The elements represent users’ ratings on items where the ratings have several grades (levels).

The question is how to determine the unknown elements of the matrix. One common assumption

is that similar users may have similar ratings on similar items. When a user is speciﬁed, the system

suggests a ranking list of items with the high grade items on the top.

Item1

Item2

Item3

...

ItemN

User1

5

4

User2

1

2

2

...

?

?

?

UserM

4

3

Figure 1.2: Collaborative Filtering


1.1. RANKING

3

MACHINE TRANSLATION

Machine translation can help people to access information cross languages and thus is very important.

Given a sentence in the source language, usually, there are a large number of possible translations

(sentences) in the target language. The quality of translations can vary, however. How to select the

most plausible translation(s) is the key question.

A popular approach to machine translation consists of two phases: candidate generation and

re-ranking (see Fig.1.3).Given a sentence in the source language,the system ﬁrst generates and ranks

2

1

e

e

ranked translation

Generation

Module

f

sentence in source language

Re-Ranking 

Module

1000

e

M

ranked translation

candidates in target 

language

e~

re-ranked top

sentence in target

language 

Figure 1.3: Machine Translation

all possible candidate translations in the target language using a generative model, then it conducts

re-ranking on the top candidate translations (say, 1,000 candidates) using a discriminative model,

and, ﬁnally, it chooses the top ranked candidate as output. The re-ranking process is performed

based on the likelihood of candidates’ being good translations, and it is critical to the performance

of machine translation.

META-SEARCH

A meta-search system is a system that sends the user’s request to several search systems and aggregates

the results from those search systems. Meta-search is becoming more and more important when web

continues to evolve, and more and more search systems (sometimes in different domains) become

available.

More formally, in meta-search, the query is submitted to several search systems and ranking

lists of documents are returned from the systems. The meta-search system then combines all the

ranking lists and generates a new ranking list (meta ranking list),which is better than all the individual

ranking lists. In practice, the sets of documents returned from different systems can be different.


4

1. LEARNING TO RANK

One can take the union of the sets of documents as the ﬁnal set of documents. Figure 1.4 illustrates

the process of meta-search.

query

rankings of 

documents

Search

System 1 

Search

Meta Search

meta ranking 

Search

System k

query

Search

System 2

System

…….

of documents

Figure 1.4: Meta-Search

1.2

LEARNING TO RANK

Recently, a new area called learning to rank has emerged in the intersection of machine learning,

information retrieval,and natural language processing.Learning to rank is about performing ranking

using machine learning techniques. It is based on previous work on ranking in machine learning and

statistics, and it also has its own characteristics.

There may be two deﬁnitions on learning to rank. In a broad sense, learning to rank refers to

any machine learning techniques for ranking. In a narrow sense, learning to rank refers to machine

learning techniques for building ranking models in ranking creation and ranking aggregation de-

scribed above. This book takes the latter deﬁnition (narrow sense). Figure 1.5 gives a taxonomy of

problems in learning to rank.

Recent years have seen signiﬁcant efforts on research and development of learning to rank

technologies.Many powerful methods have been developed and some of them have been successfully

applied to real applications such as web search. Over one hundred papers on the topic have been

published. Benchmark data sets have been released (e.g., [70]), and a competition on the task 3 has

also been carried out. Workshops (e.g., [58, 64, 65]) and journal special issues have been organized

(e.g., [69]). A book devoted to the topic has also been published [68].

3Yahoo Learning to Rank Challenge. http://learningtorankchallenge.yahoo.com/


1.3. RANKING CREATION

5

Learing to Rank







Ranking Creation







Supervised (e.g., Ranking SVM)

Unsupervised (e.g., BM25)

Ranking Aggregation







Supervised (e.g., CRank)

Unsupervised (e.g., Borda Count)

Figure 1.5: Taxonomy of Problems in Learning to Rank

1.3

RANKING CREATION

We can generalize the ranking creation problems already described as a more general task.

Suppose that there are two sets. For simplicity, we refer to them as a set of requests Q =

{q1, q2, · · · , qi, · · · , qM} and a set of offerings (or objects) O = {o1, o2, · · · , oj, · · · , oN}, respec-

tively4. Q can be a set of queries, a set of users, and a set of source sentences in document retrieval,

collaborative ﬁltering, and machine translation, respectively. O can be a set of documents, a set of

items, and a set of target sentences, respectively. Note that Q and O can be inﬁnite sets. Given an

element q of Q and a subset O of O (O ∈ 2O), we are to rank the elements in O based on the

information from q and O.

Ranking (ranking creation) is performed with ranking (scoring) function F(q, O) : Q ×

On → ℜn

SO = F(q, O)

π = sortSO(O),

where n = |O|, q denotes an element of Q, O denotes a subset of O, SO denotes a set of scores of

elements in O, and π denotes a ranking list (permutation) on elements in O sorted by SO. Note

that even for the same O, F can give two different ranking lists with two different q’s. That is to

say, we are concerned with ranking on O, with respect to a speciﬁc q.

Instead of using F(q, O), we usually use ranking (or scoring) function f (q, o) for ease of

manipulation, where q is an element of Q, o is an element of O, and so is a score of o. The ranking

function f (q, o) assigns a score to each o in O and the elements in O are then sorted by using the

scores. That means ranking is actually performed by sorting with f (q, o) : Q × O → ℜ

so = f (q, o)

π = sortso,o∈O(O).

4The naming of request and offering is proposed by Paul Kantor.


6

1. LEARNING TO RANK

io 1,

ranking of offerings

{

}

M

i

q

q

q

q

Q

L

L

,

,

,

,

2

1

=

iq

)

,

(

O

q

F

requests

in

i

i

o

o

,

2

,

M

offerings

{

}

N

j

o

o

o

o

O

L

L

,

,

,

,

2

1

=

{

}

in

i

i

i

i

o

o

o

O

,

2

,

1,

,

,

,

L

=

Ranking 

System

)

,

(

i

i O

q

F

Figure 1.6: Ranking Creation (with Global Ranking Function). Downward arrow represents ranking of

objects

requests

ranking of offerings

{

}

M

i

q

q

q

q

Q

L

L

,

,

,

,

2

1

=

iq

)

,

(

o

q

f

)

,

(

 

1,

1,

i

i

i

o

q

f

o

offerings

{

}

N

j

o

o

o

o

O

L

L

,

,

,

,

2

1

=

{

}

in

i

i

i

i

o

o

o

O

,

2

,

1,

,

,

,

L

=

Ranking

System

)

,

(

 

)

,

(

 

,

,

2

,

2

,

i

i

n

i

i

n

i

i

i

i

o

q

f

o

o

q

f

o

M

Figure 1.7: Ranking Creation (with Local Ranking Function). Downward arrow represents ranking of

objects

We refer to F(q, O) as global ranking function, f (q, o) as local ranking function because the

former works on a subset of objects while the latter works on a single object (cf., Figs. 1.6 and 1.7).

1.4

RANKING AGGREGATION

We can also deﬁne the general ranking aggregation task. Again, suppose that Q =

{q1, q2, · · · , qi, · · · , qM} and O = {o1, o2, · · · , oj, · · · , oN} are a set of requests and a set of of-

ferings, respectively. For an element q of Q and a subset O of O, there are k ranking lists on O:

� = {πi|π ∈ �, i = 1, · · · , k}, where � is the set of all ranking lists on O. Ranking aggregation

takes request q and ranking lists of offerings � as input and generates a new ranking list of offerings

π as output with ranking function F(q, �) : Q × �k → ℜn

SO = F(q, �)


1.5. LEARNING FOR RANKING CREATION

7

π = sortSO(O).

We usually simply deﬁne

F(q, �) = F(�).

That is to say, we assume that the ranking function does not depend on the request.

Ranking aggregation is actually a process of combining multiple ranking lists into a single

ranking list,which is better than any of the original ranking lists,as shown in Figure 1.8.The ranking

model is a global ranking model.

Ranking 

Aggregation

1

!

)

,

,

(

2

1

k

F

!

!

!

L

2

!

Aggregation

System

…….

2

!

k

!

Figure 1.8: Ranking Aggregation. Downward arrows represent rankings of objects

Ranking creation generates ranking based on features of request and offerings, while ranking

aggregation generates ranking based on rankings of offerings.Note that the output of ranking creation

can be used as the input of ranking aggregation.

1.5

LEARNING FOR RANKING CREATION

When learning to rank is mentioned, it usually means ranking creation using supervised learning.

This is also the main focus of this book. The learning task can be described in the following way.

There are two systems: a learning system and a ranking system.

Thelearningsystemtakestrainingdataasinput.Thetrainingdataconsistsofrequestsandtheir

associated ranking lists of offerings. For each request qi ∈ {q1, q2, · · · , qm}, there is an associated

set of offerings Oi ∈ {O1, O2, · · · , Om} (Oi = {oi,1, oi,2, · · · , oi,ni}, i = 1, · · · , m), and there is a

‘true’ ranking list on the offerings πi ∈ {π1, π2, · · · , πm}. The learning system constructs a ranking

model (usually, a local ranking model f (q, o)) on the basis of the training data.

The ranking system then makes use of the learned ranking model for ranking prediction.

Given a new request qm+1, the ranking system receives a subset of offerings Om+1, assigns scores


8

1. LEARNING TO RANK

to the offerings using the ranking model, and sorts the offerings in descending order of the scores,

obtaining a ranking list πm+1. See Fig. 1.9.

2

,1

1,1

1

o

o

q

M

M

M

m

m

m

o

o

q

2

,

1,

M

Learning 

System

{

}

N

o

o

o

O

,

,

,

2

1

L

=

1

,1 n

o

m

n

m

o ,

Ranking 

System

1

+

m

q

)

,

(

 

)

,

(

 

)

,

(

 

1

1

,1

1

,1

2

,1

1

2

,1

1,1

1

1,1

+

+

+

+

+

+

+

+

+

+

+

m

m

n

m

m

n

m

m

m

m

m

m

m

o

q

f

o

o

q

f

o

o

q

f

o

M

)

,

(

o

q

f

Figure 1.9: Learning for Ranking Creation. Downward arrows represent rankings

Here are the major characteristics of learning for ranking creation.

• Ranking creation: generating a ranking list of offerings based on the request and the offerings

• Feature-based: using features deﬁned on the request and the offerings

• Local ranking model: a local ranking model f (q, o) is utilized

• Supervised learning: the ranking model is usually created by supervised learning

1.6

LEARNING FOR RANKING AGGREGATION

Ranking aggregation can be supervised or unsupervised. In the supervised learning setting, the

learning system takes training data as input.The training data consists of requests and their associated

ranking lists of offerings.For each requestqi ∈ {q1, q2, · · · , qm},there is an associated set of offerings

Oi ∈ {O1, O2, · · · , Om} where Oi = {oi,1, oi,2, · · · , oi,ni}, i = 1, · · · , m. Furthermore, for each

Oi, there are k ranking lists on the set: �i = {πi,1, πi,2, · · · , πi,k}, as well as a ’true’ ranking list on

the set: πi. The learning system constructs a ranking model F(q, �) using the training data.

The ranking system then makes use of the learned ranking model for ranking prediction.

Given a new request qm+1, the ranking system receives k ranking lists on the associated set of

offerings Om+1: �m+1 = {πm+1,1, πm+1,2, · · · , πm+1,k}, assigns scores to the offerings with the


1.6. LEARNING FOR RANKING AGGREGATION

9

ranking model, and sorts the offerings in descending order of the scores, obtaining a ranking list

πm+1.

Here are the major characteristics of learning for ranking aggregation.

• Ranking aggregation: generate a ranking list of offerings from multiple ranking lists of the

offerings

• Ranking-based: using multiple ranking lists of the offerings

• Global ranking model: a global ranking model F(q, �) is utilized

• Supervised or unsupervised learning: the ranking model is created by either supervised or

unsupervised learning



11

C H A P T E R

2

Learning for Ranking Creation

This chapter gives a general introduction to learning for ranking creation. Ranking creation is aimed

at creating a ranking list of offerings based on the features of the offerings and the request, so that

‘good’ offerings to the request are ranked at the top. Learning for ranking creation is concerned with

automatic construction of the ranking model using machine learning techniques.

Recently intensive studies have been conducted on learning for ranking creation due to its

importance in practice. Many methods have been proposed and some of the technologies have been

successfully applied to applications such as web search.

Hereafter, we take document retrieval (or search) as an example to make the explanation.

Without loss of generality, the technologies described here can be applied to other applications.

2.1

DOCUMENT RETRIEVAL AS EXAMPLE

Learning for ranking creation (in general learning to rank) plays a very important role in document

retrieval. Traditionally, the ranking model in document retrieval f (q, d) is constructed without

training where q stands for a query and d stands for a document. In BM25 [90], the ranking model

f (q, d) is represented as a conditional probability distribution P (r|q, d) where r takes on 1 or 0 as

value and denotes being relevant or irreverent,q and d denote a query and a document,respectively.In

Language Model for IR (LMIR) [80, 113], the ranking model is deﬁned as a conditional probability

distribution P (q|d) where q denotes a query and d denotes a document. Both BM25 and LMIR

are calculated with the given query and document, and thus no training is needed (only tuning of a

few parameters is necessary).

Recently a new trend arises in IR, that is, to employ machine learning techniques to auto-

matically construct the ranking model f (q, d) for document retrieval (cf., [39]). This is motivated

by a number of facts. In document retrieval, particularly in web search, there are many signals which

can represent relevance. Incorporating such information into the ranking model and automatically

constructing the ranking model becomes a natural choice. At web search engines, a large amount

of search log data, such as click through data, is accumulated. This also brings a new opportunity

of automatically creating the ranking model with low cost by deriving training data from search

logs. All these facts have stimulated the research on learning to rank. Actually, learning to rank has

become one of the key technologies for modern web search.


12

2. LEARNING FOR RANKING CREATION

2

,1

1,1

1

d

d

q

M

M

M

m

m

m

d

d

q

2

,

1,

M

Learning 

System

{

}

N

d

d

d

D

,

,

,

2

1

L

=

1

,1 n

d

m

n

m

d

,

Ranking 

System

1

+

m

q

)

,

(

 

)

,

(

 

)

,

(

 

1

1

,1

1

,1

2

,1

1

2

,1

1,1

1

1,1

+

+

+

+

+

+

+

+

+

+

+

m

m

n

m

m

n

m

m

m

m

m

m

m

d

q

f

d

d

q

f

d

d

q

f

d

M

)

,

(

d

q

f

Figure 2.1: Learning to Rank for Document Retrieval

2.2

LEARNING TASK

We describe a number of issues in learning for ranking creation, with document retrieval as an

example. These include training and testing processes, training data creation, feature construction,

and evaluation. We also discuss the relations between ranking and other tasks such as ordinal clas-

siﬁcation.

2.2.1

TRAINING AND TESTING

Learning for ranking creation is comprised of training and testing, as a supervised learning task.

The training data contains queries and documents. Each query is associated with a number

of documents. The relevance of the documents with respect to the query is also given. As will be

explained later, the relevance information can be given in several ways. Here, we take the most

widely used approach, and we assume that the relevance of a document with respect to a query is

represented by a label. The labels are at several grades (levels). The higher grade a document has,

the more relevant the document is.

Suppose that Q is the query set and D is the document set. Suppose that Y = {1, 2, · · · , l}

is the label set, where the labels represent grades. There exists a total order between the grades

l ≻ l − 1 ≻ · · · ≻ 1, where ≻ denotes the order relation. Further suppose that {q1, q2, · · · , qm}

is the set of queries for training and qi is the i-th query. Di = {di,1, di,2, · · · , di,ni} is the set of

documents associated with query qi and yi = {yi,1, yi,2, · · · , yi,ni} is the set of labels associated

with query qi, where ni denotes the sizes of Di and yi; di,j denotes the j-th document in Di; and

yi,j ∈ Y denotes the j-th grade label in yi, representing the relevance degree of di,j with respect to

qi. The original training set is denoted as S =

�

(qi, Di), yi

�m

i=1.


2.2. LEARNING TASK

13

Table 2.1: Summary of Notations

Notations

Explanations

Q

query set

D

document set

Y = {1, 2, · · · , l}

label set (grade set) with order ≻

S = {(qi, Di), yi}m

i=1

original training data set

qi ∈ Q

i-th query in training data

Di = {di,1, di,2, · · · , di,ni}

set of documents associated with qi in training data

di,j ∈ D

j-th document in Di

yi = {yi,1, yi,2, · · · , yi,ni}

set of labels on Di with respect to qi

yi,j ∈ Y

label of di,j with respect to qi

xi = φ(qi, di,j)

feature vector from (qi, di,j)

xi = �(qi, Di)

feature vectors from (qi, Di)

�i

set of possible rankings on Di with respect to qi

πi ∈ �i

permutation on Di with respect to qi

πi(j)

rank (position) of j-th document in πi

S′ = {(xi, yi)}m

i=1

transformed training data set

f (q, d) = f (x)

local ranking model

F(q, D) = F(x)

global ranking model

T = {(qm+1, Dm+1)}

original test data set

T ′ = {xm+1}

transformed test data set

A feature vector xi,j = φ(qi, di,j) is created from each query-document pair (qi, di,j), i =

1, 2, · · · , m; j = 1, 2, · · · , ni, where φ denotes the feature functions. That is to say, features are

deﬁned as functions of query and document. Letting xi = {xi,1, xi,2, · · · , xi,ni}, we represent the

training data set as S′ =

�

(xi, yi)

�m

i=1. We aim to train a local ranking model f (q, d) = f (x) that

can assign a score to a given query document pair q and d, or equivalently to a given feature vector

x. More generally, we can also consider a global ranking model F(q, D) = F(x). Note that the local

ranking model outputs a single score, while the global ranking model outputs a set of scores.

Let the documents in Di be identiﬁed by the integers {1, 2, · · · , ni}. We deﬁne permutation

(ranking list) πi on Di as a bijection from {1, 2, · · · , ni} to itself. We use �i to denote the set of all

possible permutations on Di, use πi(j) to denote the rank (or position) of the j-th document (i.e.,

di,j) in permutation πi, and use π−1(j) to denote the document at the j-th rank in permutation πi.

Ranking is nothing but to select a permutation πi ∈ �i for the given query qi and the associated

set of documents Di using the scores given by the ranking model F(qi, Di) (or f (qi, di)).

The test data consists of a new query qm+1 and associated set of documents Dm+1. T =

{(qm+1, Dm+1)}. We create feature vector xm+1, use the trained ranking model to assign scores to

the documents in Dm+1, sort them based on the scores, and give the ranking list of documents as

output πm+1.


14

2. LEARNING FOR RANKING CREATION

Table 2.1 gives a summary of notations. Figures 2.2 and 2.3 illustrate the training and testing

processes.

Data Labeling

(rank)

Learning

)

(x

f

Feature 

Extraction

"

"

#

"

"

$

%

1

,1

2

,1

1,1

1

n

d

d

d

q M

"

"

#

"

"

$

%

1

1

,1

,1

2

,1

2

,1

1,1

1,1

1

n

n

y

d

y

d

y

d

q M

"

"

#

"

"

$

%

1

1

,1

,1

2

,1

2

,1

1,1

1,1

n

n

y

x

y

x

y

x

M

M

M

M

)

(x

f

"

"

#

"

"

$

%

m

n

m

m

m

m

d

d

d

q

,

2

,

1,

M

"

"

#

"

"

$

%

m

m

n

m

n

m

m

m

m

m

m

y

d

y

d

y

d

q

,

,

2

,

2

,

1,

1,

M

"

"

#

"

"

$

%

m

m

n

m

n

m

m

m

m

m

y

x

y

x

y

x

,

,

2

,

2

,

1,

1,

M

M

M

M

Figure 2.2: Training Process

Data Labeling

Feature

Ranking

"

"

#

"

"

$

%

+

+

+

+

+

1

,1

2

,1

1,1

1

m

n

m

m

m

m

d

d

d

q

M

"

"

#

"

"

$

%

+

+

+

+

+

+

+

+

+

 

 

 

 

 

 

1

1

,1

,1

2

,1

2

,1

1,1

1,1

1

m

m

n

m

n

m

m

m

m

m

m

y

d

y

d

y

d

q

M

"

"

#

"

"

$

%

+

+

+

+

+

+

+

+

 

 

 

 

 

 

1

1

,1

,1

2

,1

2

,1

1,1

1,1

m

m

n

m

n

m

m

m

m

m

y

x

y

x

y

x

M

Data Labeling

(rank)

Feature 

Extraction

Ranking 

with

)

(x

f

Figure 2.3: Testing Process

The training and testing data is similar to, but different from, the data in conventional su-

pervised learning such as classiﬁcation and regression. Query and its associated documents form a

group. The groups are i.i.d. data, while the instances within a group are not i.i.d. data. A (local)

ranking model is a function of query and document, or equivalently, a function of feature vector

derived from query and document.


2.2. LEARNING TASK

15

2.2.2

TRAINING DATA CREATION

Learning for ranking creation is a supervised learning task and thus how to create high quality training

data is a critical issue.

Ideally, the training data would consist of the perfect ranking lists of documents for each

query. In practice, however, such kind of data could be difﬁcult to obtain because the ranking lists

must reﬂect users’ average judgments on the relevance of the documents with respect to the queries.

Currently, there are two common ways to create training data.The ﬁrst one is human labeling,

which is widely used in the IR community. First, a set of queries is randomly selected from the query

log of a search system.Suppose that there are multiple search systems.Then the queries are submitted

to the search systems, and all the top ranked documents are collected. As a result, each query is

associated with documents from multiple search systems (it is called the pooling strategy). Human

judges are then asked to make relevance judgments on all the query document pairs. Relevance

judgments are usually conducted at ﬁve levels, for example, perfect, excellent, good, fair, and bad.

Human judges make relevance judgments from the viewpoint of average users. For example, if the

query is ‘Microsoft’, and the web page is microsoft.com, then the label is ‘perfect’. Furthermore, the

Wikipedia page about Microsoft is ‘excellent’. A page talking about Microsoft as its main topic will

be labeled as ‘good,’ a page only mentioning Microsoft will be labeled as ‘fair,’ and a page not relevant

to Microsoft will be labeled as ‘bad’. Labels representing relevance are then assigned to the query

document pairs. The labeling on query document pairs can be performed by multiple judges, and

then majority voting can be conducted. Since human labeling is expensive, it is often the case that

some query and document pairs are only judged by one single judge.Therefore, how to improve the

quality of human relevance judgments becomes an important issue in learning to rank research.

The other way of generating training data is derivation from click through data.Click-through

data at a web search engine records clicks on documents by users after they submit queries. Click-

through data represents implicit feedbacks on relevance from users and thus is useful for relevance

judgments. One method is to use the differences between numbers of clicks on documents to derive

preferences (relative relevance) on document pairs [57]. Suppose that for a query three documents A,

B,C are returned at the top 1,2,3 positions,and users’total numbers of clicks on the documents have

been recorded. If there are more clicks on document B than document A, then we can determine

that document B is more relevant than document A for this query because users seem to prefer

document B to document A, even the latter is ranked lower than the former. Given a ranking list of

documents, users tend to click documents on the top, even the documents may not be relevant. As

a result, documents on the top tend to have more clicks. This is a phenomenon referred to as ’click

bias’. Using the approach above, we can effectively deal with click bias because it derives preference

pairs of documents by looking at ‘skips’ of higher ranked documents. Therefore, this method can

generate preference pairs of documents as training data for learning to rank. Within each document

pair, one document is regarded more relevant than the other with respect to the query. See also

[87, 88].


16

2. LEARNING FOR RANKING CREATION

Table 2.2: Public DataSets for Learning to Rank

Dataset

URL

LEOTR

http://research.microsoft.com/en-us/um/

beijing/projects/letor

Microsoft Learning to Rank Dataset

http://research.microsoft.com/en-us/

projects/mslr

Yahoo Learning to Rank Challenge

http://webscope.sandbox.yahoo.com

The two approaches above both have pros and cons. It is very hard to maintain the quality of

data, when it is created by humans. Human judges are prone to errors, and their understanding on

relevance also has limitations because they are not query owners. Furthermore, manual data labeling

is also costly. In contrast, derivation of training data from click-through data is of low cost and the

data may also represent real users’judgments.The shortcoming of this approach is that click-through

data is noisy and is only available for head queries (high frequency queries).

Table 2.2 gives a list of publically available datasets for learning to rank research. They are all

datasets created by the ﬁrst approach.

2.2.3

FEATURE CONSTRUCTION

The ranking model f (q, d) is in fact deﬁned as f (x) where x is a feature vector based on q and

d. That is to say, the ranking model is feature based. That is the reason that the ranking model has

generalization ability. Speciﬁcally, it is trained from a small number of queries and their associated

documents but is applicable to any other queries and their associated documents.As in other machine

learning tasks, the performance of learning highly depends on the effectiveness of the features used.

How to deﬁne useful features thus is a critically important problem.

In web search, BM25 and PageRank are widely used ranking features. In fact, both can be

viewed as unsupervised ranking models. At the early stage of web search, the ﬁnal ranking model

was usually simply deﬁned as a linear combination of BM25 and PageRank, or something similar.

Later, more and more features have been developed. That is also the reason that a more general and

principled learning approach is needed in ranking model construction. We give the deﬁnitions of

BM25 and PageRank.

BM 25 is a probabilistic model representing the relevance of document d to query q [90]. It

actually looks at the matching degree between the query terms and document terms and utilizes the

numbers of occurrence of query terms in the document to represent relevance. Speciﬁcally, BM25

of query q and document d is calculated as

BM25(q, d) =

�

w∈q∩d

idf (w)

(k + 1)tf (w)

tf (w) + k((1 − b) + b

dl

avgdl )

,


2.2. LEARNING TASK

17

where w denotes a word in d and q, tf (w) denotes the frequency of w in d, idf (w) denotes the

inverse document frequency of w, dl denotes the length of d, avgdl denotes the average document

length, and k and b are parameters.

Page Rank represents the importance of web page [78]. It views the web as a directed graph

in which pages are vertices and hyperlinks are directed edges, deﬁnes a Markov process on the web

graph, and views the stationary distribution (Page Rank) of the Markov process as scores of page

importance. Page Rank of web page d is deﬁned as P (d)

P (d) = α

�

di∈M(d)

P (di)

L(di) + (1 − α) 1

N ,

where P (d) is the probability of visiting page d, P (di) is the probability of visiting page di, M(d) is

the set of pages linked to d, L(di) is the number of outlinks from di, N is the total number of nodes

on the graph, and α is a weight.

There are other features utilized in web search. Table 2.3 gives some examples, which have

been veriﬁed to be effective in web search. They include both query-document matching features

and document features, representing relevance of document to query and importance of document,

respectively.

Web pages usually contain a number of ﬁelds (metadata streams) such as title, anchor texts,

URL,extracted title [50,51],and associated queries in click-through data [3].One can deﬁne query-

document matching features, for example, BM25, for each ﬁeld of the web page, and thus exploit a

number of features in the same type.

2.2.4

EVALUATION

Evaluation on the performance of a ranking model is carried out by comparison between the ranking

lists output by the model and the ranking lists given as ground truth. Several evaluation measures

are widely used in IR and other ﬁelds. These include NDCG (Normalized Discounted Cumulative

Gain), DCG (Discounted Cumulative Gain) [53], MAP (Mean Average Precision) [101], WTA

(Winners Take All), MRR (Mean Reciprocal Rank), and Kendall’s Tau.

Given query qi and associated documents Di,suppose that πi is the ranking list (permutation)

on Di and yi is the set of labels (grades) of Di. DCG measures the goodness of the ranking list with

the labels. Speciﬁcally, DCG at position k for qi is deﬁned:

DCG(k) =

�

j:πi(j)≤k

G(j)D(πi(j)),

where G(·) is a gain function and D(·) is a position discount function. Note that πi(j) denotes the

position of di,j in πi.Therefore, the summation is taken over the top k positions in ranking list πi1.

DCG represents the cumulative gain of accessing the information from position one to position k

1Here, the deﬁnition of NDCG (or DCG) are formulated based on the indices of documents. It is also possible to deﬁne NDCG

(or DCG) based on the indices of positions.


18

2. LEARNING FOR RANKING CREATION

Table 2.3: Example Features of Learning to Rank for Web Search

Feature

Type

Explanation

Reference

Number of

occurrences

Matching

number of times query exactly occurs in

title, anchor, URL, extracted title, associ-

ated query, and body

BM25

Matching

BM25 scores on title, anchor, URL, ex-

tracted title, associated query, and body

[90]

N-gram BM25

Matching

BM25 scores of n-grams on title, anchor,

URL, extracted title, associated query,

and body

[109]

Edit Distance

Matching

edit distance scores between query and

title, anchor, URL, extracted title, associ-

ated query, and span in body (minimum

length of text segment including all query

words [94])

Our unpub-

lished work

Number of in-links

Document

number of in-links to the page

PageRank

Document

importance score of page calculated on

web link graph

[78]

Number of clicks

Document

number of clicks on the page in search log

BrowseRank

Document

importance score of page calculated on

user browsing graph

[72]

Spam score

Document

likelihood of spam page

[45]

Page quality score

Document

likelihood of low quality page

[10]

with discounts on the positions. NDCG is normalized DCG, and NDCG at position k for qi is

deﬁned:

NDCG(k) = DCG−1

max(k)

�

j:πi(j)≤k

G(j)D(πi(j)),

where DCGmax(k) is the normalizing factor and is chosen such that a perfect ranking π∗

i ’s NDCG

score at position k is 1. In a perfect ranking, the documents with higher grades are ranked higher.

Note that there can be multiple perfect rankings for a query and associated documents.

The gain function is normally deﬁned as an exponential function of grade. That is to say,

satisfaction of accessing information exponentially increases when grade of relevance increases. For

example,

G(j) = 2yi,j − 1,

where yi,j is the label (grade) of di,j in ranking list πi.The discount function is normally deﬁned as a

logarithmic function of position.That is to say, satisfaction of accessing information logarithmically


2.2. LEARNING TASK

19

decreases when position of information access increases.

D(πi(j)) =

1

log2(1 + πi(j)),

where πi(j) is the position of di,j in ranking list πi.

Hence, DCG and NDCG at position k for qi become

DCG(k) =

�

j:πi(j)≤k

2yi,j − 1

log2(1 + πi(j)),

NDCG(k) = DCG−1

max(k)

�

j:πi(j)≤k

2yi,j − 1

log2(1 + πi(j)).

DCG and NDCG of the whole ranking list for qi become

DCG =

�

j:πi(j)≤ni

2yi,j − 1

log2(1 + πi(j)),

NDCG = DCG−1

max

�

j:πi(j)≤ni

2yi,j − 1

log2(1 + πi(j)).

DCG and NDCG values are further averaged over queries (i = 1, · · · , m).

Table 2.4 gives examples of calculating NDCG values of two ranking lists. NDCG (DCG)

has the effect of giving high scores to the ranking lists in which relevant documents are ranked high.

See the examples in Table 2.4. For the perfect rankings, the NDCG value at each position is always

one, while for imperfect rankings, the NDCG values are less than one.

MAP is another measure widely used in IR.In MAP,it is assumed that the grades of relevance

are at two levels: 1 and 0. Given query qi, associated documents Di, ranking list πi on Di, and labels

yi of Di, Average Precision for qi is deﬁned:

AP =

�ni

j=1 P (j) · yi,j

�ni

j=1 yi,j

,

where yi,j is the label (grade) of di,j and takes on 1 or 0 as value, representing being relevant or

irrelevant. P (j) for query qi is deﬁned:

P (j) =

�

k:πi(k)≤πi(j) yi,k

πi(j)

,

where πi(j) is the position of di,j in πi. P (j) represents the precision until the position of di,j for

qi. Note that labels are either 1 or 0, and thus precision (i.e., ratio of label 1) can be deﬁned. Average

Precision represents averaged precision over all the positions of documents with label 1 for query qi.


20

2. LEARNING FOR RANKING CREATION

Table 2.4: Example of NDCG

Perfect ranking

Formula

Explanation

(3, 3, 2, 2, 1, 1, 1)

grades: 3,2,1

(7, 7, 3, 3, 1, 1, 1)

2yi,j − 1

gains

(1, 0.63, 0.5, · · · )

1/ log(πi(j) + 1)

position discounts

(7, 11.41, 12.91, · · · )

�

j:πi(j)≤k

2yi,j −1

log(πi(j)+1)

DCG scores

(1/7, 1/11.41, 1/12.91,· · · )

DCG−1

max(k)

normalizing factors

(1,1,1,· · · )

NDCG(k)

NDCG scores

Imperfect ranking

Formula

Explanation

(2, 3, 2, 3, 1, 1, 1)

grades: 3,2,1

(3, 7, 3, 7, 1, 1, 1)

2yi,j − 1

gains

(1, 0.63, 0.5, · · · )

1/ log(πi(j) + 1)

position discounts

(3, 7.41, 8.91, · · · )

�

j:πi(j)≤k

2yi,j −1

log(πi(j)+1)

DCG scores

(1/7, 1/11.41, 1/12.91,· · · )

DCG−1

max(k)

normalizing factors

(0.43, 0.65, 0.69, · · · )

NDCG(k)

NDCG scores

Table 2.5: Example of MAP

Perfect ranking

Formula

Explanation

(1, 0, 1, 1, 0, 0, 0)

Ranks:1,0

(1, -, 0.67, 0.75, −, −, −)

P (j)

Precision at position j with label 1

0.81

AP

Average Precision

Average Precision values are further averaged over queries to become Mean Average Precision

(MAP). Table 2.5 gives an example of calculating the AP value of one ranking list.

Kendall’s Tau is a measure proposed in statistics. It is deﬁned on two ranking lists: one is the

ranking list by the ranking model, and the other is by the ground truth. Kendall’s Tau of ranking list

πi with respect to ground truth π∗

i is deﬁned:

Ti =

2ci

1

2ni(ni − 1)

− 1,

where ci denotes the number of concordant pairs between the two lists, and ni denotes the length

of the two lists. For example, Kendall’s Tau between two ranking lists: (A,B,C) and (C,A,B) is as

follows.

Ti = 2 × 1

3

− 1 = −1

3.

Kendall’s Tau has values between −1 and +1. If the two ranking lists are exactly the same, then it

is +1. If one ranking list is in reverse order of the other, then it is −1. It is easy to verify Kendall’s


2.3. LEARNING APPROACHES

21

Tau can also be written as

Ti =

ci − di

1

2ni(ni − 1)

,

where di denotes the number of discordant pairs between the two lists.

2.2.5

RELATIONS WITH OTHER LEARNING TASKS

There are some similarities between ranking and other machine learning tasks such as classiﬁca-

tion, regression, and ordinal classiﬁcation (ordinal regression). Differences between them also exist,

however. That is why learning to rank is also an interesting research problem in machine learning.

In classiﬁcation, the input is a feature vector x ∈ ℜd, and the output is a label y ∈ Y, repre-

senting a class where Y is the set of class labels, and the goal of learning is to learn a classiﬁer f (x)

which can determine the class label y of a given feature vector x.

In regression, the input is a feature vector x ∈ ℜd, the output is a real number y ∈ ℜ, and the

goal of learning is to learn a function f (x) which can determine the real number y of a given feature

vector x.

Ordinal classiﬁcation (or ordinal regression) [30, 67, 92] is close to ranking, but it is also

different. The input is a feature vector x ∈ ℜd, the output is a label y ∈ Y, representing a grade

where Y is a set of grade labels. The goal of learning is to learn a model f (x) which can determine

the grade label y of a given feature vector x. The model ﬁrst calculates the score f (x), and then it

decides the grade label y using a number of thresholds. Speciﬁcally, the model segments the real

number axis into a number of intervals and assigns to each interval a grade. It then takes the grade

of the interval which f (x) falls into as the grade of x.

In ranking, one cares more about accurate ordering of objects (offerings), while in ordinal

classiﬁcation, one cares more about accurate ordered-categorization of objects. A typical example of

ordinal classiﬁcation is product rating. For example, given the features of a movie, we are to assign a

number of stars (ratings) to the movie.In that case,correct assignment of number of stars is critical.A

typical example of ranking is document retrieval. In document retrieval, given a query, the objective

is to give a right ranking on the documents, although sometimes training data and testing data are

labeled at multiple grades as in ordinal classiﬁcation. The number of documents to be ranked can

vary from query to query. There are queries for which more relevant documents are available in the

collection, and there are also queries for which only weakly relevant documents are available.

As will be seen later, ranking can be approximated by classiﬁcation, regression, and ordinal

classiﬁcation.

2.3

LEARNING APPROACHES

Learning to rank, particularly learning for ranking creation, has been intensively studied recently.

The proposed methods can be categorized as the pointwise approach,pairwise approach,and listwise

approach.There are also methods which may not belong to any of the approaches,for example,query

dependent ranking [41] and multiple nested ranking [74].


22

2. LEARNING FOR RANKING CREATION

Table 2.6: Categorization of Learning to Rank Methods

SVM

Boosting

Neural Net

Others

Pointwise

OC SVM [92]

McRank [67]

Prank [30]

Subset Ranking [29]

Pairwise

Ranking

SVM

[48]

RankBoost [37]

RankNet [11]

IR SVM [13]

GBRank [115]

Frank [97]

LambdaMART

[102]

LambdaRank

[12]

Listwise

SVM

MAP

[111]

AdaRank [108]

ListNet [14]

SoftRank [95]

PermuRank

[110]

ListMLE [104]

AppRank [81]

The pointwise and pairwise approaches transform the ranking problem into classiﬁcation,

regression, and ordinal classiﬁcation.The listwise approach takes ranking lists of objects as instances

in learning and learns the ranking model based on ranking lists. The main differences among the

approaches actually lie in the loss functions employed.

It is observed that the listwise approach and pairwise approach usually outperform the point-

wise approach. In the recent Yahoo Learning to Rank Challenge, LambdaMART, which belongs

to the pairwise approach, achieved the best performance.

The methods can also be categorized based on the learning techniques which they employ.

They include SVM techniques, Boosting techniques, Neural Net techniques, and others.

Table 2.6 gives a summary of the existing methods. Each of them will be described hereafter.

2.3.1

POINTWISE APPROACH

In the pointwise approach, the ranking problem (ranking creation) is transformed to classiﬁcation,

regression, or ordinal classiﬁcation, and existing methods for classiﬁcation, regression, or ordinal

classiﬁcation are applied. Therefore, the group structure of ranking is ignored in this approach.

More speciﬁcally, the pointwise approach takes training data in Figure 2.2 as input. It ig-

nores the group structure and combines all the groups together (xi,1, yi,1), · · · , (xi,ni, yi,ni), i =

1, · · · , m.The training data becomes typical supervised learning data (representing mapping from x

to y). When we take y as a class label, real number, and grade label, then the problem becomes clas-

siﬁcation, regression, and ordinal classiﬁcation, respectively. We can then employ existing methods

for classiﬁcation, regression, or ordinal classiﬁcation to perform the learning task.

Suppose that the learned model f (x) outputs real numbers. Then, given a query, we can use

the model to rank documents (sort documents according to the scores given by the model).The loss

function in learning is pointwise in the sense that it is deﬁned on a single object (feature vector).


2.3. LEARNING APPROACHES

23

Table 2.7: Characteristics of Pointwise Approach

Pointwise Approach (Classiﬁcation)

Learning

Ranking

Input

feature vector

feature vectors

x

x = {xi}n

i=1

Output

category

ranking list

y = classiﬁer(f (x))

sort({f (xi)}n

i=1)

Model

classiﬁer(f (x))

ranking model f (x)

Loss

classiﬁcation loss

ranking loss

Pointwise Approach (Regression)

Learning

Ranking

Input

feature vector

feature vectors

x

x = {xi}n

i=1

Output

real number

ranking list

y = f (x)

sort({f (xi)}n

i=1)

Model

regression model f (x)

ranking model f (x)

Loss

regression loss

ranking loss

Pointwise Approach (Ordinal Classiﬁcation)

Learning

Ranking

Input

feature vector

feature vectors

x

x = {xi}n

i=1

Output

ordered category

ranking list

y = threshold(f (x))

sort({f (xi)}n

i=1)

Model

threshold(f (x))

ranking model f (x)

Loss

ordinal classiﬁcation loss

ranking loss

Table 2.7 summarizes the main characteristics of the pointwise approach. The pointwise approach

includes Prank, OC SVM, McRank, and Subset Ranking. Chapter 4 explains Prank and OC SVM

in details. (See also [22, 23, 24]).

Crammer &amp; Singer [30] have studied ordinal classiﬁcation, which is to assign a grade to a

given object. The grades can be used for ranking, and thus their method can also be viewed as a

method for ranking. Crammer &amp; Singer propose a simple and efﬁcient online algorithm, called

Prank. Given training data, Prank iteratively learns a number of parallel Perceptron models, and

each model separates two neighboring grades. The authors have also conducted analysis on Prank

in terms of mistake bound.

Shashua &amp; Levin [92] propose a large margin approach to ordinal classiﬁcation, referred to as

OC SVM (Ordinal Classiﬁcation SVM) in this book. In their method, they also try to learn parallel

hyperplanes to separate neighboring grades, but their machinery is the Large Margin Principle.

They consider two ways of deﬁning the margin. The ﬁrst assumes that the margins between the


24

2. LEARNING FOR RANKING CREATION

neighboring grades are the same and the margin is maximized in learning.The second allows different

margins for different neighboring grades, and the sum of margins is maximized.

Li et al. [67] cast the ranking problem as multi-class classiﬁcation and propose the McRank

algorithm.The authors are motivated by the fact that the errors in ranking based on DCG is bounded

by the errors in multi-class classiﬁcation.They learn and exploit a classiﬁcation model that can assign

to an object probabilities of being members of grades. They then calculate the expected grades of

objects and use the expected grades to rank objects. The class probabilities are learned by using the

Gradient Boosting Tree algorithm.

Cossock &amp; Zhang [29] have developed the Subset Ranking algorithm. They ﬁrst consider

using DCG as evaluation measure. Since minimization of the loss function based on DCG is a non-

convex optimization problem, they instead manage to minimize a surrogate loss function which

is an upper bound of the original loss function. The surrogate loss function is deﬁned based on

regression errors. Therefore, the original ranking problem can be solved as a regression problem.

Cossock and Zhang then derive a learning method for the task on the basis of regression.They have

also investigated the generalization ability and the consistency of the learning method.

2.3.2

PAIRWISE APPROACH

In the pairwise approach, ranking is transformed into pairwise classiﬁcation or pairwise regression.

For example, a classiﬁer for classifying the ranking orders of document pairs can be created and

employed in ranking of documents. In the pairwise approach, the group structure of ranking is also

ignored.

Speciﬁcally, the pairwise approach takes training data in Figure 2.2 as input. From the labeled

data of query qi, (xi,1, yi,1), · · · , (xi,ni, yi,ni), i = 1, · · · , m, it creates preference pairs of feature

vectors (documents). For example, if xi,j has a higher grade than xi,k (yi,j &gt; yi,k), then xi,j ≻ xi,k

becomes a preference pair, which means that xi,j is ahead of xi,k .The preference pairs can be viewed

as instances and labels in a new classiﬁcation problem. For example, xi,j ≻ xi,k is a positive instance.

We can employ exiting classiﬁcation methods to train a classiﬁer to conduct the classiﬁcation. The

classiﬁer f (x) can then be used in ranking. More precisely, documents are assigned scores by f (x)

and sorted by the scores. Training of a good model for ranking is realized by training of a good

model for pairwise classiﬁcation. The loss function in learning is pairwise because it is deﬁned on a

pair of feature vectors (documents). Table 2.8 summarizes the main characteristics of the pairwise

approach.

The pairwise approach includes Ranking SVM, RankBoost, RankNet, IR SVM, GBRank,

Frank,LambdaRank,and LambdaMART.Chapter 4 introduces Ranking SVM,IR SVM,GBRank,

RankNet, and LambdaRank in details. (See also [35, 77, 86, 98, 116]).

Ranking SVM is one of the ﬁrst learning to rank methods, proposed by Herbrich et al. [48].

The basic idea is to formalize the ranking problem as pairwise classiﬁcation and employ the SVM

technique to perform the learning task.The objects in different grades are used to generate preference

pairs (which object is ahead of which) and viewed as data representing mappings from object pairs


2.3. LEARNING APPROACHES

25

Table 2.8: Characteristics of Pairwise Approach

Pairwise Approach (Classiﬁcation)

Learning

Ranking

Input

feature vectors

feature vectors

x(1), x(2)

x = {xi}n

i=1

Output

pairwise classiﬁcation

ranking list

classiﬁer(f (x(1)) − f (x(2)))

sort({f (xi)}n

i=1)

Model

classiﬁer(f (x))

ranking model f (x)

Loss

pairwise classiﬁcation loss

ranking loss

Pairwise Approach (Regression)

Learning

Ranking

Input

feature vectors

feature vectors

x(1), x(2)

x = {xi}n

i=1

Output

pairwise regression

ranking list

f (x(1)) − f (x(2))

sort({f (xi)}n

i=1)

Model

regression model f (x)

ranking model f (x)

Loss

pairwise regression loss

ranking loss

to orders. Herbrich et al. show that when the classiﬁer is a linear model (or a linear model after

applying the kernel trick), it can be directly employed in ranking.

TheRankBoostAlgorithmhasbeendevelopedbyFreundetal.[37].Intheirwork,theypresent

a formal framework for ranking, namely the problem of learning to rank objects by combining a

number of ranking features. They then propose the RankBoost algorithm based on the Boosting

technique. They show theoretical results describing the algorithm’s behavior both on the training

data and the test data. An efﬁcient implementation of the algorithm is also given.

Burges et al.[11] propose the RankNet algorithm,which is also based on pairwise classiﬁcation

like Ranking SVM and RankBoost. The major difference lies in that it employs Neural Network as

ranking model and uses Cross Entropy as loss function. Burges et al. also show the good properties

of Cross Entropy as loss function in ranking. Their method employs Gradient Descent to learn the

optimal Neural Network model.

The advantage of the above methods is that existing learning techniques on classiﬁcation and

regression can be directly applied.The disadvantage is that the goal of learning may not be consistent

with that of prediction. In fact, evaluation of ranking is usually conducted based on measures such as

NDCG, which are deﬁned on list of objects. In contrast, training in the pairwise approach is driven

by enhancement of accuracy of pairwise classiﬁcation or pairwise regression, which is deﬁned on

pairs of objects.

Cao et al. [13] propose employing cost sensitive Ranking SVM to overcome the shortcoming.

The authors point out that there are two factors one must consider in ranking for document retrieval.

First, correctly ranking documents on the top of list is crucial. Second, the number of relevant


26

2. LEARNING FOR RANKING CREATION

documents can vary from query to query. They modify the hinge loss function in Ranking SVM to

deal with the problems, and then they formalize the learning task as cost sensitive Ranking SVM.

The method is often referred to as IR SVM. Gradient Descent and Quadratic Programming are

respectively employed to solve the optimization problem in learning.

Zheng et al. [115] suggest employing pairwise regression for ranking. The GB (Gradient

BoostingTree) Rank algorithm is proposed.They ﬁrst introduce a regression framework for ranking,

which employs a pairwise regression loss function.They then propose a novel optimization method

based on the Gradient Boosting Tree algorithm to iteratively minimize the loss function. They use

two types of relevance judgments to derive training data: absolute relevance judgments by human

judges and relative relevance judgments extracted from click through data.

The Frank method proposed by Tsai et al. [97] is based on a similar motivation as IR SVM.

The authors propose employing a novel loss function in RankNet.The loss function named ‘Fidelity

Loss’ not only retains the good properties of the Cross Entropy loss, but it also possesses some

desirable new properties. Particularly, Fidelity Loss is bounded between 0 and 1, which makes the

learning method more robust against noises. In Frank, a Neural Network model is trained as ranking

model using Gradient Descent.

LambdaRank developed by Burges et al. [12] addresses the challenge by using an implicit

pairwise loss function2. Speciﬁcally, LambdaRank considers learning the optimal ranking model by

optimizing the loss function with Gradient Descent. In fact, it only explicitly deﬁnes the gradient

function of the loss function, referred to as Lambda Function. As example, LambdaRank employs

a Neural Network model. Burges et al. also give necessary and sufﬁcient conditions for the implicit

cost function to be convex.

Wu et al.[102] propose a new method called LambdaMART,using Boosting and the Lambda

function in LambdaRank. It employs the MART (Multiple Additive Regression Trees) algorithm

[38] to learn a boosted regression tree as ranking model. MART is actually an algorithm conducting

Gradient Descent in the functional space.LambdaMART speciﬁcally employs the Lambda function

asthegradientsinMART.Wuetal.haveveriﬁedthattheefﬁciencyofLambdaMARTissigniﬁcantly

better than LambdaRank, and the accuracy of it is also higher.

2.3.3

LISTWISE APPROACH

The listwise approach addresses the ranking problem in a more natural way.Speciﬁcally,it takes rank-

ing lists as instances in both learning and prediction. The group structure of ranking is maintained

and ranking evaluation measures can be more directly incorporated into the loss functions.

More speciﬁcally, the listwise approach takes training data in Figure 2.2 as input. It views the

labeled data (xi,1, yi,1), · · · , (xi,ni, yi,ni) associated with query qi as one instance. The approach

learns a ranking model f (x) from the training data that can assign scores to feature vectors (doc-

uments) and rank the feature vectors using the scores, such that feature vectors with higher grades

are ranked higher. This is a new problem for machine learning and conventional techniques in

2The general formulation of LambdaRank can be either listwise or pairwise, but its speciﬁc implementation in practice is pairwise.


2.3. LEARNING APPROACHES

27

machine learning cannot be directly applied. Recently, advanced learning to rank techniques have

been developed, and many of them belong to the listwise approach. Table 2.9 summarizes the main

characteristics of the listwise approach.

The listwise approach includes ListNet, ListMLE, AdaRank, SVM MAP, Soft Rank, and

AppRank. Chapter 4 describes ListNet, ListMLE, AdaRank, SVM MAP, and Soft Rank in details.

(See also [18, 85, 99, 100]).

Cao et al. [14] point out the importance of employing the listwise approach to ranking, in

which lists of objects are treated as ’instances’.They propose using the Luce-Plackett model to calcu-

late the permutation probability or top k probability of list of objects. The ListNet algorithm based

on the idea is then developed. In the method, a Neural Network model is employed as model, and

KL divergence is employed as loss function.The permutation probability or top k probability of a list

of objects is calculated by the Luce-Plackett model. KL divergence measures the difference between

the learned ranking list and the true ranking list using their permutation probability distributions

or top k probability distributions. Gradient Descent is utilized as optimization algorithm. Xia et al.

[104] extend ListNet to ListMLE, in which log likelihood is deﬁned as loss function. The learning

of ListMLE is equivalent to Maximum Likelihood Estimation on the parameterized Luce-Plackett

model.

The evaluation measures in applications such as those in IR are deﬁned on list of objects.

Ideally, a learning algorithm would train a ranking model that could directly optimize the evaluation

measures. Another group of listwise methods try to directly optimize the evaluation measures in

learning.These include AdaRank developed by Xu &amp; Li [108]. AdaRank minimizes a loss function

directly deﬁned on an evaluation measure by using the Boosting technique. It repeatedly constructs

’weak rankers’on the basis of reweighted training data and ﬁnally linearly combines the weak rankers

for ranking prediction. AdaRank is a very simple and efﬁcient learning to rank algorithm.

The algorithm of SVM MAP proposed by Yue et al. [111] also considers direct optimization

of evaluation measures, particularly, MAP used in IR. SVM MAP is an SVM learning algorithm

that can efﬁciently ﬁnd a globally optimal solution to minimization of an upper bound of the loss

function based on MAP. Xu et al. [110] show that one can extend the idea to derive a group of

Table 2.9: Characteristics of Listwise Approach

Listwise Approach

Learning

Ranking

Input

feature vectors

feature vectors

x = {xi}n

i=1

x = {xi}n

i=1

Output

ranking list

ranking list

sort({f (xi)}n

i=1)

sort({f (xi)}n

i=1)

Model

ranking model f (x)

ranking model f (x)

Loss

listwise loss function

ranking loss


28

2. LEARNING FOR RANKING CREATION

algorithms. The key idea of these algorithms is to introduce loss functions based on IR evaluation

measures (deﬁned on lists of objects), consider different upper bounds of the loss functions, and

employ the SVM technique to optimize the upper bounds. Different upper bounds and different

optimization techniques can lead to different algorithms including one called PermuRank.

Another listwise approach tries to approximate the evaluation measures. SoftRank is a repre-

sentative algorithm of such an approach. The major challenge for direct optimization of evaluation

measures is that they are not directly optimizable due to the nature of the loss functions. Taylor et

al. [95] present a soft (approximate) way of calculating the distribution of ranks of objects. With an

approximated rank distribution, it is possible to approximately calculate evaluation measures such

as NDCG. They show that SoftRank is a very effective way of optimizing evaluation measures.

Qin et al. [81] propose a general framework for direct optimization of IR measures in learning

to rank. In the framework, the IR measures such as NDCG and MAP are approximated as surrogate

functions, and the surrogate functions are then optimized.The key idea of the approach is as follows.

The difﬁculty in directly optimizing IR measures lies in that the measures are rank based and thus

are non-continuous and non-differentiable with respect to the score output by the ranking function.

The proposed approach approximates the ranks of documents by a continuous and differentiable

function. An algorithm based on the framework is developed, which is called AppRank.

2.3.4

EVALUATION RESULTS

According to the previous studies, the listwise approach and the pairwise approach usually work

better than the pointwise approach. As in other machine learning tasks, there is no single method

that can always outperforms the other methods. This is a general trend on the learning to rank

methods.

Tables 2.10-2.16 give the experimental results on a number of methods in terms of NDCG on

the LETOR datasets [82]. The LETOR data sets are benchmark data for learning to rank, derived

from TREC data by researchers in Microsoft Research 3. One can get a rough sense about the

performances achieved by the methods.

In the Yahoo Learning to Rank Challenge 4, the pairwise approach of LambdaMART

achieved the best performance. In fact, the accuracies by the top performers were very close to

each other.

3http://research.microsoft.com/en-us/um/beijing/projects/letor/

4http://learningtorankchallenge.yahoo.com/


2.3. LEARNING APPROACHES

29

Table 2.10: NDCG on TD2003 Dataset

Method

NDCG@1

NDCG@3

NDCG@5

NDCG@10

Regression

0.32

0.31

0.30

0.33

Ranking SVM

0.32

0.34

0.36

0.35

RankBoost

0.28

0.32

0.31

0.31

FRank

0.30

0.27

0.25

0.27

ListNet

0.40

0.34

0.34

0.35

AdaRank-MAP

0.26

0.31

0.30

0.31

AdaRank-NDCG

0.36

0.29

0.29

0.30

SVMMAP

0.32

0.32

0.33

0.33

Table 2.11: NDCG on TD2004 Dataset

Method

NDCG@1

NDCG@3

NDCG@5

NDCG@10

Regression

0.36

0.34

0.33

0.30

Ranking SVM

0.41

0.35

0.32

0.31

RankBoost

0.51

0.43

0.39

0.35

FRank

0.49

0.39

0.36

0.33

ListNet

0.36

0.36

0.33

0.32

AdaRank-MAP

0.41

0.38

0.36

0.33

AdaRank-NDCG

0.43

0.37

0.35

0.32

SVMMAP

0.29

0.30

0.30

0.29

Table 2.12: NDCG on NP2003 Dataset

Method

NDCG@1

NDCG@3

NDCG@5

NDCG@10

Regression

0.45

0.61

0.64

0.67

Ranking SVM

0.58

0.77

0.78

0.80

RankBoost

0.60

0.76

0.78

0.81

FRank

0.54

0.73

0.76

0.78

ListNet

0.57

0.76

0.78

0.80

AdaRank-MAP

0.58

0.73

0.75

0.76

AdaRank-NDCG

0.56

0.72

0.74

0.77

SVMMAP

0.56

0.77

0.79

0.80


30

2. LEARNING FOR RANKING CREATION

Table 2.13: NDCG on NP2004 Dataset

Method

NDCG@1

NDCG@3

NDCG@5

NDCG@10

Regression

0.37

0.56

0.61

0.65

Ranking SVM

0.51

0.75

0.80

0.81

RankBoost

0.43

0.63

0.65

0.69

FRank

0.48

0.64

0.69

0.73

ListNet

0.53

0.76

0.80

0.81

AdaRank-MAP

0.48

0.70

0.73

0.75

AdaRank-NDCG

0.51

0.67

0.71

0.74

SVMMAP

0.52

0.75

0.79

0.81

Table 2.14: NDCG on HP2003 Dataset

Method

NDCG@1

NDCG@3

NDCG@5

NDCG@10

Regression

0.42

0.51

0.55

0.59

Ranking SVM

0.69

0.77

0.80

0.81

RankBoost

0.67

0.79

0.80

0.82

FRank

0.65

0.74

0.78

0.80

ListNet

0.72

0.81

0.83

0.84

AdaRank-MAP

0.73

0.81

0.83

0.84

AdaRank-NDCG

0.71

0.79

0.80

0.81

SVMMAP

0.71

0.78

0.79

0.80

Table 2.15: NDCG on HP2004 Dataset

Method

NDCG@1

NDCG@3

NDCG@5

NDCG@10

Regression

0.39

0.58

0.61

0.65

Ranking SVM

0.57

0.71

0.75

0.77

RankBoost

0.51

0.70

0.72

0.74

FRank

0.60

0.73

0.75

0.76

ListNet

0.60

0.72

0.77

0.78

AdaRank-MAP

0.61

0.82

0.83

0.83

AdaRank-NDCG

0.59

0.75

0.79

0.81

SVMMAP

0.63

0.75

0.80

0.81


2.3. LEARNING APPROACHES

31

Table 2.16: NDCG on OHSUMED Dataset

Method

NDCG@1

NDCG@3

NDCG@5

NDCG@10

Regression

0.45

0.44

0.43

0.41

Ranking SVM

0.50

0.42

0.42

0.41

RankBoost

0.46

0.46

0.45

0.43

FRank

0.53

0.48

0.46

0.44

ListNet

0.53

0.47

0.44

0.44

AdaRank-MAP

0.54

0.47

0.46

0.44

AdaRank-NDCG

0.53

0.48

0.47

0.45

SVMMAP

0.52

0.47

0.45

0.43



33

C H A P T E R

3

Learning for Ranking

Aggregation

This chapter gives a general introduction to learning for ranking aggregation.Ranking aggregation is

aimed at combining multiple rankings into a single ranking, which is better than any of the original

rankings in terms of an evaluation measure. Learning for ranking aggregation is about building a

ranking model for ranking aggregation using machine learning techniques.

Hereafter, we take meta-search as an example to make the explanation. Without loss of

generality, the technologies described can be applied to other applications.

3.1

LEARNING TASK

In meta-search, the query from the user is sent to multiple search systems, and the ranking lists

from the search systems are then combined and presented to the user in a single ranking list. Since

the ranking lists from individual search systems may not be accurate enough, meta-search actually

takes a majority voting over search ranking lists.The question is then how to effectively perform the

majority voting. Here we call the rankings from individual search systems basic rankings, and the

ranking in meta search ﬁnal ranking.

Learning for ranking aggregation can be performed either as unsupervised learning or su-

pervised learning. In traditional IR, ranking aggregation is usually based on unsupervised learning.

Recently, supervised methods for ranking aggregation have also been proposed.

In supervised learning for ranking aggregation, the training data contains queries, their asso-

ciated documents and basic rankings on the documents, as well as the corresponding ﬁnal rankings.

The testing data includes query, associated documents, and basic rankings on the documents.

Suppose that Q is the query set, and D is the document set. Further suppose that

{q1, q2, · · · , qm} is the set of queries in training data. Di = {di,1, di,2, · · · , di,ni} is the set of docu-

ments associated with query qi, �i = {σi,1, σi,2, · · · , σi,k} is the set of basic rankings on the docu-

ments in Di with respect to query qi, and πi is the ﬁnal ranking on the documents in Di with respect

to query qi. Here, di,j denotes the jth document in Di, σi,j denotes the jth basic ranking in �i, and

k denotes the number of basic rankings. The training set is represented as S = {(qi, �i), πi}m

i=1.

In learning, a model for ranking aggregation is constructed, which takes the form of F(q, �) :

Q × �k → ℜn, where q is a query, D is a set of associated documents, � is a set of basic rankings

on the documents in D with respect to q, n denotes the number of documents, and k denotes the

number of basic rankings. F(q, �) can assign scores to the documents in D, sort the documents


34

3. LEARNING FOR RANKING AGGREGATION

Table 3.1: Summary of Notations

Notation

Explanation

Q

query set

D

document set

S = {(qi, �i), πi}m

i=1

training data set

qi ∈ Q

i-th query in training data

Di = {di,1, di,2, · · · , di,ni}

set of documents associated with qi

di,j ∈ D

j-th document in Di

�i = {σi,1, σi,2, · · · , σi,k}

set of basic rankings on Di with respect to qi

σi,j ∈ �i, (j = 1, · · · , k)

j-th basic ranking on Di with respect to qi

πi ∈ �i

ﬁnal ranking on Di for qi

�i

set of possible rankings on Di with respect to qi

F(q, �)

model for ranking aggregation

T = {(qm+1, �m+1)}

test data set

qm+1 ∈ Q

query in test data

Dm+1 = {dm+1,1, dm+1,2, · · · , dm+1,nm+1}

set of documents associated with qm+1

�m+1 = {σm+1,1, σm+1,2, · · · , σm+1,k}

set of rankings on Dm+1 with respect to qm+1

according to the scores, and generate a ﬁnal ranking.

SD = F(q, �)

π = sortSDD.

Note that F is a global ranking function in the sense that it is deﬁned on a set of documents.

The test data consists of query qm+1, associated documents Dm+1, and basic rankings on the

documents �m+1. We use the trained ranking model F(q, �) to assign scores to the documents in

Dm+1, sort them based on the scores, and give the ﬁnal ranking list. The test data set is represented

as T = {(qm+1, �m+1)}.

Table 3.1 gives a summary of notations.

Ranking aggregation is generally deﬁned as a query dependent task so far. In practice, it is

usually speciﬁed as a query independent task. That is,

F(q, �) = F(�).

Note that we use integers to represent documents. Let the documents in Di be identiﬁed by

the integers {1, 2, · · · , ni}. We deﬁne permutation πi on Di as a bijection from {1, 2, · · · , ni} to

itself. We use �i to denote the set of all possible ranking lists (permutations) on Di, and use πi(j)

and π−1

i

(j) to respectively denote the rank of the j-th document and the document at the j-th

rank.


3.2. LEARNING METHODS

35

Note that for simplicity, it is assumed here that all the basic rankings are on the same set of

documents Di.In practice,different basic rankings can be provided on different subsets of documents.

In such case, one can just take the union of the subsets and deﬁne the union as Di.

Evaluation measures in ranking aggregation can be any measure in learning to rank,depending

on how the ground truth is given. For example, if the ﬁnal ranking in ground truth is given as a

ranking list, then Kendal’s Tau can be used. If the ﬁnal ranking is based on grades, then MAP or

NDCG can be employed.

3.2

LEARNING METHODS

Existing methods for ranking aggregation includes unsupervised learning methods such as Borda

Count and Markov Chain, and supervised learning methods such as Cranking. We give detailed

explanations on Borda Count, Markov Chain, and Cranking in Chapter 4. See also [26, 59, 71].

Borda Count is an election method in which the best candidate is selected based on voters’

rankings of candidates. Aslam &amp; Montague [8] have applied it to meta-search and have veriﬁed

the effectiveness of the method in meta-search. Borda Count is a simple method as follows. First,

each voter gives a ranking of the n candidates (objects). For each ranking, the top ranked candidate

receives n points,the second ranked candidate receives n-1 points,and so on.Then the candidates are

ranked in descending order of their total points, generating the ﬁnal ranking list, and the candidate

with the most points wins.

Markov Chain based ranking aggregation assumes that there exists a Markov Chain on the

objects. The basic rankings of objects are utilized to construct the Markov Chain, in the way that

the transition probabilities are estimated based on the order relations in the rankings.The stationary

distribution of the Markov Chain is then utilized to rank the objects in the ﬁnal ranking. Dwork

et al. [34] propose four methods for constructing the transition probability matrix of the Markov

Chain, which leads to four different Markov Chain methods for ranking aggregation.

Cranking proposed by Lebanon &amp; Lafferty [63] employs a generalization of the Mallows

model for ranking aggregation.The generalized Mallows model, which is in the form of exponential

function, deﬁnes a conditional probability distribution of ﬁnal ranking of objects given multiple

basic rankings of objects. The model can be viewed as a general probabilistic model for ranking

aggregation. Lebanon &amp; Lafferty propose several learning methods for estimating the parameters

of the generalized Mallows model under different settings, particularly when the rankings in the

training data are only partially available.



37

C H A P T E R

4

Methods of Learning to Rank

This chapter describes in details about eleven methods for ranking creation, including PRank [30],

OC SVM [92], Ranking SVM [47, 48], IR SVM [13], GBRank [114, 115], RankNet [11], Lamb-

daRank [12, 32], ListNet &amp; ListMLE [14, 104], AdaRank [108], SVM MAP [111], and SoftRank

[43, 95], and three methods for ranking aggregation, including Borda Count [34], Markov Chain

[34], and CRanking [63].

4.1

PRANK

PRank (Perceptron Ranking) is an online algorithm for ordinal classiﬁcation proposed by Crammer

&amp; Singer [30]. One can employ it in ranking (ranking creation) as a pointwise method. The basic

idea of PRank is to utilize and learn a number of parallel Perceptron models while each model makes

classiﬁcation between the neighboring grades.

4.1.1

MODEL

Suppose that X ⊆ ℜd and Y = {1, 2, · · · , l} where there is a total order on Y. x ∈ X is an object

(feature vector) and y ∈ Y is a label representing grade. Given object x, we aim to predict its

label (grade) y. That is to say, this is an ordinal classiﬁcation problem. We employ a number of

linear models (Perceptrons) ⟨w, x⟩ − br, (r = 1, · · · , l − 1) to make the prediction, where w ∈ ℜd

is a weight vector and br ∈ ℜ, (r = 1, · · · , l) are biases satisfying b1 ≤ · · · ≤ bl−1 ≤ bl = +∞.

The models correspond to parallel hyperplanes ⟨w, x⟩ − br = 0 separating grades r and r + 1,

(r = 1, · · · , l − 1). Here ⟨⟩ denotes dot product. Figure 4.1 gives an example of PRank models. If

x satisﬁes ⟨w, x⟩ − br−1 ≥ 0 and ⟨w, x⟩ − br &lt; 0, then y = r, (r = 1, · · · , l). We can write it as

minr∈{1,··· ,l}{r|⟨w, x⟩ − br &lt; 0}.

4.1.2

LEARNING ALGORITHM

PRank employs the Perceptron learning algorithm [91] to simultaneously learn the linear models

online.The Perceptron learning algorithm is based on Stochastic Gradient Descent,and so is PRank.

PRank takes one input pair at each round. Suppose that for the current round, the input pair is

(x, y), and we are to update the weights w and biases br, (r = 1, · · · , l − 1). For simplicity, we omit

the superscript representing the round here. Given a feature vector x, the current models can predict

a grade ˆy for it. Speciﬁcally, if x satisﬁes ⟨w, x⟩ − br−1 ≥ 0 and ⟨w, x⟩ − br &lt; 0, then the predicted

grade should be ˆy = r,(r = 1, · · · , l).On the other hand,given the true grade label y,it is also possi-

bletosaywhichmodelsshouldpredictthefeaturevectoraspositiveexampleandwhichmodelsshould


38

4. METHODS OF LEARNING TO RANK

w

r=2

r=3

r=1

Figure 4.1: PRank Model

predict it as negative example. We use variables (z1, · · · , zl−1) = (+1, · · · , +1, −1, · · · , −1) to

represent the fact, where the ﬁrst y − 1 variables correspond to the models which should make

positive predictions (+1) and the rest variables the models which should make negative predictions

(−1).Thus,if the prediction of a model is correct,then zr(⟨w, x⟩ − br) &gt; 0 holds;if the prediction is

incorrect, then zr(⟨w, x⟩ − br) ≤ 0 holds (r = 1, · · · , l − 1). When an error occurs, PRank adjusts

the weights for all the models making the error. Speciﬁcally, PRank updates those models’ biases br

with br − zr and updates the weights w with w + (� zr)x where the sum is taken over the models

making the error.

Figure 4.2 shows the PRank algorithm.

4.2

OC SVM

The method proposed by Shashua &amp; Levin [92] also utilizes a number of parallel hyperplanes as

ranking model.Their method learns the parallel hyperplanes by the Large Margin principle. In one

implementation, the method tries to maximize a ﬁxed margin for all the neighboring grades.

4.2.1

MODEL

Suppose that X ⊆ ℜd and Y = {1, 2, · · · , l} where there exists a total order on Y. x ∈ X is feature

vector and y ∈ Y is a label representing a grade. As in PRank, we employ a number of parallel

hyperplanes ⟨w, x⟩ − br = 0, (r = 1, · · · , l − 1) to predict the label y of a given feature vector x,

where w ∈ ℜd is a weight vector and br ∈ ℜ, (r = 1, · · · , l) are biases satisfying b1 ≤ · · · ≤ bl−1 ≤


4.2. OC SVM

39

Input: training data {(xt, yt)}T

i=1

Initialize w = 0, b1, · · · , bl−1 = 0, bl = ∞.

For t = 1, · · · , T

• Get a feature vector xt ∈ Rn

• Predict ˆyt = minr∈{1,··· ,l}{r|⟨w, xt⟩ − br &lt; 0}

• Get a new label yt

• If ˆyt ̸= yt then update w

– For r = 1, · · · , l − 1 : if yt ≤ r then zr = −1, else zr = +1

– For r = 1, · · · , l − 1 : if zr(⟨w, xt⟩ − br) ≤ 0 then ζr = zr, else ζr = 0

– Update w = w + (� ζr)x

– For r = 1, · · · , l − 1 update br = br − ζr

End For

Output: the ranking model ⟨w, x⟩ − br = 0, r = 1, · · · , l − 1

Figure 4.2: PRank Algorithm

bl = +∞. If x satisﬁes ⟨w, x⟩ − br−1 ≥ 0 and ⟨w, x⟩ − br &lt; 0, then y = r, (r = 1, · · · , l). That

is to say, the prediction is based on minr∈{1,··· ,l}{r|⟨w, x⟩ − br &lt; 0}.

4.2.2

LEARNING ALGORITHM

OC SVM assumes that the parallel hyperplanes separate the instances in any two adjacent grades

with the same large margin (Figure 4.3).

Suppose that the training data is given as follows. For each grade r = 1, · · · , l, there are mr

instances: xr,i, i = 1, · · · , mr.The learning task is formalized as the following Quadratic Program-

ming (QP) problem.

minw,b,ξ 1

2||w||2 + C �l−1

r=1

�mr

i=1(ξr,i + ξ∗

r+1,i)

s. t. ⟨w, xr,i⟩ − br ≤ −1 + ξr,i

⟨w, xr+1,i⟩ − br ≥ 1 + ξ∗

r+1,i

ξr,i ≥ 0, ξ∗

r+1,i ≥ 0

i = 1, · · · , mr, r = 1, · · · , l − 1

m = m1 + · · · + ml

(4.1)


40

4. METHODS OF LEARNING TO RANK

w

r=2

r=3

r=1

Figure 4.3: OC SVM Model

where xr,i denotes the i-th instance in the r-th grade, ξr+1,i and ξ∗

r+1,i denote the corresponding

slack variables,|| · || denotes L2 norm,m is the number of training instances,and C &gt; 0 is coefﬁcient.

The detailed way of solving the problem can be found in [92].

Figure 4.4 shows the learning algorithm of OC SVM.

Input: training data {(xi, yi)}m

i=1

Solve the QP problem in (4.1)

Output: the ranking model ⟨w, x⟩ − br = 0, r = 1, · · · , l − 1

Figure 4.4: Learning Algorithm of OC SVM

4.3

RANKING SVM

Ranking SVM is one of the ﬁrst learning to rank methods, proposed by Herbrich et al. [47, 48].

The basic idea of Ranking SVM is to transform ranking into pairwise classiﬁcation and employ the

SVM technique [27] to perform the learning task.

4.3.1

LINEAR MODEL AS RANKING FUNCTION

Assume that X ⊆ ℜd is the feature space and x ∈ X is an element in the space (feature vector).

Further suppose that f is a scoring function f : X → ℜ.Then one can rank feature vectors (objects)

in X with f (x). That is to say, given any two feature vectors xi ∈ X and xj ∈ X, if f (xi) &gt; f (yj),


4.3. RANKING SVM

41

w

1x

2x

w

rank 1

rank 2

rank 3

3x

Figure 4.5: Ranking Problem

then xi should be ranked ahead of xj, and vice versa.

xi ≻ xj ⇔ f (xi) &gt; f (xj).

In principle, function f (x) can be any function. For simplicity, let us suppose that f (x) is a

linear function for the time being.

f (x) = ⟨w, x⟩,

where w denotes a weight vector and ⟨·⟩ denotes inner product.

We can transform the ranking problem into a binary classiﬁcation problem if the scoring

function is a linear function. The reason is as follows.

First, the following relation holds for any two feature vectors xi and xj, when f (x) is a linear

function.

f (xi) &gt; f (xj) ⇔ ⟨w, xi − xj⟩ &gt; 0.

Next, for any two feature vectors xi and xj, we can consider a binary classiﬁcation problem

on the difference of the feature vectors xi − xj. Speciﬁcally, we assign a label y to it.

y =

� +1,

if xi − xj &gt; 0

−1,

if xi − xj &lt; 0

Hence,

⟨w, xi − xj⟩ &gt; 0 ⇔ y = +1.


42

4. METHODS OF LEARNING TO RANK

Therefore, the following relation holds. That is to say, if xi is ranked ahead of xj, then y is

+1, otherwise, y is −11.

xi ≻ xj ⇔ y = +1.

4.3.2

RANKING SVM MODEL

We can learn and utilize a linear classiﬁer, such as Linear SVM for the ranking task. The classiﬁer

can be directly used as ranking model. One can also extend the linear model to a non-linear model

by using the kernel trick. We call the above method ‘Ranking SVM’.

Figure 4.5 shows an example ranking problem. Suppose that there are two groups of objects

(documents associated with two queries) in the feature space. Further suppose that there are three

grades (levels). For example, objects x1, x2, and x3 in the ﬁrst group are at three different grades.

The weight vector w corresponds to the linear function f (x) = ⟨w, x⟩, which can score and rank

the objects. Ranking objects with the function is equivalent to projecting the objects into the vector

and sorting the objects according to the projections on the vector. If the ranking function is ‘good’,

then there should be an effect that objects at grade 3 are ranked ahead of objects at grade 2, etc.

Note that objects belonging to different groups are incomparable.

Figure 4.6 shows how the ranking problem in Figure 4.5 can be transformed to Linear SVM

classiﬁcation. The differences between two feature vectors at different grades in the same group

are treated as new feature vectors, e.g., x1 − x2, x1 − x3, and x2 − x3. Furthermore, labels are also

assigned to the new feature vectors. For example, x1 − x2, x1 − x3, and x2 − x3 are positive. Note

that feature vectors at the same grade or feature vectors from different groups are not utilized to

create new feature vectors. One can train a Linear SVM classiﬁer, which separates the new feature

vectors as shown in Figure 4.6. Note that the hyperplane of the SVM classiﬁer passes the original,

and the positive and negative instances are anti-symmetric. For example, x1 − x2 and x2 − x1 are

positive and negative instances, respectively. In fact, we can discard the negative instances in learning

because they are redundant.

4.3.3

LEARNING ALGORITHM

More formally, Ranking SVM is formalized as the following constrained optimization problem

(Quadratic Programming). We ﬁrst consider the linear case,

minw,ξ 1

2||w||2 + C �N

i=1 ξi

s. t. yi⟨w, x(1)

i

− x(2)

i

⟩ ≥ 1 − ξi

ξi ≥ 0

i = 1, . . . , N

where x(1)

i

and x(2)

i

denote the ﬁrst and second feature vectors in a pair of feature vectors, || · ||

denotes L2 norm, N is the number of training instances, and C &gt; 0 is coefﬁcient.

1For ease of explanation, we do not consider the case in which there is a tie. One can make an extension to handle it.


4.3. RANKING SVM

43

)

;

(

w

x

f

3

1

x

x &amp;

3

2

x

x &amp;

Positive Instances

2

1

x

x &amp;

+1

-1

1

2

x

x &amp;

2

3

x

x &amp;

1

3

x

x &amp;

Negative Instances

Figure 4.6: Transformation to Pairwise Classiﬁcation

It is equivalent to the following non-constrained optimization problem, i.e., minimization of

regularized hinge loss function.

min

w

N

�

i=1

[1 − yi⟨w, x(1)

i

− x(2)

i

⟩]+ + λ||w||2,

(4.2)

where [x]+ denotes function max(x, 0) and λ =

1

2C . The primal QP problem can be solved by

solving the dual problem.

maxα − 1

2

�N

i=1

�N

j=1 αiαjyiyj⟨x(1)

i

− x(2)

i

, x(1)

j

− x(2)

j ⟩ + �N

i=1 αi

s. t. �N

i=1 αiyi = 0

0 ≤ αi ≤ C,

i = 1, · · · , N.

(4.3)

The optimal solution, used as ranking function, is given as

f (x) =

N

�

i=1

α∗

i yi⟨x, x(1)

i

− x(2)

i

⟩.

(4.4)

The learning algorithm of Ranking SVM is summarized in Figure 4.7.


44

4. METHODS OF LEARNING TO RANK

We can also generalize the above problem to the non-linear case by using the kernel trick.

maxα − 1

2

�N

i=1

�N

j=1 αiαjyiyjK(x(1)

i

− x(2)

i

, x(1)

j

− x(2)

j ) + �N

i=1 αi

s. t. �N

i=1 αiyi = 0

0 ≤ αi ≤ C,

i = 1, · · · , N.

The optimal ranking function is in the form

f (x) =

N

�

i=1

α∗

i yiK(x, x(1)

i

− x(2)

i

).

Parameter: C

Input: training data {((x(1)

i

, x(2)

i

), yi)}, i = 1, · · · , N

Solve the dual problem in (4.3) to obtain the optimal parameters α∗

i , (i = 1, · · · , N)

Output: the ranking model in (4.4)

Figure 4.7: Learning Algorithm of Ranking SVM

4.4

IR SVM

IR SVM proposed by Cao et al. [13] is an extension of Ranking SVM for information retrieval,

whose idea can be applied to other applications as well.

4.4.1

MODIFIED LOSS FUNCTION

Ranking SVM transforms ranking into pairwise classiﬁcation, and thus it actually makes use of 0-1

loss in the learning process. There exists a gap between the loss function and the IR evaluation

measures. IR SVM attempts to bridge the gap by modifying the 0-1 loss, that is, conducting cost

sensitive learning of Ranking SVM.

We ﬁrst look at the problems caused by straightforward application of Ranking SVM to

document retrieval, using examples in Figure 4.8.

One problem with direct application of Ranking SVM is that it equally treats document pairs

across different grades. In example 1, there are three pairs of documents. They are document pairs

with label pairs (grade pairs) 3-2, 3-1, and 2-1, respectively. Ranking SVM uses the same 0-1 loss

for the document pairs. This is in contrast to the fact that different document pairs should have

different importance in ranking. Actually, making correct ordering on the pair of 3-1 (ranking 3

ahead of 1) is more critical than the other pairs. Example 2 indicates the problem from another

perspective. There are two rankings for the same query. In ranking-1 the documents at positions 1

and 2 are swapped from the perfect ranking, while in ranking-2 the documents at positions 3 and 4


4.4. IR SVM

45

Grade: 3, 2, 1

Documents are represented by their grades

Example 1:

ranking for query-1: 3 2 1

Example 2:

ranking-1 for query-2: 2 3 2 1 1 1 1

ranking-2 for query-2: 3 2 1 2 1 1 1

Example 3:

ranking for query-3: 3 2 2 1 1 1 1

ranking for query-4: 3 3 2 2 2 1 1 1 1 1

Figure 4.8: Example Ranking Lists

are swapped from the perfect ranking. There is only one error for each ranking in terms of 0-1 loss.

Therefore, they have the same effect on training of Ranking SVM, which is not desirable. Actually,

ranking-2 should be better than ranking-1, from the viewpoint of IR, because the result of its top is

better. Again, to have a high accuracy on top-ranked documents is crucial for an IR system, which

is reﬂected in the IR evaluation measures.

Another issue with Ranking SVM is that it equally treats document pairs from different

queries. The numbers of documents usually vary from query to query. In example 3, there are two

queries, and the numbers of documents associated with them are different. For query3 there are 2

document pairs between grades 3-2,4 document pairs between grades 3-1,8 document pairs between

grades 2-1, and in total 14 document pairs. For query4, there are 31 document pairs. Ranking SVM

takes 14 instances (document pairs) from query3 and 31 instances (document pairs) from query4 for

training. Thus, the impact on the training process from query4 will be larger than the impact from

query3. In other words, the model learned will be biased toward query4. This is in contrast to the

fact that in IR evaluation queries are evenly important.

IR SVM addresses the above two problems by changing the 0-1 classiﬁcation into a cost

sensitive classiﬁcation.It does so by modifying the hinge loss function of Ranking SVM.Speciﬁcally,

it sets different losses for document pairs across different grades and from different queries. To

emphasize the importance of correct ranking on the top, the loss function heavily penalizes errors on

the top.To increase the inﬂuences of queries with less documents, the loss function heavily penalizes

errors from such queries.

Figure 4.9 plots the shapes of different hinge loss functions with different penalty parameters.

The x-axis represents yf (x(1)

i

− x(2)

i

) and the y-axis represents loss. When yf (x(1)

i

− x(2)

i

) ≥ 1 ,

the losses are zero. When yf (x(1)

i

− x(2)

i

) &lt; 1, the losses are linearly decreasing functions with

different slopes. If the slope equals -1, then the function is the normal hinge loss function. IR

SVM modiﬁes the hinge loss function, speciﬁcally modiﬁes the slopes for different grade pairs and


46

4. METHODS OF LEARNING TO RANK

1

=

'(

2

=

'(

Loss

0.5

=

'(

1

+

)

(x

yf

Figure 4.9: Modiﬁed Hinge Loss Functions

different queries. It assigns higher weights to document pairs belonging to important grade pairs

and normalizes weights of document pairs according to queries.

4.4.2

LEARNING ALGORITHM

The learning of IR SVM is equivalent to the following optimization problem. Speciﬁcally, mini-

mization of the modiﬁed regularized hinge loss function,

min

w

N

�

i=1

τk(i)µq(i)[1 − yi⟨w, x(1)

i

− x(2)

i

⟩]+ + λ||w||2,

where [x]+ denotes function max(x, 0),λ =

1

2C ,and τk(i) and µq(i) are weights.See the loss function

of Ranking SVM (4.2).

Here τk(i) represents the weight of instance i whose label pair belongs to the k-th grade pair.

Xu et al.propose a heuristic method to determine the value of τk.The method takes,average decrease

in NDCG@1 when randomly changing the positions of documents belonging to the grade pair, as

the value of τk. Moreover, µq(i) represents the normalization weight of instance i from query q.The

value of µq(i) is simply calculated as 1

nq , where nq is the number of document pairs for query q.

The equivalent constrained optimization (Quadratic Programming) problem is as below.

minw,ξ 1

2||w||2 + Ci

�N

i=1 ξi

s. t. yi⟨w, x(1)

i

− x(2)

i

⟩ ≥ 1 − ξi,

Ci = τk(i)µq(i)

2λ

ξi ≥ 0,

i = 1, . . . , N.


4.5. GBRANK

47

The primal problem can be solved by solving the dual problem.

maxα − 1

2

�N

i=1

�N

j=1 αiαjyiyj⟨x(1)

i

− x(2)

i

, x(1)

j

− x(2)

j ⟩ + �N

i=1 αi

s. t. �N

i=1 αiyi = 0

0 ≤ αi ≤ Ci,

Ci = τk(i)µq(i)

2λ

i = 1, · · · , N.

(4.5)

The optimal solution is in the following form.

f (x) =

N

�

i=1

α∗

i yi⟨(x(1)

i

− x(2)

i

), x⟩.

(4.6)

The learning algorithm of IR SVM is summarized in Figure 4.10.

Parameter: Ci estimated based on τ and µ

Input: training data {((x(1)

i

, x(2)

i

), yi)}N

i=1

Solve the dual problem in (4.5) to obtain α∗

i , (i = 1, · · · , N)

Output: the ranking model in (4.6)

Figure 4.10: Learning Algorithm of IR SVM

4.5

GBRANK

GBRank proposed by Zheng et al. [114, 115] is also a pairwise method, which is based on Boosting

Decision Tree.

4.5.1

LOSS FUNCTION

GBRank takes preference pairs as training data

{(x(1)

i

, x(2)

i

), x(1)

i

≻ x(2)

i

}N

i=1.

In GBRank, the following pairwise loss function is utilized.

L(f ) = 1

2

N

�

i=1

(max{0, τ − (f (x(1)

i

) − f (x(2)

i

)})2,

where f (x) is the ranking function and τ(0 &lt; τ ≤ 1) is parameter. Note that it is assumed that

x(1)

i

≻ x(2)

i

holds. The intuitive explanation to the loss function is that if f (x(1)

i

) is larger than

f (x(2)

i

) with τ, then the loss is zero; otherwise, the loss is 1

2(f (x(2)

i

) − f (x(1)

i

) + τ)2 (cf., the loss

function in Ranking SVM (4.2)).


48

4. METHODS OF LEARNING TO RANK

We can employ Functional Gradient Decent to optimize the loss function with respect to the

training instances. First, we view

f (x(1)

i

),

f (x(2)

i

),

i = 1, · · · , N.

as variables and compute the gradient of L(h) with respect to the training instances

− max{0, f (x(2)

i

) − f (x(1)

i

) + τ},

max{0, f (x(2)

i

) − f (x(1)

i

) + τ},

i = 1, · · · , N

If f (x(1)

i

) − f (x(2)

i

) ≥ τ, then the corresponding loss is zero, and there is no need to change

the ranking function. If f (x(1)

i

) − f (x(2)

i

) &lt; τ, the corresponding loss is non-zero, and we change

the ranking function using Gradient Descent

fk(x) = fk−1(x) − η▽L(fk(x)),

where ▽ stands for gradient and η is learning rate. More speciﬁcally,

fk(x(1)

i

) = fk−1(x(1)

i

) + η(fk−1(x(2)

i

) − fk−1(x(1)

i

) + τ)

fk(x(2)

i

) = fk−1(x(2)

i

) − η(fk−1(x(2)

i

) − fk−1(x(1)

i

) + τ),

where fk(x) and fk−1(x),respectively,denote the values of f (x) in the k-th and (k − 1)-th iterations,

and η is learning rate. If η equals one, then we only need to update the function in the following

way (in the k-th iteration).

fk(x(1)

i

) = fk−1(x(2)

i

) + τ

fk(x(2)

i

) = fk−1(x(1)

i

) − τ.

4.5.2

LEARNING ALGORITHM

GBRank collects all the pairs with non-zero losses (in the k-th iteration)

{(x(1)

i

, fk−1(x(2)

i

) + τ), (x(2)

i

, fk−1(x(1)

i

) − τ)}

views it as regression data and employs Gradient Boosting Tree [38] to learn a model gk(x) that can

make prediction on the regression data.The learned model gk(x) is then linearly combined with the

existing model f(k−1)(x) to create a new model fk(x) (in the k-th iteration)

fk(x) = kfk−1(x) + βgk(x)

k + 1

,

where β is shrinkage factor.

Figure 4.11 shows the GBRank algorithm.


4.6. RANKNET

49

Parameter: β (shrinkage factor), τ (threshold), and K (number of iterations)

Input: S = {((x(1)

i

, x(2)

i

), x(1)

i

≻ x(2)

i

)}N

i=1

Initialize ranking function f0(x)

For k = 1, · · · , K

• Use the previous function fk−1(x)

• Separate S into two subsets

S+ = {(x(1)

i

, x(2)

i

)|fk−1(x(1)

i

) − fk−1(x(2)

i

) ≥ τ}

S− = {(x(1)

i

, x(2)

i

)|fk−1(x(1)

i

) − fk−1(x(2)

i

) &lt; τ}

• Create regression data

{(x(1)

i

, fk−1(x(2)

i

) + τ), (x(2)

i

, fk−1(x(1)

i

) − τ)|(x(1)

i

, x(2)

i

) ∈ S−}

• Employing Gradient Boosting Tree to learn regression model gk(x) using the re-

gression data

• Construct the current function

fk(x) = kfk−1(x) + βgk(x)

k + 1

Output: the ranking function fK(x)

Figure 4.11: Learning Algorithm of GB Rank

4.6

RANKNET

RankNet developed by Burges et al. [11] is also a pairwise method.

4.6.1

LOSS FUNCTION

RankNet adopts Cross Entropy as loss function in learning.

First, it is assumed that in the training data a probability is associated with each pair of objects.

For object pair (document pair) x(1)

i

and x(2)

i

,probability ¯Pi is given,which represents the probability

that x(1)

i

is ahead of x(2)

i

(e.g., x(1)

i

has a higher grade than x(2)

i

). For example, ¯Pi = 1 means that

x(1)

i

should deﬁnitely be ahead of x(2)

i

. ¯Pi = 0.5 means that it is not certain which is ahead of which

(e.g., they belong to the same grade).

Second, it is assumed that a probability is calculated for each pair of objects using the ranking

function. For object pair (document pair) x(1)

i

and x(2)

i

, probability Pi is calculated. Suppose that


50

4. METHODS OF LEARNING TO RANK

ranking function f : ℜd → ℜ assigns scores to objects. Let s(1)

i

= f (x(1)

i

), s(2)

i

= f (x(2)

i

), si =

s(1)

i

− s(2)

i

. Then we deﬁne

Pi ≡

exp(si)

1 + exp(si).

(4.7)

If f (x(1)

i

) &gt; f (x(2)

i

), then x(1)

i

is ranked ahead of x(2)

i

with probability Pi.

Cross Entropy, which measures ‘distance’ between two probability distributions is deﬁned as

Li = − ¯Pi log Pi − (1 − ¯Pi) log(1 − Pi).

(4.8)

We make use of Cross Entropy as loss function for prediction on the order of a pair of objects.

Plugging (4.7) into (4.8) yields

Li = − ¯Pisi + log(1 + exp(si)).

When ¯Pi = 1, Cross Entropy loss becomes logistic loss

Li = log(1 + exp(−si)).

4.6.2

MODEL

RankNet employs Neural Network as model (Figure 4.12). That is why the method is called

RankNet. The Neural Network is supposed to be a three layer network with a single output node,

represented as

s = f (x; θ) = f



�

j

wj · fj

��

k

wjkx(k) + bj

�

+ b



 ,

(4.9)

where x(k) denotes the k-th element of input x, wjk, and bjk, and fj denote the weight, offset,

and activation function of the ﬁrst layer, respectively, wj, b, and f denote the weight, offset, and

activation function of the second layer, respectively, and s denotes the ﬁnal output. θ denotes the

parameter vector. The activation functions are sigmoid functions (non-linear functions).

4.6.3

LEARNING ALGORITHM

RankNet employs the Back Propagation algorithm (equivalently Stochastic Gradient Descent) to

learn the parameters of network.Given training data {(x(1)

i

, x(2)

i

), Pi}, i = 1, · · · , N.The algorithm

iteratively updates parameter θ with each training instance (preference pair) using

θ − η∂L

∂θ ,

(4.10)

where η is learning rate. For simplicity, we omit the superscript representing the index of iteration.


4.6. RANKNET

51

input

output

hidden

Figure 4.12: RankNet Model

The loss on a preference pair is calculated as L = L(s(1) − s(2)). The gradient ∂L

∂θ of loss L

with respect to parameter θ is calculated as

∂L

∂θ = L′

�

∂s(1)

∂θ

− ∂s(2)

∂θ

�

,

where L′ = dL(s)

ds

More precisely, the gradients with respect to speciﬁc parameters are calculated as follows.

∂L

∂b = L′f ′(1) − L′f ′(2) ≡ �(1) − �(2)

(4.11)

∂L

∂wj

= �(1)f (1)

j

− �(2)f (2)

j

(4.12)

∂L

∂bj

= �(1)wjf ′(1)

j

− �(2)wjf ′(2)

j

≡ �(1)

j

− �(2)

j

(4.13)

∂L

∂wjk

= �(1)

j x(1)

(k) − �(2)

j x(2)

(k).

(4.14)

Learning is actually performed by forward and backward propagation on the Neural Network.

Foreword propagation (fprop) corresponds to re-calculation of the ﬁnal score based on the new

parameters, and backward propagation (backprop) corresponds to re-calculation of the parameters.

RankNet uses a validation data set to make selection of parameters, i.e., it employs Cross

Validation in parameter selection. In this way, any IR measure can be utilized as evaluation measure.

Burges et al. take the uses of Neural Network, Cross Entropy loss, back-propagation, and Cross

Validation as the major characteristics of the RankNet method.


52

4. METHODS OF LEARNING TO RANK

Parameter: η (learning rate) and T (number of iterations)

Input: training data {((x(1)

i

, x(2)

i

), Pi)}N

i=1

For t = 1, · · · , T

• For i = 1, · · · , N

– Take the instance and preference probability in training data (x(1)

i

, x(2)

i

), Pi

– Fprop: use the current network to calculate scores s(1)

i

= f (x(1)

i

) and s(2)

i

=

f (x(2)

i

)

– Calculate loss with Pi, s(1)

i

and s(2)

i

using (4.8)

– Bprop: update the parameters of network using (4.10)-(4.14)

• End For

End For

Output: the ranking model in (4.9)

Figure 4.13: Learning Algorithm of RankNet

4.6.4

SPEED UP OF TRAINING

Burges et al. have also proposed an efﬁcient algorithm for speeding up the training process of

RankNet [12].

Two ingredients are used to make the speed up. First, instead of conducting Stochastic Gra-

dient Descent, the algorithm performs Batch Gradient Descent. Second, it stores and re-uses some

of the intermediate results, assuming that the algorithm is applied to search in which the query-

document structure can be leveraged.

Suppose that P is the set of document pairs (i, j) appearing in the training data. Further

suppose that D is the whole set of documents, Pi_ is the set of documents j for which {i, j} is a pair

in P , and P_j is the set of documents i for which {i, j} is a pair in P . Let m denote the number of

queries and n denote the number of documents per query.

The algorithm calculates the gradient of loss function with respect to the parameter over all

the training data.2

∂L

∂θ =

�

(i,j)∈P

∂L(si, sj)

∂si

∂si

∂θ + ∂L(si, sj)

∂sj

∂sj

∂θ ,

where L(·) denotes the loss function and θ denotes the parameters of model, and si and sj are the

scores of i and j.

2For ease of explanation, we change the notation L(s1, s2) = L(s1 − s2).


4.7. LAMBDARANK

53

The algorithm further rewrites the gradient as

∂L

∂θ =

�

i∈D

∂si

∂θ

�

j∈Pi_

∂L(si, sj)

∂si

+

�

j∈D

∂sj

∂θ

�

i∈P_j

∂L(si, sj)

∂sj

.

(4.15)

In each iteration of a query, n fprops are performed to compute the ﬁnal score si. Next, for each

document, �

j∈Pi_

∂L(si,sj)

∂si

and �

i∈P_j

∂L(si,sj)

∂sj

are computed and stored. After that, n fprops and

n backprops are conducted to compute the gradients ∂si

∂θ and ∂sj

∂θ .

In this way, the efﬁciency of calculation with regard to ∂L(si,sj)

∂si

and ∂L(si,sj)

∂sj

is enhanced from

the order of O(n2) to the order of O(n), where n is the number of documents per query.

4.7

LAMBDARANK

4.7.1

LOSS FUNCTION

The ranking evaluation result (the objective function in learning) is usually not continuous and

differentiable, and it depends on sorting. (The sorting function itself is not continuous and differ-

entiable as well). LambdaRank, proposed by Burges et al. [12, 32], considers employing Gradient

Descent to optimize the evaluation result and tries to directly deﬁne and utilize the gradient function

of the evaluation result.

Suppose that the rankingmodel,query,and documents are given.Then each document receives

a score from the ranking model, and a ranking list can be created by sorting the documents based on

the scores. Since the documents are assigned ground truth labels, a ranking evaluation result based

on an IR measure can be obtained. Suppose that we use a surrogate loss function L to approximate

the IR evaluation measure. Then, an evaluation result based on the surrogate loss function L can

also be obtained. It is this evaluation result which LambdaRank attempts to continuously optimize.

The surrogate loss function is deﬁned on a list of documents. In that sense, LambdaRank can

also be viewed as the listwise approach. LambdaRank does not explicitly give the deﬁnition of the

loss function. Instead it deﬁnes the gradient function of the surrogate loss function. More speciﬁcally,

the gradient function is deﬁned as

∂L

∂sj

= −λj(s1, y1, · · · , sn, yn),

where s1, s2, · · · , sn denote the scores of documents and y1, y2, · · · , yn denote the labels of doc-

uments. Note that the index j is on a single document. That is to say, the gradient of a document

depends on the scores and labels of the other documents. The sign is chosen such that a positive

value for a document means that the document must reduce the loss.The gradients of documents are

calculated after the current model generates a ranking list of documents for the query. The gradient

function is called Lambda Function, and that is why the method is called LambdaRank.

The question is then how to specify the Lambda Function, so as to effectively optimize the

ranking evaluation result. One idea is to increase the gradients of documents on top positions.


54

4. METHODS OF LEARNING TO RANK

Suppose that there are two relevant documents d1 and d2. One is at position 2 and the other n − 2.

The change of d1’s score to move it up to the top position should be less than the change of d2’s

score to move it up to the top position.Therefore, we would prefer spending a little capacity moving

d1 up to spending a large capacity moving d2 up. That is, the gradient (lambda score) of d1 should

be much larger than the gradient (lambda score) of d2. In general, for any two documents ranked at

two positions j1 and j2. Suppose that j1 ≪ j2, (that is, the former document is ranked much higher

than the latter document), we would have the gradients satisfy

| ∂L

∂sj1

| ≫| ∂L

∂sj2

|.

4.7.2

LEARNING ALGORITHM

When implementing LambdaRank,one only needs to consider a method for calculating the Lambda

Function (the gradient of loss function). One common way is to calculate it based on NDCG

λj = ∂L

∂sj

= G−1

max

�

i

�

1

1 + exp(si − sj)

�

(gi − gj)(Di − Dj),

(4.16)

where gi, Di, and si, respectively, denote the gain, discount, and score of document di. G−1

max denotes

the normalizing factor of NDCG. The Lambda Function is in fact a pairwise loss function. That is

to say, the conventional implementation of ‘LambdaRank’ is a pairwise method.

LambdaRank employs Neural Network as ranking model. In fact, it can be viewed as an

extension of RankNet. The learning algorithm is similar to that of RankNet (the fast version),

except that a different loss function is employed. Figure 4.14 summarizes the algorithm.

Parameter: η (learning rate ) and T (number of iterations)

Input: S = {((x(1)

i

, y(2)

i

), Pi)}N

i=1

Initialize parameter θ

For t = 1, · · · , T

• Compute gradient ▽L(θ) = ∂L(θ)/∂θ using (4.15)(4.16)

• Update θ = θ − η▽L(θ)

End For

Output: the ranking model f (x; θ)

Figure 4.14: Learning Algorithm of LambdaRank


4.8. LISTNET AND LISTMLE

55

4.8

LISTNET AND LISTMLE

ListNet and ListMLE are probabilistic and listwise methods for learning to rank, proposed by Cao

et al. [14] and Xia et al. [104]. The methods exploit the Plackett-Luce model studied in statistics.

See also [44].

4.8.1

PLACKETT-LUCE MODEL

Let us ﬁrst look at the Plackett-Luce model (the PL model for short).PL model deﬁnes a probability

distribution on permutations of objects, referred to as permutation probability. Suppose that there

is a set of objects o = {o1, o2, · · · , on}. Let π denote a permutation (ranking list) of the objects and

π−1(i) denote the object in the i-th rank (position) in π.Further suppose that there are non-negative

scores assigned to the objects. Let s = {s1, s2, · · · , sn} denote the scores of the objects.

The PL model deﬁnes the probability of permutation π based on scores s as follows.

Ps(π) =

n

�

i=1

sπ−1(i)

�n

j=i sπ−1(j)

.

The probabilities of permutations naturally form a probability distribution.

For example, suppose that there are three objects A, B, C and sA, sB, sC are scores of the

objects (sA &gt; sB &gt; sC). The probability of permutation ABC is

Ps(ABC) =

sA

sA + sB + sC

sB

sB + sC

sC

sC

.

The probability of permutation BCA is

Ps(BCA) =

sB

sA + sB + sC

sC

sA + sC

sA

sA

.

It is easy to verify that

Ps(ABC) + Ps(ACB) + Ps(BAC) + Ps(BCA) + Ps(CAB) + Ps(CBA) = 1.

The permutation probability has the following interpretation, as explained below with the

above example.Given three objects A,B,and C and their scores,we randomly generate a permutation

on them. If we ﬁrst select A from A, B, C based on A’s relative score, then select B from B and

C based on B’s relative score, and ﬁnally select C, then we generate the permutation ABC with

probability Ps(ABC). Permutation probability Ps(ABC) represents the likelihood of permutation

ABC being generated in the process.

The PL model has some nice properties. First, the permutation in descending order of scores

has the largest probability and the permutation in ascending order of scores has the smallest prob-

ability. In the above example, Ps(ABC) is the largest and Ps(CBA) is the smallest among the


56

4. METHODS OF LEARNING TO RANK

permutation probabilities. Furthermore, given the permutation in descending order of scores, swap-

ping any two objects in the permutation will decrease the probability.In the above example,swapping

B and C in ABC yields ACB and we have Ps(ABC) &gt; Ps(ACB).

The PL model also deﬁnes a probability distribution on top k subgroups, referred to as top k

probability. Given objects and permutations of objects, we can deﬁne top k subgroups on the objects.

Top k subgroup g[o1 · · · ok] represents all permutations whose top k objects are o1 · · · ok.The top k

probability of subgroup g[o1 · · · ok] is deﬁned as

Ps(g[o1 · · · ok]) =

k�

i=1

soi

�n

j=i soj

.

In the above example, we have

Ps(g[A]) =

sA

sA + sB + sC

.

Ps(g[AB]) =

sA

sA + sB + sC

sB

sB + sC

.

It is easy to verify the following relation between top k probability and permutation probability

holds, which is another property of the PL model.

Ps(g[o1 · · · ok]) =

�

π∈g[o1···ok]

Ps(π)

For example,

Ps(g[A]) = Ps(ABC) + Ps(ACB).

4.8.2

LISTNET

ListNet makes use of a parameterized Plackett-Luce model. The model can be based on either

permutation probability or top k probability, but due to efﬁciency consideration, it is usually based

on top k probability.Time complexity of computation of permutation probabilities is of order O(n!)

while that of top k probabilities is of order O(n!/(n − k)!).

In document retrieval, suppose that for query q and its associated documents d1, d2, · · · , dn,

the corresponding relevance labels y1, y2, · · · , yn are given. From query q and documents

d1, d2, · · · , dn, feature vectors x1, x2, · · · , xn are created.

Given feature vectors x1, x2, · · · , xn, the top k probability of subgroup g[x1 · · · xk] may be

calculated as

PF(x;θ)(g[x1 · · · xk]) =

k�

i=1

exp(f (xi; θ))

�ni

j=i exp(f (xj; θ)),

(4.17)

where f (x; θ) is a Neural Network model with parameter θ and F(x; θ) is a list of scores given by

the Neural Network. That is to say, the score of xi is determined by an exponential function of the

Neural Network, which works as ranking model.


4.8. LISTNET AND LISTMLE

57

The corresponding labels y1, y2, · · · , yn can be transformed to scores as well. Speciﬁcally, the

score of xi by yi can be determined by an exponential function of yi. Then the top k probability by

the labels may be calculated similarly.

Py(g[x1 · · · xk]) =

k�

i=1

exp(yi)

�ni

j=i exp(yj).

(4.18)

If two scoring functions have a similar effect in ranking,then the permutation distributions (or

top k probability distributions) by them should be similar in shape.Figure 4.15 gives two permutation

distributions based on two scoring functions f and g. One can measure the difference between the

two scoring functions by using KL Divergence. This is exactly the idea in ListNet.

ƒ: ƒ(A) = 3, ƒ(B)=0, ƒ(C)=1;

g: g(A) = 6, g(B)=4, g(C)=3;

Ranking by ƒ:ACB

Ranking by g:ABC

h: h(A) = 4, h(B)=6, h(C)=3;

Ranking by h:ACB

0.229

0.229

0.229

0.616

0.616

0.616

0.037

0.037

0.037

0.005

0.005

0.005

0.005

0.005

0.005

0.108

0.108

0.108

ABC

ACB

BAC

BCA

CAB

CBA

ABC

ACB

BAC

BCA

CAB

CBA

Pƒ(!"

Pg(!"

Ph(!"

!

!

!

Figure 4.15: Examples of Permutation Probability Distributions by PL Model

ListNet measures the difference between the top k probability by the Neural Network model

and the top k probability by the ground truth using KL Divergence. KL Divergence between

two probability distributions is deﬁned as D(P||Q) = �

i pi log pi

qi where P and Q are the two

probability distributions. Here, only �

i −pi log qi is used, since �

i pi log pi is constant. (Note

that KL Divergence is asymmetric).

Suppose that the training data is given as S = {(xi, yi)}m

i=1. Each instance (xi, yi) is given as

((xi,1, xi,2, · · · , xi,ni), (yi,1, yi,2, · · · , yi,ni)). A global ranking function is deﬁned based on a local

ranking function: F(xi) = (f (xi1), f (xi2), · · · , f (xini)).


58

4. METHODS OF LEARNING TO RANK

ListNet takes the KL Divergence over all the training instances as total loss and learns the

ranking model by minimizing the total loss. The total loss function is deﬁned as

L(θ) =

m

�

i=1

L(yi, F(xi; θ)).

Here the loss function for each instance is deﬁned as

L(yi, F(xi; θ))

=

−

�

g∈Gk

i

Pyi(g) log PF(xi;θ)(g)

=

−

�

g∈Gk

i

k�

j=1

exp(yi,j)

�ni

l=j exp(yi,l) log

k�

j=1

exp(f (xi,j; θ))

�ni

l=j exp(f (xi,l; θ)),

where Pyi(g) denotes the top k probability of subgroup g by the ground truth yi, PF(xi;θ)(g) denotes

the top k probability of subgroup g by Neural Network F(xi; θ), and Gk

i denotes all top k subgroups.

ListNet employs Gradient Decent to perform the optimization.Figure 4.16 gives the learning

algorithm.

∂L(θ)

∂θ

=

m

�

i=1

∂L(yi, F(xi; θ))

∂θ

(4.19)

∂L(yi, F(xi; θ))

∂θ

= −

�

g∈Gk

Pyi(g)

PF(xi;θ)(g)

∂PF(xi;θ)(g)

∂θ

.

(4.20)

When k = 1, we have

∂L(yi, F(xi; θ))

∂θ

=

−

ni

�

j=1

Pyi(xi,j)∂f (xi,j; θ)

∂θ

+

ni

�

j=1

Pyi(xi,j)

ni

�

l=1

Pf (xi,l)(xi,l)∂f (xi,l; θ)

∂θ

.

(4.21)

4.8.3

LISTMLE

Another algorithm is ListMLE, which employs the parameterized Plackett-Luce model (4.17)-

(4.18) and Maximum Likelihood Estimation. Speciﬁcally, it maximizes the following total loss

function based on logarithmic loss

L(yi, F(xi; θ)) = −

m

�

i=1

log

k�

j=1

exp(f (xi,π−1

i

(j); θ))

�ni

l=j exp(f (xi,π−1

i

(l); θ)),

where πi is a perfect ranking by yi. The learning algorithm of ListMLE is the similar to that of

ListNet. Note that when k = 1, ListMLE degenerates to Logistic Regression.


4.9. ADARANK

59

Parameter: k (top positions), η (learning rate), and T (number of iterations )

Input: S = {(xi, yi)}m

i=1

Initialize parameter θ

For t = 1, · · · , T

• For i = 1, · · · , m

– Take input (xi, yi)

– Compute gradient ▽L(θ) = ∂L(θ)/∂θ using (4.19)-(4.21)

– Update θ = θ − η▽L(θ)

• End For

End For

Output: the Neural Net model f (x; θ)

Figure 4.16: Learning Algorithm of ListNet

4.9

ADARANK

Since the evaluation measures in IR are based on lists, it is more natural and effective to directly

optimize listwise loss functions in learning to rank. AdaRank, proposed by Xu &amp; Li [108], is one of

the direction optimization algorithms.

4.9.1

LOSS FUNCTION

Suppose that the training data is given as lists of feature vectors and their corresponding lists of

labels (grades) S = {(xi, yi)}m

i=1. We are to learn a ranking model f (x) deﬁned on object (feature

vector) x. Given a new list of objects (feature vectors) x, the learned ranking model can assign a

score to each of the objects x, x ∈ x. We can then sort the objects based on the scores to generate

a ranking list (permutation) π. The evaluation is conducted at the list level, speciﬁcally, a listwise

evaluation measure E(π, y) is utilized.

In learning, ideally we would create a ranking model that can maximize the accuracy in terms

of a listwise evaluation measure on training data, or equivalently, minimizes the loss function deﬁned

below,

L(f ) =

m

�

i=1

(E(π∗

i , yi) − E(πi, yi)) =

m

�

i=1

(1 − E(πi, yi)),

(4.22)

where πi is the permutation on feature vector xi by ranking model f and yi is the corresponding list

of grades.We refer to the loss function L(·) as the ‘true loss function’(or ’empirical risk function’) and

those methods that manage to minimize the true loss function as the ‘direct optimization approach’.


60

4. METHODS OF LEARNING TO RANK

The loss function is not smooth and differentiable, and thus straightforward optimization

of the evaluation might not work. Instead, we can consider optimizing an upper bound of the loss

function.

Since inequality

exp(−x) ≥ 1 − x

holds, we can consider optimizing the following upper bound

m

�

i=1

exp(−E(πi, yi)).

Other upper bounds can also be considered, for example,

m

�

i=1

log

�

1 + exp(−E(πi, yi))

�

.

That is to say, the exponential function and logistic function may be exploited as ‘surrogate’

loss functions in learning. Note that both functions are continuous, differentiable, and even convex

with respect to E.

4.9.2

LEARNING ALGORITHM

AdaRank minimizes the exponential loss function, by taking the boosting approach. Mimicking

the famous AdaBoost algorithm [36], AdaRank conducts stepwise minimization of the exponential

loss function. More speciﬁcally, AdaRank repeats the process of re-weighting the training instances,

creating a weak ranker, and assigning a weight to the weak ranker, to minimize the loss function.

Finally, AdaRank linearly combines the weak rankers as the ranking model. Figure 4.17 shows the

algorithm of AdaRank.

We can prove that AdaRank can continuously reduce the empirical loss function during the

training process, under certain condition, as shown in [108]. When the evaluation measure is dot

product, AdaRank can reduce the loss to zero.

One advantage of AdaRank is its simplicity, and it is perhaps one of the simplest learning to

rank algorithms.

4.10

SVM MAP

Another approach of direct optimization tries to use the Structural SVM techniques to learn a

ranking model. The algorithm SVM MAP developed by Yue et al. [111] is such an algorithm. Xu

et al. [110] further generalize it to a group of algorithms, including PermuRank. See also [15, 62].

4.10.1 LOSS FUNCTION

In ranking, for query qi the ranking model f (xij) assigns a score to each feature vector xij where

xij is the feature vector derived from qi and its associated document dij. The feature vectors xi


4.10. SVM MAP

61

Input: S = {(xi, yi)}m

i=1

Parameter: T (number of iterations)

Evaluation measure: E

Initialize P1(i) = 1/m

For t = 1, · · · , T

• Create weak ranker ht with weighted distribution Pt on training data S

• Choose αt

αt = 1

2 · ln

�m

i=1 Pt(i)(1 + E(πi, yi))

�m

i=1 Pt(i)(1 − E(πi, yi))

• where πi = sortht(xi)

• Create ft

ft(x) =

t�

k=1

αkhk(x)

• Update Pt+1

Pt+1(i) =

exp(−E(πi, yi))

�m

j=1 exp(−E(πj, yj))

• where πi = sortft(xi)

End For

Output: the ranking model f (x) = fT (x)

Figure 4.17: Learning Algorithm of AdaRank

(documents di) are then sorted based on their scores, and a ranking denoted as ˜πi is obtained. The

labels of feature vectors xi are also given as yi.

For simplicity, suppose that the ranking model f (xij) is a linear model:

f (xij) = ⟨w, xij⟩,

(4.23)

where w denotes the weight vector.

We consider using a scoring function S(xi, πi) to measure the goodness of a given permutation

(ranking) πi. S(xi, πi) is deﬁned as

S(xi, πi) = ⟨w, σ(xi, πi)⟩,


62

4. METHODS OF LEARNING TO RANK

where w is still the weight vector and vector σ(xi, πi) is deﬁned as

σ(xi, πi) =

2

ni(ni − 1)

�

k,l:k&lt;l

zkl(xik − xil),

where zkl = +1 if πi(k) &lt; πi(l) (xik is ranked ahead of xil in πi), and −1, otherwise.

We can use the scoring function in learning. For query qi, we calculate S(xi, πi) for each

permutation πi and select the permutation ˜πi with the largest score:

˜πi = arg max

πi∈�i

S(xi, πi),

(4.24)

where �i denotes the set of all the possible permutations for xi.

It can be easily shown that, the ranking ˜πi selected by Eq.(4.24) is equivalent to the ranking

created by the ranking model f (xij) (when both of them are linear functions). Figure 4.18 gives an

example. It is easy to verify that both f (x) and S(xi, π) will output ABC as the most preferable

ranking (permutation).

Objects: A, B, C

fA = ⟨w, xA⟩, fB = ⟨w, xB⟩, fC = ⟨w, xC⟩

Suppose fA &gt; fB &gt; fC

For example:

Permutation1: ABC

Permutation2: ACB

SABC = 1

6⟨w, ((xA − xB) + (xB − xC) + (xA − xC))⟩

SACB = 1

6⟨w, ((xA − xC) + (xC − xB) + (xA − xB))⟩

SABC &gt; SACB

Figure 4.18: Example of Scoring Function

In this way, we can view the problem of learning a ranking model as the optimization problem

in which the following loss function is minimized.

m

�

i=1

max

π∗

i ∈�∗

i ;πi∈�i\�∗

i

��

E(π∗

i , yi) − E(πi, yi)

�

·

�

[S(xi, π∗

i ) ≤ S(xi, πi)

�

]

�

,

(4.25)

where [[c]] is one if condition c is satisﬁed; otherwise, it is zero. π∗

i ∈ �∗

i ⊆ �i denotes any of the

perfect permutations for qi.

The loss function measures the loss when the most preferred ranking by the ranking model

is not the perfect ranking. One can prove that the true loss function in (4.22) is upper bounded by

the new loss function in (4.25).


4.10. SVM MAP

63

4.10.2 LEARNING ALGORITHMS

The loss function (4.25) is still not continuous and differentiable.We can consider using continuous,

differentiable, and even convex upper bounds of the loss function (4.25).

1) The 0-1 function in (4.25) can be replaced with its upper bounds, for example, hinge,

exponential, and logistic functions, yielding

m

�

i=1

max

π∗

i ∈�∗

i ,πi∈�i\�∗

i

�

E(π∗

i , yi) − E(πi, yi)

�

· exp(−

�

S(xi, π∗

i ) − S(xi, πi)

�

),

m

�

i=1

max

π∗

i ∈�∗

i ,πi∈�i\�∗

i

�

E(π∗

i , yi) − E(πi, yi)

�

· log

�

1 + exp(−

�

S(xi, π∗

i ) − S(xi, πi)

�

)

�

,

m

�

i=1

max

π∗

i ∈�∗

i ,πi∈�i\�∗

i

�

E(π∗

i , yi) − E(πi, yi)

�

· [1 −

�

S(xi, π∗

i ) − S(xi, πi)

�

]+,

m

�

i=1

�

max

π∗

i ∈�∗

i ,πi∈�i\�∗

i

��

E(π∗

i , yi) − E(πi, yi)

�

−

�

S(xi, π∗

i ) − S(xi, πi)

��

�

+

,

where [x]+ denotes function max(0, x).

2) The max function can also be replaced with its upper bound, the sum function. This is

because �

i xi ≥ maxi xi if xi ≥ 0 holds for all i.

3) Relaxations 1 and 2 can be applied simultaneously.

For example, utilizing hinge function and taking the true loss as MAP, we obtain SVM MAP.

More precisely, SVM MAP solves the following optimization problem:

minw;ξ≥0 1

2||w||2 + C

m

�m

i=1 ξi

s.t.

∀i, ∀π∗

i ∈ �∗

i , ∀πi ∈ �i \ �∗

i :

S(xi, π∗

i ) − S(xi, πi) ≥ E(π∗

i , yi) − E(πi, yi) − ξi,

(4.26)

where C is coefﬁcient and ξi is the maximum loss among all the losses for permutations of query qi.

Equivalently, SVM MAP minimizes the following regularized hinge loss function

m

�

i=1

�

max

π∗

i ∈�∗

i ;πi∈�i\�∗

i

(E(π∗

i , yi) − E(πi, yi)) − (S(xi, π∗

i ) − S(xi, πi))

�

+

+ λ||w||2.

(4.27)

Intuitively, the ﬁrst term calculates the total maximum loss when selecting the best permutation

for each of the queries. Speciﬁcally, if the difference between the scores S(xi, π∗

i ) − S(xi, πi) is

less than the difference between the corresponding evaluation measures E(π∗

i , yi) − E(πi, yi), then

there will be a loss, otherwise not. Next, the maximum loss is selected for each query, and they are

summed up over all the queries. One can also consider an NDCG version of the method, with a

similar formulation.


64

4. METHODS OF LEARNING TO RANK

Since c · [[x ≤ 0]] &lt; [c − x]+ holds for all c ∈ ℜ+ and x ∈ ℜ, it is easy to see that the upper

bound in (4.27) also bounds the true loss function in (4.22).

Actually, it is possible to derive a number of algorithms for optimizing the upper bounds

(surrogate loss functions). Xu et al. gives an example, which they call PermuRank, and they show

that PermuRank can perform equally well as SVM MAP.

PermuRank minimizes the following regularized hinge loss function.

m

�

i=1

�

π∗

i ∈�∗

i ;πi∈�i\�∗

i

�

E(π∗

i , yi) − E(πi, yi)

�

·

�

1 −

�

S(xi, π∗

i ) − S(xi, πi)

��

+ .

The number of possible permutations is of exponential order, and thus it is not feasible

to directly implement SVM MAP and PermuRank. Both SVM MAP and PermuRank utilize a

working set to cope with the difﬁculty. The set contains arbitrary perfect and imperfect rankings at

the beginning, and the most violated perfect and imperfect rankings are added to the set at each

round of learning.

Figure 4.19 summarizes the learning algorithm of SVM MAP.

Parameter: C

Input: training data {(xi, yi)}m

i=1

Solve the optimization problem in (4.26) to obtain the optimal ranking model

Output: the ranking model in (4.23)

Figure 4.19: Learning Algorithm of SVM MAP

4.11

SOFTRANK

SoftRank is a direct optimization method of learning to rank, proposed by Taylor et al. [43, 95].

Because the ranking evaluation results in IR are usually not smooth and not differentiable, SoftRank

tries to optimize a probabilistic approximation of ranking evaluation result. Speciﬁcally, it introduces

an approximation of NDCG called Soft NDCG, optimizes the ranking evaluation result in terms of

Soft NDCG, and employs a Neural Network model and Gradient Descent to perform the learning

task.

4.11.1 SOFT NDCG

Let us ﬁrst look at the deﬁnition of Soft NDCG. For ease of explanation, suppose that the number

of documents to rank for each query is the same and equals n.


4.11. SOFTRANK

65

We re-write the deﬁnition of NDCG and consider NDCG at position n for permutation

(ranking) π as

G = G−1

max

n

�

j=1

G(j)D(rj),

where G(j) denotes the gain of document j, rj denotes the rank (position) of document j, and

D(rj) denotes the position discount of document j.

Suppose that each document j has a score sj (j = 1, · · · , n). Then, sorting the documents

according to their scores will yield a ranking of documents.An NDCG value can be calculated for the

ranking using the deﬁnition above.The ranking evaluation result in terms of NDCG is determined

by the scores as well as the sorting, which makes it non-smooth and non-differentiable.

In calculation of Soft NDCG, we assume that the ranking of documents is decided based

on the scores of documents probabilistically rather than deterministically. We can calculate the

probability of each document’s being ranked at a position and the expectation of position discount

of each document. In this way, the evaluation result based on NDCG can be approximated by that

based on Soft NDCG and the use of sorting can be avoided.

Speciﬁcally, Soft NDCG is deﬁned as

G

=

G−1

max

n

�

j=1

G(j)E(D(rj))

(4.28)

=

G−1

max

n

�

j=1

G(j)

n

�

r=1

D(r)Pj(r),

(4.29)

where E(D(rj)) denotes the expectation of position discount of document j, and Pj(r) denotes

the probability of document j’s being ranked at rank r. The question then is how to calculate the

probability distribution Pj(r) and then the expectation of position discount E(D(rj)).

2

1.5

0.5

0

1

-1

-0.8

-0.6

-0.4

-0.2

0.2

0.4

0.6

0.8

1

0

s1

s2

p(s)

s3

2

1.5

0.5

0

1

-1

-0.8

-0.6

-0.4

-0.2

0.2

0.4

0.6

0.8

1

0

Score s

Score s

s1

s2

p(s)

s3

Figure 4.20: Deterministic Score v.s. Probabilistic Score


66

4. METHODS OF LEARNING TO RANK

4.11.2 APPROXIMATION OF RANK DISTRIBUTION

There might be different ways to estimate the probability of a document’s being ranked at a position.

Given a document, SoftRank calculates the probability of its being ranked at each position

by recursively calculating the probabilities that the document is ranked ahead of or behind of the

other n − 1 documents in n − 1 Bernoulli trials (with different success probabilities).

SoftRank assumes that the score of document xi given by model f (xi; θ) follows the Gaussian

distribution N((f (xi; θ), σ 2) with a known variance σ 2 (cf., Figure 4.20).

P (s ≤ si) =

� si

−∞

N(s|(f (xi; θ), σ 2)ds.

Therefore, for any two documents xi and xj, the difference between their scores follows the

Gaussian distribution N((f (xi; θ) − f (xj; θ), 2σ 2). The probability that xi is ranked ahead of xj

is thus

πij = P (si − sj ≥ 0) =

� ∞

0

N(s|(f (xi; θ) − f (xj; θ)), 2σ 2)ds.

SoftRank calculates the probability distributionPj(r) of document j over ranks r in a recursive

manner. Suppose that it is to position document j among the n documents. First, there is only one

rank, namely 1, available and document j is ranked at rank 1. The initial rank distribution P (1)

j

(1)

for document j is deﬁned as

P (1)

j

(1) = 1.

Next, the remaining n − 1 documents are assumed to be added one by one into the rank distribution.

When there are i − 1 documents in the rank distribution and document i is added, there are two

possible results.The score of document i is larger than the score of document j, and thus document i

is ranked ahead of document j. Or, the score of document i is smaller than the score of document j,

and thus document i is ranked behind of document j.In the former case,the probability of document

j being at rank r equals the probability of document j at rank r − 1 in the previous iteration. In

the latter case, the probability of document j being at rank r is the same as in the previous iteration.

The two cases can be linearly combined and the rank distribution P (i)

j (r) for document j in the

i-th iteration can be deﬁned as

P (i)

j (r) = πijP (i−1)

j

(r − 1) + (1 − πij)P (i−1)

j

(r).

During the calculation, it is assumed

P (i)

j (r) = 0, if r ≤ 0.

Finally, the probability of document j ranked at rank r is deﬁned as

Pj(r) = P (n)

j

(r).

In this way, each document has a distribution of ranks, as shown in Figure 4.21. Note that

the distribution is an approximation of the true rank distribution.


4.11. SOFTRANK

67

1

0.8

0.6

0.4

0.2

0

0

Rank r1

1

2

p(r1)

p(r2)

p(r3)

p(r1)

p(r2)

p(r3)

1

0.8

0.6

0.4

0.2

0

0

Rank r1

1

2

1

0.8

0.6

0.4

0.2

0

0

Rank r2

1

2

1

0.8

0.6

0.4

0.2

0

0

Rank r2

1

2

1

0.8

0.6

0.4

0.2

0

0

Rank r3

1

2

1

0.8

0.6

0.4

0.2

0

0

Rank r3

1

2

Figure 4.21: Deterministic Rank Distribution v.s. Probabilistic Rank Distribution

4.11.3 LEARNING ALGORITHM

SoftRank uses Neural Network as model and Gradient Descent as optimization technique. Suppose

that there are k parameters in the model. In learning, SoftRank calculates the gradient of Soft

NDCG of n documents with respect to the parameters

∂G

∂θ =





∂s1

∂θ1

· · ·

∂sn

∂θ1

· · ·

· · ·

· · ·

∂s1

∂θk

· · ·

∂sn

∂θk









∂G

∂s1

· · ·

∂G

∂sn



 .

(4.30)

The gradient of Soft NDCG with respect to the score of document j is calculated as (l =

1, · · · , n).

∂G

∂sl

= G−1

max

n

�

j=1

G(j)

n

�

r=1

D(r)∂Pj(r)

∂sl

.

(4.31)

Since Pj(r) is a recursively deﬁned function, its derivative also needs to be calculated recur-

sively. Denoting ψj,l(r) = ∂Pj(r)

∂sl

, we recursively calculate the derivative as follows:

ψ(1)

j,l (1) = 0

(4.32)

ψ(i)

j,l (r) = ψ(i−1)

j,l

(r − 1)πij + ψ(i)

j,l (r)(1 − πij) +

�

P (i−1)

j

(r − 1) − P (i−1)

j

(r)

� ∂πij

∂sl

.

(4.33)

Furthermore, ∂πij

∂sl can be calculated in three cases (note that i ̸= j).

∂πij

∂sl

=







N(0|sl − sj, 2σ 2)

l = i, l ̸= j

−N(0|si − sl, 2σ 2)

l ̸= i, l = j

0

l ̸= i, l ̸= j.

(4.34)


68

4. METHODS OF LEARNING TO RANK

Parameter: η (learning rate ) and T ( number of iterations )

Input: S = {(xi, yi)}m

i=1

Initialize parameter θ

For t = 1, · · · , T

• For i = 1, · · · , m

– Take input (xi, yi)

– Compute gradient ▽G(θ) = ∂G(θ)/∂θ using (4.30)-(4.34)

– Update θ = θ − η▽G(θ)

• End For

End For

Output Neural Net model f (x; θ)

Figure 4.22: Learning Algorithm of SoftRank

4.12

BORDA COUNT

Borda Count is an unsupervised method for ranking aggregation. Aslam &amp; Montague [8] propose

employing Borda Count in meta search. In such case, Borda Count ranks documents in the ﬁnal

ranking based on their positions in the basic rankings. More speciﬁcally, in the ﬁnal ranking, doc-

uments are sorted according to the numbers of documents that are ranked below them in the basic

rankings. If a document is ranked high in many basic rankings, then it will be ranked high in the

ﬁnal ranking list.

The ranking scores of documents in the ﬁnal ranking SD are calculated as

SD = F(�) =

k

�

i=1

Si

Si ≡





si,1

...

si,j

...

si,n





si,j = n − σi(j),

where si,j denotes the number of documents ranked behind document j in basic ranking σi, σi(j)

denotes the rank of document j in basic ranking σi, and n denotes the number of documents.


4.13. MARKOV CHAIN

69

For example, documents A, B, C are ranked in three basic rankings: σ1, σ2, and σ3.

σ1





A

B

C





σ2





A

C

B





σ3





B

A

C





The ranking scores of documents SD are as follows.

SD =





2

1

0



 +





2

0

1



 +





1

2

0



 =





5

3

1





The ﬁnal ranking list π is created by Borda Count based on the scores SD.

π





A

B

C





Borda Count can be viewed as a method of assigning a vector of k values (i.e., n − σi(j)) to

each document and sorting the documents by L1 norms of the vectors. One can easily come up with

other alternatives, for example, sorting by medians of the vectors or Lp norms of the vectors. This

leads to several different methods.

4.13

MARKOV CHAIN

The Markov Chain method for ranking aggregation, referred to as Markov Chain, assumes that

there exists a Markov Chain on the documents to be ranked, and the preference relations between

documents in the basic rankings represent the transitions between the documents in the Markov

Chain. The stationary distribution of the Markov Chain is then utilized to rank the documents.

Dwork et al. [34] have proposed four methods (denoted as MC1, MC2, MC3, and MC4) to

construct the transition probability matrix of the Markov Chain.

MC1 is deﬁned as follows. If the current state is document i, then the next state is chosen

uniformly from the set of documents that are ranked higher than or equal to i in the basic rankings,

that is, from the multiset ∪k{j|j ⪰k i}, where j ⪰k i means that j is ranked higher than or equal

to i in ranking k. The transition probability matrix is deﬁned as follows.

P ≡ (p(i, j))n×n = diag

�

1

�n

j=1 q(1, j), · · · ,

1

�n

j=1 q(n, j)

�

Q

Q ≡ (q(i, j))n×n =

�

k

Qk


70

4. METHODS OF LEARNING TO RANK

Qk = (qk(i, j))n×n

qk(i, j) =

� 1

j ⪰k i

0

otherwise.

MC2 is deﬁned as follows. If the current state is document i, then the next state is determined

by ﬁrst selecting a basic ranking σk uniformly from all rankings and then selecting document j

uniformly from the set of documents that are ranked higher than or equal to i: {j|j ⪰k i}.

P ≡ (p(i, j))n×n = 1

k

�

k

Pk

Pk ≡ (pk(i, j))n×n

pk(i, j) =

�

1

m

j ⪰k i

0

otherwise,

where m = |{j|j ⪰k i}|.

In MC3, if the current state is document i, then the next state is determined as follows. First,

we choose ranking σk uniformly from the basic rankings, next for document j, if j ≻k i, then we

go to j; otherwise, we stay at i.

P ≡ (p(i, j))n×n = 1

k

�

k

Pk

Pk ≡ (pk(i, j))n×n

pk(i, j) =







1

n

j ≻k i

n−m

n

j =k i

0

otherwise,

where m = |{j|j ≻k i}|.

In MC4,if the current state is document i,then the next state is decided as follows.Document

j is selected uniformly from the union of all documents. If j ≻k i holds for the majority of the basic

rankings, then we go to j; otherwise, we stay at i.

P ≡ (p(i, j))n×n

p(i, j) =







1

n

q(i, j) &gt; q(j, i)

n−m

n

j = i

0

otherwise,


4.14. CRANKING

71

where m = |{j|q(i, j) &gt; q(j, i)}|

Q = (q(i, j))n×n =

�

k

Qk

Qk ≡ (qk(i, j))n×n

qk(i, j) =

� 1

j ≻k i

0

otherwise.

4.14

CRANKING

The unsupervised methods described above conduct majority voting in their ﬁnal ranking decisions.

In fact, the methods treat all the basic ranking lists equally and give high scores to those documents

ranked high in most of basic ranking lists.The uniform weight assumption may not hold in practice,

however. For example, in meta search, ranking lists generated by different search engines may have

different accuracies and reliabilities. One may want to learn the weights of basic ranking lists.

Supervised learning methods like Cranking proposed by Lebanon &amp; Lafferty [63] can address the

problem.

4.14.1 MODEL

Cranking employs the following probability model

P (π|θ, �) =

1

Z(θ, �) exp(

k

�

j=1

θj · d(π, σj)),

(4.35)

where π denotes the ﬁnal ranking, � = (σ1, · · · , σk) denotes the basic rankings, d denotes the

distance between two rankings, and θ denotes weight parameters. Distance d can be, for example,

Kendal’s Tau. Furthermore, Z is the normalizing factor over all the possible rankings, as deﬁned

below.

Z(θ, �) =

�

π

exp(

k

�

j=1

θj · d(π, σj)).

The model in Cranking is an extension of the Mallows model in statistics, in which there is

only a single ’basic ranking’.

P(π|θ, σ) =

1

Z(θ, �) exp(θ · d(π, σ))

Z(θ, σ) =

�

π

exp(θ · d(π, σ)).


72

4. METHODS OF LEARNING TO RANK

4.14.2 LEARNING ALGORITHM

In learning, the training data is given as S = {(�i, πi)}m

i=1, and the goal is to build the model for

rankingaggregationbasedonthedata.WecanconsideremployingMaximumLikelihoodEstimation

to learn the parameters of the model. If the ﬁnal ranking and the basic rankings are all full ranking

lists in the training data, then the log likelihood function is calculated as follows.

L(θ) = log

m

�

i=1

P (πi|θ, �i) =

m

�

i=1

log

exp(�k

j=1 θj · d(πi, σi,j))

�

πi∈� exp �k

j=1 θj · d(πi, σi,j)

.

We can employ Gradient Descent to estimate the optimal parameters.

In practice, sometimes only partial lists are given in the training data. If the ﬁnal ranking lists

are given as partial lists, then the likelihood function is calculated as

L(θ) = log

m

�

i=1

P (πi|θ, �i) = log

m

�

i=1

�

π′

i ∈G(πi)

P (π

′

i|θ, �i)

(4.36)

=

m

�

i=1

log

�

π′

i ∈G(πi) exp(�k

j=1 θj · d(π

′

i, σi,j))

�

π′

i ∈� exp �k

j=1 θj · d(π

′

i, σi,j)

,

where πi is a partial list, and G(πi) denotes the group of full lists with πi as the top partial list.

If both the ﬁnal ranking lists and the basic ranking lists are given as partial lists, then the

likelihood function is calculated as

L(θ) = log

m

�

i=1

P (πi|θ, �i) = log

m

�

i=1

�

π′

i ∈G(πi)

1

�k

j=1 |G(σi,j)|

�

σ ′

i,j∈G(σi,j)

P (π

′

i|θ, �′i)

(4.37)

=

m

�

i=1

log

�

π′

i ∈G(πi)

1

�k

j=1 |G(σi,j)|

�

σ ′

i,j∈G(σi,j)

exp(�k

j=1 θj · d(π

′

i, σ

′

i,j))

�

π′

i ∈� exp(�k

j=1 θj · d(π

′

i, σ

′

i,j))

,

where πi is a partial list and G(πi) denotes the group of full lists with πi as the top partial list, and

similarly σi,j is a partial list and G(σi,j) denotes the group of full lists with σi,j as the top partial

list. Furthermore, it is assumed here that the full lists in group G(σi,j) are uniformly distributed.

The above two likelihood functions (Eq.(4.36)-(4.37)) cannot be directly optimized.Cranking

employs Markov Chain Monte Carlo (MCMC) to perform parameter estimation. Figure 4.23

summarizes the learning algorithm.

4.14.3 PREDICTION

In prediction, given the learned model (i.e., the parameters θ) and the basic rankings �, Cranking

ﬁrst calculates the probability distribution of ﬁnal ranking π: P (π|θ, �). It uses the probability


4.14. CRANKING

73

Input: training data {(�i, πi)}m

i=1

Learn parameter θ using (Eq. (4.36)-(4.37)) and MCMC

Output: ranking model P (π|θ, �) (4.35)

Figure 4.23: Learning Algorithm of Cranking

distribution to calculate the expected rank of each document.

E(π(i)|θ, �) =

n

�

r=1

r · P (π(i) = r|θ, �) =

n

�

r=1

r ·

�

π∈�,π(i)=r

P (π|θ, �).

It then sorts the documents based on their expected ranks.



75

C H A P T E R

5

Applications of Learning to

Rank

Learning to rank can be applied to a wide variety of applications in information retrieval and natural

language processing. Typical applications are document retrieval, expert search, deﬁnition search,

meta-search, personalized search, online advertisement, collaborative ﬁltering, question answering,

keyphrase extraction, document summarization, and machine translation.

In the applications, the objects (offerings) to be ranked can be documents, document units

such as sentences and paragraphs, entities such as people and products. Ranking can be based on

importance, preference, and quality, and it can be employed as an end-to-end solution or as a part

of a solution.

This chapter introduces some example applications of learning to rank (ranking creation).

WEB SEARCH

Learning to rank has been successfully applied to web search. It is known that the ranking models

at several web search engines are built by learning to rank technologies. Usually, a large number of

signals representing relevance are used as features in the models. Training data is created by a group

of professional judges. Moreover, powerful computing platforms for scalable and efﬁcient training

of ranking models are employed.

Learning to rank is also applied to different problems in web search, including context aware

search [105], recency ranking [31], federated search [79], personalized search, online advertisement,

etc.

COLLABORATIVE FILTERING

Collaborative ﬁltering, also known as recommender system, is a task as follows. The users are asked

to give ratings to the items.The system examines the ratings of items by the users and offers each user

a ranking list of items. The ranking lists represent recommendations to the users from the system,

while higher ranked items are more likely to be preferred by the users.

Collaborative ﬁltering can be formalized as an ordinal classiﬁcation or classiﬁcation problem

because users give ratings to items. Sometimes it is more natural to formalize it as ranking (ranking

creation). This is because ratings from different users are on different scales and are not directly

comparable, and thus it is better to view the ratings from each user as a ranking list.


76

5. APPLICATIONS OF LEARNING TO RANK

Freund et al. [37] have applied RankBoost to collaborative ﬁltering, speciﬁcally movie rec-

ommendation. RankBoost is a pairwise method for ranking in which AdaBoost is employed as the

learning algorithm. In Freund et al.’s method, the ratings from the target user are viewed as training

data, and the ratings from the other users are utilized as features. A RankBoost model is created with

the training data. The trained model is then used for ranking of all the movies (including unrated

movies) for the target user. See also [46].

DEFINITION SEARCH

In deﬁnition search, given a terminology query, the system returns a ranking list of deﬁnitions

(deﬁnitional paragraphs) of the terminology. Xu et al. propose a method of deﬁnition search using

learning to rank [107].

The method ﬁrst automatically extracts all likely deﬁnitional paragraphs from the documents

with several heuristic rules. For example, paragraphs with the ﬁrst sentence being "X is a" are taken

as candidates. Their method then applies a Ranking SVM model to assign to all the candidate

paragraphs scores representing their likelihood of being good deﬁnitions, removes redundant para-

graphs, and stores the paragraphs in a database with the terminologies as keys (e.g., X in ‘X is a’). In

deﬁnition search, given a terminology the system retrieves the related deﬁnitional paragraphs and

returns the ranking list of deﬁnitional paragraphs.

The Ranking SVM model utilizes a number of features, including both positive and negative

features. For example, if the term (e.g., X in ’X is a’) repeatedly occurs in the paragraph, then it is

likely the paragraph is a deﬁnition of the term. If words like ’she’, ’he’, or ’said’ occur in the paragraph,

it is likely the paragraph is not a deﬁnition.

KEYPHRASE EXTRACTION

Keyphrase extraction is a problem as follows. Given a document, a number of phrases (usually noun

phrases) are output, which can precisely and compactly represent the content of the document.

Traditionally keyphrase extraction is formalized as classiﬁcation and classiﬁcation methods such as

decision tree and Naive Bayes are employed. Jiang et al. formalize the keyphrase extraction problem

as ranking instead of classiﬁcation [55]. In fact, keyphrase extraction can be viewed as the inverse

problem of document retrieval.

Suppose that there are some training data in which a number of documents are assigned

keyphrases and non-keyphrases.Jiang et al.’s method takes ordered phrase pairs as training instances,

each of which consists of a keyphrase and a non-keyphrase, and builds a Ranking SVM model with

the training data.The method then sorts the candidate phrases of a new document with the trained

model, and selects the top ranked candidate phrases as keyphrases. Experimental results show that

Ranking SVM statistically signiﬁcantly outperforms the classiﬁcation methods of SVM and Naive

Bayes.

Jiang et al. give two reasons on the better performance of the ranking approach over the

classiﬁcation approach. First, it is more natural to consider the likelihood of a phrase’s being a


77

keyphrase in a relative sense than in an absolute sense. Second, features for determining whether a

phrase is a keyphrase are also relative.

QUERY DEPENDENT SUMMARIZATION

When the search system presents a search result to the user, it is important to show the titles and

summaries of the documents because they are helpful for the user to judge whether the documents

are relevant or not. This is the problem referred to as query-dependent summarization or snippet

generation. Query dependent summarization usually contains two steps: relevant sentence selection

and summary composition. In sentence selection, the most informative sentences are identiﬁed.

Metzler &amp; Kanungo [76] propose using learning to rank techniques to conduct sentence

selection in query dependent summarization. They apply GBRank and Support Vector Regression

to the task. Given a query and a retrieved document, their method treats all the sentences in the

document as candidate sentences and ranks the sentences based on their relevance to the query

and appropriateness as a sentence in the summary. They train a model for the ranking with some

labeled data. A number of features are deﬁned in the model. For example, whether the query has

an exact match in the sentence, the fraction of query terms in the sentence, the length of sentence

(neither short nor long sentence is preferred), and the position of sentence in the document. They

demonstrate that GBRank is an effective algorithm for the task.

MACHINE TRANSLATION

Re-ranking in machine translation is also a typical ranking problem. The state-of-the-art machine

translation approach generates many candidate translations using a generative model, conducts re-

ranking on the candidate translations using a discriminative model, and then selects the top ranked

result. Features that can discriminate between good and bad translations are used in the re-ranking

model.

There are several advantages by taking the re-ranking approach. First, the accuracy of trans-

lation may be enhanced because the discriminative model can further leverage global features and

discriminative training in the ﬁnal translation selection. Second, the efﬁciency of translation may

be improved. The top n candidates are ﬁrst selected with the generative model, and then the best

translation is chosen from a small set of candidates.

For example, Shen at al. propose using learning to rank techniques in re-ranking of machine

translation. They have proposed two methods [93] similar to the Prank algorithm. One of the

algorithms is called Splitting, which tries to ﬁnd parallel hyperplanes separating the top k good

translations, the bottom l bad translations and the translations in between, for each sentence, where

k and l are pre-determined. Figure 5.1 illustrates the Splitting model.


78

5. APPLICATIONS OF LEARNING TO RANK

w

k=2

k=2

grade 1

grade 2

grade 3

l=3

l=3

Figure 5.1: Splitting Model for Machine Translation


79

C H A P T E R

6

Theory of Learning to Rank

This chapter gives a statistical learning formulation of learning to rank (ranking creation) and explains

the issues in theoretical study of learning to rank.

6.1

STATISTICAL LEARNING FORMULATION

Learning to rank (ranking creation) is a supervised learning task. Suppose that X is the input space

consisting of lists of feature vectors and Y is the output space consisting of lists of grades. Further

suppose that x is an element of X representing a list of feature vectors and y is an element of

Y representing a list of grades. Let P (X, Y) be an unknown joint probability distribution where

random variable X takes x as its value and random variable Y takes y as its value.

Assume that F is a function mapping from a list of feature vectors x to a list of scores.

The goal of the learning task is to automatically learn a function ˆF(x), given training data

(x1, y1), (x2, y2), . . . , (xm, ym). Each training instance is comprised of feature vectors xi and the

corresponding grades yi (i = 1, · · · , m). Here m denotes the number of training instances.

F(x) and y can be further written as F(x) = [f (x1), f (x2), · · · , f (xn)] and y =

[y1, y2, · · · , yn]. Here f (x) denotes a local ranking function and n denotes the number of fea-

ture vectors and grades. The feature vectors correspond to the objects to be ranked, denoted as

O = [1, 2, · · · , n].

We make use of a loss function L(·, ·) to evaluate the prediction result of F(x). First, the

feature vectors x are ranked according to F(x). Then the ranking results are evaluated against the

corresponding grades y. If the feature vectors with higher grades are ranked higher, then the loss

will be small. Otherwise, the loss will be large. The loss function is speciﬁcally represented as

L(F(x), y).

Note that the loss function for ranking is slightly different from the loss functions in other statistical

learning tasks, in the sense that it makes use of sorting.

We further deﬁne the risk function R(·) as the expected loss function with respect to the joint

distribution P (X, Y),

R(F) =

�

X×Y

L(F(x), y)dP (x, y).

Given training data, we calculate the empirical risk function as follows,

ˆR(F) = 1

m

m

�

i=1

L(F(xi), yi).


80

6. THEORY OF LEARNING TO RANK

We can formalize the learning task as minimization of the empirical risk function, as in

other learning tasks. We can also introduce regularizer to conduct minimization of the regularized

empirical risk function.

The minimization of empirical risk function could be difﬁcult due to the nature of the loss

function (it is not continuous and it uses sorting). We can consider using a surrogate loss function

denoted as

L′(F(x), y).

The corresponding risk function and empirical risk functions are deﬁned as follows.

R′(F) =

�

X×Y

L′(F(x), y)dP (x, y)

ˆR′(F) = 1

m

m

�

i=1

L′(F(xi), yi).

In such case, the learning problem becomes that of minimization of (regularized) empirical risk

function based on surrogate loss.

Note that we adopt a machine learning formulation here. In IR, the feature vectors x are

derived from a query and its associated documents. The grades y represent the relevance degrees of

the documents with respect to the query.We make use of a global ranking function F(x). In practice,

it is usually a local ranking function f (x). The possible number of feature vectors in x can be very

large, even inﬁnite. The evaluation (loss function) is, however, only concerned with n results. In IR,

n can be determined by the pooling strategy (cf., Section 2.2.2).

6.2

LOSS FUNCTIONS

In binary classiﬁcation, the true loss function is usually 0-1 loss. In contrast, in ranking, there are

different ways to deﬁne the true loss function. In IR, the true loss functions can be those deﬁned

based on NDCG (Normalized Discounted Cumulative Gain) and MAP (Mean Average Precision).

Speciﬁcally, we have

L(F(x), y) = 1 − NDCG

(6.1)

and

L(F(x), y) = 1 − MAP.

(6.2)

Given permutation π by F(x), NDCG of it (for n objects) is deﬁned as follows.

NDCG =

1

Gmax

�

i:π(i)≤n

G(i)D(π(i))

(6.3)

G(i) = 2yi − 1,

D(π(i)) =

1

log2(1 + π(i)),


6.2. LOSS FUNCTIONS

81

where yi is the grade of object i, π(i) is the rank of object i in π, G(·) is the gain function, D(·) is

the position discount function, and Gmax is the normalizing factor.

Given permutation π by F(x), MAP of it (for n objects)1 is deﬁned as follows.

MAP =

�n

i=1 P (i) · yi

�n

i=1 yi

,

where yi is the grade of object i taking on 1 or 0 as value, π(i) is the rank of object i in π, and P(i)

represents the precision until the rank of object i, deﬁned as

P (i) =

�

j:π(j)≤π(i) yj

π(i)

.

Note that the true loss functions (NDCG loss and MAP loss) are not continuous, and they

depend on sorting by F(x).

For the surrogate loss function,there are also different ways to deﬁne it,which leads to different

approaches to learning to rank. For example, one can deﬁne pointwise loss, pairwise loss, and listwise

loss functions, respectively.

Squared Loss, which is a pointwise loss, is deﬁned as

L′(F(x), y) =

n

�

i=1

(f (xi) − yi)2.

The loss function is the one used in Subset Regression.

The pointwise loss in McRank is as follows

L′(F(x), y) =

n

�

i=1

I[classiﬁer(f (xi)) ̸= yi],

where I[·] is the indicator function and the output of classiﬁer(f (xi)) is a label (grade).

Pairwise losses can be hinge loss, exponential loss, and logistic loss as deﬁned as follows.They

are used in Ranking SVM, RankBoost, and RankNet, respectively.

L′(F(x), y) =

n−1

�

i=1

n

�

j=i+1

[1 − sign(yi − yj)(f (xi) − f (xj))]+, when yi ̸= yj,

(6.4)

where it is assumed that L′ = 0, when yi = yj.

L′(F(x), y) =

n−1

�

i=1

n

�

j=i+1

exp

�

−sign(yi − yj)(f (xi) − f (xj))

�

, when yi ̸= yj.

(6.5)

1Here, we abuse terminology for ease of explanation. Mean Average Precision is in fact averaged over queries. The MAP here is

only deﬁned on one query. In that case, it should be called AP (Average Precision).


82

6. THEORY OF LEARNING TO RANK

L′(F(x), y) =

n−1

�

i=1

n

�

j=i+1

log

�

1 + exp(−sign(yi − yj)(f (xi) − f (xj)))

�

, when yi ̸= yj. (6.6)

Listwise losses can be KL loss and logarithmic loss utilized in ListNet and ListMLE, respec-

tively.

KL Loss in ListNet is deﬁned as

L′(F(x), y) = D(Py(π)||PF (π)),

(6.7)

where D(·||·) is KL Divergence, Py(π) is the permutation probability distribution (or top k prob-

ability distribution) induced by y, and PF (π) is the permutation probability distribution (or top k

probability distribution) induced by F(x). Both distributions are calculated by the Plackett-Luce

model.

Logarithmic Loss in ListMLE is deﬁned as

L′(F(x), y) = − log PF (π∗

y ),

(6.8)

where PF (π∗

y ) is the probability of perfect permutation by y, calculated by F(x) and the Plackett-

Luce model.

Obviously, the surrogate loss function in AdaRank is also a listwise loss.

L′(F(x), y) = exp(−NDCG),

where NDCG is calculated on the basis of F(x) and y.

6.3

RELATIONS BETWEEN LOSS FUNCTIONS

Previous work has shown that the pointwise losses, pairwise losses (6.4-6.6) and listwise loss (6.8)

in existing methods are upper bounds of the true losses (6.1-6.2).

L(F(x), y) ≤ L′(F(x), y).

Thatmeansthatexistinglearningtorankmethods,suchasSubsetRanking,McRank,RankingSVM,

RankBoost, RankNet, ListMLE, and AdaRank are methods of optimizing different surrogate loss

functions.

Below we give a summary of the relations between the surrogate loss functions used in existing

methods and the true loss function (1-NDCG).

The pointwise loss function in Subset Ranking is an upper bound of (1-NDCG) [29].

1 − NDCG ≤

1

Gmax

�

2

n

�

i=1

D(π(i))2

�1/2

L′(F(x), y)1/2,


6.4. THEORETICAL ANALYSIS

83

where D(π(i)) is the position discount of object i and L′(F(x), y) is the surrogate loss function in

Subset Ranking.

The pointwise loss function in McRank is an upper bound of (1-NDCG) [67].

1 − NDCG ≤ 15

√

2

Gmax

� n

�

i=1

D(π(i))2 − n

n

�

i=1

D(π(i))2/n

�1/2

L′(F(x), y)1/2,

where D(π(i)) is the position discount of object i and L′(F(x), y) is the surrogate loss function in

McRank.

The pairwise loss functions in Ranking SVM, RankBoost, and RankNet are upper bounds of

(1-NDCG) [20].

1 − NDCG ≤ maxi(G(i)D(π(i)))

Gmax

L′(F(x), y),

where G(i) is the gain of object i and D(π(i)) is the position discount of object i and L′(F(x), y)

is the surrogate loss function in the above pairwise methods.

The listwise loss function in ListMLE is an upper bound of (1-NDCG) [20].

1 − NDCG ≤ maxi(G(i)D(π(i)))

ln 2 · Gmax

L′(F(x), y),

where G(i) is the gain of object i and D(π(i)) is the position discount of object i, and L′(F(x), y)

is the surrogate loss function in ListMLE.

6.4

THEORETICAL ANALYSIS

There are two major issues with regard to theoretical analysis of learning to rank, namely general-

ization ability and statistical consistency.

Generalization ability of a method represents the relation between the empirical risk function

and the expected risk function. It is usually represented by a bound between the two risk functions.

Cossock &amp; Zhang show the generalization ability of Subset Ranking. Lan et al. give generalization

bounds of Ranking SVM, IR SVM, ListNet, and ListMLE [60, 61]. Recently, Chen et al. have

proved a generalization bound of pairwise methods, in a more natural framework [21].

Statistical consistency is to answer the question whether optimization of a surrogate loss

function can lead to optimization of the true loss function. Xia et al. have studied the consistency of

ListNet and ListMLE [103, 104].

For other work on theoretical analysis of learning to rank, see [2, 5, 25, 28].



85

C H A P T E R

7

Ongoing and Future Work

Learning to rank is a hot area in machine learning and related ﬁelds, including information retrieval,

natural language processing and data mining, and intensive study is being conducted.

In Chapter 4, methods of learning to rank have been described. It is still necessary to develop

more advanced technologies. It is also clear from the discussions in Chapters 5 and 6, there are still

many open questions with regard to theory and applications of learning to rank.

Let us look at some ongoing and future work on several topics with regard to learning to rank,

particularly learning for ranking creation.

• Training data creation

• Semi-supervised learning and active learning

• Feature learning

• Scalable and efﬁcient training

• Domain adaptation

• Ranking by ensemble learning

• Global ranking

• Ranking of objects in graph

TRAINING DATA CREATION

The quality of training data largely affects the performance of learning to rank, as in other machine

learning tasks.If the quality of training data is low, then the accuracy of the trained model will also be

low.The so-called ‘garbage in garbage out’ phenomenon also occurs in learning to rank. In addition,

reducing the cost of training data construction is another issue which needs to be considered.

In IR, training data for ranking is usually annotated by humans, which is costly and error

prone. As a result, the amount of training data tends to be small and the quality of data cannot be

guaranteed.

As explained, one way to cope with the challenge is to automatically derive training data from

click-through data. Click-through data represents users’ implicit feedbacks and thus is a valuable

data source for training data creation. The problem which we need to address is to eliminate noise

and position bias. For example, one can employ the method proposed by Joachims [57], to use the


86

7. ONGOING AND FUTURE WORK

skips of documents in search as signals of relative relevance judgments. Another method developed

by Radlinski and Joachims [87] can also be exploited, which makes use of the queries and clicks in

search sessions. Speciﬁcally, a clicked document with the current query is preferred over an examined

but not clicked document with the previous query in the same session, under the assumption that

the user may have not found relevant result with the previous query.

Since training data labeled by human judges inevitably contains errors, another related issue

is to automatically correct errors in the training data. One approach is to use click-through data to

make error detection and correction. For example, Xu et al. [106] propose a method for detecting

human labeling errors using click-through data. A discriminative model for predicting relevance

labels from click-through patterns is employed.

For other methods with regard to training data creation, see also [4, 7, 88].

SEMI-SUPERVISED LEARNING AND ACTIVE LEARNING

Since creation of training data is expensive, using both labeled and unlabeled data in learning to

rank naturally arises as an important issue to investigate. A key question then is how to leverage the

useful information in the unlabeled data to enhance the performance of learning. Several methods

on semi-supervised learning have been proposed [6, 33, 49, 56, 66]. Further investigations on the

problem appear to be necessary.

Another related issue is active learning. Long et al. [73] point out that a general principle for

active learning, named Expected Loss Minimization (ELO), can be employed in ranking just like

in classiﬁcation, regression, and other tasks. ELO suggests selecting the queries or documents with

the largest expected losses.They propose an algorithm called ELO-DCG for active learning at both

the query and document levels.

FEATURE LEARNING

In practice, the features used in the ranking model are more critical for the accuracy of learning to

rank. Developing powerful features is an important step in building practical ranking systems.

In IR, BM25 and LM4IR (unsupervised ranking models) can be used as features of a rank-

ing model. BM25 and LM4IR actually represent the relevance of query and document, using the

matching degree of their terms. How to enrich a matching model and learn the model from data

is an interesting topic. Metzler &amp; Croft [75] propose employing a Markov Random Field model

to represent the matching degree between query and document and to use the model in relevance

ranking.The key idea is to take into account dependency between the terms in the query and repre-

sent their relations in a probabilistic dependency graph (MRF). An algorithm for learning the MRF

model is also developed. See also [96].

PageRank is a document feature widely used in learning to rank. One can also think about

enhancing the model. For example, Liu et al. propose exploiting user browsing graph built upon

user behavior data, constructing a continuous time Markov model on the graph, and calculating the


87

stationary distribution as page importance. Their algorithm referred to as BrowseRank is a natural

extension of PageRank [72].

More studies on supervised or unsupervised learning of features for ranking are certainly

needed. Automatic selection of features also needs more investigations [42].

SCALABLE AND EFFICIENT TRAINING

Training data for learning to rank can also be large as in other learning tasks. How to make the

training of learning to rank scalable and efﬁcient is an important issue. Chapelle &amp; Keerthi [16]

have developed an efﬁcient algorithm for training Ranking SVM. They employ the primal Newton

method to speed up the training process and show that their implementation of Ranking SVM is

ﬁve orders of magnitude faster than SVMLight, the widely used Ranking SVM learning tool.

DOMAIN ADAPTATION

Domain adaptation or transfer learning is a popular research topic in machine learning, which is

also true for ranking. Another related issue is multi-task learning. There are domains for which it

is easy to obtain training data and build reliable ranking models, while there are domains for which

this is not the case. How to adapt a model trained in one domain to another domain then becomes

important.

Methods for domain adaptation,transfer learning,and multi-task learning have been proposed

[9, 17, 19, 40]. For example, Chapelle et al. [17] propose a boosting algorithm to multi-task ranking.

Their method learns a joint model for several different tasks, which addresses the speciﬁcs of each

task with task-speciﬁc parameters and the commonalities among the tasks with shared parameters.

RANKING BY ENSEMBLE LEARNING

To enhance the accuracy of ranking, a divide-and-conquer approach can be effective. That is, for

different queries in document retrieval one creates and utilizes different rankers and maximizes the

overall ranking accuracy.

In general web search, users’ search needs are very diverse, and thus it appears more necessary

to adopt the query dependent approach. How to automatically classify queries into classes, train a

ranking model for each class,and combine the ranking models becomes an important area to explore.

Geng et al. [41] propose a query dependent ranking method. Given a query, the method tries to ﬁnd

the k nearest training queries and construct a ranking model with the data in the neighborhood.

Efﬁcient ways of performing k nearest neighbor training are given. There exist challenging yet

interesting problems along the direction.

GLOBAL RANKING

In ranking creation, usually a local model is utilized. The local model assigns a score to each object,

and the objects are ranked according to their scores. The use of local model in ranking has certain


88

7. ONGOING AND FUTURE WORK

advantages such as efﬁciency in processing. However, as explained in Chapter 1, ranking creation

is by nature a global ranking issue, and thus it would be better to learn and utilize a global ranking

model.

In document retrieval, ranking should be performed based on not only relevance, but also

diversity, novelty, etc. Qin et al. [83] propose employing a Continuous Conditional Random Fields

model for global ranking.The model represents documents and their scores as vertices and relations

between document scores as edges in an undirected graph. A method for learning the CRF model

from supervised learning data is also developed. They show that the CRF mode can effectively

utilize similarity relation between documents and link relation between documents. Yue et al. [112]

propose a method for conducting ranking based on diversity (for ambiguous queries like “Jaguar”, it

is better to rank the relevant documents of all the major senses on the top). The method takes the

relevant documents as input and then groups them into diverse subsets. It formalizes the training of

the model as a learning problem using Structural SVM. For other related work, see [52, 54, 84, 89].

RANKING OF NODES IN GRAPH

Sometimes information on the relations between the objects to be ranked is also available. The

relations are often represented in a directed or undirected graph on the objects. Therefore, how to

leverage the information in ranking becomes an interesting question. This kind of setting is partic-

ularly common in social search and social data mining. Note that PageRank [78] and BrowseRank

[72] are also methods of ranking objects in a graph, but they only make use of the link information

and are unsupervised learning methods.

Agrawal et al.[1],for example,propose a supervised learning method for ranking the objects in

a graph.Their method employs the Markov random walk model, as in PageRank, and automatically

learns the transition probabilities from the preference pairs of objects in the training data. The

method formalizes the learning task as a constrained network ﬂow problem in which the objective is

maximum entropy and the Markov property and the preference pairs are represented as constraints.


89

Bibliography

[1] Alekh Agarwal, Soumen Chakrabarti, and Sunny Aggarwal. Learning to rank networked

entities. In KDD, pages 14–23, 2006. DOI: 10.1145/1150402.1150409 88

[2] Shivani Agarwal and Partha Niyogi. Stability and generalization of bipartite ranking algo-

rithms. In COLT, pages 32–47, 2005. 83

[3] Eugene Agichtein, Eric Brill, and Susan Dumais. Improving web search ranking by incorpo-

rating user behavior information. In Proceedings of the 29th annual international ACM SIGIR

conference on Research and development in information retrieval, SIGIR ’06, pages 19–26, New

York, NY, USA, 2006. ACM. DOI: 10.1145/1148170.1148177 17

[4] Rakesh Agrawal, Alan Halverson, Krishnaram Kenthapadi, Nina Mishra, and Panayi-

otis Tsaparas.

Generating labels from clicks.

In WSDM, pages 172–181, 2009.

DOI: 10.1145/1498759.1498824 86

[5] Nir Ailon and Mehryar Mohri. An efﬁcient reduction of ranking to classiﬁcation. In COLT,

pages 87–98, 2008. 83

[6] Massih Reza Amini, Tuong Vinh Truong, and Cyril Goutte. A boosting algorithm for

learning bipartite ranking functions with partially labeled data. In SIGIR ’08: Proceedings of

the 31st annual international ACM SIGIR conference on Research and development in information

retrieval, pages 99–106, New York, NY, USA, 2008. ACM. DOI: 10.1145/1390334.1390354

86

[7] Javed A. Aslam, Evangelos Kanoulas, Virgil Pavlu, Stefan Savev, and Emine Yilmaz.

Document selection methodologies for efﬁcient and effective learning-to-rank.

In SI-

GIR ’09: Proceedings of the 32nd international ACM SIGIR conference on Research and de-

velopment in information retrieval, pages 468–475, New York, NY, USA, 2009. ACM.

DOI: 10.1145/1571941.1572022 86

[8] Javed A.Aslam and Mark Montague. Models for metasearch. In Proceedings of the 24th annual

international ACM SIGIR conference on Research and development in information retrieval,

SIGIR ’01,pages 276–284,NewYork,NY,USA,2001.ACM.DOI: 10.1145/383952.384007

35, 68

[9] Jing Bai, Ke Zhou, Guirong Xue, Hongyuan Zha, Gordon Sun, BelleTseng, Zhaohui Zheng,

and Yi Chang. Multi-task learning for learning to rank in web search. In CIKM ’09: Proceeding


90

BIBLIOGRAPHY

of the 18th ACM conference on Information and knowledge management, pages 1549–1552, New

York, NY, USA, 2009. ACM. DOI: 10.1145/1645953.1646169 87

[10] Michael Bendersky, W. Bruce Croft, and Yanlei Diao. Quality-biased ranking of web docu-

ments. In WSDM, pages 95–104, 2011. DOI: 10.1145/1935826.1935849 18

[11] Chris Burges,Tal Shaked,Erin Renshaw,Ari Lazier,Matt Deeds,Nicole Hamilton,and Greg

Hullender. Learning to rank using gradient descent. In ICML’05:Proceedingsof the22ndinter-

national conference on Machine learning, pages 89–96, 2005. DOI: 10.1145/1102351.1102363

22, 25, 37, 49

[12] C.J.C. Burges, R. Ragno, and Q.V. Le. Learning to rank with nonsmooth cost functions. In

Advances in Neural Information Processing Systems 18, pages 395–402. MIT Press, Cambridge,

MA, 2006. 22, 26, 37, 52, 53

[13] Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou Huang, and Hsiao-Wuen Hon.

Adapting ranking SVM to document retrieval.

In SIGIR’ 06, pages 186–193, 2006.

DOI: 10.1145/1148170.1148205 22, 25, 37, 44

[14] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from

pairwise approach to listwise approach. In ICML ’07: Proceedings of the 24th international

conference on Machine learning, pages 129–136, 2007. DOI: 10.1145/1273496.1273513 22,

27, 37, 55

[15] Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and Chiru Bhattacharyya. Structured

learning for non-smooth ranking losses. In KDD ’08: Proceeding of the 14th ACM SIGKDD

international conference on Knowledge discovery and data mining, pages 88–96, New York, NY,

USA, 2008. ACM. DOI: 10.1145/1401890.1401906 60

[16] Olivier Chapelle and S. Sathiya Keerthi. Efﬁcient algorithms for ranking with svms. Inf.

Retr., 13(3):201–215, 2010. DOI: 10.1007/s10791-009-9109-9 87

[17] Olivier Chapelle, Pannagadatta K. Shivaswamy, Srinivas Vadrevu, Kilian Q. Weinberger,

Ya Zhang, and Belle L. Tseng. Multi-task learning for boosting with application to web

search ranking. In KDD, pages 1189–1198, 2010. DOI: 10.1145/1835804.1835953 87

[18] Olivier Chapelle and Mingrui Wu. Gradient descent optimization of smoothed information

retrieval metrics. Inf. Retr., 13(3):216–235, 2010. DOI: 10.1007/s10791-009-9110-3 27

[19] Depin Chen, Yan Xiong, Jun Yan, Gui-Rong Xue, Gang Wang, and Zheng Chen.

Knowledge transfer for cross domain learning to rank.

Inf. Retr., 13(3):236–253, 2010.

DOI: 10.1007/s10791-009-9111-2 87

[20] Wei Chen, Tie-Yan Liu, Yanyan Lan, Zhi-Ming Ma, and Hang Li. Ranking measures and

loss functions in learning to rank. In NIPS ’09, 2009. 83


BIBLIOGRAPHY

91

[21] Wei Chen, Tie-Yan Liu, and Zhi-Ming Ma. Two-layer generalization analysis for ranking

using rademacher average. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel,

and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 370–378.

2010. 83

[22] Weiwei Cheng, Jens Hühn, and Eyke Hüllermeier.

Decision tree and instance-based

learning for label ranking.

In ICML ’09: Proceedings of the 26th Annual International

Conference on Machine Learning, pages 161–168, New York, NY, USA, 2009. ACM.

DOI: 10.1145/1553374.1553395 23

[23] Wei Chu and Zoubin Ghahramani. Gaussian processes for ordinal regression. J. Mach. Learn.

Res., 6:1019–1041, 2005. 23

[24] Wei Chu and S. Sathiya Keerthi. New approaches to support vector ordinal regression. In

ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 145–152,

2005. DOI: 10.1145/1102351.1102370 23

[25] Stéphan J.M.Clémençon and NicolasVayatis. Empirical performance maximization for linear

rank statistics. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in

Neural Information Processing Systems 21, pages 305–312. 2009. 83

[26] W. William Cohen, R. E. Schapire, and Yoram Singer. Learning to order things. JAIR,

10:243–270, 1999. 35

[27] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.

DOI: 10.1023/A:1022627411411 40

[28] Corinna Cortes, Mehryar Mohri, and Ashish Rastogi. Magnitude-preserving ranking algo-

rithms. In ICML ’07: Proceedings of the 24th international conference on Machine learning, pages

169–176, New York, NY, USA, 2007. ACM. DOI: 10.1145/1273496.1273518 83

[29] David Cossock and Tong Zhang.

Subset ranking using regression.

In COLT ’06:

Proceedings of the 19th Annual Conference on Learning Theory, pages 605–619, 2006.

DOI: 10.1007/11776420_44 22, 24, 82

[30] Koby Crammer and Yoram Singer. Pranking with ranking. In NIPS, pages 641–647, 2001.

21, 22, 23, 37

[31] Anlei Dong, Yi Chang, Zhaohui Zheng, Gilad Mishne, Jing Bai, Ruiqiang Zhang, Karolina

Buchner, Ciya Liao, and Fernando Diaz. Towards recency ranking in web search. In WSDM,

pages 11–20, 2010. DOI: 10.1145/1718487.1718490 75

[32] Pinar Donmez, Krysta M. Svore, and Christopher J.C. Burges. On the local optimality of

lambdarank. In SIGIR ’09: Proceedings of the 32nd international ACM SIGIR conference on


92

BIBLIOGRAPHY

Research and development in information retrieval, pages 460–467, New York, NY, USA, 2009.

ACM. DOI: 10.1145/1571941.1572021 37, 53

[33] Kevin Duh and Katrin Kirchhoff. Learning to rank with partially-labeled data. In SI-

GIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and

development in information retrieval, pages 251–258, New York, NY, USA, 2008. ACM.

DOI: 10.1145/1390334.1390379 86

[34] Cynthia Dwork, Ravi Kumar, Moni Naor, and D. Sivakumar. Rank aggregation methods for

the web. In Proceedings of the 10th international conference on World Wide Web, WWW ’01,

pages 613–622, New York, NY, USA, 2001. ACM. DOI: 10.1145/371920.372165 35, 37,

69

[35] Jonathan L. Elsas, Vitor R. Carvalho, and Jaime G. Carbonell. Fast learning of document

ranking functions with the committee perceptron. In WSDM ’08: Proceedings of the interna-

tional conference on Web search and web data mining, pages 55–64, New York, NY, USA, 2008.

ACM. DOI: 10.1145/1341531.1341542 24

[36] Y. Freund and R. Schapire. A short introduction to boosting. Journal of Japanese Society for

Artiﬁcial Intelligence, 14(5):771–780, 1999. 60

[37] Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram Singer. An efﬁcient boosting

algorithm for combining preferences. Journal of Machine Learning Research, 4:933–969, 2003.

DOI: 10.1162/jmlr.2003.4.6.933 22, 25, 76

[38] Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. The

Annals of Statistics, 29(5):1189—1232, 2001. DOI: 10.1214/aos/1013203450 26, 48

[39] Jianfeng Gao, Haoliang Qi, Xinsong Xia, and Jian-Yun Nie. Linear discriminant model for

information retrieval. In SIGIR, pages 290–297, 2005. DOI: 10.1145/1076034.1076085 11

[40] Wei Gao, Peng Cai, Kam-Fai Wong, and Aoying Zhou. Learning to rank only using training

data from related domain. In Proceeding of the 33rd international ACM SIGIR conference on

Research and development in information retrieval, SIGIR ’10, pages 162–169, New York, NY,

USA, 2010. ACM. DOI: 10.1145/1835449.1835478 87

[41] Xiubo Geng, Tie-Yan Liu, Tao Qin, Andrew Arnold, Hang Li, and Heung-Yeung Shum.

Query dependent ranking using k-nearest neighbor. In SIGIR ’08: Proceedings of the 31st an-

nual international ACM SIGIR conference on Research and development in information retrieval,

pages 115–122, New York, NY, USA, 2008. ACM. DOI: 10.1145/1390334.1390356 21, 87

[42] Xiubo Geng, Tie-Yan Liu, Tao Qin, and Hang Li. Feature selection for ranking. In SI-

GIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research

and development in information retrieval, pages 407–414, New York, NY, USA, 2007. ACM.

DOI: 10.1145/1277741.1277811 87


BIBLIOGRAPHY

93

[43] John Guiver and Edward Snelson. Learning to rank with softrank and gaussian processes.

In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research

and development in information retrieval, pages 259–266, New York, NY, USA, 2008. ACM.

DOI: 10.1145/1390334.1390380 37, 64

[44] John Guiver and Edward Snelson. Bayesian inference for plackett-luce ranking models. In

ICML ’09: Proceedings of the 26th Annual International Conference on Machine Learning, pages

377–384, New York, NY, USA, 2009. ACM. DOI: 10.1145/1553374.1553423 55

[45] Zoltán Gyöngyi, Hector Garcia-Molina, and Jan Pedersen. Combating web spam with

trustrank. In Proceedings of the Thirtieth international conference on Very large data bases -

Volume 30, VLDB ’04, pages 576–587. VLDB Endowment, 2004. 18

[46] Edward F.Harrington. Online ranking/collaborative ﬁltering using the perceptron algorithm.

In ICML, pages 250–257, 2003. 76

[47] Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Support vector learning for ordinal

regression. May 20 1999. DOI: 10.1049/cp:19991091 37, 40

[48] Ralf Herbrich,Thore Graepel,and Klaus Obermayer. Large Margin rank boundaries for ordinal

regression. MIT Press, Cambridge, MA, 2000. 22, 24, 37, 40

[49] Steven C. H. Hoi and Rong Jin. Semi-supervised ensemble ranking. In Dieter Fox and

Carla P. Gomes, editors, AAAI, pages 634–639. AAAI Press, 2008. 86

[50] Yunhua Hu, Hang Li, Yunbo Cao, Dmitriy Meyerzon, and Qinghua Zheng. Automatic

extraction of titles from general documents using machine learning. In Mary Marlino,Tamara

Sumner, and Frank M. Shipman III, editors, JCDL, pages 145–154. ACM, 2005. 17

[51] Yunhua Hu, Guomao Xin, Ruihua Song, Guoping Hu, Shuming Shi, Yunbo Cao, and Hang

Li. Title extraction from bodies of HTML documents and its application to web page

retrieval. In Ricardo A. Baeza-Yates, Nivio Ziviani, Gary Marchionini, Alistair Moffat, and

John Tait, editors, SIGIR, pages 250–257. ACM, 2005. 17

[52] Jim C. Huang and Brendan J. Frey. Structured ranking learning using cumulative distribution

networks. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural

Information Processing Systems 21, pages 697–704. 2009. 88

[53] Kalervo Järvelin and Jaana Kekäläinen. In evaluation methods for retrieving highly relevant

documents. In Proceedings of the 23rd annual international ACM SIGIR conference on Research

and development in information retrieval, SIGIR ’00, pages 41–48, New York, NY, USA, 2000.

ACM. DOI: 10.1145/345508.345545 17


94

BIBLIOGRAPHY

[54] Shihao Ji, Ke Zhou, Ciya Liao, Zhaohui Zheng, Gui-Rong Xue, Olivier Chapelle, Gordon

Sun, and Hongyuan Zha. Global ranking by exploiting user clicks. In SIGIR ’09: Proceedings

of the 32nd international ACM SIGIR conference on Research and development in information

retrieval, pages 35–42, New York, NY, USA, 2009. ACM. DOI: 10.1145/1571941.1571950

88

[55] Xin Jiang, Yunhua Hu, and Hang Li. A ranking approach to keyphrase extraction. In James

Allan, Javed A. Aslam, Mark Sanderson, ChengXiang Zhai, and Justin Zobel, editors, SIGIR,

pages 756–757. ACM, 2009. 76

[56] Rong Jin, Hamed Valizadegan, and Hang Li. Ranking reﬁnement and its application to

information retrieval. InWWW’08:Proceedingof the17thinternationalconferenceonWorldWide

Web, pages 397–406, New York, NY, USA, 2008. ACM. DOI: 10.1145/1367497.1367552

86

[57] T.Joachims. Optimizing search engines using clickthrough data. In KDD’02,pages 133–142,

2002. DOI: 10.1145/775047.775067 15, 85

[58] Thorsten Joachims, Hang Li, Tie-Yan Liu, and ChengXiang Zhai.

Learning to

rank for information retrieval (LR4IR 2007).

SIGIR Forum, 41(2):58–62, 2007.

DOI: 10.1145/1328964.1328974 4

[59] Alexandre Klementiev, Dan Roth, and Kevin Small.

Unsupervised rank aggrega-

tion with distance-based models.

In Proceedings of the 25th international conference

on Machine learning, ICML ’08, pages 472–479, New York, NY, USA, 2008. ACM.

DOI: 10.1145/1390156.1390216 35

[60] Yanyan Lan, Tie-Yan Liu, Zhiming Ma, and Hang Li. Generalization analysis of listwise

learning-to-rank algorithms. In ICML ’09: Proceedings of the 26th Annual International Con-

ference on Machine Learning, pages 577–584, New York, NY, USA, 2009. ACM. 83

[61] Yanyan Lan, Tie-Yan Liu, Tao Qin, Zhiming Ma, and Hang Li.

Query-level stability

and generalization in learning to rank.

In ICML ’08: Proceedings of the 25th interna-

tional conference on Machine learning, pages 512–519, New York, NY, USA, 2008. ACM.

DOI: 10.1145/1390156.1390221 83

[62] Quoc V. Le and Alexander J. Smola. Direct optimization of ranking measures. CoRR,

abs/0704.3359, 2007. informal publication; informal publication. 60

[63] Guy Lebanon and John D. Lafferty. Cranking: Combining rankings using conditional prob-

ability models on permutations. In ICML ’02: Proceedings of the Nineteenth International

Conference on Machine Learning, pages 363–370, San Francisco, CA, USA, 2002. Morgan

Kaufmann Publishers Inc. 35, 37, 71


BIBLIOGRAPHY

95

[64] Hang Li, Tie-Yan Liu, and ChengXiang Zhai. Learning to rank for information retrieval

(LR4IR 2008). SIGIR Forum, 42(2):76–79, 2008. DOI: 10.1145/1480506.1480519 4

[65] Hang Li, Tie-Yan Liu, and ChengXiang Zhai. Learning to rank for information retrieval

(LR4IR 2009). SIGIR Forum, 43(2):41–45, 2009. DOI: 10.1145/1670564.1670571 4

[66] Ming Li, Hang Li, and Zhi-Hua Zhou. Semi-supervised document retrieval. Inf. Process.

Manage, 45(3):341–355, 2009. DOI: 10.1016/j.ipm.2008.11.002 86

[67] Ping Li, Christopher Burges, and Qiang Wu. Mcrank: Learning to rank using multiple

classiﬁcation and gradient boosting. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,

Advances in Neural Information Processing Systems 20, pages 897–904. MIT Press, Cambridge,

MA, 2008. 21, 22, 24, 83

[68] Tie-Yan Liu. Learning to rank for information retrieval. Foundations andTrends in Information

Retrieval, 3(3):225–331, 2009. DOI: 10.1561/1500000016 4

[69] Tie-Yan Liu, Thorsten Joachims, Hang Li, and Chengxiang Zhai. Introduction to spe-

cial issue on learning to rank for information retrieval.

Inf. Retr., 13(3):197–200, 2010.

DOI: 10.1007/s10791-009-9120-1 4

[70] Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, and Hang Li. Letor: Benchmark dataset for

research on learning to rank for information retrieval. In Proceedings of SIGIR 2007 Workshop

on Learning to Rank for Information Retrieval, 2007. DOI: 10.1561/1500000016 4

[71] Yu-Ting Liu, Tie-Yan Liu, Tao Qin, Zhi-Ming Ma, and Hang Li. Supervised rank aggre-

gation. In WWW ’07: Proceedings of the 16th international conference on World Wide Web, pages

481–490, New York, NY, USA, 2007. ACM. DOI: 10.1145/1242572.1242638 35

[72] Yuting Liu, Bin Gao, Tie-Yan Liu, Ying Zhang, Zhiming Ma, Shuyuan He, and Hang Li.

Browserank: letting web users vote for page importance. In Sung-Hyon Myaeng, Douglas W.

Oard, Fabrizio Sebastiani,Tat-Seng Chua, and Mun-Kew Leong, editors,SIGIR, pages 451–

458. ACM, 2008. 18, 87, 88

[73] Bo Long, Olivier Chapelle, Ya Zhang, Yi Chang, Zhaohui Zheng, and Belle L. Tseng. Ac-

tive learning for ranking through expected loss optimization. In Fabio Crestani, Stéphane

Marchand-Maillet, Hsin-Hsi Chen, Efthimis N. Efthimiadis, and Jacques Savoy, editors,

SIGIR, pages 267–274. ACM, 2010. 86

[74] Irina Matveeva,Chris Burges,Timo Burkard,Andy Laucius,and Leon Wong. High accuracy

retrieval with multiple nested ranker. In SIGIR ’06: Proceedings of the 29th annual international

ACM SIGIR conference on Research and development in information retrieval, pages 437–444,

New York, NY, USA, 2006. ACM. DOI: 10.1145/1148170.1148246 21


96

BIBLIOGRAPHY

[75] Donald Metzler and W. Bruce Croft. A markov random ﬁeld model for term dependencies.

In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research

and development in information retrieval, pages 472–479, New York, NY, USA, 2005. ACM

Press. DOI: 10.1145/1076034.1076115 86

[76] Donald Metzler and Tapas Kanungo.

Machine learned sentence selection strategies for

query-biased summarization. sigir learning to rank workshop, 2008. 77

[77] Taesup Moon, Alex J. Smola, Yi Chang, and Zhaohui Zheng.

Intervalrank: isotonic

regression with listwise and pairwise constraints.

In WSDM, pages 151–160, 2010.

DOI: 10.1145/1718487.1718507 24

[78] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing

order to the web. Technical report, Stanford University, Stanford, CA, 1998. 17, 18, 88

[79] Ashok Kumar Ponnuswami, Kumaresh Pattabiraman, Qiang Wu, Ran Gilad-Bachrach, and

Tapas Kanungo. On composition of a federated web search result page: using online users

to provide pairwise preference for heterogeneous verticals. In WSDM, pages 715–724, 2011.

DOI: 10.1145/1935826.1935922 75

[80] Jay M. Ponte and W. Bruce Croft. A language modeling approach to information retrieval.

In Proceedings of the 21st annual international ACM SIGIR conference on Research and develop-

ment in information retrieval, SIGIR ’98, pages 275–281, New York, NY, USA, 1998. ACM.

DOI: 10.1145/290941.291008 11

[81] Tao Qin, Tie-Yan Liu, and Hang Li.

A general approximation framework for di-

rect optimization of information retrieval measures.

Inf. Retr., 13(4):375–397, 2010.

DOI: 10.1007/s10791-009-9124-x 22, 28

[82] Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li.

Letor: A benchmark collection for re-

search on learning to rank for information retrieval.

Inf. Retr., 13(4):346–374, 2010.

DOI: 10.1007/s10791-009-9123-y 28

[83] Tao Qin, Tie-Yan Liu, Xu-Dong Zhang, De-Sheng Wang, and Hang Li. Global ranking

using continuous conditional random ﬁelds. In D. Koller, D. Schuurmans, Y. Bengio, and

L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1281–1288.

2009. 88

[84] Tao Qin, Tie-Yan Liu, Xu-Dong Zhang, De-Sheng Wang, Wen-Ying Xiong, and Hang Li.

Learning to rank relational objects and its application to web search. In WWW ’08: Proceeding

of the 17th international conference on World Wide Web, pages 407–416, New York, NY, USA,

2008. ACM. DOI: 10.1145/1367497.1367553 88


BIBLIOGRAPHY

97

[85] Tao Qin, Xu-Dong Zhang, Ming-Feng Tsai, De-Sheng Wang, Tie-Yan Liu, and Hang Li.

Query-level loss functions for information retrieval. Inf. Process. Manage., 44(2):838–855,

2008. DOI: 10.1016/j.ipm.2007.07.016 27

[86] Tao Qin, Xu-Dong Zhang, De-Sheng Wang, Tie-Yan Liu, Wei Lai, and Hang Li. Rank-

ing with multiple hyperplanes. In Proceedings of the 30th annual international ACM SIGIR

conference, pages 279–286, 2007. DOI: 10.1145/1277741.1277791 24

[87] Filip Radlinski andThorsten Joachims. Query chains:learning to rank from implicit feedback.

In KDD ’05: Proceeding of the eleventh ACM SIGKDD international conference on Knowledge

discovery in data mining, pages 239–248, 2005. DOI: 10.1145/1081870.1081899 15, 86

[88] Filip Radlinski and Thorsten Joachims. Active exploration for learning rankings from click-

through data. In KDD ’07: Proceedings of the 13th ACM SIGKDD international conference

on Knowledge discovery and data mining, pages 570–579, New York, NY, USA, 2007. ACM.

DOI: 10.1145/1281192.1281254 15, 86

[89] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. Learning diverse rankings with

multi-armed bandits. In ICML ’08: Proceedings of the 25th international conference on Machine

learning,pages784–791,NewYork,NY,USA,2008.ACM.DOI: 10.1145/1390156.1390255

88

[90] S. E. Robertson and S.Walker. Some simple effective approximations to the 2-poisson model

for probabilistic weighted retrieval. In Proceedings of the 17th annual international ACM SIGIR

conference on Research and development in information retrieval,SIGIR ’94,pages 232–241,New

York, NY, USA, 1994. Springer-Verlag New York, Inc. 11, 16, 18

[91] F. Rosenblatt. The perceptron: a probabilistic model for information storage and organization

in the brain. Psychological Review, 65:386–408, 1958. DOI: 10.1037/h0042519 37

[92] Amnon Shashua and Anat Levin. Ranking with large margin principle: Two approaches.

In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing

Systems 15. MIT Press. 21, 22, 23, 37, 38, 40

[93] Libin Shen, Anoop Sarkar, and Franz Josef Och. Discriminative reranking for machine

translation. In HLT-NAACL, pages 177–184, 2004. 77

[94] Tao Tao and ChengXiang Zhai. An exploration of proximity measures in information re-

trieval. In Proceedings of the 30th annual international ACM SIGIR conference on Research and

development in information retrieval, SIGIR ’07, pages 295–302, New York, NY, USA, 2007.

ACM. DOI: 10.1145/1277741.1277794 18

[95] Michael Taylor, John Guiver, Stephen Robertson, and Tom Minka.

Softrank: optimiz-

ing non-smooth rank metrics.

In WSDM ’08: Proceedings of the international conference


98

BIBLIOGRAPHY

on Web search and web data mining, pages 77–86, New York, NY, USA, 2008. ACM.

DOI: 10.1145/1341531.1341544 22, 28, 37, 64

[96] MichaelTaylor,Hugo Zaragoza,Nick Craswell,Stephen Robertson,and Chris Burges. Opti-

misation methods for ranking functions with multiple parameters. In CIKM ’06: Proceedings

of the 15th ACM international conference on Information and knowledge management, pages

585–593, New York, NY, USA, 2006. ACM. DOI: 10.1145/1183614.1183698 86

[97] Ming-FengTsai,Tie-Yan Liu,Tao Qin, Hsin-Hsi Chen, and Wei-Ying Ma. Frank: a ranking

method with ﬁdelity loss. In Proceedings of the 30th annual international ACM SIGIRconference,

pages 383–390, 2007. DOI: 10.1145/1277741.1277808 22, 26

[98] Nicolas Usunier, David Buffoni, and Patrick Gallinari.

Ranking with ordered weighted

pairwise classiﬁcation.

In ICML ’09: Proceedings of the 26th Annual International Con-

ference on Machine Learning, pages 1057–1064, New York, NY, USA, 2009. ACM.

DOI: 10.1145/1553374.1553509 24

[99] Hamed Valizadegan, Rong Jin, Ruofei Zhang, and Jianchang Mao. Learning to rank by

optimizing ndcg measure. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and

A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1883–1891.

2009. 27

[100] Maksims N.Volkovs and Richard S.Zemel. Boltzrank:learning to maximize expected ranking

gain. InICML’09:Proceedingsof the26thAnnualInternationalConferenceonMachineLearning,

pages 1089–1096, New York, NY, USA, 2009. ACM. DOI: 10.1145/1553374.1553513 27

[101] Ellen M. Voorhees and Donna Harman. TREC: Experiment and Evaluation in Information

Retrieval. MIT, 2005. 17

[102] Qiang Wu, Christopher J. C. Burges, Krysta Marie Svore, and Jianfeng Gao.

Adapt-

ing boosting for information retrieval measures.

Inf. Retr., 13(3):254–270, 2010.

DOI: 10.1007/s10791-009-9112-1 22, 26

[103] Fen Xia, Tie-Yan Liu, and Hang Li. Statistical consistency of top-k ranking. In Y. Bengio,

D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural

Information Processing Systems 22, pages 2098–2106. 2009. 83

[104] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li.

Listwise approach

to learning to rank: theory and algorithm. In ICML ’08: Proceedings of the 25th interna-

tional conference on Machine learning, pages 1192–1199, New York, NY, USA, 2008. ACM.

DOI: 10.1145/1390156.1390306 22, 27, 37, 55, 83

[105] Biao Xiang, Daxin Jiang, Jian Pei, Xiaohui Sun, Enhong Chen, and Hang Li. Context-aware

ranking in web search. In Fabio Crestani, Stéphane Marchand-Maillet, Hsin-Hsi Chen,

Efthimis N. Efthimiadis, and Jacques Savoy, editors, SIGIR, pages 451–458. ACM, 2010. 75


BIBLIOGRAPHY

99

[106] Jingfang Xu, Chuanliang Chen, Gu Xu, Hang Li, and Elbio Renato Torres Abib. Improving

quality of training data for learning to rank using click-through data. In Brian D. Davison,

Torsten Suel, Nick Craswell, and Bing Liu, editors, WSDM, pages 171–180. ACM, 2010. 86

[107] Jun Xu, Yunbo Cao, Hang Li, and Min Zhao.

Ranking deﬁnitions with supervised

learning methods.

In Special interest tracks and posters of the 14th international confer-

ence on World Wide Web, WWW ’05, pages 811–819, New York, NY, USA, 2005. ACM.

DOI: 10.1145/1062745.1062761 76

[108] Jun Xu and Hang Li. Adarank: a boosting algorithm for information retrieval. In SI-

GIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research

and development in information retrieval, pages 391–398, New York, NY, USA, 2007. ACM.

DOI: 10.1145/1277741.1277809 22, 27, 37, 59, 60

[109] Jun Xu, Hang Li, and Chaoliang Zhong. Relevance ranking using kernels. In Pu-Jen Cheng,

Min-Yen Kan, Wai Lam, and Preslav Nakov, editors, AIRS, volume 6458 of Lecture Notes in

Computer Science, pages 1–12. Springer, 2010. 18

[110] Jun Xu, Tie-Yan Liu, Min Lu, Hang Li, and Wei-Ying Ma. Directly optimizing evaluation

measures in learning to rank. In SIGIR ’08: Proceedings of the 31st annual international ACM

SIGIR conference on Research and development in information retrieval, pages 107–114, New

York, NY, USA, 2008. ACM. DOI: 10.1145/1390334.1390355 22, 27, 60

[111] YisongYue,Thomas Finley,Filip Radlinski,andThorsten Joachims. A support vector method

for optimizing average precision. In Proceedings of the 30th annual international ACM SIGIR

conference, pages 271–278, 2007. DOI: 10.1145/1277741.1277790 22, 27, 37, 60

[112] Yisong Yue and T. Joachims.

Predicting diverse subsets using structural SVMs.

In International

Conference

on

Machine

Learning

(ICML), pages 271–278, 2008.

DOI: 10.1145/1390156.1390310 88

[113] Chengxiang Zhai and John Lafferty. A study of smoothing methods for language mod-

els applied to information retrieval.

ACM Trans. Inf. Syst., 22:179–214, April 2004.

DOI: 10.1145/984321.984322 11

[114] Zhaohui Zheng, Hongyuan Zha, Keke Chen, and Gordon Sun. A regression framework

for learning ranking functions using relative relevance judgments. In Proceedings of the 30th

annual international ACM SIGIR conference, 2007. DOI: 10.1145/1277741.1277792 37, 47

[115] Zhaohui Zheng, Hongyuan Zha, Tong Zhang, Olivier Chapelle, Keke Chen, and Gordon

Sun. A general boosting method and its application to learning ranking functions for web

search. In J.C.Platt,D.Koller,Y.Singer,and S.Roweis,editors,AdvancesinNeuralInformation

Processing Systems 20, pages 1697–1704. MIT Press, Cambridge, MA, 2008. 22, 26, 37, 47


100

BIBLIOGRAPHY

[116] Ke Zhou, Gui-Rong Xue, Hongyuan Zha, and Yong Yu. Learning to rank with ties. In

Proceedings of the 31st annual international ACM SIGIR conference, pages 275–282, 2008.

DOI: 10.1145/1390334.1390382 24


101

Author’s Biography

HANG LI

Hang Li is senior researcher and research manager at Microsoft



Research Asia. He is also adjunct professor at Peking University,

Nanjing University, Xi’an Jiaotong University, and Nankai Uni-

versity. His research areas include information retrieval, natural

language processing, statistical machine learning, and data min-

ing. He graduated from Kyoto University in 1988 and earned his

PhD from the University of Tokyo in 1998. He worked at the

NEC lab in Japan during 1991 and 2001. He joined Microsoft

Research Asia in 2001 and has been working there until present.

Hang has about 100 publications at top international journals and

conferences,including SIGIR,WWW,WSDM,ACL,EMNLP,

ICML, NIPS, and SIGKDD. He and his colleagues’ papers re-

ceived the SIGKDD’08 best application paper award and the

SIGIR’08 best student paper award. Hang has also been working on the development of several

products. These include Microsoft SQL Server 2005, Microsoft Ofﬁce 2007 and Ofﬁce 2010, Mi-

crosoft Live Search 2008, Microsoft Bing 2009 and Bing 2010. He has also been very active in

the research communities and served or is serving the top conferences and journals. For example, in

2011,he is PC co-chair of WSDM’11; area chairs of SIGIR’11,AAAI’11,NIPS’11; PC members of

WWW’11, ACL-HLT’11, SIGKDD’11, ICDM’11, EMNLP’11; and an editorial board member

on both the Journal of the American Society for Information Science and the Journal of Computer

Science &amp; Technology.

