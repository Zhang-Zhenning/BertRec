
Lecture 4: Sequence Models I

Alan Ri7er

(many slides from Greg Durrett, Dan Klein, Vivek Srikumar, Chris Manning, Yoav Artzi)


This Lecture

‣ Sequence modeling

‣ HMMs for POS tagging

‣ Viterbi, forward-backward

‣ HMM parameter esGmaGon


LinguisGc Structures

‣ Language is tree-structured


LinguisGc Structures

‣ Language is tree-structured

I ate the spaghe* with chops/cks


LinguisGc Structures

‣ Language is tree-structured

I ate the spaghe* with chops/cks

I ate the spaghe* with meatballs


LinguisGc Structures

‣ Language is tree-structured

I ate the spaghe* with chops/cks

I ate the spaghe* with meatballs


LinguisGc Structures

‣ Language is tree-structured

I ate the spaghe* with chops/cks

I ate the spaghe* with meatballs


LinguisGc Structures

‣ Language is tree-structured

I ate the spaghe* with chops/cks

I ate the spaghe* with meatballs

‣ Understanding syntax fundamentally requires trees — the sentences 

have the same shallow analysis


LinguisGc Structures

‣ Language is tree-structured

I ate the spaghe* with chops/cks

I ate the spaghe* with meatballs

‣ Understanding syntax fundamentally requires trees — the sentences 

have the same shallow analysis

I    ate  the spaghe* with chops/cks

I     ate  the spaghe* with meatballs

PRP VBZ  DT       NN        IN        NNS  

PRP VBZ  DT       NN        IN        NNS  


LinguisGc Structures

‣ Language is sequenGally structured: interpreted in an online way

Tanenhaus et al. (1995)


LinguisGc Structures

‣ Language is sequenGally structured: interpreted in an online way



Tanenhaus et al. (1995)


LinguisGc Structures

‣ Language is sequenGally structured: interpreted in an online way



Tanenhaus et al. (1995)




POS Tagging

Ghana ’s ambassador should have set up the big mee/ng in DC yesterday .

‣ What tags are out there?

NNP  POS NN                    MD  VB   VBN   RP DT JJ         NN    IN NNP NN       .


POS Tagging



Slide credit: Dan Klein


POS Tagging


POS Tagging

Fed raises interest rates 0.5 percent


POS Tagging

Fed raises interest rates 0.5 percent



I hereby 

increase interest 

rates 0.5%


POS Tagging

Fed raises interest rates 0.5 percent

VBD

VBN

NNP



I hereby 

increase interest 

rates 0.5%


POS Tagging

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS



I hereby 

increase interest 

rates 0.5%


POS Tagging

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN



I hereby 

increase interest 

rates 0.5%


POS Tagging

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS



I hereby 

increase interest 

rates 0.5%


POS Tagging

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN



I hereby 

increase interest 

rates 0.5%


POS Tagging

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN



I hereby 

increase interest 

rates 0.5%

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN


POS Tagging

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN

I’m 0.5% interested 

in the Fed’s raises!



I hereby 

increase interest 

rates 0.5%

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN




POS Tagging

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN

I’m 0.5% interested 

in the Fed’s raises!



I hereby 

increase interest 

rates 0.5%

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN

‣ Other paths are also plausible but even more semanGcally weird…




POS Tagging

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN

I’m 0.5% interested 

in the Fed’s raises!



I hereby 

increase interest 

rates 0.5%

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN

‣ Other paths are also plausible but even more semanGcally weird…

‣ What governs the correct choice? Word + context




POS Tagging

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN

I’m 0.5% interested 

in the Fed’s raises!



I hereby 

increase interest 

rates 0.5%

Fed raises interest rates 0.5 percent

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN

‣ Other paths are also plausible but even more semanGcally weird…

‣ What governs the correct choice? Word + context

‣ Word idenGty: most words have &lt;=2 tags, many have one (percent, the) 

‣ Context: nouns start sentences, nouns follow verbs, etc.




POS Tagging






What is this good for?


What is this good for?

‣ Text-to-speech: record, lead


What is this good for?

‣ Text-to-speech: record, lead

‣ Preprocessing step for syntacGc parsers


What is this good for?

‣ Text-to-speech: record, lead

‣ Preprocessing step for syntacGc parsers

‣ Domain-independent disambiguaGon for other tasks


What is this good for?

‣ Text-to-speech: record, lead

‣ Preprocessing step for syntacGc parsers

‣ Domain-independent disambiguaGon for other tasks

‣ (Very) shallow informaGon extracGon


Sequence Models


Sequence Models

‣ Input x = (x1, ..., xn)

y = (y1, ..., yn)

Output 


Sequence Models

‣ Input x = (x1, ..., xn)

y = (y1, ..., yn)

Output 

‣ POS tagging: x is a sequence of words, y is a sequence of tags


Sequence Models

‣ Input x = (x1, ..., xn)

y = (y1, ..., yn)

Output 

‣ POS tagging: x is a sequence of words, y is a sequence of tags

‣ Today: generaGve models P(x, y); discriminaGve models next Gme


Hidden Markov Models

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)


Hidden Markov Models

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)

‣ Model the sequence of y as a Markov process (dynamics model)


Hidden Markov Models

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)

‣ Model the sequence of y as a Markov process (dynamics model)

‣ Markov property: future is condiGonally independent of the past given 

the present


Hidden Markov Models

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)

‣ Model the sequence of y as a Markov process (dynamics model)

y1

y2

‣ Markov property: future is condiGonally independent of the past given 

the present

y3

P(y3|y1, y2) = P(y3|y2)


Hidden Markov Models

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)

‣ Model the sequence of y as a Markov process (dynamics model)

y1

y2

‣ Markov property: future is condiGonally independent of the past given 

the present

y3

P(y3|y1, y2) = P(y3|y2)

‣ Lots of mathemaGcal theory about how Markov chains behave


Hidden Markov Models

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)

‣ Model the sequence of y as a Markov process (dynamics model)

y1

y2

‣ Markov property: future is condiGonally independent of the past given 

the present

‣ If y are tags, this roughly corresponds to assuming that the next tag 

only depends on the current tag, not anything before

y3

P(y3|y1, y2) = P(y3|y2)

‣ Lots of mathemaGcal theory about how Markov chains behave


Hidden Markov Models

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)


Hidden Markov Models

y1

y2

yn

x1

x2

xn

…

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)

Fed     raises

percent

…

NNP

VBZ

NN

…


Hidden Markov Models

y1

y2

yn

x1

x2

xn

…

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)


Hidden Markov Models

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)


Hidden Markov Models

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

IniGal 

distribuGon

}

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)


Hidden Markov Models

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

IniGal 

distribuGon

TransiGon 

probabiliGes

}

}

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)


Hidden Markov Models

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

IniGal 

distribuGon

TransiGon 

probabiliGes

Emission 

probabiliGes

}

}

}

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)


Hidden Markov Models

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

IniGal 

distribuGon

TransiGon 

probabiliGes

Emission 

probabiliGes

}

}

}

‣ ObservaGon (x) depends 

only on current state (y)

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)


Hidden Markov Models

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

IniGal 

distribuGon

TransiGon 

probabiliGes

Emission 

probabiliGes

}

}

}

‣ MulGnomials: tag x tag 

transiGons, tag x word 

emissions

‣ ObservaGon (x) depends 

only on current state (y)

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)


Hidden Markov Models

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

IniGal 

distribuGon

TransiGon 

probabiliGes

Emission 

probabiliGes

}

}

}

‣ P(x|y) is a distribuGon over 

all words in the vocabulary 

— not a distribuGon over 

features (but could be!)

‣ MulGnomials: tag x tag 

transiGons, tag x word 

emissions

‣ ObservaGon (x) depends 

only on current state (y)

y = (y1, ..., yn)

Output 

‣ Input x = (x1, ..., xn)


TransiGons in POS Tagging

‣ Dynamics model

Fed raises interest rates 0.5 percent .

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN

P(y1)

n

Y

i=2

P(yi|yi�1)

 NNP - proper noun, singular 

 VBZ  - verb, 3rd ps. sing. present 

 NN   - noun, singular or mass

.


TransiGons in POS Tagging

‣ Dynamics model

Fed raises interest rates 0.5 percent .

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN

‣                            likely because start of sentence

P(y1 = NNP)

P(y1)

n

Y

i=2

P(yi|yi�1)

 NNP - proper noun, singular 

 VBZ  - verb, 3rd ps. sing. present 

 NN   - noun, singular or mass

.


TransiGons in POS Tagging

‣ Dynamics model

Fed raises interest rates 0.5 percent .

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN

‣                            likely because start of sentence

‣                                                 likely because verb oien follows noun

P(y1 = NNP)

P(y2 = VBZ|y1 = NNP)

P(y1)

n

Y

i=2

P(yi|yi�1)

 NNP - proper noun, singular 

 VBZ  - verb, 3rd ps. sing. present 

 NN   - noun, singular or mass

.


TransiGons in POS Tagging

‣ Dynamics model

Fed raises interest rates 0.5 percent .

VBD

VBN

NNP

VBZ

NNS

VB

VBP

NN

VBZ

NNS CD NN

‣                            likely because start of sentence

‣                                                 likely because verb oien follows noun

‣                                           direct object follows verb, other verb rarely 

follows past tense verb (main verbs can follow modals though!)

P(y1 = NNP)

P(y2 = VBZ|y1 = NNP)

P(y3 = NN|y2 = VBZ)

P(y1)

n

Y

i=2

P(yi|yi�1)

 NNP - proper noun, singular 

 VBZ  - verb, 3rd ps. sing. present 

 NN   - noun, singular or mass

.


EsGmaGng TransiGons

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

.


EsGmaGng TransiGons

‣ Similar to Naive Bayes esGmaGon: maximum likelihood soluGon = 

normalized counts (with smoothing) read oﬀ supervised data

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

.


EsGmaGng TransiGons

‣ Similar to Naive Bayes esGmaGon: maximum likelihood soluGon = 

normalized counts (with smoothing) read oﬀ supervised data

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

‣ P(tag | NN)

.


EsGmaGng TransiGons

‣ Similar to Naive Bayes esGmaGon: maximum likelihood soluGon = 

normalized counts (with smoothing) read oﬀ supervised data

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

‣ P(tag | NN)

.

= (0.5 ., 0.5 NNS)


EsGmaGng TransiGons

‣ Similar to Naive Bayes esGmaGon: maximum likelihood soluGon = 

normalized counts (with smoothing) read oﬀ supervised data

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

‣ How to smooth?

‣ P(tag | NN)

.

= (0.5 ., 0.5 NNS)


EsGmaGng TransiGons

‣ Similar to Naive Bayes esGmaGon: maximum likelihood soluGon = 

normalized counts (with smoothing) read oﬀ supervised data

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

‣ How to smooth?

‣ One method: smooth with unigram distribuGon over tags

‣ P(tag | NN)

.

= (0.5 ., 0.5 NNS)


EsGmaGng TransiGons

‣ Similar to Naive Bayes esGmaGon: maximum likelihood soluGon = 

normalized counts (with smoothing) read oﬀ supervised data

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

‣ How to smooth?

‣ One method: smooth with unigram distribuGon over tags

‣ P(tag | NN)

P(tag|tag�1) = (1 � �) ˆP(tag|tag�1) + � ˆP(tag)

.

= (0.5 ., 0.5 NNS)


EsGmaGng TransiGons

‣ Similar to Naive Bayes esGmaGon: maximum likelihood soluGon = 

normalized counts (with smoothing) read oﬀ supervised data

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

‣ How to smooth?

‣ One method: smooth with unigram distribuGon over tags

‣ P(tag | NN)

P(tag|tag�1) = (1 � �) ˆP(tag|tag�1) + � ˆP(tag)

= empirical distribuGon (read oﬀ from data)

ˆP

.

= (0.5 ., 0.5 NNS)


Emissions in POS Tagging

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

.


‣ Emissions P(x | y) capture the distribuGon of words occurring with a 

given tag

Emissions in POS Tagging

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

.


‣ Emissions P(x | y) capture the distribuGon of words occurring with a 

given tag

Emissions in POS Tagging

‣ P(word | NN) = (0.05 person, 0.04 oﬃcial, 0.03 interest, 0.03 percent …)

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

.


‣ Emissions P(x | y) capture the distribuGon of words occurring with a 

given tag

Emissions in POS Tagging

‣ P(word | NN) = (0.05 person, 0.04 oﬃcial, 0.03 interest, 0.03 percent …)

‣ When you compute the posterior for a given word’s tags, the distribuGon 

favors tags that are more likely to generate that word

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

.


‣ Emissions P(x | y) capture the distribuGon of words occurring with a 

given tag

Emissions in POS Tagging

‣ P(word | NN) = (0.05 person, 0.04 oﬃcial, 0.03 interest, 0.03 percent …)

‣ When you compute the posterior for a given word’s tags, the distribuGon 

favors tags that are more likely to generate that word

‣ How should we smooth this?

Fed raises interest rates 0.5 percent .

NNP VBZ

NN

NNS CD

NN

.


EsGmaGng Emissions

Fed raises interest rates 0.5 percent

NNP VBZ

NN

NNS CD

NN


EsGmaGng Emissions

‣ P(word | NN) = (0.5 interest, 0.5 percent) — hard to smooth!

Fed raises interest rates 0.5 percent

NNP VBZ

NN

NNS CD

NN


EsGmaGng Emissions

‣ P(word | NN) = (0.5 interest, 0.5 percent) — hard to smooth!

Fed raises interest rates 0.5 percent

NNP VBZ

NN

NNS CD

NN

‣ Can interpolate with distribuGon looking at word shape 

P(word shape | tag) (e.g., P(capitalized word of len &gt;= 8 | tag))


EsGmaGng Emissions

‣ P(word | NN) = (0.5 interest, 0.5 percent) — hard to smooth!

Fed raises interest rates 0.5 percent

NNP VBZ

NN

NNS CD

NN

‣ AlternaGve: use Bayes’ rule

‣ Can interpolate with distribuGon looking at word shape 

P(word shape | tag) (e.g., P(capitalized word of len &gt;= 8 | tag))


EsGmaGng Emissions

‣ P(word | NN) = (0.5 interest, 0.5 percent) — hard to smooth!

Fed raises interest rates 0.5 percent

NNP VBZ

NN

NNS CD

NN

P(word|tag) = P(tag|word)P(word)

P(tag)

‣ AlternaGve: use Bayes’ rule

‣ Can interpolate with distribuGon looking at word shape 

P(word shape | tag) (e.g., P(capitalized word of len &gt;= 8 | tag))


EsGmaGng Emissions

‣ P(word | NN) = (0.5 interest, 0.5 percent) — hard to smooth!

‣ Fancy techniques from language modeling, e.g. look at type ferGlity 

— P(tag|word) is ﬂa7er for some kinds of words than for others)

Fed raises interest rates 0.5 percent

NNP VBZ

NN

NNS CD

NN

P(word|tag) = P(tag|word)P(word)

P(tag)

‣ AlternaGve: use Bayes’ rule

‣ Can interpolate with distribuGon looking at word shape 

P(word shape | tag) (e.g., P(capitalized word of len &gt;= 8 | tag))


EsGmaGng Emissions

‣ P(word | NN) = (0.5 interest, 0.5 percent) — hard to smooth!

‣ Fancy techniques from language modeling, e.g. look at type ferGlity 

— P(tag|word) is ﬂa7er for some kinds of words than for others)

Fed raises interest rates 0.5 percent

NNP VBZ

NN

NNS CD

NN

P(word|tag) = P(tag|word)P(word)

P(tag)

‣ AlternaGve: use Bayes’ rule

‣ Can interpolate with distribuGon looking at word shape 

P(word shape | tag) (e.g., P(capitalized word of len &gt;= 8 | tag))

‣ P(word|tag) can be a log-linear model — we’ll see this in a few lectures


Inference in HMMs

‣ Input x = (x1, ..., xn)

y = (y1, ..., yn)

Output 

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)


Inference in HMMs

‣ Inference problem:

‣ Input x = (x1, ..., xn)

y = (y1, ..., yn)

Output 

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

argmaxyP(y|x) = argmaxy

P(y, x)

P(x)


Inference in HMMs

‣ Inference problem:

‣ Input x = (x1, ..., xn)

y = (y1, ..., yn)

Output 

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

argmaxyP(y|x) = argmaxy

P(y, x)

P(x)


Inference in HMMs

‣ Inference problem:

‣ ExponenGally many possible y here!

‣ Input x = (x1, ..., xn)

y = (y1, ..., yn)

Output 

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

argmaxyP(y|x) = argmaxy

P(y, x)

P(x)


Inference in HMMs

‣ Inference problem:

‣ ExponenGally many possible y here!

‣ SoluGon: dynamic programming (possible because of Markov structure!)

‣ Input x = (x1, ..., xn)

y = (y1, ..., yn)

Output 

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

argmaxyP(y|x) = argmaxy

P(y, x)

P(x)


Inference in HMMs

‣ Inference problem:

‣ ExponenGally many possible y here!

‣ SoluGon: dynamic programming (possible because of Markov structure!)

‣ Many neural sequence models depend on enGre previous tag 

sequence, need to use approximaGons like beam search

‣ Input x = (x1, ..., xn)

y = (y1, ..., yn)

Output 

y1

y2

yn

x1

x2

xn

…

P(y, x) = P(y1)

n

Y

i=2

P(yi|yi�1)

n

Y

i=1

P(xi|yi)

argmaxyP(y|x) = argmaxy

P(y, x)

P(x)


Viterbi Algorithm



slide credit: Vivek Srikumar


Viterbi Algorithm



slide credit: Vivek Srikumar


Viterbi Algorithm



slide credit: Vivek Srikumar


Viterbi Algorithm



slide credit: Vivek Srikumar

best (parGal) score for  

a sequence ending in state s


Viterbi Algorithm





slide credit: Vivek Srikumar


Viterbi Algorithm





slide credit: Vivek Srikumar


Viterbi Algorithm

slide credit: Dan Klein




Viterbi Algorithm

slide credit: Dan Klein

‣ “Think about” all possible immediate 

prior state values. Everything before 

that has already been accounted for by 

earlier stages.




Viterbi Algorithm





slide credit: Vivek Srikumar


Viterbi Algorithm





slide credit: Vivek Srikumar


Viterbi Algorithm





slide credit: Vivek Srikumar


Forward-Backward Algorithm


Forward-Backward Algorithm

‣ In addiGon to ﬁnding the best path, we may want to compute 

marginal probabiliGes of paths P(yi = s|x)


Forward-Backward Algorithm

‣ In addiGon to ﬁnding the best path, we may want to compute 

marginal probabiliGes of paths P(yi = s|x)

P(yi = s|x) =

X

y1,...,yi�1,yi+1,...,yn

P(y|x)


Forward-Backward Algorithm

‣ In addiGon to ﬁnding the best path, we may want to compute 

marginal probabiliGes of paths P(yi = s|x)

P(yi = s|x) =

X

y1,...,yi�1,yi+1,...,yn

P(y|x)

‣ What did Viterbi compute? P(ymax|x) = max

y1,...,yn P(y|x)


Forward-Backward Algorithm

‣ In addiGon to ﬁnding the best path, we may want to compute 

marginal probabiliGes of paths P(yi = s|x)

P(yi = s|x) =

X

y1,...,yi�1,yi+1,...,yn

P(y|x)

‣ What did Viterbi compute? P(ymax|x) = max

y1,...,yn P(y|x)

‣ Can compute marginals with dynamic programming as well using an 

algorithm called forward-backward


Forward-Backward Algorithm




Forward-Backward Algorithm

P(y3 = 2|x) =




Forward-Backward Algorithm

P(y3 = 2|x) =

sum of all paths through state 2 at time 3

sum of all paths




Forward-Backward Algorithm

P(y3 = 2|x) =

sum of all paths through state 2 at time 3

sum of all paths




Forward-Backward Algorithm

P(y3 = 2|x) =

sum of all paths through state 2 at time 3

sum of all paths




Forward-Backward Algorithm

P(y3 = 2|x) =

sum of all paths through state 2 at time 3

sum of all paths




Forward-Backward Algorithm







slide credit: Dan Klein

P(y3 = 2|x) =

sum of all paths through state 2 at time 3

sum of all paths


Forward-Backward Algorithm







slide credit: Dan Klein

P(y3 = 2|x) =

sum of all paths through state 2 at time 3

sum of all paths

=


Forward-Backward Algorithm







slide credit: Dan Klein

P(y3 = 2|x) =

sum of all paths through state 2 at time 3

sum of all paths

=

‣ Easiest and most ﬂexible to do one 

pass to compute        and one to 

compute 


Forward-Backward Algorithm








Forward-Backward Algorithm







‣ IniGal:


Forward-Backward Algorithm







↵1(s) = P(s)P(x1|s)

‣ IniGal:


Forward-Backward Algorithm







↵1(s) = P(s)P(x1|s)

‣ IniGal:

‣ Recurrence:


Forward-Backward Algorithm







↵1(s) = P(s)P(x1|s)

↵t(st) =

X

st�1

↵t�1(st�1)P(st|st�1)P(xt|st)

‣ IniGal:

‣ Recurrence:


Forward-Backward Algorithm







↵1(s) = P(s)P(x1|s)

↵t(st) =

X

st�1

↵t�1(st�1)P(st|st�1)P(xt|st)

‣ IniGal:

‣ Recurrence:

‣ Same as Viterbi but summing 

instead of maxing!


Forward-Backward Algorithm







↵1(s) = P(s)P(x1|s)

↵t(st) =

X

st�1

↵t�1(st�1)P(st|st�1)P(xt|st)

‣ IniGal:

‣ Recurrence:

‣ Same as Viterbi but summing 

instead of maxing!

‣ These quanGGes get very small! 

Store everything as log probabiliGes


Forward-Backward Algorithm








Forward-Backward Algorithm







‣ IniGal:


Forward-Backward Algorithm







‣ IniGal:

�n(s) = 1


Forward-Backward Algorithm







‣ IniGal:

�n(s) = 1

‣ Recurrence:


Forward-Backward Algorithm







‣ IniGal:

�n(s) = 1

�t(st) =

X

st+1

�t+1(st+1)P(st+1|st)P(xt+1|st+1)

‣ Recurrence:


Forward-Backward Algorithm







‣ IniGal:

�n(s) = 1

�t(st) =

X

st+1

�t+1(st+1)P(st+1|st)P(xt+1|st+1)

‣ Recurrence:

‣ Big diﬀerences: count emission for 

the next Gmestep (not current one)


Forward-Backward Algorithm

↵1(s) = P(s)P(x1|s)

↵t(st) =

X

st�1

↵t�1(st�1)P(st|st�1)P(xt|st)







�n(s) = 1

�t(st) =

X

st+1

�t+1(st+1)P(st+1|st)P(xt+1|st+1)

‣ Big diﬀerences: count emission for 

the next Gmestep (not current one)


Forward-Backward Algorithm

↵1(s) = P(s)P(x1|s)

↵t(st) =

X

st�1

↵t�1(st�1)P(st|st�1)P(xt|st)







�n(s) = 1

�t(st) =

X

st+1

�t+1(st+1)P(st+1|st)P(xt+1|st+1)

=


Forward-Backward Algorithm

↵1(s) = P(s)P(x1|s)

↵t(st) =

X

st�1

↵t�1(st�1)P(st|st�1)P(xt|st)







�n(s) = 1

�t(st) =

X

st+1

�t+1(st+1)P(st+1|st)P(xt+1|st+1)

P(s3 = 2|x) =

↵3(2)�3(2)

P

i ↵3(i)�3(i) =


Forward-Backward Algorithm

↵1(s) = P(s)P(x1|s)

↵t(st) =

X

st�1

↵t�1(st�1)P(st|st�1)P(xt|st)







�n(s) = 1

�t(st) =

X

st+1

�t+1(st+1)P(st+1|st)P(xt+1|st+1)

P(s3 = 2|x) =

↵3(2)�3(2)

P

i ↵3(i)�3(i)

‣ What is the denominator here?

=


Forward-Backward Algorithm

↵1(s) = P(s)P(x1|s)

↵t(st) =

X

st�1

↵t�1(st�1)P(st|st�1)P(xt|st)







�n(s) = 1

�t(st) =

X

st+1

�t+1(st+1)P(st+1|st)P(xt+1|st+1)

P(s3 = 2|x) =

↵3(2)�3(2)

P

i ↵3(i)�3(i)

‣ What is the denominator here? P(x)

=


HMM POS Tagging

Slide credit: Dan Klein


HMM POS Tagging

‣ Baseline: assign each word its most frequent tag: ~90% accuracy

Slide credit: Dan Klein


HMM POS Tagging

‣ Baseline: assign each word its most frequent tag: ~90% accuracy

‣ Trigram HMM: ~95% accuracy / 55% on unknown words

Slide credit: Dan Klein


Trigram Taggers

Fed raises interest rates 0.5 percent

NNP VBZ

NN

NNS CD

NN


Trigram Taggers

‣ Trigram model: y1 = (&lt;S&gt;, NNP), y2 = (NNP, VBZ), …

Fed raises interest rates 0.5 percent

NNP VBZ

NN

NNS CD

NN


Trigram Taggers

‣ Trigram model: y1 = (&lt;S&gt;, NNP), y2 = (NNP, VBZ), …

‣ P((VBZ, NN) | (NNP, VBZ)) — more context! Noun-verb-noun S-V-O

Fed raises interest rates 0.5 percent

NNP VBZ

NN

NNS CD

NN


Trigram Taggers

‣ Trigram model: y1 = (&lt;S&gt;, NNP), y2 = (NNP, VBZ), …

‣ P((VBZ, NN) | (NNP, VBZ)) — more context! Noun-verb-noun S-V-O

Fed raises interest rates 0.5 percent

NNP VBZ

NN

NNS CD

NN

‣ Tradeoﬀ between model capacity and data size — trigrams are a 

“sweet spot” for POS tagging


HMM POS Tagging

‣ Baseline: assign each word its most frequent tag: ~90% accuracy

‣ Trigram HMM: ~95% accuracy / 55% on unknown words

Slide credit: Dan Klein


HMM POS Tagging

‣ Baseline: assign each word its most frequent tag: ~90% accuracy

‣ Trigram HMM: ~95% accuracy / 55% on unknown words

‣ TnT tagger (Brants 1998, tuned HMM): 96.2% accuracy / 86.0% on unks

Slide credit: Dan Klein


HMM POS Tagging

‣ Baseline: assign each word its most frequent tag: ~90% accuracy

‣ Trigram HMM: ~95% accuracy / 55% on unknown words

‣ TnT tagger (Brants 1998, tuned HMM): 96.2% accuracy / 86.0% on unks

Slide credit: Dan Klein

‣ State-of-the-art (BiLSTM-CRFs): 97.5% / 89%+


Errors



Slide credit: Dan Klein / Toutanova + Manning (2000)


Errors



oﬃcial knowledge

JJ/NN       NN

Slide credit: Dan Klein / Toutanova + Manning (2000)


Errors



oﬃcial knowledge

JJ/NN       NN

Slide credit: Dan Klein / Toutanova + Manning (2000)

(NN NN: tax cut, art gallery, …)


Errors



oﬃcial knowledge

made  up  the story

JJ/NN       NN

VBD  RP/IN DT  NN

Slide credit: Dan Klein / Toutanova + Manning (2000)

(NN NN: tax cut, art gallery, …)


Errors



oﬃcial knowledge

made  up  the story

recently  sold  shares

JJ/NN       NN

VBD  RP/IN DT  NN

RB    VBD/VBN  NNS

Slide credit: Dan Klein / Toutanova + Manning (2000)

(NN NN: tax cut, art gallery, …)


Remaining Errors

Manning 2011 “Part-of-Speech Tagging from 97% to 100%: Is It Time for Some LinguisGcs?”


Remaining Errors

‣ Lexicon gap (word not seen with that tag in training) 4.5%

Manning 2011 “Part-of-Speech Tagging from 97% to 100%: Is It Time for Some LinguisGcs?”


Remaining Errors

‣ Lexicon gap (word not seen with that tag in training) 4.5%

‣ Unknown word: 4.5%

Manning 2011 “Part-of-Speech Tagging from 97% to 100%: Is It Time for Some LinguisGcs?”


Remaining Errors

‣ Lexicon gap (word not seen with that tag in training) 4.5%

‣ Unknown word: 4.5%

‣ Could get right: 16% (many of these involve parsing!)

Manning 2011 “Part-of-Speech Tagging from 97% to 100%: Is It Time for Some LinguisGcs?”


Remaining Errors

‣ Lexicon gap (word not seen with that tag in training) 4.5%

‣ Unknown word: 4.5%

‣ Could get right: 16% (many of these involve parsing!)

‣ Diﬃcult linguisGcs: 20%

Manning 2011 “Part-of-Speech Tagging from 97% to 100%: Is It Time for Some LinguisGcs?”


Remaining Errors

‣ Lexicon gap (word not seen with that tag in training) 4.5%

‣ Unknown word: 4.5%

‣ Could get right: 16% (many of these involve parsing!)

‣ Diﬃcult linguisGcs: 20%

They      set       up absurd situa/ons, detached from reality

VBD / VBP? (past or present?)

Manning 2011 “Part-of-Speech Tagging from 97% to 100%: Is It Time for Some LinguisGcs?”


Remaining Errors

‣ Underspeciﬁed / unclear, gold standard inconsistent / wrong: 58%

‣ Lexicon gap (word not seen with that tag in training) 4.5%

‣ Unknown word: 4.5%

‣ Could get right: 16% (many of these involve parsing!)

‣ Diﬃcult linguisGcs: 20%

They      set       up absurd situa/ons, detached from reality

VBD / VBP? (past or present?)

Manning 2011 “Part-of-Speech Tagging from 97% to 100%: Is It Time for Some LinguisGcs?”


Remaining Errors

‣ Underspeciﬁed / unclear, gold standard inconsistent / wrong: 58%

‣ Lexicon gap (word not seen with that tag in training) 4.5%

‣ Unknown word: 4.5%

‣ Could get right: 16% (many of these involve parsing!)

‣ Diﬃcult linguisGcs: 20%

They      set       up absurd situa/ons, detached from reality

VBD / VBP? (past or present?)

a $ 10 million fourth-quarter charge against discon/nued opera/ons

adjecGve or verbal parGciple? JJ / VBN?

Manning 2011 “Part-of-Speech Tagging from 97% to 100%: Is It Time for Some LinguisGcs?”


Other Languages



Petrov et al. 2012


Next Time


Next Time

‣ CRFs: feature-based discriminaGve models


Next Time

‣ CRFs: feature-based discriminaGve models

‣ Structured SVM for sequences


Next Time

‣ CRFs: feature-based discriminaGve models

‣ Structured SVM for sequences

‣ Named enGty recogniGon

