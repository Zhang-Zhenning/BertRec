
Understanding Maximum Likelihood

An Interactive Visualization

Created by Kristoffer Magnusson

Share

The maximum likelihood method is used to fit many models in statistics. In this post I

will present some interactive visualizations to try to explain maximum likelihood

estimation and some common hypotheses tests (the likelihood ratio test, Wald test,

and Score test).

We will use a simple model with only two unknown parameters: the mean and variance.

Our primary focus will be on the mean and we'll treat the variance as a nuisance

parameter.

Likelihood Calculation

Before we do any calculations, we need some data. So, here's 10 random observations

from a normal distribution with unknown mean (μ) and variance (σ²).

Y = [1.0, 2.0]

We also need to assume a model, we're gonna go with the model that we know

generated this data: 

. The challenge now is to find what combination of

values for μ and σ² maximize the likelihood of observing this data (given our assumed

model). Try moving the sliders around to see what happens.

Mean (μ)

Variance

(σ²)

We can calculate the joint likelihood by multiplying the densities for all observations. However, often we

calculate the log-likelihood instead, which is

y ∼ N(μ, σ )

2

80

 

100

 







NEW SAMPLE


-34.4 + -33.6 = -68.1

The combination of parameter values that give the largest log-likelihood is the maximum likelihood estimates

(MLEs).

Finding the Maximum Likelihood Estimates

Since we use a very simple model, there's a couple of ways to find the MLEs. If we

repeat the above calculation for a wide range of parameter values, we get the plots

below. The joint MLEs can be found at the top of contour plot, which shows the

likelihood function for a grid of parameter values. We can also find the MLEs

analytically by using some calculus. We find the top of the hill by using the partial

derivatives with regard to μ and σ² - which is generally called the score function (U).

Solving the score equations mean that we find which combination of μ and σ² leads to

both partial derivates being zero.

Mean

For more challenging models, we often need to use some optimization algorithm. Basically, we let the

computer iteratively climb towards the top of the hill. You can use the controls below to see how a gradient

ascent or Newton-Raphson algorithm finds its way to the maximum likelihood estimate.

Iterations: 0

Variance

Tip: You can move the values around by dragging them.

Inference

After we've found the MLEs we usually want to make some inferences, so let's focus on

three common hypothesis tests. Use the sliders below to change the null hypothesis

and the sample size.

Illustration

Sample Size

(n)

Null (μ0)

The score function evaluated at the null is,

ℓ(μ, σ ) =

2

ln f (y ) =

∑i

n

y

i

Algorithm

Gradient ascent

 

+1 -1

10

 

80

 


The observed Fisher information is the negative of the second derivative. This is related to the curvature of

the likelihood function -- try increasing the sample size and note that the peak gets narrower around the MLE

and that the information increases. The inverse of I is also the variance of the MLE.

Hypothesis Tests

We have the following null and alternative hypothesis,

The likelihood ratio test compares the likelihood ratios of two models. In this example it's the likelihood

evaluated at the MLE and at the null. This is illustrated in the plot by the vertical distance between the

two horizontal lines. If we multiply the difference in log-likelihood by -2 we get the statistic,

Asymptotically LR follows a 

 distribution with 1 degrees of freedom, which gives p = NaN.

Note: The figure is simplified and does not account for the fact that each likelihood is based on

different variance estimates.

Written by Kristoffer Magnusson, a researcher in clinical psychology. You should follow

him on Twitter and come hang out on the open science discord Git Gud Science.

FAQ

U(μ ,

) =

0 σ^02

ℓ(μ ,

) =

∂μ0

∂

0 σ^02

−Infinity

I(μ ,

) =

0 σ^02

ℓ(μ ,

) =

∂μ02

∂2

0 σ^02

Infinity

H :

0

μ = 80

versus

H :

1

μ = 80

LRT





LR = −2[ℓ(μ ,

)−[ℓ( ,

)]

0 σ^0

2

μ^ σ^2

= NaN

χ2









What are the formulas?







How do I cite this page?








Contribute/Donate

There are many ways to contribute to free and open software. If you like my work and

want to support it you can:

A huge thanks to the 145 supporters who've bought me a 338 coffees!

Jason Rinaldo bought ���������� (10) coffees

I've been looking for applets that show this for YEARS, for demonstrations for classes.

Thank you so much! Students do not need to tolarate my whiteboard scrawl now. I'm

sure they'd appreciate you, too.l

Someone bought ����� (5) coffees

@metzpsych bought ����� (5) coffees

Always the clearest, loveliest simulations for complex concepts. Amazing resource for

teaching intro stats!

Ryo bought ����� (5) coffees

For a couple years now I've been wanting to create visualizations like these as a way to

commit these foundational concepts to memory. But after finding your website I'm both

relieved that I don't have to do that now and pissed off that I couldn't create anything half

as beautiful and informative as you have done here. Wonderful job.













I found a bug/error/typo or want to make a suggestion!







I'm gonna ask a large number of students to visit this site. Will

it crash your server?







Can I include this visualization in my book/article/etc?







Buy Me a Coffee �

(or use PayPal)







SHOW ALL


Sponsors

You can sponsor my open source work using GitHub Sponsors and have your name

shown here.

Backers ���

Bazyli Brzóska

Joseph Bulbulia

Darren L Dahly

bradfde

Your Name

Pull requests are also welcome, or you can contribute by suggesting new features, add

useful references, or help fix typos. Just open a issues on GitHub.

Webmentions

0 

 0

What's this?

Yann de Mey 

 2022-11-09

I am indeed using your medium guide to bring this into practice in @Stata on Friday ;) I also

link to these pages as they nicely add some visual intuition for the students as well:

aboveintelligent.com/deep-learning-… and rpsychologist.com/likelihood

Andrew Camp 

 2022-02-14

@Kate__Barnes @AlisonHeape @SarahR_Morris @gema_zamarro Check this out -

rpsychologist.com/likelihood/

������� 

 2021-08-11

������� Understanding Maximum Likelihood An Interactive Visualization

rpsychologist.com/likelihood/

Farzana Khatun Tania 

 2021-05-14

This �! I try to illustrate the maximum likelihood method. I've also included the likelihood

ratio test, Wald test, and Score test. rpsychologist.com/d3/likelihood/

twitter.com/krstoffr/statu…

Lucille Rausch 

 2021-03-01

Understanding Maximum Likelihood Estimation - an interactive visualization by @krstoffr

rpsychologist.com/likelihood




















(Webmentions sent before 2021 will unfortunately not show up here.)

More Visualizations

Understanding p-

values Through

Simulations

An interactive

simulation to help

explain p-values

Maximum

Likelihood

An interactive

post covering

various aspects of

maximum

likelihood

estimation.

Cohen's d

An interactive app

to visualize and

understand

standardized

effect sizes.

Statistical Power

and Significance

Testing

An interactive

version of the


























traditional Type I

and II error

illustration.

Confidence

Intervals

An interactive

simulation of

confidence

intervals

Bayesian

Inference

An interactive

illustration of

prior, likelihood,

and posterior.

Correlations

Interactive

scatterplot that

lets you visualize

correlations of

various

magnitudes.

Equivalence and

Non-Inferiority

Testing

Explore how

superiority, non-

inferiority, and

equivalence


























testing relates to

a confidence

interval

P-value

distribution

Explore the

expected

distribution of p-

values under

varying

alternative

hypothesises.

t-distribution

Interactively

compare the t-

and normal

distribution.

Designed and built by Kristoffer Magnusson. Powered by Gatsby.

Connect













Twitter

Mastodon

GitHub

LinkedIn


Donate

License

Version 0.1.2, last updated 2020-10-24. License MIT (source code), the visualization is CC0,

and the text content is CC-BY 4.0.

© 2023 Kristoffer Magnusson.

Blog

Discord

GitHub Sponsors

Buy Me a Coffee

PayPal

