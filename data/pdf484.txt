


Jan 3, 2019

·

7 min read

Hidden Markov Models — Part 2: the Decoding

Problem

Photo by fotografierende on Unsplash

Problem 2 — Decoding




Q = ???

The Viterbi Algorithm

Initialization

Viterbi Initialization Equation

Initialization of Viterbi Algorithm

Viterbi Initialization Array Equation

ψ(Sunny) = [ 0 ]

ψ(Rainy) = [ 0 ]

Recursion


Recursion

Viterbi Recursion Equation

Recursion of Viterbi Algorithm 1

0.12 * 0.8 * 0.1 = 0.0096

0.08 * 0.4 * 0.1 = 0.0032

0.0096 &gt; 0.0032 

δ = 0.0096

ψ(Sunny) = [ 0, Sunny ]

Recursion of Viterbi Algorithm 2

ψ(Rainy) = [ 0, Rainy ]

Recursion of Viterbi Algorithm 3

ψ(Sunny) = [ 0, Sunny, Rainy, Sunny ]

ψ(Rainy) = [ 0, Rainy, Rainy, Sunny ]


Termination

0.00082944 &gt; 0.00015552 =&gt; P = 0.00082944

Sunny

Sunny

Backtracking

Sunny state

Q = [ ?, ?, ?, Sunny ]

Sunny

4

ψ(Sunny)

Sunny


Q = [ ?, ?, Sunny, Sunny ]

Sunny

3

ψ(Sunny)

Rainy

Q = [ ?, Rainy, Sunny, Sunny ]

Rainy

Rainy

ψ(Rainy) 

Rainy

Shop =&gt; Clean =&gt; Bike =&gt; Paint

Q = [ Rainy, Rainy, Sunny, Sunny ]

Hidden State Sequence derived from Backtracking

Conclusion &amp; What’s next

*Bonus tip for developers*

npm package called mary-markov

here

here

npm install --save mary-markov


*********

Machine Learning

Artificial Intelligence

Hidden Markov Models

Viterbi Algorithm

Backtracking


6



Follow



JavaScript Full Stack Developer | Web &amp; Graphic Designer | Passionate reader and writer | A bit of a loner sometimes :)





