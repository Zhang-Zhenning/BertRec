


Member-only story

GloVe, ELMo &amp; BERT

·

Published in

Towards Data Science

10 min read

·

Mar 16, 2021

Listen

Share

Photo by Amador Loureiro on Unsplash

They 

Pandas

They

Pandas








Spark NLP

GloVe

ELMo

BERT

What is Spark NLP ?


The dataset


0 = no disaster

1 = disaster


1211 null values


Building the preprocessing pipeline

DocumentAssembler()

Tokenizer()


Normalizer()


StopWordsCleaner()

Lemmatizer()


→ write


Visualizing the pipeline’s effect


13%.


→

→

→


Word embeddings

GloVe

ELMo

BERT


disaster

no disaster

ratio

P(k|disaster),

P(k|no disaster)

disaster

no disaster

disaster

no

disaster

wind

wind


wind

and

disaster

no disaster

Results


GloVe



ELMo



BERT



What did we learn ?

GloVe

ELMo

BERT


Helpful resources



Follow



NLP

Transfer Learning

Text Classification

Spark Nlp

Word Embeddings






182 Followers

·

Writer for 

Towards Data Science

Data scientist and a life-long learner.



TabNet — Deep Neural Network for Structured, Tabular Data

·

·






Zero-ETL, ChatGPT, And The Future of Data Engineering

·






The Portfolio that Got Me a Data Scientist Job

·

·






How to Implement Deep Neural Networks for Time-to-Event Analyses

·

·

See all from Ryan Burke

·

See all from Towards Data Science






How to Train a Word2Vec Model from Scratch with Gensim

·

·






Generating Word Embeddings from Text Data using Skip-Gram Algorithm and Deep

Learning in Python

·

·






Topic Modeling For Beginners Using BERTopic and Python

·

·






Understanding and Coding the Attention Mechanism — The Magic Behind

Transformers

·

·






Interpreting the Prediction of BERT Model for Text Classification

·

·






Essential Guide to Foundation Models and Large Language Models

·

·

See more recommendations



