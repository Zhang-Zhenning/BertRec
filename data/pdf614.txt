
Pretrained Transformers for Text Ranking: BERT and Beyond

Andrew Yates, Rodrigo Nogueira, Jimmy Lin



 

Anthology ID:

2021.naacl-tutorials.1

Volume:

Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:

Human Language Technologies: Tutorials

Month:

June

Year:

2021

Address:

Online

Venue:

NAACL

SIG:

–

Publisher:

Association for Computational Linguistics

Note:

–

Pages:

1–4

Language:

–

URL:

https://aclanthology.org/2021.naacl-tutorials.1

DOI:

10.18653/v1/2021.naacl-tutorials.1

Bibkey:

Cite (ACL):

Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained Transformers for Text Ranking: BERT and Beyond. In

Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:



Abstract

The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a

particular task. Although the most common formulation of text ranking is search, instances of the task can also be found

in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures

known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known

example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is

based on the preprint of a forthcoming book to be published by Morgan and &amp; Claypool under the Synthesis Lectures on

Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners

who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in

this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking

in multi-stage ranking architectures and learned dense representations that perform ranking directly.


ACL materials are Copyright © 1963–2023 ACL; other materials are copyrighted by their respective copyright holders. Materials

prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License.

Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed

on a Creative Commons Attribution 4.0 International License.

The ACL Anthology is managed and built by the ACL Anthology team of volunteers.

Site last built on 29 April 2023 at 17:40 UTC with commit cc1e6b15.

Human Language Technologies: Tutorials, pages 1–4, Online. Association for Computational Linguistics.

Cite (Informal):

Pretrained Transformers for Text Ranking: BERT and Beyond (Yates et al., NAACL 2021)

Copy Citation:

More options…

PDF:

https://aclanthology.org/2021.naacl-tutorials.1.pdf

Video:



 https://aclanthology.org/2021.naacl-tutorials.1.mp4

Data

ASNQ, BEIR, C4, CORD-19, DL-HARD, MS MARCO, Natural Questions, SNLI, SQuAD, SST, TREC-

COVID, TriviaQA, WebQuestions

 PDF

 



 Cite

 



 



  Video

