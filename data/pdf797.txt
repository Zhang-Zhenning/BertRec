
Home &gt; AI technologies

Enterprise

AI

DEFINITION

BERT language model

Ben Lutkevich, Technical Features Writer

BERT is an open source machine learning framework for natural language processing (NLP). BERT is designed to help

computers understand the meaning of ambiguous language in text by using surrounding text to establish context. The BERT

framework was pre-trained using text from Wikipedia and can be fine-tuned with question and answer datasets.

BERT, which stands for Bidirectional Encoder Representations from Transformers, is based on Transformers, a deep

learning model in which every output element is connected to every input element, and the weightings between them are

dynamically calculated based upon their connection. (In NLP, this process is called attention.)

Historically, language models could only read text input sequentially -- either left-to-right or right-to-left -- but couldn't do both at

the same time. BERT is different because it is designed to read in both directions at once. This capability, enabled by the

introduction of Transformers, is known as bidirectionality. 

Using this bidirectional capability, BERT is pre-trained on two different, but related, NLP tasks: Masked Language Modeling and

Next Sentence Prediction.

The objective of Masked Language Model (MLM) training is to hide a word in a sentence and then have the program predict

what word has been hidden (masked) based on the hidden word's context. The objective of Next Sentence Prediction training

is to have the program predict whether two given sentences have a logical, sequential connection or whether their relationship

is simply random.

Transformers were first introduced by Google in 2017. At the time of their introduction, language models primarily used

recurrent neural networks (RNN) and convolutional neural networks (CNN) to handle NLP tasks.

Although these models are competent, the Transformer is considered a significant improvement because it doesn't require

sequences of data to be processed in any fixed order, whereas RNNs and CNNs do. Because Transformers can process

data in any order, they enable training on larger amounts of data than ever was possible before their existence. This, in turn,

facilitated the creation of pre-trained models like BERT, which was trained on massive amounts of language data prior to its

release. 

In 2018, Google introduced and open-sourced BERT. In its research stages, the framework achieved groundbreaking results

in 11 natural language understanding tasks, including sentiment analysis, semantic role labeling, sentence classification and

the disambiguation of polysemous words, or words with multiple meanings.

Completing these tasks distinguished BERT from previous language models such as word2vec and GloVe, which are limited

when interpreting context and polysemous words. BERT effectively addresses ambiguity, which is the greatest challenge to

natural language understanding according to research scientists in the field. It is capable of parsing language with a relatively

human-like "common sense".

In October 2019, Google announced that they would begin applying BERT to their United States based production search

algorithms.

BERT is expected to affect 10% of Google search queries. Organizations are recommended not to try and optimize content

for BERT, as BERT aims to provide a natural-feeling search experience. Users are advised to keep queries and content

focused on the natural subject matter and natural user experience.

Background


In December 2019, BERT was applied to more than 70 different languages.

The goal of any given NLP technique is to understand human language as it is spoken naturally. In BERT's case, this typically

means predicting a word in a blank. To do this, models typically need to train using a large repository of specialized, labeled

training data. This necessitates laborious manual data labeling by teams of linguists.

BERT, however, was pre-trained using only an unlabeled, plain text corpus (namely the entirety of the English Wikipedia, and

the Brown Corpus). It continues to learn unsupervised from the unlabeled text and improve even as its being used in practical

applications (ie Google search). Its pre-training serves as a base layer of "knowledge" to build from. From there, BERT can

adapt to the ever-growing body of searchable content and queries and be fine-tuned to a user's specifications. This process is

known as transfer learning.

As mentioned above, BERT is made possible by Google's research on Transformers. The transformer is the part of the model

that gives BERT its increased capacity for understanding context and ambiguity in language. The transformer does this by

processing any given word in relation to all other words in a sentence, rather than processing them one at a time. By looking at

all surrounding words, the Transformer allows the BERT model to understand the full context of the word, and therefore better

understand searcher intent.

This is contrasted against the traditional method of language processing, known as word embedding, in which previous

models like GloVe and word2vec would map every single word to a vector, which represents only one dimension, a sliver, of

that word's meaning.

These word embedding models require large datasets of labeled data. While they are adept at many general NLP tasks, they

fail at the context-heavy, predictive nature of question answering, because all words are in some sense fixed to a vector or

meaning. BERT uses a method of masked language modeling to keep the word in focus from "seeing itself" -- that is, having a

fixed meaning independent of its context. BERT is then forced to identify the masked word based on context alone. In BERT

words are defined by their surroundings, not by a pre-fixed identity. In the words of English linguist John Rupert Firth, "You shall

know a word by the company it keeps."

BERT chart

BERT chart

BERT is also the first NLP technique to rely solely on self-attention mechanism, which is made possible by the bidirectional

Transformers at the center of BERT's design. This is significant because often, a word may change meaning as a sentence

develops. Each word added augments the overall meaning of the word being focused on by the NLP algorithm. The more

words that are present in total in each sentence or phrase, the more ambiguous the word in focus becomes. BERT accounts

for the augmented meaning by reading bidirectionally, accounting for the effect of all other words in a sentence on the focus

word and eliminating the left-to-right momentum that biases words towards a certain meaning as a sentence progresses.

For example, in the image above, BERT is determining which prior word in the sentence the word "is" referring to, and then

using its attention mechanism to weigh the options. The word with the highest calculated score is deemed the correct

association (i.e., "is" refers to "animal", not "he"). If this phrase was a search query, the results would reflect this subtler, more

precise understanding the BERT reached.

BERT is currently being used at Google to optimize the interpretation of user search queries. BERT excels at several

functions that make this possible, including:

Sequence-to-sequence based language generation tasks such as:

Question answering

Abstract summarization

Sentence prediction

Conversational response generation

Natural language understanding tasks such as:

Polysemy and Coreference (words that sound or look the same but have different meanings) resolution

Word sense disambiguation

How BERT works

What is BERT used for?


Natural language inference

Sentiment classification

BERT is expected to have a large impact on voice search as well as text-based search, which has been error-prone with

Google's NLP techniques to date. BERT is also expected to drastically improve international SEO, because its proficiency in

understanding context helps it interpret patterns that different languages share without having to understand the language

completely. More broadly, BERT has the potential to drastically improve artificial intelligence systems across the board.

BERT is open source, meaning anyone can use it. Google claims that users can train a state-of-the-art question and answer

system in just 30 minutes on a cloud tensor processing unit (TPU), and in a few hours using a graphic processing unit (GPU).

Many other organizations, research groups and separate factions of Google are fine-tuning the BERT model architecture with

supervised training to either optimize it for efficiency (modifying the learning rate, for example) or specialize it for certain tasks

by pre-training it with certain contextual representations. Some examples include:

patentBERT - a BERT model fine-tuned to perform patent classification.

docBERT - a BERT model fine-tuned for document classification.

bioBERT - a pre-trained biomedical language representation model for biomedical text mining.

VideoBERT - a joint visual-linguistic model for process unsupervised learning of an abundance of unlabeled data on

Youtube. 

SciBERT - a pretrained BERT model for scientific text

G-BERT - a BERT model pretrained using medical codes with hierarchical representations using graph neural networks

(GNN) and then fine-tuned for making medical recommendations.

TinyBERT by Huawei - a smaller, "student" BERT that learns from the original "teacher" BERT, performing transformer

distillation to improve efficiency. TinyBERT produced promising results in comparison to BERT-base while being 7.5 times

smaller and 9.4 times faster at inference.

DistilBERT by HuggingFace - a supposedly smaller, faster, cheaper version of BERT that is trained from BERT, and then

certain architectural aspects are removed for the sake of efficiency.

This was last updated in January 2020

Related Terms

narrow AI (weak AI)

Narrow AI is an application of artificial intelligence technologies to enable a high-functioning system that replicates -- and ... See complete definition

Turing Test

A Turing Test is a method of inquiry in artificial intelligence (AI) for determining whether or not a computer is capable of ... See complete definition

Continue Reading About BERT language model

NLP uses in BI and analytics speak softly but carry a big stick

∙ Key considerations for operationalizing machine learning

∙ Wayfair takes a dip into NLP image processing technology

∙ AI for knowledge management boosts information accessibility

∙ BERT practical guide

∙


Latest TechTarget resources

Business Analytics

CIO

Data Management

ERP

Business Analytics

 QlikWorld 2023 recap: The future is bright for Qlik

What is generative AI? Everything you need to know

Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery,... See complete definition

natural language generation (NLG)

By: TechTarget Contributor

large language model (LLM)



By: Sean Kerner

How to detect AI-generated content



By: Ron Karjian

lemmatization



By: Alexander Gillis

Dig Deeper on AI technologies


Qlik celebrated analytics advancements at QlikWorld 2023, highlighting its acquisition of Talend, the evolution of Qlik Cloud, ...

 Sisense's Orad stepping down, Katz named new CEO

With the embedded analytics specialist recently reaching important financial milestones, the vendor's longtime leader is handing ...

 Knime updates Business Hub to ease data science deployment

The vendor is adding a tool that enables continuous integration and deployment of AI and ML models to help organizations better ...

CIO

 AI policy advisory group talks competition in draft report

The National AI Advisory Committee's first draft report points out how investing in AI research and development can help the U.S....

 ChatGPT use policy up to businesses as regulators struggle

As regulators struggle to keep up with emerging AI tech such as ChatGPT, businesses will be responsible for creating use policies...

 Federal agencies promise action against 'AI-driven harm'

Federal enforcement agencies cracked down on artificial intelligence systems Tuesday, noting that the same consumer protection ...

Data Management

 New Starburst, DBT integration eases data transformation

The integration enables data mesh adopters to work with data from multiple sources without having to move it in and out of a ...

 InfluxData update ups speed, power of time series database

The new version of InfluxDB features an architecture built with Apache Arrow and written in Rust to improve the speed and ...

 IBM acquires Ahana, steward of open source PrestoDB

The purchase not only gives IBM a managed SaaS and AWS marketplace version of the popular open-source Presto database, but ...

ERP

 3D printing has a complex relationship with sustainability

3D printing promises some sustainability benefits, including creating lighter parts and shorter supply chains, but the overall ...

 What adding a decision intelligence platform can do for ERP

Tom Oliver of AI vendor Faculty makes the case for decision intelligence technology as the solution to the data-silo problems of ...

 7 3PL KPIs that can help you evaluate success

Supply chain leaders should look at some particular KPIs to determine whether their company's 3PL provider is meeting their needs...

About Us

Editorial Ethics Policy

Meet The Editors

Contact Us

Advertisers

Partner with Us

Media Kit

Corporate Site

Contributors

Reprints

Answers

Definitions

E-Products

Events

Features

Guides

Opinions


All Rights Reserved, Copyright 2018 - 2023, TechTarget

Privacy Policy 

Cookie Preferences 

Do Not Sell or Share My Personal Information

Close

Photo Stories

Quizzes

Tips

Tutorials

Videos

