


Log in

Sign up



Mari153

707

6

18



user20160

31.1k

3

71

106

Are Mutual Information and Kullback–Leibler divergence equivalent?

Asked 2 years, 7 months ago

Modified 2 years, 7 months ago

Viewed 6k times

13

 

 

From my readings, I understand that:

1. Mutual information (MI) is a metric as it meets the triangle inequality, non-negativity, indiscernability and symmetry criteria.

2. The Kullback–Leibler divergence (DKL) is not a metric as it does not obey the triangle inequality

However, one answer on Cross Validated (Information gain, mutual information and related measures) [the second answer], it was shown that mutual information and Kullback–Leibler divergence are equivalent. How can this be given

that MI is a metric and DKL is not? I can only assume that I am missing something here.

Share

Improve this question

asked Sep 11, 2020 at 3:34

mutual information is not a metric. Variation of information is a metric

– develarist

Sep 11, 2020 at 4:21

@develarist - thanks for this. This is very interesting as going by a Google search, it is clear that mutual information is treated as a metric by some data analysts.

– Mari153

Sep 11, 2020 at 5:51

1 Answer

Sorted by:

16

 

Mutual information is not a metric. A metric d satisfies the identity of indisceribles: d(x,y) =0 if and only if x =y. This is not true of mutual information, which behaves in the opposite manner--zero mutual information implies that

two random variables are independent (as far from identical as you can get). And, if two random variables are identical, they have maximal mutual information (as far from zero as you can get).

You're correct that KL divergence is not a metric. It's not symmetric and doesn't satisfy the triangle inequality.

Mutual information and KL divergence are not equivalent. However, the mutual information I(X,Y) between random variables X and Y is given by the KL divergence between the joint distribution pXY and the product of the marginal

distributions pX ⊗ pY (what the joint distribution would be if X and Y were independent).

I(X,Y) =DKL(pXY ∥ pX ⊗ pY)

Although mutual information is not itself a metric, there are metrics based on it. For example, the variation of information:

VI(X,Y) =H(X,Y)−I(X,Y) =H(X)+H(Y)−2I(X,Y)

where H(X) and H(Y) are the marginal entropies and H(X,Y) is the joint entropy.

Share

Improve this answer

edited Sep 11, 2020 at 5:45

answered Sep 11, 2020 at 4:35

1

This is most interesting as when I type the absolute phrase "mutual information in a metric" into Google, I get something like "about 367,000 results". Some 103 references on Google Scholar also return the use the phrase. So

clearly there is some interpretation that MI is being treated as a metric.

– Mari153

Sep 11, 2020 at 5:46 

4

@MurrayB By "metric", I mean a formal distance metric, satisfying the conditions you mentioned in the question (symmetry, triangle inequality, identity of indiscernibles). However, it's quite common to use the word "metric"

more informally, to describe a way of measuring or quantifying something. I suspect this is probably what's happening in your google results.

– user20160

Sep 11, 2020 at 6:04

Thanks for this. I was assuming the same. The misuse of terms makes understanding maths, statistics and data analysis all the more challenging.

– Mari153

Sep 11, 2020 at 7:09

Your Answer

Ask Question

kullback-leibler

mutual-information

Cite

Follow





Highest score (default)

Cite

Follow






CROSS VALIDATED

Tour

Help

Chat

Contact

Feedback

COMPANY

Stack Overflow

Teams

Advertising

Collectives

Talent

About

Press

Legal

Privacy Policy

Terms of Service

Cookie Settings

Cookie Policy

STACK EXCHANGE NETWORK

Technology

Culture &amp; recreation

Life &amp; arts

Science

Professional

Business

API

Data

Blog

Facebook

Twitter

LinkedIn

Instagram

Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2023.4.21.43403

By clicking “Post Your Answer”, you agree to our terms of service, privacy policy and cookie policy

Not the answer you're looking for? Browse other questions tagged kullback-leibler

mutual-information  or ask your own question.

Linked

39

Information gain, mutual information and related measures

Related

4

Symmetric Kullback-Leibler divergence OR Mutual Information as a metric of distance between two distributions?

7

Feature Selection: Information Gain VS Mutual Information

28

Kullback-Leibler divergence WITHOUT information theory

2

Kullback–Leibler divergence when one measure is a sum of diracs

4

If Mutual Information is KLD(P(X,Y)||P(X)P(Y)), why is KLD(P(X)P(Y)||P(X,Y)) never mentioned/used?

2

Why does mutual information use KL divergence?

2

Why is mutual information symmetric?

Hot Network Questions



"them" vs. "those"



verb "ausmachen" goes to the end



A minor scale definition: am I missing something?



What is scrcpy OTG mode and how does it work?



Elect the Doge of Venice

more hot questions

 Question feed

Post Your Answer

Featured on Meta



New blog post from our CEO Prashanth: Community is the future of AI



Improving the copy in the close modal and post notices - 2023 edition

Your privacy

By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.

 

Accept all cookies

Necessary cookies only

Customize settings

