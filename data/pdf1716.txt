
arXiv:0705.1161v1  [cs.IR]  8 May 2007

IDF Revisited: A Simple New Derivation within the

Robertson-Sp¨arck Jones Probabilistic Model

Lillian Lee

Dept. of Computer Science, Cornell University

Ithaca, NY 14853-7501 USA

http://www.cs.cornell.edu/home/llee

llee@cs.cornell.edu

ABSTRACT

There have been a number of prior attempts to theoretically

justify the eﬀectiveness of the inverse document frequency

(IDF). Those that take as their starting point Robertson

and Sp¨arck Jones’s probabilistic model are based on strong

or complex assumptions. We show that a more intuitively

plausible assumption suﬃces. Moreover, the new assump-

tion, while conceptually very simple, provides a solution to

an estimation problem that had been deemed intractable by

Robertson and Walker (1997).

Categories and Subject Descriptors: H.3.3 [Informa-

tion Search and Retrieval]: Retrieval models

General Terms: Theory, Algorithms

Keywords: inverse document frequency, IDF, probabilistic

model, term weighting

1.

INTRODUCTION

The inverse document frequency (IDF) [12] has been “in-

corporated in (probably) all information retrieval systems”

([6], pg. 77). Attempts to theoretically explain its empirical

successes abound ([2, 14, 1, 11, 5, 8, 4, 3], inter alia). Our

focus here is on explanations based on Robertson and Sp¨arck

Jones’s probabilistic-model (RSJ-PM) paradigm of informa-

tion retrieval [10], not because of any prejudice against other

paradigms, but because a certain RSJ-PM-based justiﬁca-

tion of the IDF in the absence of relevance information has

been promulgated by several inﬂuential authors [2, 9, 7].

RSJ-PM-based accounts use either an assumption due to

Croft and Harper [2] that is mathematically convenient but

not plausible in real settings, or a complex assumption due

to Robertson and Walker [11]. We show that the IDF can be

derived within the RSJ-PM framework via a new assump-

tion that directly instantiates a highly intuitive notion, and

that, while conceptually simple, solves an estimation prob-

lem deemed intractable by Robertson and Walker [11].

2.

CROFT-HARPER DERIVATION

In the (binary-independence version of the) RSJ-PM, the

ith term is assigned weight

log pi(1 − qi)



qi(1 − pi) .

(1)

Copyright is held by the author/owner(s).

SIGIR’07, July 23–27, 2007, Amsterdam, The Netherlands.

ACM 978-1-59593-597-7/07/0007.

where pi

def

= P(Xi = 1 | R = y), qi

def

= P(Xi = 1 | R = n),

Xi is an indicator variable for the presence of the ith term,

and R is a relevance random variable.

Croft and Harper

[2] proposed the use of two assumptions to estimate pi and

qi in the absence of relevance information. CH-1, which is

unobjectionable, simply states that most of the documents

in the corpus are not relevant to the query. This allows us

to set d

qCH

i

def

=

ni



N , where ni is the number of documents in

the corpus that contain the ith term, and N is the num-

ber of documents in the corpus. The second assumption,

CH-2, is that all query terms share the same probability π

of occurring in a relevant document1. Under CH-2, one sets

d

pCH

i

def

= π, and thus (1) becomes

π′ + log N − ni



ni

,

(2)

where π′ = log (π/(1 − π)) is constant (and is 0 if π = .5).

Quantity (2) is essentially the IDF.

CH-2 is an ingenious device for pushing the derivation

above through.

However, intuition suggests that the oc-

currence probability of query terms in relevant documents

should be at least somewhat correlated with their occur-

rence probability in arbitrary documents within the corpus,

and hence not constant. For example, a very frequent term

can be expected to occur in a noticeably large fraction of

any particular subset of the corpus, including the relevant

documents. Contrariwise, a query term might be relatively

infrequent overall due to having a more commonly used syn-

onym; such a term would still occur relatively infrequently

even within the set of (truly) relevant documents.2

3.

ROBERTSON-WALKER DERIVATION

Robertson and Walker (RW) [11] also object to CH-2, on

the grounds that for query terms with very large document

frequencies, weight (2) can be negative. This anomaly, they

show, arises precisely because d

pCH

i

is constant. They then

propose the following alternative:

d

pRW

i

def

=

π



π + (1 − π) N−ni



N

,

where π is the Croft-Harper constant, but reinterpreted as

the estimate for pi just when ni = 0. One can check that

d

pRW

i

∈ [π, 1] slopes up hyperbolically in ni. Applying d

pRW

i



1This can be relaxed to apply to just the query terms in the

document in question.

2Indeed, one study [5] did ﬁnd pi increasing with ni.


and d

qCH

i

to the term-weight scheme (1) yields

π′ + log N



ni

(3)

(which is positive as long as π ≥ .5).

4.

NEW ASSUMPTION

The estimate d

pRW

i

increases monotonically in ni, which is

a desirable property, as we have argued above. However, its

exact functional form does not seem particularly intuitive.

RW motivate it simply as an approximation to a linear form;

approximation is necessary, they claim, because

the straight-line model [i.e., pi linear in qi and

hence ni by CH-1] is actually rather intractable,

and does not lead to a simple weighting formula

([11], pg. 18).3

Despite this claim, we show here that there exists a highly

intuitive linear estimate that leads to a term weight varying

inversely with document frequency.

There are two main principles that motivate our new es-

timate. First, as already stated, any estimate of pi should

be positively correlated with ni. The second and key insight

is that query terms should have a higher occurrence proba-

bility within relevant documents than within the document

collection as a whole. Thus, if the ith term appears in the

query, we should “lift” its estimated occurrence probability

in relevant documents above ni/N, which is its estimated

occurrence probability in general documents. This leads us

to the following intuitive estimate, which is reminiscent of

“add-one smoothing” used in language modeling (more on

this below):

bpi

def

= ni + L



N + L .

(4)

Here the L &gt; 0 in the numerator4 is a “lift” or “boost”

constant.5

Plugging bpi and d

qCH

i

into (1) yields the term

weight

log

„ ni+L



N+L



ni



N

N−ni



N



N−ni



N+L

«

= log

„

1 + L



ni

«

,

which varies inversely in ni, as desired.

Furthermore, as hinted at above, selecting L’s value is

equivalent to selecting bpi’s value for query terms whose doc-

ument frequency is 0. That is, L/(N + L) is directly analo-

gous to π in RW’s derivation. Indeed, choosing L = N is just

like choosing π = 0.5, which is commonly done in presen-

tations of the Croft-Harper derivation in order to eliminate

the leading constant π′ in (2); doing so in our case yields

the following term weight, which is the “usual” form of the

IDF ([13], pg. 184):

log

„

1 + N



ni

«

.



3The fact that RW’s Figure 2 depicts the linear scenario

graphically appears to have led to some mistaken impres-

sions (e.g., [5], pg. 18, coincidentally) that this is the math-

ematical model that RW actually employed.

4The L in the denominator ensures that bpi ≤ 1.

5Since the RSJ-PM document-scoring function only accu-

mulates weights for terms appearing in the query, it is ﬁne

to compute the bpi’s oﬄine, that is, before the query is seen.

Finally, note that bpi is linear in ni; we have thus contradicted

the assertion quoted above that developing a “straight-line”

model is “intractable” [11].

5.

ONWARD AND UPWARD

An interesting direction for future work is to consider lift

functions L(ni) that depend on ni. It can be shown that

diﬀerent choices of L(ni) allow one to model non-linear de-

pendencies of pi on ni that occur in real data, such as the

approximately logarithmic dependence observed in TREC

corpora by Greiﬀ [5]. Importantly, seemingly similar choices

of L(ni) yield strikingly diﬀerent term-weighting schemes;

it would be interesting to empirically compare these new

schemes against the classic IDF.

Acknowledgments. We thank Jon Kleinberg and the anony-

mous reviewers for helpful comments. This paper is based

upon work supported in part by the National Science Foun-

dation under grant no.

IIS-0329064, a Yahoo!

Research

Alliance gift, and an Alfred P. Sloan Research Fellowship.

Any opinions, ﬁndings, and conclusions or recommendations

expressed are those of the author and do not necessarily re-

ﬂect the views or oﬃcial policies, either expressed or implied,

of any sponsoring institutions, the U.S. government, or any

other entity.

6.

REFERENCES

[1] K. W. Church and W. A. Gale. Inverse document frequency

(IDF): A measure of deviations from Poisson. In Proceedings of

the Third Workshop on Very Large Corpora (WVLC), pages

121–130, 1995.

[2] W. B. Croft and D. J. Harper. Using probabilistic models of

document retrieval without relevance information. Journal of

Documentation, 35(4):285–295, 1979. Reprinted in Karen

Sp¨arck Jones and Peter Willett, eds., Readings in Information

Retrieval, Morgan Kaufmann, pp. 339–344, 1997.

[3] A. P. de Vries and T. Roelleke. Relevance information: A loss

of entropy but a gain for idf? In Proceedings of SIGIR, pages

282–289, 2005.

[4] H. Fang, T. Tao, and C. Zhai. A formal study of information

retrieval heuristics. In Proceedings of SIGIR, pages 49–56,

2004.

[5] W. R. Greiﬀ. A theory of term weighting based on exploratory

data analysis. In Proceedings of SIGIR, pages 11–19, New

York, NY, USA, 1998.

[6] D. Harman. The history of IDF and its inﬂuences on IR and

other ﬁelds. In Charting a New Course: Natural Language

Processing and Information Retrieval: Essays in Honour of

Karen Sp¨arck Jones, pages 69–79. Springer, 2005.

[7] C. D. Manning, P. Raghavan, and H. Sch¨utze. Introduction to

Information Retrieval, chapter 11 (Probabilistic information

retrieval). Cambridge University Press, 2007. Draft of April 28.

[8] K. Papineni. Why inverse document frequency? In Proceedings

of the NAACL, pages 1–8, 1995.

[9] S. E. Robertson. Understanding inverse document frequency:

On theoretical arguments for IDF. Journal of Documentation,

60(5):503–520, 2004.

[10] S. E. Robertson and K. Sp¨arck Jones. Relevance weighting of

search terms. Journal of the American Society for

Information Science, 27(3):129–146, 1976.

[11] S. E. Robertson and S. Walker. On relevance weights with little

relevance information. In Proceedings of SIGIR, pages 16–24,

1997.

[12] K. Sp¨arck Jones. A statistical interpretation of term speciﬁcity

and its application in retrieval. Journal of Documentation,

28:11–21, 1972.

[13] I. H. Witten, A. Moﬀat, and T. C. Bell. Managing Gigabytes:

Compressing and Indexing Documents and Images. Morgan

Kaufmann, second edition, 1999.

[14] S. K. M. Wong and Y. Y. Yao. A note on inverse document

frequency weighting scheme [sic]. Technical Report TR-89-990,

Cornell University, Ithaca, NY, USA, 1989.

