
Speech and Language Processing.

Daniel Jurafsky &amp; James H. Martin.

Copyright © 2023.

All

rights reserved.

Draft of January 7, 2023.

CHAPTER

4

Naive Bayes and Sentiment

Classiﬁcation

Classiﬁcation lies at the heart of both human and machine intelligence. Deciding

what letter, word, or image has been presented to our senses, recognizing faces

or voices, sorting mail, assigning grades to homeworks; these are all examples of

assigning a category to an input. The potential challenges of this task are highlighted

by the fabulist Jorge Luis Borges (1964), who imagined classifying animals into:

(a) those that belong to the Emperor, (b) embalmed ones, (c) those that

are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray

dogs, (h) those that are included in this classiﬁcation, (i) those that

tremble as if they were mad, (j) innumerable ones, (k) those drawn with

a very ﬁne camel’s hair brush, (l) others, (m) those that have just broken

a ﬂower vase, (n) those that resemble ﬂies from a distance.

Many language processing tasks involve classiﬁcation, although luckily our classes

are much easier to deﬁne than those of Borges. In this chapter we introduce the naive

Bayes algorithm and apply it to text categorization, the task of assigning a label or

text

categorization

category to an entire text or document.

We focus on one common text categorization task, sentiment analysis, the ex-

sentiment

analysis

traction of sentiment, the positive or negative orientation that a writer expresses

toward some object. A review of a movie, book, or product on the web expresses the

author’s sentiment toward the product, while an editorial or political text expresses

sentiment toward a candidate or political action. Extracting consumer or public sen-

timent is thus relevant for ﬁelds from marketing to politics.

The simplest version of sentiment analysis is a binary classiﬁcation task, and

the words of the review provide excellent cues. Consider, for example, the follow-

ing phrases extracted from positive and negative reviews of movies and restaurants.

Words like great, richly, awesome, and pathetic, and awful and ridiculously are very

informative cues:

+ ...zany characters and richly applied satire, and some great plot twists

− It was pathetic. The worst part about it was the boxing scenes...

+ ...awesome caramel sauce and sweet toasty almonds. I love this place!

− ...awful pizza and ridiculously overpriced...

Spam detection is another important commercial application, the binary clas-

spam detection

siﬁcation task of assigning an email to one of the two classes spam or not-spam.

Many lexical and other features can be used to perform this classiﬁcation. For ex-

ample you might quite reasonably be suspicious of an email containing phrases like

“online pharmaceutical” or “WITHOUT ANY COST” or “Dear Winner”.

Another thing we might want to know about a text is the language it’s written

in. Texts on social media, for example, can be in any number of languages and

we’ll need to apply different processing. The task of language id is thus the ﬁrst

language id

step in most language processing pipelines. Related text classiﬁcation tasks like au-

thorship attribution— determining a text’s author— are also relevant to the digital

authorship

attribution

humanities, social sciences, and forensic linguistics.


2

CHAPTER 4

•

NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

Finally, one of the oldest tasks in text classiﬁcation is assigning a library sub-

ject category or topic label to a text. Deciding whether a research paper concerns

epidemiology or instead, perhaps, embryology, is an important component of infor-

mation retrieval. Various sets of subject categories exist, such as the MeSH (Medical

Subject Headings) thesaurus. In fact, as we will see, subject category classiﬁcation

is the task for which the naive Bayes algorithm was invented in 1961 Maron (1961).

Classiﬁcation is essential for tasks below the level of the document as well.

We’ve already seen period disambiguation (deciding if a period is the end of a sen-

tence or part of a word), and word tokenization (deciding if a character should be

a word boundary). Even language modeling can be viewed as classiﬁcation: each

word can be thought of as a class, and so predicting the next word is classifying the

context-so-far into a class for each next word. A part-of-speech tagger (Chapter 8)

classiﬁes each occurrence of a word in a sentence as, e.g., a noun or a verb.

The goal of classiﬁcation is to take a single observation, extract some useful

features, and thereby classify the observation into one of a set of discrete classes.

One method for classifying text is to use handwritten rules. There are many areas of

language processing where handwritten rule-based classiﬁers constitute a state-of-

the-art system, or at least part of it.

Rules can be fragile, however, as situations or data change over time, and for

some tasks humans aren’t necessarily good at coming up with the rules. Most cases

of classiﬁcation in language processing are instead done via supervised machine

learning, and this will be the subject of the remainder of this chapter. In supervised

supervised

machine

learning

learning, we have a data set of input observations, each associated with some correct

output (a ‘supervision signal’). The goal of the algorithm is to learn how to map

from a new observation to a correct output.

Formally, the task of supervised classiﬁcation is to take an input x and a ﬁxed

set of output classes Y = {y1,y2,...,yM} and return a predicted class y ∈ Y. For

text classiﬁcation, we’ll sometimes talk about c (for “class”) instead of y as our

output variable, and d (for “document”) instead of x as our input variable. In the

supervised situation we have a training set of N documents that have each been hand-

labeled with a class: {(d1,c1),....,(dN,cN)}. Our goal is to learn a classiﬁer that is

capable of mapping from a new document d to its correct class c ∈ C, where C is

some set of useful document classes. A probabilistic classiﬁer additionally will tell

us the probability of the observation being in the class. This full distribution over

the classes can be useful information for downstream decisions; avoiding making

discrete decisions early on can be useful when combining systems.

Many kinds of machine learning algorithms are used to build classiﬁers. This

chapter introduces naive Bayes; the following one introduces logistic regression.

These exemplify two ways of doing classiﬁcation. Generative classiﬁers like naive

Bayes build a model of how a class could generate some input data. Given an ob-

servation, they return the class most likely to have generated the observation. Dis-

criminative classiﬁers like logistic regression instead learn what features from the

input are most useful to discriminate between the different possible classes. While

discriminative systems are often more accurate and hence more commonly used,

generative classiﬁers still have a role.

4.1

Naive Bayes Classiﬁers

In this section we introduce the multinomial naive Bayes classiﬁer, so called be-

naive Bayes

classiﬁer


4.1

•

NAIVE BAYES CLASSIFIERS

3

cause it is a Bayesian classiﬁer that makes a simplifying (naive) assumption about

how the features interact.

The intuition of the classiﬁer is shown in Fig. 4.1. We represent a text document

as if it were a bag of words, that is, an unordered set of words with their position

bag of words

ignored, keeping only their frequency in the document. In the example in the ﬁgure,

instead of representing the word order in all the phrases like “I love this movie” and

“I would recommend it”, we simply note that the word I occurred 5 times in the

entire excerpt, the word it 6 times, the words love, recommend, and movie once, and

so on.

it

it

it

it

it

it

I

I

I

I

I

love

recommend

movie

the

the

the

the

to

to

to

and

and

and

seen

seen

yet

would

with

who

whimsical

while

whenever

times

sweet

several

scenes

satirical

romantic

of

manages

humor

have

happy

fun

friend

fairy

dialogue

but

conventions

areanyone

adventure

always

again

about

I love this movie! It's sweet, 

but with satirical humor. The 

dialogue is great and the 

adventure scenes are fun... 

It manages to be whimsical 

and romantic while laughing 

at the conventions of the 

fairy tale genre. I would 

recommend it to just about 

anyone. I've seen it several 

times, and I'm always happy 

to see it again whenever I 

have a friend who hasn't 

seen it yet!

it 

I

the

to

and

seen

yet

would

whimsical

times

sweet

satirical

adventure

genre

fairy

humor

have

great

…

6 

5

4

3

3

2

1

1

1

1

1

1

1

1

1

1

1

1

…

Figure 4.1

Intuition of the multinomial naive Bayes classiﬁer applied to a movie review. The position of the

words is ignored (the bag-of-words assumption) and we make use of the frequency of each word.

Naive Bayes is a probabilistic classiﬁer, meaning that for a document d, out of

all classes c ∈ C the classiﬁer returns the class ˆc which has the maximum posterior

probability given the document. In Eq. 4.1 we use the hat notation ˆ to mean “our

ˆ

estimate of the correct class”.

ˆc = argmax

c∈C

P(c|d)

(4.1)

This idea of Bayesian inference has been known since the work of Bayes (1763),

Bayesian

inference

and was ﬁrst applied to text classiﬁcation by Mosteller and Wallace (1964). The

intuition of Bayesian classiﬁcation is to use Bayes’ rule to transform Eq. 4.1 into

other probabilities that have some useful properties. Bayes’ rule is presented in

Eq. 4.2; it gives us a way to break down any conditional probability P(x|y) into

three other probabilities:

P(x|y) = P(y|x)P(x)

P(y)

(4.2)

We can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3:

ˆc = argmax

c∈C

P(c|d) = argmax

c∈C

P(d|c)P(c)

P(d)

(4.3)


4

CHAPTER 4

•

NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

We can conveniently simplify Eq. 4.3 by dropping the denominator P(d). This

is possible because we will be computing P(d|c)P(c)

P(d)

for each possible class. But P(d)

doesn’t change for each class; we are always asking about the most likely class for

the same document d, which must have the same probability P(d). Thus, we can

choose the class that maximizes this simpler formula:

ˆc = argmax

c∈C

P(c|d) = argmax

c∈C

P(d|c)P(c)

(4.4)

We call Naive Bayes a generative model because we can read Eq. 4.4 as stating

a kind of implicit assumption about how a document is generated: ﬁrst a class is

sampled from P(c), and then the words are generated by sampling from P(d|c). (In

fact we could imagine generating artiﬁcial documents, or at least their word counts,

by following this process). We’ll say more about this intuition of generative models

in Chapter 5.

To return to classiﬁcation: we compute the most probable class ˆc given some

document d by choosing the class which has the highest product of two probabilities:

the prior probability of the class P(c) and the likelihood of the document P(d|c):

prior

probability

likelihood

ˆc = argmax

c∈C

likelihood

� �� �

P(d|c)

prior

����

P(c)

(4.5)

Without loss of generalization, we can represent a document d as a set of features

f1, f2,..., fn:

ˆc = argmax

c∈C

likelihood

�

��

�

P(f1, f2,...., fn|c)

prior

����

P(c)

(4.6)

Unfortunately, Eq. 4.6 is still too hard to compute directly: without some sim-

plifying assumptions, estimating the probability of every possible combination of

features (for example, every possible set of words and positions) would require huge

numbers of parameters and impossibly large training sets. Naive Bayes classiﬁers

therefore make two simplifying assumptions.

The ﬁrst is the bag-of-words assumption discussed intuitively above: we assume

position doesn’t matter, and that the word “love” has the same effect on classiﬁcation

whether it occurs as the 1st, 20th, or last word in the document. Thus we assume

that the features f1, f2,..., fn only encode word identity and not position.

The second is commonly called the naive Bayes assumption: this is the condi-

naive Bayes

assumption

tional independence assumption that the probabilities P(fi|c) are independent given

the class c and hence can be ‘naively’ multiplied as follows:

P(f1, f2,...., fn|c) = P( f1|c)·P( f2|c)·...·P( fn|c)

(4.7)

The ﬁnal equation for the class chosen by a naive Bayes classiﬁer is thus:

cNB = argmax

c∈C

P(c)

�

f∈F

P(f|c)

(4.8)

To apply the naive Bayes classiﬁer to text, we need to consider word positions, by

simply walking an index through every word position in the document:

positions ← all word positions in test document

cNB = argmax

c∈C

P(c)

�

i∈positions

P(wi|c)

(4.9)


4.2

•

TRAINING THE NAIVE BAYES CLASSIFIER

5

Naive Bayes calculations, like calculations for language modeling, are done in log

space, to avoid underﬂow and increase speed. Thus Eq. 4.9 is generally instead

expressed as

cNB = argmax

c∈C

logP(c)+

�

i∈positions

logP(wi|c)

(4.10)

By considering features in log space, Eq. 4.10 computes the predicted class as a lin-

ear function of input features. Classiﬁers that use a linear combination of the inputs

to make a classiﬁcation decision —like naive Bayes and also logistic regression—

are called linear classiﬁers.

linear

classiﬁers

4.2

Training the Naive Bayes Classiﬁer

How can we learn the probabilities P(c) and P( fi|c)? Let’s ﬁrst consider the maxi-

mum likelihood estimate. We’ll simply use the frequencies in the data. For the class

prior P(c) we ask what percentage of the documents in our training set are in each

class c. Let Nc be the number of documents in our training data with class c and

Ndoc be the total number of documents. Then:

ˆP(c) = Nc

Ndoc

(4.11)

To learn the probability P(fi|c), we’ll assume a feature is just the existence of a word

in the document’s bag of words, and so we’ll want P(wi|c), which we compute as

the fraction of times the word wi appears among all words in all documents of topic

c. We ﬁrst concatenate all documents with category c into one big “category c” text.

Then we use the frequency of wi in this concatenated document to give a maximum

likelihood estimate of the probability:

ˆP(wi|c) =

count(wi,c)

�

w∈V count(w,c)

(4.12)

Here the vocabulary V consists of the union of all the word types in all classes, not

just the words in one class c.

There is a problem, however, with maximum likelihood training. Imagine we

are trying to estimate the likelihood of the word “fantastic” given class positive, but

suppose there are no training documents that both contain the word “fantastic” and

are classiﬁed as positive. Perhaps the word “fantastic” happens to occur (sarcasti-

cally?) in the class negative. In such a case the probability for this feature will be

zero:

ˆP(“fantastic”|positive) = count(“fantastic”,positive)

�

w∈V count(w,positive)

= 0

(4.13)

But since naive Bayes naively multiplies all the feature likelihoods together, zero

probabilities in the likelihood term for any class will cause the probability of the

class to be zero, no matter the other evidence!

The simplest solution is the add-one (Laplace) smoothing introduced in Chap-

ter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing


6

CHAPTER 4

•

NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

algorithms in language modeling, it is commonly used in naive Bayes text catego-

rization:

ˆP(wi|c) =

count(wi,c)+1

�

w∈V (count(w,c)+1) =

count(wi,c)+1

��

w∈V count(w,c)

�

+|V|

(4.14)

Note once again that it is crucial that the vocabulary V consists of the union of all the

word types in all classes, not just the words in one class c (try to convince yourself

why this must be true; see the exercise at the end of the chapter).

What do we do about words that occur in our test data but are not in our vocab-

ulary at all because they did not occur in any training document in any class? The

solution for such unknown words is to ignore them—remove them from the test

unknown word

document and not include any probability for them at all.

Finally, some systems choose to completely ignore another class of words: stop

words, very frequent words like the and a. This can be done by sorting the vocabu-

stop words

lary by frequency in the training set, and deﬁning the top 10–100 vocabulary entries

as stop words, or alternatively by using one of the many predeﬁned stop word lists

available online. Then each instance of these stop words is simply removed from

both training and test documents as if it had never occurred. In most text classiﬁca-

tion applications, however, using a stop word list doesn’t improve performance, and

so it is more common to make use of the entire vocabulary and not use a stop word

list.

Fig. 4.2 shows the ﬁnal algorithm.

function TRAIN NAIVE BAYES(D, C) returns log P(c) and log P(w|c)

for each class c ∈ C

# Calculate P(c) terms

Ndoc = number of documents in D

Nc = number of documents from D in class c

logprior[c]← log Nc

Ndoc

V←vocabulary of D

bigdoc[c]←append(d) for d ∈ D with class c

for each word w in V

# Calculate P(w|c) terms

count(w,c)←# of occurrences of w in bigdoc[c]

loglikelihood[w,c]← log

count(w,c) + 1

�

w′ in V (count (w′,c) + 1)

return logprior, loglikelihood, V

function TEST NAIVE BAYES(testdoc,logprior, loglikelihood, C, V) returns best c

for each class c ∈ C

sum[c]← logprior[c]

for each position i in testdoc

word←testdoc[i]

if word ∈ V

sum[c]←sum[c]+ loglikelihood[word,c]

return argmaxc sum[c]

Figure 4.2

The naive Bayes algorithm, using add-1 smoothing. To use add-α smoothing

instead, change the +1 to +α for loglikelihood counts in training.


4.3

•

WORKED EXAMPLE

7

4.3

Worked example

Let’s walk through an example of training and testing naive Bayes with add-one

smoothing. We’ll use a sentiment analysis domain with the two classes positive

(+) and negative (-), and take the following miniature training and test documents

simpliﬁed from actual movie reviews.

Cat

Documents

Training -

just plain boring

-

entirely predictable and lacks energy

-

no surprises and very few laughs

+

very powerful

+

the most fun ﬁlm of the summer

Test

?

predictable with no fun

The prior P(c) for the two classes is computed via Eq. 4.11 as

Nc

Ndoc :

P(−) = 3

5

P(+) = 2

5

The word with doesn’t occur in the training set, so we drop it completely (as

mentioned above, we don’t use unknown word models for naive Bayes). The like-

lihoods from the training set for the remaining three words “predictable”, “no”, and

“fun”, are as follows, from Eq. 4.14 (computing the probabilities for the remainder

of the words in the training set is left as an exercise for the reader):

P(“predictable”|−) =

1+1

14+20

P(“predictable”|+) = 0+1

9+20

P(“no”|−) =

1+1

14+20

P(“no”|+) = 0+1

9+20

P(“fun”|−) =

0+1

14+20

P(“fun”|+) = 1+1

9+20

For the test sentence S = “predictable with no fun”, after removing the word ‘with’,

the chosen class, via Eq. 4.9, is therefore computed as follows:

P(−)P(S|−) = 3

5 × 2×2×1

343

= 6.1×10−5

P(+)P(S|+) = 2

5 × 1×1×2

293

= 3.2×10−5

The model thus predicts the class negative for the test sentence.

4.4

Optimizing for Sentiment Analysis

While standard naive Bayes text classiﬁcation can work well for sentiment analysis,

some small changes are generally employed that improve performance.

First, for sentiment classiﬁcation and a number of other text classiﬁcation tasks,

whether a word occurs or not seems to matter more than its frequency. Thus it often

improves performance to clip the word counts in each document at 1 (see the end


8

CHAPTER 4

•

NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

of the chapter for pointers to these results). This variant is called binary multino-

mial naive Bayes or binary naive Bayes. The variant uses the same algorithm as

binary naive

Bayes

in Fig. 4.2 except that for each document we remove all duplicate words before con-

catenating them into the single big document during training and we also remove

duplicate words from test documents. Fig. 4.3 shows an example in which a set

of four documents (shortened and text-normalized for this example) are remapped

to binary, with the modiﬁed counts shown in the table on the right. The example

is worked without add-1 smoothing to make the differences clearer. Note that the

results counts need not be 1; the word great has a count of 2 even for binary naive

Bayes, because it appears in multiple documents.

Four original documents:

− it was pathetic the worst part was the

boxing scenes

− no plot twists or great scenes

+ and satire and great plot twists

+ great scenes great ﬁlm

After per-document binarization:

− it was pathetic the worst part boxing

scenes

− no plot twists or great scenes

+ and satire great plot twists

+ great scenes ﬁlm

NB

Binary

Counts

Counts

+

−

+

−

and

2

0

1

0

boxing

0

1

0

1

ﬁlm

1

0

1

0

great

3

1

2

1

it

0

1

0

1

no

0

1

0

1

or

0

1

0

1

part

0

1

0

1

pathetic

0

1

0

1

plot

1

1

1

1

satire

1

0

1

0

scenes

1

2

1

2

the

0

2

0

1

twists

1

1

1

1

was

0

2

0

1

worst

0

1

0

1

Figure 4.3

An example of binarization for the binary naive Bayes algorithm.

A second important addition commonly made when doing text classiﬁcation for

sentiment is to deal with negation. Consider the difference between I really like this

movie (positive) and I didn’t like this movie (negative). The negation expressed by

didn’t completely alters the inferences we draw from the predicate like. Similarly,

negation can modify a negative word to produce a positive review (don’t dismiss this

ﬁlm, doesn’t let us get bored).

A very simple baseline that is commonly used in sentiment analysis to deal with

negation is the following: during text normalization, prepend the preﬁx NOT to

every word after a token of logical negation (n’t, not, no, never) until the next punc-

tuation mark. Thus the phrase

didn’t like this movie , but I

becomes

didn’t NOT_like NOT_this NOT_movie , but I

Newly formed ‘words’ like NOT like, NOT recommend will thus occur more of-

ten in negative document and act as cues for negative sentiment, while words like

NOT bored, NOT dismiss will acquire positive associations. We will return in Chap-

ter 20 to the use of parsing to deal more accurately with the scope relationship be-

tween these negation words and the predicates they modify, but this simple baseline

works quite well in practice.


4.5

•

NAIVE BAYES FOR OTHER TEXT CLASSIFICATION TASKS

9

Finally, in some situations we might have insufﬁcient labeled training data to

train accurate naive Bayes classiﬁers using all words in the training set to estimate

positive and negative sentiment. In such cases we can instead derive the positive

and negative word features from sentiment lexicons, lists of words that are pre-

sentiment

lexicons

annotated with positive or negative sentiment. Four popular lexicons are the General

Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon

General

Inquirer

LIWC

of Hu and Liu (2004) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).

For example the MPQA subjectivity lexicon has 6885 words each marked for

whether it is strongly or weakly biased positive or negative. Some examples:

+ : admirable, beautiful, conﬁdent, dazzling, ecstatic, favor, glee, great

− : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate

A common way to use lexicons in a naive Bayes classiﬁer is to add a feature

that is counted whenever a word from that lexicon occurs. Thus we might add a

feature called ‘this word occurs in the positive lexicon’, and treat all instances of

words in the lexicon as counts for that one feature, instead of counting each word

separately. Similarly, we might add as a second feature ‘this word occurs in the

negative lexicon’ of words in the negative lexicon. If we have lots of training data,

and if the test data matches the training data, using just two features won’t work as

well as using all the words. But when training data is sparse or not representative of

the test set, using dense lexicon features instead of sparse individual-word features

may generalize better.

We’ll return to this use of lexicons in Chapter 25, showing how these lexicons

can be learned automatically, and how they can be applied to many other tasks be-

yond sentiment classiﬁcation.

4.5

Naive Bayes for other text classiﬁcation tasks

In the previous section we pointed out that naive Bayes doesn’t require that our

classiﬁer use all the words in the training data as features. In fact features in naive

Bayes can express any property of the input text we want.

Consider the task of spam detection, deciding if a particular piece of email is

spam detection

an example of spam (unsolicited bulk email)—one of the ﬁrst applications of naive

Bayes to text classiﬁcation (Sahami et al., 1998).

A common solution here, rather than using all the words as individual features,

is to predeﬁne likely sets of words or phrases as features, combined with features

that are not purely linguistic. For example the open-source SpamAssassin tool1

predeﬁnes features like the phrase “one hundred percent guaranteed”, or the feature

mentions millions of dollars, which is a regular expression that matches suspiciously

large sums of money. But it also includes features like HTML has a low ratio of text

to image area, that aren’t purely linguistic and might require some sophisticated

computation, or totally non-linguistic features about, say, the path that the email

took to arrive. More sample SpamAssassin features:

• Email subject line is all capital letters

• Contains phrases of urgency like “urgent reply”

• Email subject line contains “online pharmaceutical”

• HTML has unbalanced “head” tags

1

https://spamassassin.apache.org


10

CHAPTER 4

•

NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

• Claims you can be removed from the list

For other tasks, like language id—determining what language a given piece

language id

of text is written in—the most effective naive Bayes features are not words at all,

but character n-grams, 2-grams (‘zw’) 3-grams (‘nya’, ‘ Vo’), or 4-grams (‘ie z’,

‘thei’), or, even simpler byte n-grams, where instead of using the multibyte Unicode

character representations called codepoints, we just pretend everything is a string of

raw bytes. Because spaces count as a byte, byte n-grams can model statistics about

the beginning or ending of words. A widely used naive Bayes system, langid.py

(Lui and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using fea-

ture selection to winnow down to the most informative 7000 ﬁnal features.

Language ID systems are trained on multilingual text, such as Wikipedia (Wiki-

pedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire.

To make sure that this multilingual text correctly reﬂects different regions, dialects,

and socioeconomic classes, systems also add Twitter text in many languages geo-

tagged to many regions (important for getting world English dialects from countries

with large Anglophone populations like Nigeria or India), Bible and Quran transla-

tions, slang websites like Urban Dictionary, corpora of African American Vernacular

English (Blodgett et al., 2016), and so on (Jurgens et al., 2017).

4.6

Naive Bayes as a Language Model

As we saw in the previous section, naive Bayes classiﬁers can use any sort of fea-

ture: dictionaries, URLs, email addresses, network features, phrases, and so on. But

if, as in the previous section, we use only individual word features, and we use all

of the words in the text (not a subset), then naive Bayes has an important similar-

ity to language modeling. Speciﬁcally, a naive Bayes model can be viewed as a

set of class-speciﬁc unigram language models, in which the model for each class

instantiates a unigram language model.

Since the likelihood features from the naive Bayes model assign a probability to

each word P(word|c), the model also assigns a probability to each sentence:

P(s|c) =

�

i∈positions

P(wi|c)

(4.15)

Thus consider a naive Bayes model with the classes positive (+) and negative (-)

and the following model parameters:

w

P(w|+) P(w|-)

I

0.1

0.2

love 0.1

0.001

this

0.01

0.01

fun

0.05

0.005

ﬁlm 0.1

0.1

...

...

...

Each of the two columns above instantiates a language model that can assign a

probability to the sentence “I love this fun ﬁlm”:

P(“I love this fun ﬁlm”|+) = 0.1×0.1×0.01×0.05×0.1 = 0.0000005

P(“I love this fun ﬁlm”|−) = 0.2×0.001×0.01×0.005×0.1 = .0000000010


4.7

•

EVALUATION: PRECISION, RECALL, F-MEASURE

11

As it happens, the positive model assigns a higher probability to the sentence:

P(s|pos) &gt; P(s|neg). Note that this is just the likelihood part of the naive Bayes

model; once we multiply in the prior a full naive Bayes model might well make a

different classiﬁcation decision.

4.7

Evaluation: Precision, Recall, F-measure

To introduce the methods for evaluating text classiﬁcation, let’s ﬁrst consider some

simple binary detection tasks. For example, in spam detection, our goal is to label

every text as being in the spam category (“positive”) or not in the spam category

(“negative”). For each item (email document) we therefore need to know whether

our system called it spam or not. We also need to know whether the email is actually

spam or not, i.e. the human-deﬁned labels for each document that we are trying to

match. We will refer to these human labels as the gold labels.

gold labels

Or imagine you’re the CEO of the Delicious Pie Company and you need to know

what people are saying about your pies on social media, so you build a system that

detects tweets concerning Delicious Pie. Here the positive class is tweets about

Delicious Pie and the negative class is all other tweets.

In both cases, we need a metric for knowing how well our spam detector (or

pie-tweet-detector) is doing. To evaluate any system for detecting things, we start

by building a confusion matrix like the one shown in Fig. 4.4. A confusion matrix

confusion

matrix

is a table for visualizing how an algorithm performs with respect to the human gold

labels, using two dimensions (system output and gold labels), and each cell labeling

a set of possible outcomes. In the spam detection case, for example, true positives

are documents that are indeed spam (indicated by human-created gold labels) that

our system correctly said were spam. False negatives are documents that are indeed

spam but our system incorrectly labeled as non-spam.

To the bottom right of the table is the equation for accuracy, which asks what

percentage of all the observations (for the spam or pie examples that means all emails

or tweets) our system labeled correctly. Although accuracy might seem a natural

metric, we generally don’t use it for text classiﬁcation tasks. That’s because accuracy

doesn’t work well when the classes are unbalanced (as indeed they are with spam,

which is a large majority of email, or with tweets, which are mainly not about pie).

true positive

false negative

false positive

true negative

gold positive

gold negative

system

positive

system

negative

gold standard labels

system

output

labels

recall = tp

tp+fn

precision = 

tp

tp+fp

accuracy = 

tp+tn

tp+fp+tn+fn

Figure 4.4

A confusion matrix for visualizing how well a binary classiﬁcation system per-

forms against gold standard labels.

To make this more explicit, imagine that we looked at a million tweets, and

let’s say that only 100 of them are discussing their love (or hatred) for our pie,


12

CHAPTER 4

•

NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

while the other 999,900 are tweets about something completely unrelated. Imagine a

simple classiﬁer that stupidly classiﬁed every tweet as “not about pie”. This classiﬁer

would have 999,900 true negatives and only 100 false negatives for an accuracy of

999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should

be happy with this classiﬁer? But of course this fabulous ‘no pie’ classiﬁer would

be completely useless, since it wouldn’t ﬁnd a single one of the customer comments

we are looking for. In other words, accuracy is not a good metric when the goal is

to discover something that is rare, or at least not completely balanced in frequency,

which is a very common situation in the world.

That’s why instead of accuracy we generally turn to two other metrics shown in

Fig. 4.4: precision and recall. Precision measures the percentage of the items that

precision

the system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,

are positive according to the human gold labels). Precision is deﬁned as

Precision =

true positives

true positives + false positives

Recall measures the percentage of items actually present in the input that were

recall

correctly identiﬁed by the system. Recall is deﬁned as

Recall =

true positives

true positives + false negatives

Precision and recall will help solve the problem with the useless “nothing is

pie” classiﬁer. This classiﬁer, despite having a fabulous accuracy of 99.99%, has

a terrible recall of 0 (since there are no true positives, and 100 false negatives, the

recall is 0/100). You should convince yourself that the precision at ﬁnding relevant

tweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize

true positives: ﬁnding the things that we are supposed to be looking for.

There are many ways to deﬁne a single metric that incorporates aspects of both

precision and recall. The simplest of these combinations is the F-measure (van

F-measure

Rijsbergen, 1975) , deﬁned as:

Fβ = (β 2 +1)PR

β 2P+R

The β parameter differentially weights the importance of recall and precision,

based perhaps on the needs of an application. Values of β &gt; 1 favor recall, while

values of β &lt; 1 favor precision. When β = 1, precision and recall are equally bal-

anced; this is the most frequently used metric, and is called Fβ=1 or just F1:

F1

F1 = 2PR

P+R

(4.16)

F-measure comes from a weighted harmonic mean of precision and recall. The

harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-

rocals:

HarmonicMean(a1,a2,a3,a4,...,an) =

n

1

a1 + 1

a2 + 1

a3 +...+ 1

an

(4.17)

and hence F-measure is

F =

1

α 1

P +(1−α) 1

R

or

�

with β 2 = 1−α

α

�

F = (β 2 +1)PR

β 2P+R

(4.18)


4.8

•

TEST SETS AND CROSS-VALIDATION

13

Harmonic mean is used because it is a conservative metric; the harmonic mean of

two values is closer to the minimum of the two values than the arithmetic mean is.

Thus it weighs the lower of the two numbers more heavily.

4.7.1

Evaluating with more than two classes

Up to now we have been describing text classiﬁcation tasks with only two classes.

But lots of classiﬁcation tasks in language processing have more than two classes.

For sentiment analysis we generally have 3 classes (positive, negative, neutral) and

even more classes are common for tasks like part-of-speech tagging, word sense

disambiguation, semantic role labeling, emotion detection, and so on. Luckily the

naive Bayes algorithm is already a multi-class classiﬁcation algorithm.

8

5

10

60

urgent

normal

gold labels

system

output

recallu = 

8

8+5+3

precisionu= 

8

8+10+1

1

50

30

200

spam

urgent

normal

spam

3

recalln = recalls = 

precisionn= 

60

5+60+50

precisions= 

200

3+30+200

60

10+60+30

200

1+50+200

Figure 4.5

Confusion matrix for a three-class categorization task, showing for each pair of

classes (c1,c2), how many documents from c1 were (in)correctly assigned to c2.

But we’ll need to slightly modify our deﬁnitions of precision and recall. Con-

sider the sample confusion matrix for a hypothetical 3-way one-of email catego-

rization decision (urgent, normal, spam) shown in Fig. 4.5. The matrix shows, for

example, that the system mistakenly labeled one spam document as urgent, and we

have shown how to compute a distinct precision and recall value for each class. In

order to derive a single metric that tells us how well the system is doing, we can com-

bine these values in two ways. In macroaveraging, we compute the performance

macroaveraging

for each class, and then average over classes. In microaveraging, we collect the de-

microaveraging

cisions for all classes into a single confusion matrix, and then compute precision and

recall from that table. Fig. 4.6 shows the confusion matrix for each class separately,

and shows the computation of microaveraged and macroaveraged precision.

As the ﬁgure shows, a microaverage is dominated by the more frequent class (in

this case spam), since the counts are pooled. The macroaverage better reﬂects the

statistics of the smaller classes, and so is more appropriate when performance on all

the classes is equally important.

4.8

Test sets and Cross-validation

The training and testing procedure for text classiﬁcation follows what we saw with

language modeling (Section ??): we use the training set to train the model, then use

the development test set (also called a devset) to perhaps tune some parameters,

development

test set

devset


14

CHAPTER 4

•

NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

8

8

11

340

true

urgent

true

not

system

urgent

system

not

60

40

55

212

true

normal

true

not

system

normal

system

not

200

51

33

83

true

spam

true

not

system

spam

system

not

268

99

99

635

true

yes

true

no

system

yes

system

no

precision = 8+11

8

= .42

precision = 200+33

200

= .86

precision = 60+55

60

= .52

microaverage

precision

268+99

268

= .73

=

macroaverage

precision

3

.42+.52+.86

= .60

=

Pooled

Class 3: Spam

Class 2: Normal

Class 1: Urgent

Figure 4.6

Separate confusion matrices for the 3 classes from the previous ﬁgure, showing the pooled confu-

sion matrix and the microaveraged and macroaveraged precision.

and in general decide what the best model is. Once we come up with what we think

is the best model, we run it on the (hitherto unseen) test set to report its performance.

While the use of a devset avoids overﬁtting the test set, having a ﬁxed train-

ing set, devset, and test set creates another problem: in order to save lots of data

for training, the test set (or devset) might not be large enough to be representative.

Wouldn’t it be better if we could somehow use all our data for training and still use

all our data for test? We can do this by cross-validation.

cross-validation

In cross-validation, we choose a number k, and partition our data into k disjoint

subsets called folds. Now we choose one of those k folds as a test set, train our

folds

classiﬁer on the remaining k − 1 folds, and then compute the error rate on the test

set. Then we repeat with another fold as the test set, again training on the other k−1

folds. We do this sampling process k times and average the test set error rate from

these k runs to get an average error rate. If we choose k = 10, we would train 10

different models (each on 90% of our data), test the model 10 times, and average

these 10 values. This is called 10-fold cross-validation.

10-fold

cross-validation

The only problem with cross-validation is that because all the data is used for

testing, we need the whole corpus to be blind; we can’t examine any of the data

to suggest possible features and in general see what’s going on, because we’d be

peeking at the test set, and such cheating would cause us to overestimate the perfor-

mance of our system. However, looking at the corpus to understand what’s going

on is important in designing NLP systems! What to do? For this reason, it is com-

mon to create a ﬁxed training set and test set, then do 10-fold cross-validation inside

the training set, but compute error rate the normal way in the test set, as shown in

Fig. 4.7.

4.9

Statistical Signiﬁcance Testing

In building systems we often need to compare the performance of two systems. How

can we know if the new system we just built is better than our old one? Or better

than some other system described in the literature? This is the domain of statistical

hypothesis testing, and in this section we introduce tests for statistical signiﬁcance

for NLP classiﬁers, drawing especially on the work of Dror et al. (2020) and Berg-

Kirkpatrick et al. (2012).

Suppose we’re comparing the performance of classiﬁers A and B on a metric M


4.9

•

STATISTICAL SIGNIFICANCE TESTING

15

Training Iterations

1

3

4

5

2

6

7

8

9

10

Dev

Dev

Dev

Dev

Dev

Dev

Dev

Dev

Dev

Dev

Training

Training

Training

Training

Training

Training

Training

Training

Training

Training

Training

Test 

Set

Testing

Figure 4.7

10-fold cross-validation

such as F1, or accuracy. Perhaps we want to know if our logistic regression senti-

ment classiﬁer A (Chapter 5) gets a higher F1 score than our naive Bayes sentiment

classiﬁer B on a particular test set x. Let’s call M(A,x) the score that system A gets

on test set x, and δ(x) the performance difference between A and B on x:

δ(x) = M(A,x)−M(B,x)

(4.19)

We would like to know if δ(x) &gt; 0, meaning that our logistic regression classiﬁer

has a higher F1 than our naive Bayes classiﬁer on X. δ(x) is called the effect size;

effect size

a bigger δ means that A seems to be way better than B; a small δ means A seems to

be only a little better.

Why don’t we just check if δ(x) is positive? Suppose we do, and we ﬁnd that

the F1 score of A is higher than B’s by .04. Can we be certain that A is better? We

cannot! That’s because A might just be accidentally better than B on this particular x.

We need something more: we want to know if A’s superiority over B is likely to hold

again if we checked another test set x′, or under some other set of circumstances.

In the paradigm of statistical hypothesis testing, we test this by formalizing two

hypotheses.

H0 : δ(x) ≤ 0

H1 : δ(x) &gt; 0

(4.20)

The hypothesis H0, called the null hypothesis, supposes that δ(x) is actually nega-

null hypothesis

tive or zero, meaning that A is not better than B. We would like to know if we can

conﬁdently rule out this hypothesis, and instead support H1, that A is better.

We do this by creating a random variable X ranging over all test sets. Now we

ask how likely is it, if the null hypothesis H0 was correct, that among these test sets

we would encounter the value of δ(x) that we found. We formalize this likelihood

as the p-value: the probability, assuming the null hypothesis H0 is true, of seeing

p-value

the δ(x) that we saw or one even greater

P(δ(X) ≥ δ(x)|H0 is true)

(4.21)

So in our example, this p-value is the probability that we would see δ(x) assuming

A is not better than B. If δ(x) is huge (let’s say A has a very respectable F1 of .9

and B has a terrible F1 of only .2 on x), we might be surprised, since that would be

extremely unlikely to occur if H0 were in fact true, and so the p-value would be low


16

CHAPTER 4

•

NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

(unlikely to have such a large δ if A is in fact not better than B). But if δ(x) is very

small, it might be less surprising to us even if H0 were true and A is not really better

than B, and so the p-value would be higher.

A very small p-value means that the difference we observed is very unlikely

under the null hypothesis, and we can reject the null hypothesis. What counts as very

small? It is common to use values like .05 or .01 as the thresholds. A value of .01

means that if the p-value (the probability of observing the δ we saw assuming H0 is

true) is less than .01, we reject the null hypothesis and assume that A is indeed better

than B. We say that a result (e.g., “A is better than B”) is statistically signiﬁcant if

statistically

signiﬁcant

the δ we saw has a probability that is below the threshold and we therefore reject

this null hypothesis.

How do we compute this probability we need for the p-value? In NLP we gen-

erally don’t use simple parametric tests like t-tests or ANOVAs that you might be

familiar with. Parametric tests make assumptions about the distributions of the test

statistic (such as normality) that don’t generally hold in our cases. So in NLP we

usually use non-parametric tests based on sampling: we artiﬁcially create many ver-

sions of the experimental setup. For example, if we had lots of different test sets x′

we could just measure all the δ(x′) for all the x′. That gives us a distribution. Now

we set a threshold (like .01) and if we see in this distribution that 99% or more of

those deltas are smaller than the delta we observed, i.e., that p-value(x)—the proba-

bility of seeing a δ(x) as big as the one we saw—is less than .01, then we can reject

the null hypothesis and agree that δ(x) was a sufﬁciently surprising difference and

A is really a better algorithm than B.

There are two common non-parametric tests used in NLP: approximate ran-

domization (Noreen, 1989) and the bootstrap test. We will describe bootstrap

approximate

randomization

below, showing the paired version of the test, which again is most common in NLP.

Paired tests are those in which we compare two sets of observations that are aligned:

paired

each observation in one set can be paired with an observation in another. This hap-

pens naturally when we are comparing the performance of two systems on the same

test set; we can pair the performance of system A on an individual observation xi

with the performance of system B on the same xi.

4.9.1

The Paired Bootstrap Test

The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from pre-

bootstrap test

cision, recall, or F1 to the BLEU metric used in machine translation. The word

bootstrapping refers to repeatedly drawing large numbers of samples with replace-

bootstrapping

ment (called bootstrap samples) from an original set. The intuition of the bootstrap

test is that we can create many virtual test sets from an observed test set by repeat-

edly sampling from it. The method only makes the assumption that the sample is

representative of the population.

Consider a tiny text classiﬁcation example with a test set x of 10 documents. The

ﬁrst row of Fig. 4.8 shows the results of two classiﬁers (A and B) on this test set,

with each document labeled by one of the four possibilities: (A and B both right,

both wrong, A right and B wrong, A wrong and B right); a slash through a letter

(B) means that that classiﬁer got the answer wrong. On the ﬁrst document both A

and B get the correct class (AB), while on the second document A got it right but B

got it wrong (AB). If we assume for simplicity that our metric is accuracy, A has an

accuracy of .70 and B of .50, so δ(x) is .20.

Now we create a large number b (perhaps 105) of virtual test sets x(i), each of size

n = 10. Fig. 4.8 shows a couple of examples. To create each virtual test set x(i), we


4.9

•

STATISTICAL SIGNIFICANCE TESTING

17

repeatedly (n = 10 times) select a cell from row x with replacement. For example, to

create the ﬁrst cell of the ﬁrst virtual test set x(1), if we happened to randomly select

the second cell of the x row; we would copy the value AB into our new cell, and

move on to create the second cell of x(1), each time sampling (randomly choosing)

from the original x with replacement.

1

2

3

4

5

6

7

8

9

10 A% B% δ()

x

AB AB AB �

�

AB AB �

�

AB AB AB �

�

AB AB .70

.50

.20

x(1)

AB AB AB �

�

AB �

�

AB AB �

�

AB AB �

�

AB AB .60

.60

.00

x(2)

AB AB �

�

AB �

�

AB �

�

AB AB �

�

AB AB AB AB .60

.70 -.10

...

x(b)

Figure 4.8

The paired bootstrap test: Examples of b pseudo test sets x(i) being created

from an initial true test set x. Each pseudo test set is created by sampling n = 10 times with

replacement; thus an individual sample is a single cell, a document with its gold label and

the correct or incorrect performance of classiﬁers A and B. Of course real test sets don’t have

only 10 examples, and b needs to be large as well.

Now that we have the b test sets, providing a sampling distribution, we can do

statistics on how often A has an accidental advantage. There are various ways to

compute this advantage; here we follow the version laid out in Berg-Kirkpatrick

et al. (2012). Assuming H0 (A isn’t better than B), we would expect that δ(X), esti-

mated over many test sets, would be zero; a much higher value would be surprising,

since H0 speciﬁcally assumes A isn’t better than B. To measure exactly how surpris-

ing our observed δ(x) is, we would in other circumstances compute the p-value by

counting over many test sets how often δ(x(i)) exceeds the expected zero value by

δ(x) or more:

p-value(x) = 1

b

b

�

i=1

1

�

δ(x(i))−δ(x) ≥ 0

�

(We use the notation 1(x) to mean “1 if x is true, and 0 otherwise”.) However,

although it’s generally true that the expected value of δ(X) over many test sets,

(again assuming A isn’t better than B) is 0, this isn’t true for the bootstrapped test

sets we created. That’s because we didn’t draw these samples from a distribution

with 0 mean; we happened to create them from the original test set x, which happens

to be biased (by .20) in favor of A. So to measure how surprising is our observed

δ(x), we actually compute the p-value by counting over many test sets how often

δ(x(i)) exceeds the expected value of δ(x) by δ(x) or more:

p-value(x) = 1

b

b

�

i=1

1

�

δ(x(i))−δ(x) ≥ δ(x)

�

= 1

b

b

�

i=1

1

�

δ(x(i)) ≥ 2δ(x)

�

(4.22)

So if for example we have 10,000 test sets x(i) and a threshold of .01, and in only

47 of the test sets do we ﬁnd that δ(x(i)) ≥ 2δ(x), the resulting p-value of .0047 is

smaller than .01, indicating δ(x) is indeed sufﬁciently surprising, and we can reject

the null hypothesis and conclude A is better than B.


18

CHAPTER 4

•

NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

function BOOTSTRAP(test set x, num of samples b) returns p-value(x)

Calculate δ(x) # how much better does algorithm A do than B on x

s = 0

for i = 1 to b do

for j = 1 to n do

# Draw a bootstrap sample x(i) of size n

Select a member of x at random and add it to x(i)

Calculate δ(x(i))

# how much better does algorithm A do than B on x(i)

s←s + 1 if δ(x(i)) ≥ 2δ(x)

p-value(x) ≈ s

b

# on what % of the b samples did algorithm A beat expectations?

return p-value(x)

# if very few did, our observed δ is probably not accidental

Figure 4.9

A version of the paired bootstrap algorithm after Berg-Kirkpatrick et al. (2012).

The full algorithm for the bootstrap is shown in Fig. 4.9. It is given a test set x, a

number of samples b, and counts the percentage of the b bootstrap test sets in which

δ(x∗(i)) &gt; 2δ(x). This percentage then acts as a one-sided empirical p-value

4.10

Avoiding Harms in Classiﬁcation

It is important to avoid harms that may result from classiﬁers, harms that exist both

for naive Bayes classiﬁers and for the other classiﬁcation algorithms we introduce

in later chapters.

One class of harms is representational harms (Crawford 2017, Blodgett et al.

representational

harms

2020), harms caused by a system that demeans a social group, for example by per-

petuating negative stereotypes about them. For example Kiritchenko and Moham-

mad (2018) examined the performance of 200 sentiment analysis systems on pairs of

sentences that were identical except for containing either a common African Amer-

ican ﬁrst name (like Shaniqua) or a common European American ﬁrst name (like

Stephanie), chosen from the Caliskan et al. (2017) study discussed in Chapter 6.

They found that most systems assigned lower sentiment and more negative emotion

to sentences with African American names, reﬂecting and perpetuating stereotypes

that associate African Americans with negative emotions (Popp et al., 2003).

In other tasks classiﬁers may lead to both representational harms and other

harms, such as censorship. For example the important text classiﬁcation task of

toxicity detection is the task of detecting hate speech, abuse, harassment, or other

toxicity

detection

kinds of toxic language. While the goal of such classiﬁers is to help reduce soci-

etal harm, toxicity classiﬁers can themselves cause harms. For example, researchers

have shown that some widely used toxicity classiﬁers incorrectly ﬂag as being toxic

sentences that are non-toxic but simply mention minority identities like women

(Park et al., 2018), blind people (Hutchinson et al., 2020) or gay people (Dixon

et al., 2018), or simply use linguistic features characteristic of varieties like African-

American Vernacular English (Sap et al. 2019, Davidson et al. 2019). Such false

positive errors, if employed by toxicity detection systems without human oversight,

could lead to the censoring of discourse by or about these groups.

These model problems can be caused by biases or other problems in the training

data; in general, machine learning systems replicate and even amplify the biases

in their training data. But these problems can also be caused by the labels (for


4.11

•

SUMMARY

19

example due to biases in the human labelers), by the resources used (like lexicons,

or model components like pretrained embeddings), or even by model architecture

(like what the model is trained to optimize). While the mitigation of these biases

(for example by carefully considering the training data sources) is an important area

of research, we currently don’t have general solutions. For this reason it’s important,

when introducing any NLP model, to study these kinds of factors and make them

clear. One way to do this is by releasing a model card (Mitchell et al., 2019) for

model card

each version of a model. A model card documents a machine learning model with

information like:

• training algorithms and parameters

• training data sources, motivation, and preprocessing

• evaluation data sources, motivation, and preprocessing

• intended use and users

• model performance across different demographic or other groups and envi-

ronmental situations

4.11

Summary

This chapter introduced the naive Bayes model for classiﬁcation and applied it to

the text categorization task of sentiment analysis.

• Many language processing tasks can be viewed as tasks of classiﬁcation.

• Text categorization, in which an entire text is assigned a class from a ﬁnite set,

includes such tasks as sentiment analysis, spam detection, language identi-

ﬁcation, and authorship attribution.

• Sentiment analysis classiﬁes a text as reﬂecting the positive or negative orien-

tation (sentiment) that a writer expresses toward some object.

• Naive Bayes is a generative model that makes the bag-of-words assumption

(position doesn’t matter) and the conditional independence assumption (words

are conditionally independent of each other given the class)

• Naive Bayes with binarized features seems to work better for many text clas-

siﬁcation tasks.

• Classiﬁers are evaluated based on precision and recall.

• Classiﬁers are trained using distinct training, dev, and test sets, including the

use of cross-validation in the training set.

• Statistical signiﬁcance tests should be used to determine whether we can be

conﬁdent that one version of a classiﬁer is better than another.

• Designers of classiﬁers should carefully consider harms that may be caused

by the model, including its training data and other components, and report

model characteristics in a model card.

Bibliographical and Historical Notes

Multinomial naive Bayes text classiﬁcation was proposed by Maron (1961) at the

RAND Corporation for the task of assigning subject categories to journal abstracts.

His model introduced most of the features of the modern form presented here, ap-

proximating the classiﬁcation task with one-of categorization, and implementing

add-δ smoothing and information-based feature selection.


20

CHAPTER 4

•

NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

The conditional independence assumptions of naive Bayes and the idea of Bayes-

ian analysis of text seems to have arisen multiple times. The same year as Maron’s

paper, Minsky (1961) proposed a naive Bayes classiﬁer for vision and other arti-

ﬁcial intelligence problems, and Bayesian techniques were also applied to the text

classiﬁcation task of authorship attribution by Mosteller and Wallace (1963). It had

long been known that Alexander Hamilton, John Jay, and James Madison wrote

the anonymously-published Federalist papers in 1787–1788 to persuade New York

to ratify the United States Constitution. Yet although some of the 85 essays were

clearly attributable to one author or another, the authorship of 12 were in dispute

between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian

probabilistic model of the writing of Hamilton and another model on the writings

of Madison, then computed the maximum-likelihood author for each of the disputed

essays. Naive Bayes was ﬁrst applied to spam detection in Heckerman et al. (1998).

Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show

that using boolean attributes with multinomial naive Bayes works better than full

counts. Binary multinomial naive Bayes is sometimes confused with another variant

of naive Bayes that also uses a binary representation of whether a term occurs in

a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead

estimates P(w|c) as the fraction of documents that contain a term, and includes a

probability for whether a term is not in a document. McCallum and Nigam (1998)

and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive

Bayes doesn’t work as well as the multinomial algorithm for sentiment or other text

tasks.

There are a variety of sources covering the many kinds of text classiﬁcation

tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).

Stamatatos (2009) surveys authorship attribute algorithms. On language identiﬁca-

tion see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural

system. The task of newswire indexing was often used as a test case for text classi-

ﬁcation algorithms, based on the Reuters-21578 collection of newswire articles.

See Manning et al. (2008) and Aggarwal and Zhai (2012) on text classiﬁcation;

classiﬁcation in general is covered in machine learning textbooks (Hastie et al. 2001,

Witten and Frank 2005, Bishop 2006, Murphy 2012).

Non-parametric methods for computing statistical signiﬁcance were used ﬁrst in

NLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech

recognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the

bootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work

has focused on issues including multiple test sets and multiple metrics (Søgaard et al.

2014, Dror et al. 2017).

Feature selection is a method of removing features that are unlikely to generalize

well. Features are generally ranked by how informative they are about the classiﬁca-

tion decision. A very common metric, information gain, tells us how many bits of

information

gain

information the presence of the word gives us for guessing the class. Other feature

selection metrics include χ2, pointwise mutual information, and GINI index; see

Yang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an

introduction to feature selection.

Exercises

4.1

Assume the following likelihoods for each word being part of a positive or

negative movie review, and equal prior probabilities for each class.


EXERCISES

21

pos

neg

I

0.09 0.16

always 0.07 0.06

like

0.29 0.06

foreign 0.04 0.15

ﬁlms

0.08 0.11

What class will Naive bayes assign to the sentence “I always like foreign

ﬁlms.”?

4.2

Given the following short movie reviews, each labeled with a genre, either

comedy or action:

1. fun, couple, love, love

comedy

2. fast, furious, shoot

action

3. couple, ﬂy, fast, fun, fun

comedy

4. furious, shoot, shoot, fun

action

5. ﬂy, fast, shoot, love

action

and a new document D:

fast, couple, shoot, ﬂy

compute the most likely class for D. Assume a naive Bayes classiﬁer and use

add-1 smoothing for the likelihoods.

4.3

Train two models, multinomial naive Bayes and binarized naive Bayes, both

with add-1 smoothing, on the following document counts for key sentiment

words, with positive or negative class assigned as noted.

doc “good” “poor” “great” (class)

d1. 3

0

3

pos

d2. 0

1

2

pos

d3. 1

3

0

neg

d4. 1

5

2

neg

d5. 0

2

0

neg

Use both naive Bayes models to assign a class (pos or neg) to this sentence:

A good, good plot and great characters, but poor acting.

Recall from page 6 that with naive Bayes text classiﬁcation, we simply ignore

(throw out) any word that never occurred in the training document. (We don’t

throw out words that appear in some classes but not others; that’s what add-

one smoothing is for.) Do the two models agree or disagree?


22

Chapter 4

•

Naive Bayes, Text Classiﬁcation, and Sentiment

Aggarwal, C. C. and C. Zhai. 2012. A survey of text classiﬁ-

cation algorithms. In C. C. Aggarwal and C. Zhai, editors,

Mining text data, pages 163–222. Springer.

Bayes, T. 1763. An Essay Toward Solving a Problem in the

Doctrine of Chances, volume 53. Reprinted in Facsimiles

of Two Papers by Bayes, Hafner Publishing, 1963.

Berg-Kirkpatrick, T., D. Burkett, and D. Klein. 2012. An

empirical investigation of statistical signiﬁcance in NLP.

EMNLP.

Bisani, M. and H. Ney. 2004. Bootstrap estimates for conﬁ-

dence intervals in ASR performance evaluation. ICASSP.

Bishop, C. M. 2006. Pattern recognition and machine learn-

ing. Springer.

Blodgett, S. L., S. Barocas, H. Daum´e III, and H. Wallach.

2020. Language (technology) is power: A critical survey

of “bias” in NLP. ACL.

Blodgett, S. L., L. Green, and B. O’Connor. 2016. Demo-

graphic dialectal variation in social media: A case study

of African-American English. EMNLP.

Borges, J. L. 1964. The analytical language of John Wilkins.

University of Texas Press. Trans. Ruth L. C. Simms.

Caliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman-

tics derived automatically from language corpora contain

human-like biases. Science, 356(6334):183–186.

Chinchor, N., L. Hirschman, and D. L. Lewis. 1993. Eval-

uating Message Understanding systems: An analysis of

the third Message Understanding Conference. Computa-

tional Linguistics, 19(3):409–449.

Crawford, K. 2017.

The trouble with bias.

Keynote at

NeurIPS.

Davidson, T., D. Bhattacharya, and I. Weber. 2019. Racial

bias in hate speech and abusive language detection

datasets. Third Workshop on Abusive Language Online.

Dixon, L., J. Li, J. Sorensen, N. Thain, and L. Vasserman.

2018. Measuring and mitigating unintended bias in text

classiﬁcation. 2018 AAAI/ACM Conference on AI, Ethics,

and Society.

Dror, R., G. Baumer, M. Bogomolov, and R. Reichart. 2017.

Replicability analysis for natural language processing:

Testing signiﬁcance with multiple datasets. TACL, 5:471–

–486.

Dror, R., L. Peled-Cohen, S. Shlomov, and R. Reichart.

2020. Statistical Signiﬁcance Testing for Natural Lan-

guage Processing, volume 45 of Synthesis Lectures on

Human Language Technologies. Morgan &amp; Claypool.

Efron, B. and R. J. Tibshirani. 1993. An introduction to the

bootstrap. CRC press.

Gillick, L. and S. J. Cox. 1989. Some statistical issues in the

comparison of speech recognition algorithms. ICASSP.

Guyon, I. and A. Elisseeff. 2003. An introduction to variable

and feature selection. JMLR, 3:1157–1182.

Hastie, T., R. J. Tibshirani, and J. H. Friedman. 2001. The

Elements of Statistical Learning. Springer.

Heckerman, D., E. Horvitz, M. Sahami, and S. T. Dumais.

1998. A bayesian approach to ﬁltering junk e-mail. AAAI-

98 Workshop on Learning for Text Categorization.

Hu, M. and B. Liu. 2004. Mining and summarizing customer

reviews. KDD.

Hutchinson, B., V. Prabhakaran, E. Denton, K. Webster,

Y. Zhong, and S. Denuyl. 2020. Social biases in NLP

models as barriers for persons with disabilities. ACL.

Jaech, A., G. Mulcaire, S. Hathi, M. Ostendorf, and N. A.

Smith. 2016. Hierarchical character-word models for lan-

guage identiﬁcation. ACL Workshop on NLP for Social

Media.

Jauhiainen, T., M. Lui, M. Zampieri, T. Baldwin, and

K. Lind´en. 2019. Automatic language identiﬁcation in

texts: A survey. JAIR, 65(1):675–682.

Jurgens, D., Y. Tsvetkov, and D. Jurafsky. 2017. Incorpo-

rating dialectal variability for socially equitable language

identiﬁcation. ACL.

Kiritchenko, S. and S. M. Mohammad. 2018. Examining

gender and race bias in two hundred sentiment analysis

systems. *SEM.

Liu, B. and L. Zhang. 2012. A survey of opinion mining

and sentiment analysis. In C. C. Aggarwal and C. Zhai,

editors, Mining text data, pages 415–464. Springer.

Lui, M. and T. Baldwin. 2011. Cross-domain feature selec-

tion for language identiﬁcation. IJCNLP.

Lui, M. and T. Baldwin. 2012. langid.py: An off-the-shelf

language identiﬁcation tool. ACL.

Manning, C. D., P. Raghavan, and H. Sch¨utze. 2008. Intro-

duction to Information Retrieval. Cambridge.

Maron, M. E. 1961. Automatic indexing: an experimental

inquiry. Journal of the ACM, 8(3):404–417.

McCallum, A. and K. Nigam. 1998. A comparison of event

models for naive bayes text classiﬁcation. AAAI/ICML-98

Workshop on Learning for Text Categorization.

Metsis, V., I. Androutsopoulos, and G. Paliouras. 2006.

Spam ﬁltering with naive bayes-which naive bayes?

CEAS.

Minsky, M. 1961. Steps toward artiﬁcial intelligence. Pro-

ceedings of the IRE, 49(1):8–30.

Mitchell, M., S. Wu, A. Zaldivar, P. Barnes, L. Vasserman,

B. Hutchinson, E. Spitzer, I. D. Raji, and T. Gebru. 2019.

Model cards for model reporting. ACM FAccT.

Mosteller, F. and D. L. Wallace. 1963. Inference in an au-

thorship problem: A comparative study of discrimination

methods applied to the authorship of the disputed feder-

alist papers. Journal of the American Statistical Associa-

tion, 58(302):275–309.

Mosteller, F. and D. L. Wallace. 1964. Inference and Dis-

puted Authorship: The Federalist. Springer-Verlag. 1984

2nd edition: Applied Bayesian and Classical Inference.

Murphy, K. P. 2012. Machine learning: A probabilistic per-

spective. MIT Press.

Noreen, E. W. 1989. Computer Intensive Methods for Testing

Hypothesis. Wiley.

Pang, B. and L. Lee. 2008. Opinion mining and sentiment

analysis. Foundations and trends in information retrieval,

2(1-2):1–135.

Pang, B., L. Lee, and S. Vaithyanathan. 2002.

Thumbs

up? Sentiment classiﬁcation using machine learning tech-

niques. EMNLP.

Park, J. H., J. Shin, and P. Fung. 2018. Reducing gender bias

in abusive language detection. EMNLP.


Exercises

23

Pennebaker, J. W., R. J. Booth, and M. E. Francis. 2007.

Linguistic Inquiry and Word Count: LIWC 2007. Austin,

TX.

Popp, D., R. A. Donovan, M. Crawford, K. L. Marsh, and

M. Peele. 2003. Gender, race, and speech style stereo-

types. Sex Roles, 48(7-8):317–325.

Sahami, M., S. T. Dumais, D. Heckerman, and E. Horvitz.

1998. A Bayesian approach to ﬁltering junk e-mail. AAAI

Workshop on Learning for Text Categorization.

Sap, M., D. Card, S. Gabriel, Y. Choi, and N. A. Smith. 2019.

The risk of racial bias in hate speech detection. ACL.

Søgaard, A., A. Johannsen, B. Plank, D. Hovy, and H. M.

Alonso. 2014. What’s in a p-value in NLP? CoNLL.

Stamatatos, E. 2009. A survey of modern authorship attribu-

tion methods. JASIST, 60(3):538–556.

Stone, P., D. Dunphry, M. Smith, and D. Ogilvie. 1966.

The General Inquirer: A Computer Approach to Content

Analysis. MIT Press.

van Rijsbergen, C. J. 1975. Information Retrieval. Butter-

worths.

Wang, S. and C. D. Manning. 2012. Baselines and bigrams:

Simple, good sentiment and topic classiﬁcation. ACL.

Wilson, T., J. Wiebe, and P. Hoffmann. 2005.

Recogniz-

ing contextual polarity in phrase-level sentiment analysis.

EMNLP.

Witten, I. H. and E. Frank. 2005.

Data Mining: Practi-

cal Machine Learning Tools and Techniques, 2nd edition.

Morgan Kaufmann.

Yang, Y. and J. Pedersen. 1997.

A comparative study on

feature selection in text categorization. ICML.

