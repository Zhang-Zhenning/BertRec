
Additive smoothing

Toggle the table of contents



 1

language

Article

Talk

Tools From Wikipedia, the free encyclopedia

For the image processing technique, see Laplacian smoothing.

In statistics, additive smoothing, also called Laplace smoothing[1] or Lidstone smoothing, is a technique used to smooth

categorical data. Given a set of observation counts 

 from a -dimensional multinomial distribution with  trials, a

"smoothed" version of the counts gives the estimator:

where the smoothed count 

 and the "pseudocount" α &gt; 0 is a smoothing parameter. α = 0 corresponds to no smoothing. (This

parameter is explained in § Pseudocount below.) Additive smoothing is a type of shrinkage estimator, as the resulting estimate will be

between the empirical probability (relative frequency) 

, and the uniform probability 

. Invoking Laplace's rule of succession, some

authors have argued[citation needed] that α should be 1 (in which case the term add-one smoothing[2][3] is also

used)[further explanation needed], though in practice a smaller value is typically chosen.

From a Bayesian point of view, this corresponds to the expected value of the posterior distribution, using a symmetric Dirichlet

distribution with parameter α as a prior distribution. In the special case where the number of categories is 2, this is equivalent to using

a Beta distribution as the conjugate prior for the parameters of Binomial distribution.

History [edit]

Laplace came up with this smoothing technique when he tried to estimate the chance that the sun will rise tomorrow. His rationale was

that even given a large sample of days with the rising sun, we still can not be completely sure that the sun will still rise tomorrow (known

as the sunrise problem).[4]

Pseudocount [edit]

A pseudocount is an amount (not generally an integer, despite its name) added to the number of observed cases in order to change

the expected probability in a model of those data, when not known to be zero. It is so named because, roughly speaking, a pseudo-

count of value  weighs into the posterior distribution similarly to each category having an additional count of . If the frequency of each

item  is  out of  samples, the empirical probability of event  is

but the posterior probability when additively smoothed is

as if to increase each count  by  a priori.

Depending on the prior knowledge, which is sometimes a subjective value, a pseudocount may have any non-negative finite value. It

may only be zero (or the possibility ignored) if impossible by definition, such as the possibility of a decimal digit of pi being a letter, or a

physical possibility that would be rejected and so not counted, such as a computer printing a letter when a valid program for pi is run,

or excluded and not counted because of no interest, such as if only interested in the zeros and ones. Generally, there is also a

possibility that no value may be computable or observable in a finite time (see the halting problem). But at least one possibility must

have a non-zero pseudocount, otherwise no prediction could be computed before the first observation. The relative values of

pseudocounts represent the relative prior expected probabilities of their possibilities. The sum of the pseudocounts, which may be very

large, represents the estimated weight of the prior knowledge compared with all the actual observations (one for each) when

determining the expected probability.

In any observed data set or sample there is the possibility, especially with low-probability events and with small data sets, of a possible

event not occurring. Its observed frequency is therefore zero, apparently implying a probability of zero. This oversimplification is

inaccurate and often unhelpful, particularly in probability-based machine learning techniques such as artificial neural networks and

hidden Markov models. By artificially adjusting the probability of rare (but not impossible) events so those probabilities are not exactly








zero, zero-frequency problems are avoided. Also see Cromwell's rule.

The simplest approach is to add one to each observed number of events including the zero-count possibilities. This is sometimes

called Laplace's Rule of Succession. This approach is equivalent to assuming a uniform prior distribution over the probabilities for

each possible event (spanning the simplex where each probability is between 0 and 1, and they all sum to 1).

Using the Jeffreys prior approach, a pseudocount of one half should be added to each possible outcome.

Pseudocounts should be set to one only when there is no prior knowledge at all — see the principle of indifference. However, given

appropriate prior knowledge, the sum should be adjusted in proportion to the expectation that the prior probabilities should be

considered correct, despite evidence to the contrary — see further analysis. Higher values are appropriate inasmuch as there is prior

knowledge of the true values (for a mint condition coin, say); lower values inasmuch as there is prior knowledge that there is probable

bias, but of unknown degree (for a bent coin, say).

A more complex approach is to estimate the probability of the events from other factors and adjust accordingly.

Examples [edit]

One way to motivate pseudocounts, particularly for binomial data, is via a formula for the midpoint of an interval estimate, particularly a

binomial proportion confidence interval. The best-known is due to Edwin Bidwell Wilson, in Wilson (1927): the midpoint of the Wilson

score interval corresponding to  standard deviations on either side is:

Taking 

 standard deviations to approximate a 95% confidence interval (

) yields pseudocount of 2 for each outcome, so 4 in

total, colloquially known as the "plus four rule":

This is also the midpoint of the Agresti–Coull interval, (Agresti &amp; Coull 1998).

Generalized to the case of known incidence rates [edit]

Often you are testing the bias of an unknown trial population against a control population with known parameters (incidence rates) 

. In this case the uniform probability  should be replaced by the known incidence rate of the control population  to

calculate the smoothed estimator :

As a consistency check, if the empirical estimator happens to equal the incidence rate, i.e. 

, the smoothed estimator is

independent of  and also equals the incidence rate.

Applications [edit]

Classification [edit]

Additive smoothing is commonly a component of naive Bayes classifiers.

Statistical language modelling [edit]

In a bag of words model of natural language processing and information retrieval, the data consists of the number of occurrences of

each word in a document. Additive smoothing allows the assignment of non-zero probabilities to words which do not occur in the

sample. Recent studies have proven that additive smoothing is more effective than other probability smoothing methods in several

retrieval tasks such as language-model-based pseudo-relevance feedback and recommender systems.[5][6]

See also [edit]

Bayesian average

Prediction by partial matching

Categorical distribution

References [edit]

1. ^ C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to Information Retrieval. Cambridge University Press, p. 260.

2. ^ Jurafsky, Daniel; Martin, James H. (June 2008). Speech and Language Processing (2nd ed.). Prentice Hall. p. 132. ISBN 978-0-13-

187321-6.

3. ^ Russell, Stuart; Norvig, Peter (2010). Artificial Intelligence: A Modern Approach (2nd ed.). Pearson Education, Inc. p. 863.

4. ^ Lecture 5 | Machine Learning (Stanford)  at 1h10m into the lecture

5. ^ Hazimeh, Hussein; Zhai, ChengXiang. "Axiomatic Analysis of Smoothing Methods in Language Models for Pseudo-Relevance

Feedback" . ICTIR '15 Proceedings of the 2015 International Conference on the Theory of Information Retrieval.


Privacy policy About Wikipedia Disclaimers

Contact Wikipedia Mobile view Developers

Statistics

Cookie statement





This page was last edited on 24 January 2023, at 09:01 (UTC).

Text is available under the Creative Commons Attribution-ShareAlike License 3.0; additional terms may apply. By using this site, you agree to the Terms of

Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

6. ^ Valcarce, Daniel; Parapar, Javier; Barreiro, Álvaro. "Additive Smoothing for Relevance-Based Language Modelling of Recommender

Systems" . CERI '16 Proceedings of the 4th Spanish Conference on Information Retrieval.

Sources [edit]

Wilson, E. B. (1927). "Probable inference, the law of succession, and statistical inference". Journal of the American Statistical Association. 22

(158): 209–212. doi:10.1080/01621459.1927.10502953 . JSTOR 2276774 .

Agresti, Alan; Coull, Brent A. (1998). "Approximate is better than 'exact' for interval estimation of binomial proportions". The American

Statistician. 52 (2): 119–126. doi:10.2307/2685469 . JSTOR 2685469 . MR 1628435 .

External links [edit]

SF Chen, J Goodman (1996). "



An empirical study of smoothing techniques for language modeling

". Proceedings of the 34th

annual meeting on Association for Computational Linguistics.

Pseudocounts

Bayesian interpretation of pseudocount regularizers

Categories: Statistical natural language processing

Categorical data

Probability theory



