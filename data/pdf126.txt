
I529: Machine Learning in Bioinformatics (Spring 2018)

Expectation-Maximization (EM) 

algorithm

Yuzhen Ye

School of Informatics and Computing

Indiana University, Bloomington

Spring 2018


Contents

§ Introduce EM algorithm using the flipping coin 

experiment

§ Formal definition of the EM algorithm

§ Two toy examples

– Coin toss with missing data

– Coin toss with hidden data

§ Applications of the EM algorithm

– Motif finding

– Baum-Welch algorithm

– Binning of metagenomes




A coin-flipping experiment

Ref: What is the expectation maximization algorithm? 

Nature Biotechnology 26, 897 - 899 (2008)

θ: the probability of getting heads

θA: the probability of coin A landing on head

θB: the probability of coin B landing on head




When the identities of the coins are unknown



Instead of picking up the single best guess, 

the EM algorithm computes probabilities for 

each possible completion of the missing 

data, using the current parameters  

E(H) for coin B


Main applications of the EM algorithm

§ When the data indeed has missing values, due 

to problems with or limitations of the observation 

process

§ When optimizing the likelihood function is 

analytically intractable but it can be simplified by 

assuming the existence of and values for 

additional but missing (or hidden) parameters. 


The EM algorithm handles hidden data�

Consider a model where, for observed data x and model 

parameters θ:

p(x|θ)=∑z p(x,z|θ).

z  is the �hidden” variable that is marginalized out

Finding θ* which maximizes ∑z p(x,z|θ) is hard!

The EM algorithm reduces the difficult task of optimizing log 

P(x; θ) into a sequence of simpler optimization subproblems.

In each iteration, The EM algorithm receives parameters θ(t), and 

returns new parameters θ(t+1), s.t. p(x|θ(t+1)) &gt; p(x|θ(t)). 


In each iteration the EM algorithm does the following. 

E step: Calculate 

M step: Find

which maximizes the Q function

(Next iteration sets  θ(t) ¬

and repeats).

The EM update rule:

The EM algorithm�

The EM Algorithm

Yuzhen Ye

March 1, 2013

EM algorithm

date rule:

ˆ✓(t+1) = arg max

✓

X

z

P(z|x; ˆ✓(t))logP(x, z; ✓)

hat EM update rule maximizes the log-likelihood of a dataset exp

ble completions of unobserved variables z, where each completio

ˆ( )

Yuzhen Ye

March 1, 2013

The EM algorithm

e EM update rule:

ˆ✓(t+1) = arg max

✓

X

z

P(z|x; ˆ✓(t))logP(x, z; ✓)

see the that EM update rule maximizes the log-likelihood of a da

n all possible completions of unobserved variables z, where each

the posterior probability, P(z|x; ˆ✓(t)).

Convergence of the EM algorithm

1 demonstrates the convergence of the EM algorithm Starting

g

Yuzhen Ye

March 1, 2013

1

The EM algorithm

The EM update rule:

ˆ✓(t+1) = arg max

✓

X

z

P(z|x; ˆ✓(t))l

We see the that EM update rule maximizes the log-likeli

tain all possible completions of unobserved variables z,

by the posterior probability, P(z|x; ˆ✓(t)).

2

Convergence of the EM algorithm

he EM algorithm

update rule:

ˆ✓(t+1) = arg max

✓

X

z

P(z|x; ˆ✓(t))logP(x, z; ✓)

unction is,

Qt(✓) =

X

z

P(z|x; ˆ✓(t))logP(x, z; ✓)

he that EM update rule maximizes the log-likelihood of a dataset expand

ossible completions of unobserved variables z, where each completion i

osterior probability, P(z|x; ˆ✓(t)).

nvergence of the EM algorithm

monstrates the convergence of the EM algorithm. Starting from initial p

E-step of the EM algorithm constructs a function gt that lower-bounds

tion logP(x; ✓) (i.e., gt  logP(x; ✓); and gt(ˆ✓(t)) = logP(x; ˆ✓(t)). In t

computed as the maximum of gt. In the next E-step, a new lower-bou


Templates for equations

Yuzhen Ye

March 1, 2013

on for slides

gt(✓) =

X

z

P(z|x; ˆ✓(t))log P(x, z; ✓)

P(z|x; ˆ✓(t))

(1)

can be speciﬁed as ✓ = (S, , A, b),

S2, · · · , SM}, a set of states

 2, · · · , M}, a set of associated phylogenetic models

(1  j, k  M), a matrix of state-transition probabilities

· , bM), a vector of state-initial probabilities

itional probability of visiting state k at some site i given that state l is

� 1. bj is the probability that state j is visited ﬁrst.

Yuzhen Ye

March 1, 2013

1

The EM algorithm

The EM update rule:

ˆ✓(t+1) = arg max

✓

X

z

P(z|x; ˆ✓(t))logP(x, z; ✓)

(1)

We see the that EM update rule maximizes the log-likelihood of a dataset expanded to con-

tain all possible completions of unobserved variables z, where each completion is weighted

by the posterior probability, P(z|x; ˆ✓(t)).

2

Convergence of the EM algorithm

Fig 1 demonstrates the convergence of the EM algorithm. Starting from initial parameters

✓(t), the E-step of the EM algorithm constructs a function gt that lower-bounds the objec-

tive function logP(x; ✓) (i.e., gt  logP(x; ✓); and gt(ˆ✓(t)) = logP(x; ˆ✓(t)). In the M-step,

✓(t+1) is computed as the maximum of gt. In the next E-step, a new lower-bound gt+1 is

constructed; maximization of gt+1 in the next M-step gives ✓(t+2), etc.

As the value of the lower-bound gt matches the objective function at ˆ✓(t), if follows that

logP(x; ˆ✓(t)) = gt(ˆ✓(t))  gt(ˆ✓(t+1) = logP(x; ˆ✓(t+1))

(2)

So the objective function monotonically increases during each iteration of expectation

maximization!

3

Choose the function that lower-bound the objective func-

tion



Convergence of the 

EM algorithm

Fig. 1

algorithm

rule:

ˆ✓(t+1) = arg max

✓

X

z

P(z|x; ˆ✓(t))logP(x, z; ✓)

(1)

s,

Qt(✓) =

X

z

P(z|x; ˆ✓(t))logP(x, z; ✓)

(2)

EM update rule maximizes the log-likelihood of a dataset expanded to con-

ompletions of unobserved variables z, where each completion is weighted

probability, P(z|x; ˆ✓(t)).

ence of the EM algorithm

es the convergence of the EM algorithm. Starting from initial parameters

f the EM algorithm constructs a function gt that lower-bounds the objec-

P(x; ✓) (i.e., gt  logP(x; ✓); and gt(ˆ✓(t)) = logP(x; ˆ✓(t)). In the M-step,

d as the maximum of gt. In the next E-step, a new lower-bound gt+1 is

imization of gt+1 in the next M-step gives ✓(t+2), etc.

of the lower-bound gt matches the objective function at ˆ✓(t), if follows that

logP(x; ˆ✓(t)) = gt(ˆ✓(t))  gt(ˆ✓(t+1) = logP(x; ˆ✓(t+1))

(3)

Compare the Q function and the g function

Ref: Nature Biotechnology 26, 897 - 899 (2008)


EM algorithm

ate rule:

ˆ✓(t+1) = arg max

✓

X

z

P(z|x; ˆ✓(t))logP(x, z; ✓)

at EM update rule maximizes the log-likelihood of a dataset expa

ble completions of unobserved variables z, where each completion

or probability, P(z|x; ˆ✓(t)).

ergence of the EM algorithm

trates the convergence of the EM algorithm. Starting from initia

p of the EM algorithm constructs a function gt that lower-boun

logP(x; ✓) (i.e., gt  logP(x; ✓); and gt(ˆ✓(t)) = logP(x; ˆ✓(t)). In

puted as the maximum of gt

In the next E-step a new lower-b

The EM update rule

The EM update rule maximizes the log likelihood of a 

dataset expanded to contain all possible completions 

of the unobserved variables, where each completion 

is weighted by the posterior probability!


Coin toss with missing data�

• Given a coin with two possible outcomes: H (head) and T 

(tail), with probabilities θ and 1-θ, respectively.

• The coin is tossed twice, but only the 1st outcome, T, is 

seen. So the data is x = (T,*) (with incomplete data!)

• We wish to apply the EM algorithm to get parameters that 

increase the likelihood of the data. 

• Let the initial parameters be θ = ¼.






The EM algorithm at work



X

z

=

P(z1|x; ✓t)logP(x, z1; ✓) + P(z2|x; ✓t)logP(x, z2; ✓)

=

P(z1|x; ✓t)logP(z1; ✓) + P(z2|x; ✓t)logP(z2; ✓)

=

P(z1|x; ✓t)log[✓nH(z1) ⇥ (1 � ✓)nT (z1)] + P(z2|x; ✓t)lo

=

P(z1|x; ✓t)[nH(z1)log✓ + nT (z1)log(1 � ✓)] + P(z2|x

=

[P(z1|x; ✓t)nH(z1) + P(z2|x; ✓t)nH(z2)]log✓ + [P(z1

where nH(z1) is the number of heads in z1 (i.e., T, T), and so o

nHlog✓ + nT log(1 � ✓) is maximized when ✓ =

nH

nH+nT (the M

If we denote P(z1|x; ✓t)nH(z1) + P(z2|x; ✓t)nH(z2) as nH

P(z2|x; ✓t)nT (z2) as nT , then we have our solution!

Now let’s calculate nH and nT .

P(x; ✓) = P(z1; ✓) + P(z2; ✓) = (1 � ✓)2 + (1 � ✓

Qt(✓t)

=

X

z

P(z|x; ✓t)logP(x, z; ✓)

=

P(z1|x; ✓t)logP(x, z1; ✓) + P(z2|x; ✓t)logP(x, z2; ✓)

=

P(z1|x; ✓t)logP(z1; ✓) + P(z2|x; ✓t)logP(z2; ✓)

=

P(z1|x; ✓t)log[✓nH(z1) ⇥ (1 � ✓)nT (z1)] + P(z2|x; ✓t)log[✓nH(z2) ⇥ (1 � ✓)nT (z2)]

=

P(z1|x; ✓t)[nH(z1)log✓ + nT (z1)log(1 � ✓)] + P(z2|x; ✓t)[nH(z2)log✓ + nT (z2)log(

=

[P(z1|x; ✓t)nH(z1) + P(z2|x; ✓t)nH(z2)]log✓ + [P(z1|x; ✓t)nT (z1) + P(z2|x; ✓t)nT (

where nH(z1) is the number of heads in z1 (i.e., T, T), and so on.

nHlog✓ + nT log(1 � ✓) is maximized when ✓ =

nH

nH+nT (the MLE)!

If we denote P(z1|x; ✓t)nH(z1) + P(z2|x; ✓t)nH(z2) as nH and P(z1|x; ✓t)nT (z1) +

P(z2|x; ✓t)nT (z2) as nT , then we have our solution!

Now let’s calculate nH and nT .

P(x; ✓) = P(z1; ✓) + P(z2; ✓) = (1 � ✓)2 + (1 � ✓)✓ = 3/4

(8)

P(z1|x; ✓) = P(x, z1; ✓)/P(x; ✓) = (1 � ✓)2/P(x; ✓) = 3/4 ⇥ 3/4

3/4

= 3/4

(9)

P(z2|x; ✓) = 1 � P(z1|x; ✓) = 1/4

(10)

And we have nH(z1) = 0, nT (z1) = 2, nH(z2) = 1, and nT (z2) = 1.

So we have,

nH = 3/4 ⇥ 0 + 1/4 ⇥ 1 = 1/4

nT = 3/4 ⇥ 2 + 1/4 ⇥ 1 = 7/8

✓ =

nH

nH+nT =

1/4

1/4+7/8 = 1/8.

Assume we have an initial guess of the ✓ = 1/4.

Remember the EM update rule is Qt(✓) = P

z P(z|x; ✓t)logP(x, z; ✓), where ✓t is given

parameters for current iteration (in this case, ✓t = 1/4).

Qt(✓t)

=

X

z

P(z|x; ✓t)logP(x, z; ✓)

=

P(z1|x; ✓t)logP(x, z1; ✓) + P(z2|x; ✓t)logP(x, z2; ✓)

=

P(z1|x; ✓t)logP(z1; ✓) + P(z2|x; ✓t)logP(z2; ✓)

=

P(z1|x; ✓t)log[✓nH(z1) ⇥ (1 � ✓)nT (z1)] + P(z2|x; ✓t)log[✓nH(z2) ⇥ (1 � ✓)nT (z2)]

=

P(z1|x; ✓t)[nH(z1)log✓ + nT (z1)log(1 � ✓)] + P(z2|x; ✓t)[nH(z2)log✓ + nT (z2)log(1 � ✓)]

=

[P(z1|x; ✓t)nH(z1) + P(z2|x; ✓t)nH(z2)]log✓ + [P(z1|x; ✓t)nT (z1) + P(z2|x; ✓t)nT (z2)]log(1 � ✓)

(7)

where nH(z1) is the number of heads in z1 (i.e., T, T), and so on.

nHlog✓ + nT log(1 � ✓) is maximized when ✓ =

nH

nH+nT (the MLE)!

If we deﬁne,

nH = P(z1|x; ✓t)nH(z1) + P(z2|x; ✓t)nH(z2)

(8)

nT = P(z1|x; ✓t)nT (z1) + P(z2|x; ✓t)nT (z2)

(9)

we have our solution!

Now let’s calculate nH and nT .

P(x; ✓) = P(z1; ✓) + P(z2; ✓) = (1 � ✓)2 + (1 � ✓)✓ = 3/4

(10)

P(z1|x; ✓) = P(x, z1; ✓)/P(x; ✓) = (1 � ✓)2/P(x; ✓) = 3/4 ⇥ 3/4

3/4

= 3/4

(11)

P(z2|x; ✓) = 1 � P(z1|x; ✓) = 1/4

(12)

And we have nH(z1) = 0, nT (z1) = 2, nH(z2) = 1, and nT (z2) = 1.

So we have,

nH = 1/4 ⇥ 1 = 1/4, nT = 3/4 ⇥ 2 + 1/4 ⇥ 1 = 7/8, ✓ =

nH

nH+nT =

1/4

1/4+7/8 = 1/8

Remember the EM update rule is Qt(✓) = P

z P(z|x; ✓t)logP(x, z; ✓), wher

parameters for current iteration (in this case, ✓t = 1/4).

Qt(✓t)

=

X

z

P(z|x; ✓t)logP(x, z; ✓)

=

P(z1|x; ✓t)logP(x, z1; ✓) + P(z2|x; ✓t)logP(x, z2; ✓)

=

P(z1|x; ✓t)logP(z1; ✓) + P(z2|x; ✓t)logP(z2; ✓)

=

P(z1|x; ✓t)log[✓nH(z1) ⇥ (1 � ✓)nT (z1)] + P(z2|x; ✓t)log[✓nH(z2) ⇥ (1

=

P(z1|x; ✓t)[nH(z1)log✓ + nT (z1)log(1 � ✓)] + P(z2|x; ✓t)[nH(z2)lo

=

[P(z1|x; ✓t)nH(z1) + P(z2|x; ✓t)nH(z2)]log✓ + [P(z1|x; ✓t)nT (z1) +

where nH(z1) is the number of heads in z1 (i.e., T, T), and so on.

nHlog✓ + nT log(1 � ✓) is maximized when ✓ =

nH

nH+nT (the MLE)!

If we deﬁne,

nH = P(z1|x; ✓t)nH(z1) + P(z2|x; ✓t)nH(z2)

nT = P(z1|x; ✓t)nT (z1) + P(z2|x; ✓t)nT (z2)

we have our solution!

Now let’s calculate nH and nT .

P(x; ✓) = P(z1; ✓) + P(z2; ✓) = (1 � ✓)2 + (1 � ✓)✓ = 3/4

P(z1|x; ✓) = P(x, z1; ✓)/P(x; ✓) = (1 � ✓)2/P(x; ✓) = 3/4 ⇥ 3/4

3/4

= 3/

P(z2|x; ✓) = 1 � P(z1|x; ✓) = 1/4

And we have nH(z1) = 0, nT (z1) = 2, nH(z2) = 1, and nT (z2) = 1.

So we have,

nH = 1/4 ⇥ 1 = 1/4, nT = 3/4 ⇥ 2 + 1/4 ⇥ 1 = 7/8, ✓ =

nH

nH+nT =

1/4

1/4+7/8 = 1

=

P(z1|x; ✓ )logP(x, z1; ✓) + P(z2|x; ✓ )logP(x, z2; ✓)

=

P(z1|x; ✓t)logP(z1; ✓) + P(z2|x; ✓t)logP(z2; ✓)

=

P(z1|x; ✓t)log[✓nH(z1) ⇥ (1 � ✓)nT (z1)] + P(z2|x; ✓t)log[✓nH(z2) ⇥ (1 � ✓)nT (z2)]

=

P(z1|x; ✓t)[nH(z1)log✓ + nT (z1)log(1 � ✓)] + P(z2|x; ✓t)[nH(z2)log✓ + nT (z2)log(1 � ✓

=

[P(z1|x; ✓t)nH(z1) + P(z2|x; ✓t)nH(z2)]log✓ + [P(z1|x; ✓t)nT (z1) + P(z2|x; ✓t)nT (z2)]l

where nH(z1) is the number of heads in z1 (i.e., T, T), and so on.

nHlog✓ + nT log(1 � ✓) is maximized when ✓ =

nH

nH+nT (the MLE)!

If we deﬁne,

nH = P(z1|x; ✓t)nH(z1) + P(z2|x; ✓t)nH(z2)

(8)

nT = P(z1|x; ✓t)nT (z1) + P(z2|x; ✓t)nT (z2)

(9)

we have our solution!

Now let’s calculate nH and nT .

P(x; ✓t) = P(z1; ✓t) + P(z2; ✓t) = (1 � ✓t)2 + (1 � ✓t)✓=3/4

(10)

P(z1|x; ✓) = P(x, z1; ✓t)/P(x; ✓t) = (1 � ✓t)2/P(x; ✓t) = 3/4 ⇥ 3/4

3/4

= 3/4

(11)

P(z2|x; ✓t) = 1 � P(z1|x; ✓t) = 1/4

(12)

And we have nH(z1) = 0, nT (z1) = 2, nH(z2) = 1, and nT (z2) = 1.

So we have,

nH = 1/4 ⇥ 1 = 1/4, nT = 3/4 ⇥ 2 + 1/4 ⇥ 1 = 7/8, ✓ =

nH

nH+nT =

1/4

1/4+7/8 = 1/8

Inputs:

Observation: x=(T,*)

Hidden data: z1=(T,T)  z2=(T,H)

Initial guess: θ t = ¼ 


The EM algorithm at work: continue�

§ Initial guess θ = ¼

§ After one iteration θ = 1/8

§ …

§ The optimal parameter θ will never be reached by the 

EM algorithm!�


Coin toss with hidden data

E.g., initial parameter θ: θA=0.60, θB=0.50

(x1, x2,..x5 are independent observations) 

Two coins A and B, with parameters θ={θA, θB}; compute θ that maximizes 

the log likelihood of the observed data x={x1,x2,..x5} 

3/4

P(z2|x; ✓t) = 1 � P(z1|x; ✓t) = 1/4

(12)

nd we have nH(z1) = 0, nT (z1) = 2, nH(z2) = 1, and nT (z2) = 1.

we have,

1/4 ⇥ 1 = 1/4, nT = 3/4 ⇥ 2 + 1/4 ⇥ 1 = 7/8, ✓ =

nH

nH+nT =

1/4

1/4+7/8 = 1/8

Two-coin experiment with hidden data

P(z1 = A|x; ✓t)

=

P(z1 = A|x1; ✓t)

=

P(z1 = A, x1; ✓t)

P(z1 = A, x1; ✓t) + P(z1 = B, x1; ✓t)

=

0.65 ⇥ 0.45

0.65 ⇥ 0.45 + 0.55 ⇥ 0.55 = 0.58

(13)

4

observation

nH

nT

P(A)

P(B)

nH

nT

nH

nT

x1: HTTTHHTHTH

5

5

0.58

0.42

2.9

2.9

2.1

2.1

x2: HHHHTHHHHH

9

1

0.84

0.16

7.6

0.8

1.4

0.2

x3: HTHHHHHTHH

8

2

0.81

0.19

6.4

1.6

1.6

0.4

x4: HTHTTTHHTT

4

6

0.25

0.75

1.0

1.5

3.0

4.5

x5: THHHTHHHTH

8

2

0.81

0.19

6.4

1.6

1.6

0.4

24.3H   8.4T    9.7H    7.6T     

Coin A               B

New parameter: θA=24.3/(24.3+8.4)=0.74, θB=9.7/(9.7+7.6)=0.56 


Motif finding problem

§ Motif finding problem is not that different from 

the coin toss problem!

§ Probabilistic approaches to motif finding  

– EM

– Gibbs sampling (a generalized EM algorithm)

§ There are also combinatorial approaches


Motif finding problem

§ Given a set of DNA sequences:

cctgatagacgctatctggctatccacgtacgtaggtcctctgtgcgaatctatgcgtttccaaccat

agtactggtgtacatttgatacgtacgtacaccggcaacctgaaacaaacgctcagaaccagaagtgc

aaacgtacgtgcaccctctttcttcgtggctctggccaacgagggctgatgtataagacgaaaatttt

agcctccgatgtaagtcatagctgtaactattacctgccacccctattacatcttacgtacgtataca

ctgttatacaacgcgtcatggcggggtatgcgttttggtcgtcgtacgctcgatcgttaacgtacgtc

§ Find the motif in each of the individual sequences


The MEME algorithm

§ Collect all substrings with the same length w

from the input sequences: X = (X1,…,Xn)

§ Treat sequences as bags of subsequences: a 

bag for motif, and a bag for background

§ Need to figure out two models (one for motif, 

and one for the background), and assign each of 

the subsequences to one of the bags, such that 

the likelihood of the data (subsequences) is 

maximized

– Difficult problem

– Solved by the EM algorithm


0.3x       0.7x

Motif finding vs coin toss



tagacgctatc

gctatccacgt

gtaggtcctct

M

M

M

B

B

B

0.7x       0.3x

0.2x       0.8x

B

M

Motif

Background model

θ: the probability of getting heads

θA: P(head) for coin A

θB: P(head) for coin B

Probability of an observation sequence: 

P(x|θ)=θ#(heads)(1-θ) #(tails)



Probability of a subsequence:

P(x|M), or P(x|B)


Fitting a mixture model by EM 

§ A finite mixture model:

– data X = (X1,…,Xn) arises from two or more 

groups with g models q = (q1, …,  qg).

§ Indicator vectors Z = (Z1,…,Zn), where Zi = 

(Zi1,…,Zig), and Zij = 1 if Xi is from group j, 

and = 0 otherwise. 

§ P(Zij= 1|qj) = lj . For any given i, all Zij   are 0 

except one j;

§ g=2: class 1 (the motif) and class 2 (the 

background) are given by position specific and a 

general multinomial distribution


The E- and M-step

§ E-step:  Since the log likelihood is the sum of 

over i and j of terms multiplying Zij,  and these 

are independent across i, we need only consider 

the expectation of one such, given Xi.  Using 

initial parameter values q’ and  l’, and the fact 

that the Zij are binary, we get

E(Zij |X,q’,l’)=l’jP(Xi|q’j)/ ∑k l’kP(Xi|q’k)=Z’ij

§ M-step: The maximization over l is independent 

of the rest and is readily achieved with 

lj’’ =  ∑iZ’ij / n.


Baum-Welch algorithm for HMM 

parameter estimation

Akl =

1

p(x j )

j =1

n

∑

p(si −1=k ,si =l ,x j |θ)

i =1

L

∑

Akl =

1

p(x j )

j =1

n

∑

f k

j (i −1)akl el (xi )bl

j (i )

i =1

L

∑

Ek (b) =

1

p(x j )

j =1

n

∑

 

f k

j (i ) f k

j (i )

i:xi

j =b

∑

During each iteration, compute the expected transitions between any pair of 

states, and expected emissions from any state, using averaging process (E-

step), which are then used to compute new parameters (M-step).


Application of EM algorithms in 

metagenomics: Binning

§ AbundanceBin

– Binning of short reads into bins (species)

– A Novel Abundance-Based Algorithm for Binning 

Metagenomic Sequences Using l-tuples ( JCB 

2011, 18(3): 523-534. RECOMB 2010)

§ MaxBin/MaxBin2

– Binning of assembled metagenomic scaffolds 

using an EM algorithm (Microbiome, 2014 doi: 

10.1186/2049-2618-2-26)


Pros and Cons

§ Cons

– Slow convergence

– Converge to local optima

§ Pros

– The E-step and M-step are often easy to implement 

for many problems, thanks to the nice form of the 

complete-data likelihood function

– Solutions to the M-steps often exist in the closed form 

§ Ref

–

On the convergence properties of the EM algorithm. CFJ WU, 1983

–

A gentle tutorial of the EM algorithm and its applications to parameter estimation 

for Gaussian mixture and hidden Markov models, JA Bilmes, 1998

–

What is the expectation maximization algorithm? 2008

