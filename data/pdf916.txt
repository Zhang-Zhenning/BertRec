
Published in

Walmart Global Tech Blog



Nov 10, 2020

·

14 min read

The Journey of Open AI GPT models

Photo Credit : Image by Free-Photos from Pixabay

Improving Language Understanding by Generative Pre-training (GPT-1):






Improving Language Understanding by Generative Pre-training (GPT-1):

Learning Objectives and Concepts

 Unsupervised Language Modelling

 Supervised Fine-Tuning

auxiliary

learning objective

 Task Specific Input Transformations


Dataset

Model Architecture and Implementation Details

 For Unsupervised Training

For Supervised Fine-tuning

Performance and Summary

zero-shot performance


Language Models are unsupervised multitask learners (GPT-2):

Learning Objectives and Concepts

Task Conditioning

Zero Shot Learning and Zero Short Task Transfer

 Dataset

Model architecture and Implementation Details


√

Performance and Summary

Language models are few shot learners (GPT-3):


Learning Objectives and Concepts

In-context learning

Few-shot, one-shot and zero-shot setting

 Dataset

 Model and Implementation details

Performance and Summary


Limitations and Broader Impacts

Ending Note:

Glossary:


References:

10

We’re powering the next great retail disruption. Learn more about us — https://www.linkedin.com/company/walmartglobaltech/



Read more from Walmart Global Tech Blog

NLP

Data Science

Gpt

Gpt 2

Gpt 3






