


Published in

Towards Data Science



Apr 30, 2018

·

11 min read

Save

Intuitive Guide to Understanding KL Divergence

Code:

A B C D* E F G* H I J K L* M N O P Q R S T U V W X Y Z

Concept Grounding

What is a Distribution

X 

Y

X

Y X

X








y=p(x)

X

Y

What is an event?

X=1

P(X=1)

X=1

0.95&lt; X&lt;1.05

Back to KL divergence

Problem we’re trying to solve

X

Y

x teeth / total number of worms

Intuition: KL divergence is a way of measuring the matching between

two distributions (e.g. threads)

Let’s change a few things in the example


First try: Model this with a uniform distribution

Second try: Model this with a binomial distribution

p

n

k

Breaking down the equation


p^k

k

p

k

p^k

n

n-k

(1-p)

k

p^k (1-p)^{n-k}

k

n

k

Mean and variance of the binomial distribution

np

n

n=1

variance=p(1-p)

p=0.5

p=1

p=0

Back to modeling

p

n

n

100

11

pk^{bi}

k

k

pk^{bi}

k th


Let’s summarize what we have

0.0909

n=10 p=0.544

k

How do we quantitatively decide which ones the best?

q(x)

p(x)

q(x)

D_{KL} (p||q) = 0

0

∞

Intuitive breakdown of the KL divergence

log(p(x_i)/q(x_i))

q(x_i)

p(x_i)

q(x_i)

p(x_i)

p(x_i)=q(x_i)

p(x_i)

p(x_i)

p(x_i)


log(0)

Computing KL divergence

KL Divergence with respect to Binomial Mean

n

Δ

Conclusion

Fun with KL divergence

Reference

Small note


12



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





Statistics

Machine Learning

Data Science

Probability

Light On Math

