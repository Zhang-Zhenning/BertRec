


Log in

Sign up

user88



triomphe

837

1

7

9



CommunityBot

What is the difference in Bayesian estimate and maximum likelihood estimate?

Asked 9 years, 6 months ago

Modified 7 months ago

Viewed 102k times

67

 

 

Please explain to me the difference in Bayesian estimate and Maximum likelihood estimate?

Share

Improve this question

edited Oct 30, 2013 at 10:14

asked Oct 29, 2013 at 23:15

10

Depends on the kind of Bayesian estimate. MAP? Posterior mean? The result of minimizing Bayes risk for some loss function? Each of the above? Something else?

– Glen_b

Oct 29, 2013 at 23:34 

2

I've answered this question, or an analogue, here. stats.stackexchange.com/questions/73439/… What issues are you having understanding the two? More details will help us give a better answer.

– Sycorax ♦

Oct 30, 2013 at 12:15 

3

From STAN reference manual: "If the prior is uniform, the posterior mode corresponds to the maximum likelihood estimate (MLE) of the parameters. If the prior is not uniform, the posterior mode is sometimes called the

maximum a posterior (MAP) estimate."

– Neerav

Dec 22, 2015 at 18:03 

@Neerav that's the answer I needed. thx

– WestCoastProjects

Nov 26, 2017 at 1:33

A possibly useful answer for the specific case of Bayesian maximum a posteriori estimate is given here.

– pglpm

Mar 28, 2018 at 13:24

Show 1 more comment

4 Answers

Sorted by:

87

 

It is a very broad question and my answer here only begins to scratch the surface a bit. I will use the Bayes's rule to explain the concepts.

Let’s assume that a set of probability distribution parameters, θ, best explains the dataset D. We may wish to estimate the parameters θ with the help of the Bayes’ Rule:

p(θ|D) =

p(D|θ) ∗ p(θ)

p(D)

posterior=

likelihood ∗ prior

evidence

The explanations follow:

Maximum Likelihood Estimate

With MLE,we seek a point value for θ which maximizes the likelihood, p(D|θ), shown in the equation(s) above. We can denote this value as ˆθ. In MLE, ˆθ is a point estimate, not a random variable.

In other words, in the equation above, MLE treats the term 

p(θ)

p(D) as a constant and does NOT allow us to inject our prior beliefs, p(θ), about the likely values for θ in the estimation calculations.

Bayesian Estimate

Bayesian estimation, by contrast, fully calculates (or at times approximates) the posterior distribution p(θ|D). Bayesian inference treats θ as a random variable. In Bayesian estimation, we put in probability density functions and get

out probability density functions, rather than a single point as in MLE.

Of all the θ values made possible by the output distribution p(θ|D), it is our job to select a value that we consider best in some sense. For example, we may choose the expected value of θ assuming its variance is small enough. The

variance that we can calculate for the parameter θ from its posterior distribution allows us to express our confidence in any specific value we may use as an estimate. If the variance is too large, we may declare that there does not

exist a good estimate for θ.

As a trade-off, Bayesian estimation is made complex by the fact that we now have to deal with the denominator in the Bayes' rule, i.e. evidence. Here evidence -or probability of evidence- is represented by:

p(D) =∫θp(D|θ) ∗ p(θ)dθ

This leads to the concept of 'conjugate priors' in Bayesian estimation. For a given likelihood function, if we have a choice regarding how we express our prior beliefs, we must use that form which allows us to carry out the integration

shown above. The idea of conjugate priors and how they are practically implemented are explained quite well in this post by COOlSerdash.

Share

Improve this answer

edited Apr 13, 2017 at 12:44

Ask Question

bayesian

maximum-likelihood

Cite

Follow









Highest score (default)

Cite

Follow


1



Zhubarb

7,993

3

30

47



AdamO

57.6k

6

115

228

answered Oct 30, 2013 at 10:07

1

Would you elaborate more on this? : "the denominator in the Bayes' rule, i.e. evidence."

– Daniel

Oct 30, 2013 at 17:17

1

I extended my answer.

– Zhubarb

Oct 31, 2013 at 9:41

1

@Berkan in the equation here, P(D|theta) is likelihood. However, likelihood function is defined as P(theta|D), that is the function of parameter, given data. I'm always confused about this. The term likelihood is refering to

different things here? Could you elaborate on that? Thanks a lot!

– zesla

Jul 25, 2018 at 19:46

2

@zesla if my understanding is correct, P(theta|D) is not the likelihood — it’s the posterior. That is, the distribution of theta conditional on the data source you have samples of. Likelihood is as you said: P(D|theta) — the

distribution of your data as parameterized by theta, or put perhaps more intuitively, the “likelihood of seeing what you see” as a function of theta. Does that make sense? Everyone else: please correct me where I’m wrong.

– william_grisaitis

Aug 1, 2018 at 4:54 

@zesla, the explanation given by grisaitis is correct.

– Zhubarb

Aug 1, 2018 at 7:57

Show 2 more comments

16

 

 

I think you're talking about point estimation as in parametric inference, so that we can assume a parametric probability model for a data generating mechanism but the actual value of the parameter is unknown.

Maximum likelihood estimation refers to using a probability model for data and optimizing the joint likelihood function of the observed data over one or more parameters. It's therefore seen that the estimated parameters are most

consistent with the observed data relative to any other parameter in the parameter space. Note such likelihood functions aren't necessarily viewed as being "conditional" upon the parameters since the parameters aren't random

variables, hence it's somewhat more sophisticated to conceive of the likelihood of various outcomes comparing two different parameterizations. It turns out this is a philosophically sound approach.

Bayesian estimation is a bit more general because we're not necessarily maximizing the Bayesian analogue of the likelihood (the posterior density). However, the analogous type of estimation (or posterior mode estimation) is seen as

maximizing the probability of the posterior parameter conditional upon the data. Usually, Bayes' estimates obtained in such a manner behave nearly exactly like those of ML. The key difference is that Bayes inference allows for an

explicit method to incorporate prior information.

Also 'The Epic History of Maximum Likelihood makes for an illuminating read

http://arxiv.org/pdf/0804.2996.pdf

Share

Improve this answer

edited Apr 7, 2017 at 14:44

answered Oct 30, 2013 at 0:08

Would you elaborate more on this? "However, the analogous type of estimation (or posterior mode estimation) is seen as maximizing the probability of the posterior parameter conditional upon the data."

– Daniel

Oct 30, 2013 at 17:14

The posterior mode is a bit of a misnomer because, with continuous DFs, the value is well defined. Posterior densities are related to the likelihood in the frequentist case, except that it allows you to simulate parameters from the

posterior density. Interestingly, one most intuitively thinks of the "posterior mean" as being the best point estimate of the parameter. This approach is often done and, for symmetric unimodal densities, this produces valid

credible intervals that are consistent with ML. The posterior mode is just the parameter value at the apex of the posterior density.

– AdamO

Oct 30, 2013 at 17:29

About "this produces valid credible intervals that are consistent with ML.": It really depends on the model, right? They might be consistent or not ...

– Daniel

Oct 30, 2013 at 18:19

1

The issue of underlying parametric assumptions motivates a discussion about fully parametric vs. semi-parametric or non-parametric inference. That is not a ML vs Bayesian issue and you're not the first to make that mistake. ML

is a fully parametric approach, it allows you to estimate some things which SP or NP can't (and often more efficiently when they can). Correctly specifying the probability model in ML is exactly like choosing the correct prior

and all the robustness properties (and sensitivity issues) that implies.

– AdamO

Oct 30, 2013 at 18:55 

BTW, your comments ignited this question in my mind. Any comments on this? stats.stackexchange.com/questions/74164/…

– Daniel

Oct 30, 2013 at 19:14

Show 1 more comment

3

 

 

The Bayesian estimate is Bayesian inference while the MLE is a type of frequentist inference method.

According to the Bayesian inference, f(x1,...,xn;θ) =

f(θ;x1,...,xn) ∗ f(x1,...,xn)

f(θ)

 holds, that is likelihood =

posterior ∗ evidence

prior

. Notice that the maximum likelihood estimate treats the ratio of evidence to prior as a constant(setting the

prior distribution as uniform distribution/diffuse prior/uninformative prior, p(θ) =1/6 in playing a dice for instance), which omits the prior beliefs, thus MLE is considered to be a frequentist technique(rather than Bayesian). And the

prior can be not the same in this scenario, because if the size of the sample is large enough MLE amounts to MAP(for detailed deduction please refer to this answer).

MLE's alternative in Bayesian inference is called maximum a posteriori estimation(MAP for short), and actually MLE is a special case of MAP where the prior is uniform, as we see above and as stated in Wikipedia:

For details please refer to this awesome article: MLE vs MAP: the connection between Maximum Likelihood and Maximum A Posteriori Estimation.

And one more difference is that maximum likelihood is overfitting-prone, but if you adopt the Bayesian approach the over-fitting problem can be avoided.

Share



Cite

Follow



From the point of view of Bayesian inference, MLE is a special case of maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters.




Lerner Zhang

5,908

1

36

64



TA72

119

2

Improve this answer

edited May 29, 2021 at 2:23

answered Feb 20, 2018 at 2:04

3

One of the cool things about Bayes is that you are not obligated to compute any point estimate at all. The entire posterior density can be your "estimate".

– Frank Harrell

Feb 20, 2018 at 2:26

@FrankHarrell Dear Prof. Harrell, could you please help me edit the answer if I made some terrible mistakes somewhere? Thanks very much!

– Lerner Zhang

Feb 20, 2018 at 2:35 

1

I didn't mean to imply you had made a mistake.

– Frank Harrell

Feb 20, 2018 at 4:54

1

This is covered in multiple Bayesian texts. The end result is the posterior distribution. If you want to distill that to one point you need a loss function to reward or penalize certain properties of such point estimates. With squared

error loss the best guess is the mean. With absolute error loss the best guess is the posterior median.

– Frank Harrell

Apr 12, 2021 at 20:53

1

Yes, just keep in mind that we were talking about point estimates, not actual decisions. Bayesian decision making by optimizing expected utility does not involve any point estimates but instead uses only whole distributions.

– Frank Harrell

Apr 13, 2021 at 20:06

Show 3 more comments

0

 

 

In principle the difference is precisely 0 - asymptotically speaking :)

Share

Improve this answer

answered Sep 29, 2022 at 1:14



Highly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity.

Not the answer you're looking for? Browse other questions tagged bayesian

maximum-likelihood  or ask your own question.

Linked

1

When are is Bayes estimator the same as the maximum likelihood estimator?

152

Help me understand Bayesian prior and posterior distributions

100

What is the reason that a likelihood function is not a pdf?

15

Comparing maximum likelihood estimation (MLE) and Bayes' Theorem

9

Is the invariance property of the ML estimator nonsensical from a Bayesian perspective?

9

Maximum likelihood equivalent to maximum a posterior estimation

5

MLE in context: why is maximum likelihood estimation a thing?

3

Relation Between Bayesian Estimation and Maximum a posteriori estimation

2

Modelling parameters in maximum likelihood

2

Why semi/nonparametric models?

See more linked questions

Related

9

Is a Bayesian estimate with a "flat prior" the same as a maximum likelihood estimate?

2

Finding the maximum likelihood estimate

1

Tossing coin and classical ML estimate

2

Maximum likelihood in Naive Bayes classifier

1

What is the difference between log-likelihood and expected pointwise log predictive density

1

Cite

Follow



Cite

Follow



Featured on Meta



New blog post from our CEO Prashanth: Community is the future of AI



Improving the copy in the close modal and post notices - 2023 edition


CROSS VALIDATED

Tour

Help

Chat

Contact

Feedback

COMPANY

Stack Overflow

Teams

Advertising

Collectives

Talent

About

Press

Legal

Privacy Policy

Terms of Service

Cookie Settings

Cookie Policy

STACK EXCHANGE NETWORK

Technology

Culture &amp; recreation

Life &amp; arts

Science

Professional

Business

API

Data

Blog

Facebook

Twitter

LinkedIn

Instagram

Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2023.4.21.43403

Difference between maximum likelihood estimate of variance and its expectation?

8

Maximum likelihood estimation, Restricted maximum likelihood estimation and Profile likelihood estimation

Hot Network Questions



Is Brownian motion truly random?



Has depleted uranium been considered for radiation shielding in crewed spacecraft beyond LEO?



How to combine independent probability distributions?



Interpreting non-statistically significant results: Do we have "no evidence" or "insufficient evidence" to reject the null?



What's the cheapest way to buy out a sibling's share of our parents house if I have no cash and want to pay less than the appraised value?

more hot questions

 Question feed

Your privacy

By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.

 

Accept all cookies

Necessary cookies only

Customize settings

