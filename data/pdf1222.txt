
Online edition (c)ÔøΩ2009 Cambridge UP

DRAFT! ¬© April 1, 2009 Cambridge University Press. Feedback welcome.

253

13

Text classiÔ¨Åcation and Naive

Bayes

Thus far, this book has mainly discussed the process of ad hoc retrieval, where

users have transient information needs that they try to address by posing

one or more queries to a search engine. However, many users have ongoing

information needs. For example, you might need to track developments in

multicore computer chips. One way of doing this is to issue the query multi-

core AND computer AND chip against an index of recent newswire articles each

morning. In this and the following two chapters we examine the question:

How can this repetitive task be automated? To this end, many systems sup-

port standing queries. A standing query is like any other query except that it

STANDING QUERY

is periodically executed on a collection to which new documents are incre-

mentally added over time.

If your standing query is just multicore AND computer AND chip, you will tend

to miss many relevant new articles which use other terms such as multicore

processors. To achieve good recall, standing queries thus have to be reÔ¨Åned

over time and can gradually become quite complex. In this example, using a

Boolean search engine with stemming, you might end up with a query like

(multicore OR multi-core) AND (chip OR processor OR microprocessor).

To capture the generality and scope of the problem space to which stand-

ing queries belong, we now introduce the general notion of a classiÔ¨Åcation

CLASSIFICATION

problem. Given a set of classes, we seek to determine which class(es) a given

object belongs to. In the example, the standing query serves to divide new

newswire articles into the two classes: documents about multicore computer chips

and documents not about multicore computer chips. We refer to this as two-class

classiÔ¨Åcation. ClassiÔ¨Åcation using standing queries is also called routing or

ROUTING

Ô¨Ålteringand will be discussed further in Section 15.3.1 (page 335).

FILTERING

A class need not be as narrowly focused as the standing query multicore

computer chips. Often, a class is a more general subject area like China or coffee.

Such more general classes are usually referred to as topics, and the classiÔ¨Åca-

tion task is then called text classiÔ¨Åcation, text categorization, topic classiÔ¨Åcation,

TEXT CLASSIFICATION

or topic spotting. An example for China appears in Figure 13.1. Standing

queries and topics differ in their degree of speciÔ¨Åcity, but the methods for


Online edition (c)ÔøΩ2009 Cambridge UP

254

13

Text classiÔ¨Åcation and Naive Bayes

solving routing, Ô¨Åltering, and text classiÔ¨Åcation are essentially the same. We

therefore include routing and Ô¨Åltering under the rubric of text classiÔ¨Åcation

in this and the following chapters.

The notion of classiÔ¨Åcation is very general and has many applications within

and beyond information retrieval (IR). For instance, in computer vision, a

classiÔ¨Åer may be used to divide images into classes such as landscape, por-

trait, and neither. We focus here on examples from information retrieval such

as:

‚Ä¢ Several of the preprocessing steps necessary for indexing as discussed in

Chapter 2: detecting a document‚Äôs encoding (ASCII, Unicode UTF-8 etc;

page 20); word segmentation (Is the white space between two letters a

word boundary or not? page 24 ) ; truecasing (page 30); and identifying

the language of a document (page 46).

‚Ä¢ The automatic detection of spam pages (which then are not included in

the search engine index).

‚Ä¢ The automatic detection of sexually explicit content (which is included in

search results only if the user turns an option such as SafeSearch off).

‚Ä¢ Sentiment detection or the automatic classiÔ¨Åcation of a movie or product

SENTIMENT DETECTION

review as positive or negative. An example application is a user search-

ing for negative reviews before buying a camera to make sure it has no

undesirable features or quality problems.

‚Ä¢ Personal email sorting. A user may have folders like talk announcements,

EMAIL SORTING

electronic bills, email from family and friends, and so on, and may want a

classiÔ¨Åer to classify each incoming email and automatically move it to the

appropriate folder. It is easier to Ô¨Ånd messages in sorted folders than in

a very large inbox. The most common case of this application is a spam

folder that holds all suspected spam messages.

‚Ä¢ Topic-speciÔ¨Åc or vertical search. Vertical search engines restrict searches to

VERTICAL SEARCH

ENGINE

a particular topic. For example, the query computer science on a vertical

search engine for the topic China will return a list of Chinese computer

science departments with higher precision and recall than the query com-

puter science China on a general purpose search engine. This is because the

vertical search engine does not include web pages in its index that contain

the term china in a different sense (e.g., referring to a hard white ceramic),

but does include relevant pages even if they do not explicitly mention the

term China.

‚Ä¢ Finally, the ranking function in ad hoc information retrieval can also be

based on a document classiÔ¨Åer as we will explain in Section 15.4 (page 341).


Online edition (c)ÔøΩ2009 Cambridge UP

255

This list shows the general importance of classiÔ¨Åcation in IR. Most retrieval

systems today contain multiple components that use some form of classiÔ¨Åer.

The classiÔ¨Åcation task we will use as an example in this book is text classiÔ¨Å-

cation.

A computer is not essential for classiÔ¨Åcation. Many classiÔ¨Åcation tasks

have traditionally been solved manually. Books in a library are assigned

Library of Congress categories by a librarian. But manual classiÔ¨Åcation is

expensive to scale. The multicore computer chips example illustrates one al-

ternative approach: classiÔ¨Åcation by the use of standing queries ‚Äì which can

be thought of as rules ‚Äì most commonly written by hand. As in our exam-

RULES IN TEXT

CLASSIFICATION

ple (multicore OR multi-core) AND (chip OR processor OR microprocessor), rules are

sometimes equivalent to Boolean expressions.

A rule captures a certain combination of keywords that indicates a class.

Hand-coded rules have good scaling properties, but creating and maintain-

ing them over time is labor intensive. A technically skilled person (e.g., a

domain expert who is good at writing regular expressions) can create rule

sets that will rival or exceed the accuracy of the automatically generated clas-

siÔ¨Åers we will discuss shortly; however, it can be hard to Ô¨Ånd someone with

this specialized skill.

Apart from manual classiÔ¨Åcation and hand-crafted rules, there is a third

approach to text classiÔ¨Åcation, namely, machine learning-based text classiÔ¨Å-

cation. It is the approach that we focus on in the next several chapters. In

machine learning, the set of rules or, more generally, the decision criterion of

the text classiÔ¨Åer, is learned automatically from training data. This approach

is also called statistical text classiÔ¨Åcation if the learning method is statistical.

STATISTICAL TEXT

CLASSIFICATION

In statistical text classiÔ¨Åcation, we require a number of good example docu-

ments (or training documents) for each class. The need for manual classiÔ¨Å-

cation is not eliminated because the training documents come from a person

who has labeled them ‚Äì where labeling refers to the process of annotating

LABELING

each document with its class. But labeling is arguably an easier task than

writing rules. Almost anybody can look at a document and decide whether

or not it is related to China. Sometimes such labeling is already implicitly

part of an existing workÔ¨Çow. For instance, you may go through the news

articles returned by a standing query each morning and give relevance feed-

back (cf. Chapter 9) by moving the relevant articles to a special folder like

multicore-processors.

We begin this chapter with a general introduction to the text classiÔ¨Åcation

problem including a formal deÔ¨Ånition (Section 13.1); we then cover Naive

Bayes, a particularly simple and effective classiÔ¨Åcation method (Sections 13.2‚Äì

13.4). All of the classiÔ¨Åcation algorithms we study represent documents in

high-dimensional spaces. To improve the efÔ¨Åciency of these algorithms, it

is generally desirable to reduce the dimensionality of these spaces; to this

end, a technique known as feature selection is commonly applied in text clas-


Online edition (c)ÔøΩ2009 Cambridge UP

256

13

Text classiÔ¨Åcation and Naive Bayes

siÔ¨Åcation as discussed in Section 13.5. Section 13.6 covers evaluation of text

classiÔ¨Åcation. In the following chapters, Chapters 14 and 15, we look at two

other families of classiÔ¨Åcation methods, vector space classiÔ¨Åers and support

vector machines.

13.1

The text classiÔ¨Åcation problem

In text classiÔ¨Åcation, we are given a description d ‚àà X of a document, where

X is the document space; and a Ô¨Åxed set of classes C = {c1, c2, . . . , cJ}. Classes

DOCUMENT SPACE

CLASS

are also called categories or labels. Typically, the document space X is some

type of high-dimensional space, and the classes are human deÔ¨Åned for the

needs of an application, as in the examples China and documents that talk

about multicore computer chips above. We are given a training set D of labeled

TRAINING SET

documents ‚ü®d, c‚ü©,where ‚ü®d, c‚ü© ‚àà X √ó C. For example:

‚ü®d, c‚ü© = ‚ü®Beijing joins the World Trade Organization, China‚ü©

for the one-sentence document Beijing joins the World Trade Organization and

the class (or label) China.

Using a learning method or learning algorithm, we then wish to learn a clas-

LEARNING METHOD

siÔ¨Åer or classiÔ¨Åcation function Œ≥ that maps documents to classes:

CLASSIFIER

Œ≥ : X ‚Üí C

(13.1)

This type of learning is called supervised learning because a supervisor (the

SUPERVISED LEARNING

human who deÔ¨Ånes the classes and labels training documents) serves as a

teacher directing the learning process. We denote the supervised learning

method by Œì and write Œì(D) = Œ≥. The learning method Œì takes the training

set D as input and returns the learned classiÔ¨Åcation function Œ≥.

Most names for learning methods Œì are also used for classiÔ¨Åers Œ≥. We

talk about the Naive Bayes (NB) learning method Œì when we say that ‚ÄúNaive

Bayes is robust,‚Äù meaning that it can be applied to many different learning

problems and is unlikely to produce classiÔ¨Åers that fail catastrophically. But

when we say that ‚ÄúNaive Bayes had an error rate of 20%,‚Äù we are describing

an experiment in which a particular NB classiÔ¨Åer Œ≥ (which was produced by

the NB learning method) had a 20% error rate in an application.

Figure 13.1 shows an example of text classiÔ¨Åcation from the Reuters-RCV1

collection, introduced in Section 4.2, page 69. There are six classes (UK, China,

..., sports), each with three training documents. We show a few mnemonic

words for each document‚Äôs content. The training set provides some typical

examples for each class, so that we can learn the classiÔ¨Åcation function Œ≥.

Once we have learned Œ≥, we can apply it to the test set (or test data), for ex-

TEST SET

ample, the new document Ô¨Årst private Chinese airline whose class is unknown.


Online edition (c)ÔøΩ2009 Cambridge UP

13.1

The text classiÔ¨Åcation problem

257

classes:

training

set:

test

set:

regions

industries

subject areas

Œ≥(d‚Ä≤) =China

Ô¨Årst

private

Chinese

airline

UK

China

poultry

coffee

elections

sports

London

congestion

Big Ben

Parliament

the Queen

Windsor

Beijing

Olympics

Great Wall

tourism

communist

Mao

chicken

feed

ducks

pate

turkey

bird Ô¨Çu

beans

roasting

robusta

arabica

harvest

Kenya

votes

recount

run-off

seat

campaign

TV ads

baseball

diamond

soccer

forward

captain

team

d‚Ä≤

‚óÆ Figure 13.1

Classes, training set, and test set in text classiÔ¨Åcation .



In Figure 13.1, the classiÔ¨Åcation function assigns the new document to class

Œ≥(d) = China, which is the correct assignment.

The classes in text classiÔ¨Åcation often have some interesting structure such

as the hierarchy in Figure 13.1. There are two instances each of region cate-

gories, industry categories, and subject area categories. A hierarchy can be

an important aid in solving a classiÔ¨Åcation problem; see Section 15.3.2 for

further discussion. Until then, we will make the assumption in the text clas-

siÔ¨Åcation chapters that the classes form a set with no subset relationships

between them.

DeÔ¨Ånition (13.1) stipulates that a document is a member of exactly one

class. This is not the most appropriate model for the hierarchy in Figure 13.1.

For instance, a document about the 2008 Olympics should be a member of

two classes: the China class and the sports class. This type of classiÔ¨Åcation

problem is referred to as an any-of problem and we will return to it in Sec-

tion 14.5 (page 306). For the time being, we only consider one-of problems

where a document is a member of exactly one class.

Our goal in text classiÔ¨Åcation is high accuracy on test data or new data ‚Äì for

example, the newswire articles that we will encounter tomorrow morning

in the multicore chip example. It is easy to achieve high accuracy on the

training set (e.g., we can simply memorize the labels). But high accuracy on

the training set in general does not mean that the classiÔ¨Åer will work well on


Online edition (c)ÔøΩ2009 Cambridge UP

258

13

Text classiÔ¨Åcation and Naive Bayes

new data in an application. When we use the training set to learn a classiÔ¨Åer

for test data, we make the assumption that training data and test data are

similar or from the same distribution. We defer a precise deÔ¨Ånition of this

notion to Section 14.6 (page 308).

13.2

Naive Bayes text classiÔ¨Åcation

The Ô¨Årst supervised learning method we introduce is the multinomial Naive

MULTINOMIAL NAIVE

BAYES

Bayes or multinomial NB model, a probabilistic learning method. The proba-

bility of a document d being in class c is computed as

P(c|d) ‚àù P(c) ‚àè

1‚â§k‚â§nd

P(tk|c)

(13.2)

where P(tk|c) is the conditional probability of term tk occurring in a docu-

ment of class c.1 We interpret P(tk|c) as a measure of how much evidence

tk contributes that c is the correct class. P(c) is the prior probability of a

document occurring in class c. If a document‚Äôs terms do not provide clear

evidence for one class versus another, we choose the one that has a higher

prior probability. ‚ü®t1, t2, . . . , tnd‚ü© are the tokens in d that are part of the vocab-

ulary we use for classiÔ¨Åcation and nd is the number of such tokens in d. For

example, ‚ü®t1, t2, . . . , tnd‚ü© for the one-sentence document Beijing and Taipei join

the WTO might be ‚ü®Beijing, Taipei, join, WTO‚ü©, with nd = 4, if we treat the terms

and and the as stop words.

In text classiÔ¨Åcation, our goal is to Ô¨Ånd the best class for the document. The

best class in NB classiÔ¨Åcation is the most likely or maximum a posteriori (MAP)

MAXIMUM A

POSTERIORI CLASS

class cmap:

cmap = arg max

c‚ààC

ÀÜP(c|d) = arg max

c‚ààC

ÀÜP(c) ‚àè

1‚â§k‚â§nd

ÀÜP(tk|c).

(13.3)

We write ÀÜP for P because we do not know the true values of the parameters

P(c) and P(tk|c), but estimate them from the training set as we will see in a

moment.

In Equation (13.3), many conditional probabilities are multiplied, one for

each position 1 ‚â§ k ‚â§ nd. This can result in a Ô¨Çoating point underÔ¨Çow.

It is therefore better to perform the computation by adding logarithms of

probabilities instead of multiplying probabilities. The class with the highest

log probability score is still the most probable; log(xy) = log(x) + log(y)

and the logarithm function is monotonic. Hence, the maximization that is



1. We will explain in the next section why P(c|d) is proportional to (‚àù), not equal to the quantity

on the right.


Online edition (c)ÔøΩ2009 Cambridge UP

13.2

Naive Bayes text classiÔ¨Åcation

259

actually done in most implementations of NB is:

cmap = arg max

c‚ààC

[log ÀÜP(c) + ‚àë

1‚â§k‚â§nd

log ÀÜP(tk|c)].

(13.4)

Equation (13.4) has a simple interpretation. Each conditional parameter

log ÀÜP(tk|c) is a weight that indicates how good an indicator tk is for c. Sim-

ilarly, the prior log ÀÜP(c) is a weight that indicates the relative frequency of

c. More frequent classes are more likely to be the correct class than infre-

quent classes. The sum of log prior and term weights is then a measure of

how much evidence there is for the document being in the class, and Equa-

tion (13.4) selects the class for which we have the most evidence.

We will initially work with this intuitive interpretation of the multinomial

NB model and defer a formal derivation to Section 13.4.

How do we estimate the parameters ÀÜP(c) and ÀÜP(tk|c)? We Ô¨Årst try the

maximum likelihood estimate (MLE; Section 11.3.2, page 226), which is sim-

ply the relative frequency and corresponds to the most likely value of each

parameter given the training data. For the priors this estimate is:

ÀÜP(c) = Nc



N ,

(13.5)

where Nc is the number of documents in class c and N is the total number of

documents.

We estimate the conditional probability ÀÜP(t|c) as the relative frequency of

term t in documents belonging to class c:

ÀÜP(t|c) =

Tct



‚àët‚Ä≤‚ààV Tct‚Ä≤ ,

(13.6)

where Tct is the number of occurrences of t in training documents from class

c, including multiple occurrences of a term in a document. We have made the

positional independence assumption here, which we will discuss in more detail

in the next section: Tct is a count of occurrences in all positions k in the doc-

uments in the training set. Thus, we do not compute different estimates for

different positions and, for example, if a word occurs twice in a document,

in positions k1 and k2, then ÀÜP(tk1|c) = ÀÜP(tk2|c).

The problem with the MLE estimate is that it is zero for a term‚Äìclass combi-

nation that did not occur in the training data. If the term WTO in the training

data only occurred in China documents, then the MLE estimates for the other

classes, for example UK, will be zero:

ÀÜP(WTO|UK) = 0.

Now, the one-sentence document Britain is a member of the WTO will get a

conditional probability of zero for UK because we are multiplying the condi-

tional probabilities for all terms in Equation (13.2). Clearly, the model should


Online edition (c)ÔøΩ2009 Cambridge UP

260

13

Text classiÔ¨Åcation and Naive Bayes

TRAINMULTINOMIALNB(C, D)

1

V ‚Üê EXTRACTVOCABULARY(D)

2

N ‚Üê COUNTDOCS(D)

3

for each c ‚àà C

4

do Nc ‚Üê COUNTDOCSINCLASS(D, c)

5

prior[c] ‚Üê Nc/N

6

textc ‚Üê CONCATENATETEXTOFALLDOCSINCLASS(D, c)

7

for each t ‚àà V

8

do Tct ‚Üê COUNTTOKENSOFTERM(textc, t)

9

for each t ‚àà V

10

do condprob[t][c] ‚Üê

Tct+1



‚àët‚Ä≤(Tct‚Ä≤+1)

11

return V, prior, condprob

APPLYMULTINOMIALNB(C, V, prior, condprob, d)

1

W ‚Üê EXTRACTTOKENSFROMDOC(V, d)

2

for each c ‚àà C

3

do score[c] ‚Üê log prior[c]

4

for each t ‚àà W

5

do score[c] += log condprob[t][c]

6

return arg maxc‚ààC score[c]

‚óÆ Figure 13.2

Naive Bayes algorithm (multinomial model): Training and testing.



assign a high probability to the UK class because the term Britain occurs. The

problem is that the zero probability for WTO cannot be ‚Äúconditioned away,‚Äù

no matter how strong the evidence for the class UK from other features. The

estimate is 0 because of sparseness: The training data are never large enough

SPARSENESS

to represent the frequency of rare events adequately, for example, the fre-

quency of WTO occurring in UK documents.

To eliminate zeros, we use add-one or Laplace smoothing, which simply adds

ADD-ONE SMOOTHING

one to each count (cf. Section 11.3.2):

ÀÜP(t|c) =

Tct + 1



‚àët‚Ä≤‚ààV(Tct‚Ä≤ + 1) =

Tct + 1



(‚àët‚Ä≤‚ààV Tct‚Ä≤) + B,

(13.7)

where B = |V| is the number of terms in the vocabulary. Add-one smoothing

can be interpreted as a uniform prior (each term occurs once for each class)

that is then updated as evidence from the training data comes in. Note that

this is a prior probability for the occurrence of a term as opposed to the prior

probability of a class which we estimate in Equation (13.5) on the document

level.


Online edition (c)ÔøΩ2009 Cambridge UP

13.2

Naive Bayes text classiÔ¨Åcation

261

‚óÆ Table 13.1

Data for parameter estimation examples.

docID

words in document

in c = China?



training set

1

Chinese Beijing Chinese

yes

2

Chinese Chinese Shanghai

yes

3

Chinese Macao

yes

4

Tokyo Japan Chinese

no



test set

5

Chinese Chinese Chinese Tokyo Japan

?

‚óÆ Table 13.2

Training and test times for NB.

mode

time complexity



training

Œò(|D|Lave + |C||V|)

testing

Œò(La + |C|Ma) = Œò(|C|Ma)



We have now introduced all the elements we need for training and apply-

ing an NB classiÔ¨Åer. The complete algorithm is described in Figure 13.2.



Example 13.1:

For the example in Table 13.1, the multinomial parameters we

need to classify the test document are the priors ÀÜP(c) = 3/4 and ÀÜP(



c) = 1/4 and the

following conditional probabilities:

ÀÜP(Chinese|c)

=

(5 + 1)/(8 + 6) = 6/14 = 3/7

ÀÜP(Tokyo|c) = ÀÜP(Japan|c)

=

(0 + 1)/(8 + 6) = 1/14

ÀÜP(Chinese|



c)

=

(1 + 1)/(3 + 6) = 2/9

ÀÜP(Tokyo|



c) = ÀÜP(Japan|



c)

=

(1 + 1)/(3 + 6) = 2/9

The denominators are (8 + 6) and (3 + 6) because the lengths of textc and text



c are 8

and 3, respectively, and because the constant B in Equation (13.7) is 6 as the vocabu-

lary consists of six terms.

We then get:

ÀÜP(c|d5)

‚àù

3/4 ¬∑ (3/7)3 ¬∑ 1/14 ¬∑ 1/14 ‚âà 0.0003.

ÀÜP(



c|d5)

‚àù

1/4 ¬∑ (2/9)3 ¬∑ 2/9 ¬∑ 2/9 ‚âà 0.0001.

Thus, the classiÔ¨Åer assigns the test document to c = China. The reason for this clas-

siÔ¨Åcation decision is that the three occurrences of the positive indicator Chinese in d5

outweigh the occurrences of the two negative indicators Japan and Tokyo.

What is the time complexity of NB? The complexity of computing the pa-

rameters is Œò(|C||V|) because the set of parameters consists of |C||V| con-

ditional probabilities and |C| priors. The preprocessing necessary for com-

puting the parameters (extracting the vocabulary, counting terms, etc.) can

be done in one pass through the training data. The time complexity of this


Online edition (c)ÔøΩ2009 Cambridge UP

262

13

Text classiÔ¨Åcation and Naive Bayes

component is therefore Œò(|D|Lave), where |D| is the number of documents

and Lave is the average length of a document.

We use Œò(|D|Lave) as a notation for Œò(T) here, where T is the length of the

training collection. This is nonstandard; Œò(.) is not deÔ¨Åned for an average.

We prefer expressing the time complexity in terms of D and Lave because

these are the primary statistics used to characterize training collections.

The time complexity of APPLYMULTINOMIALNB in Figure 13.2 is Œò(|C|La).

La and Ma are the numbers of tokens and types, respectively, in the test doc-

ument. APPLYMULTINOMIALNB can be modiÔ¨Åed to be Œò(La + |C|Ma) (Ex-

ercise 13.8). Finally, assuming that the length of test documents is bounded,

Œò(La + |C|Ma) = Œò(|C|Ma) because La &lt; b|C|Ma for a Ô¨Åxed constant b.2

Table 13.2 summarizes the time complexities. In general, we have |C||V| &lt;

|D|Lave, so both training and testing complexity are linear in the time it takes

to scan the data. Because we have to look at the data at least once, NB can be

said to have optimal time complexity. Its efÔ¨Åciency is one reason why NB is

a popular text classiÔ¨Åcation method.

13.2.1

Relation to multinomial unigram language model

The multinomial NB model is formally identical to the multinomial unigram

language model (Section 12.2.1, page 242). In particular, Equation (13.2) is

a special case of Equation (12.12) from page 243, which we repeat here for

Œª = 1:

P(d|q) ‚àù P(d)‚àè

t‚ààq

P(t|Md).

(13.8)

The document d in text classiÔ¨Åcation (Equation (13.2)) takes the role of the

query in language modeling (Equation (13.8)) and the classes c in text clas-

siÔ¨Åcation take the role of the documents d in language modeling. We used

Equation (13.8) to rank documents according to the probability that they are

relevant to the query q. In NB classiÔ¨Åcation, we are usually only interested

in the top-ranked class.

We also used MLE estimates in Section 12.2.2 (page 243) and encountered

the problem of zero estimates owing to sparse data (page 244); but instead

of add-one smoothing, we used a mixture of two distributions to address the

problem there. Add-one smoothing is closely related to add- 1



2 smoothing in

Section 11.3.4 (page 228).

?

Exercise 13.1

Why is |C||V| &lt; |D|Lave in Table 13.2 expected to hold for most text collections?



2. Our assumption here is that the length of test documents is bounded. La would exceed

b|C|Ma for extremely long test documents.


Online edition (c)ÔøΩ2009 Cambridge UP

13.3

The Bernoulli model

263

TRAINBERNOULLINB(C, D)

1

V ‚Üê EXTRACTVOCABULARY(D)

2

N ‚Üê COUNTDOCS(D)

3

for each c ‚àà C

4

do Nc ‚Üê COUNTDOCSINCLASS(D, c)

5

prior[c] ‚Üê Nc/N

6

for each t ‚àà V

7

do Nct ‚Üê COUNTDOCSINCLASSCONTAININGTERM(D, c, t)

8

condprob[t][c] ‚Üê (Nct + 1)/(Nc + 2)

9

return V, prior, condprob

APPLYBERNOULLINB(C, V, prior, condprob, d)

1

Vd ‚Üê EXTRACTTERMSFROMDOC(V, d)

2

for each c ‚àà C

3

do score[c] ‚Üê log prior[c]

4

for each t ‚àà V

5

do if t ‚àà Vd

6

then score[c] += log condprob[t][c]

7

else score[c] += log(1 ‚àí condprob[t][c])

8

return arg maxc‚ààC score[c]

‚óÆ Figure 13.3

NB algorithm (Bernoulli model): Training and testing. The add-one

smoothing in Line 8 (top) is in analogy to Equation (13.7) with B = 2.



13.3

The Bernoulli model

There are two different ways we can set up an NB classiÔ¨Åer. The model we in-

troduced in the previous section is the multinomial model. It generates one

term from the vocabulary in each position of the document, where we as-

sume a generative model that will be discussed in more detail in Section 13.4

(see also page 237).

An alternative to the multinomial model is the multivariate Bernoulli model

or Bernoulli model. It is equivalent to the binary independence model of Sec-

BERNOULLI MODEL

tion 11.3 (page 222), which generates an indicator for each term of the vo-

cabulary, either 1 indicating presence of the term in the document or 0 indi-

cating absence. Figure 13.3 presents training and testing algorithms for the

Bernoulli model. The Bernoulli model has the same time complexity as the

multinomial model.

The different generation models imply different estimation strategies and

different classiÔ¨Åcation rules. The Bernoulli model estimates ÀÜP(t|c) as the frac-

tion of documents of class c that contain term t (Figure 13.3, TRAINBERNOULLI-


Online edition (c)ÔøΩ2009 Cambridge UP

264

13

Text classiÔ¨Åcation and Naive Bayes

NB, line 8). In contrast, the multinomial model estimates ÀÜP(t|c) as the frac-

tion of tokens or fraction of positions in documents of class c that contain term

t (Equation (13.7)). When classifying a test document, the Bernoulli model

uses binary occurrence information, ignoring the number of occurrences,

whereas the multinomial model keeps track of multiple occurrences. As a

result, the Bernoulli model typically makes many mistakes when classifying

long documents. For example, it may assign an entire book to the class China

because of a single occurrence of the term China.

The models also differ in how nonoccurring terms are used in classiÔ¨Åca-

tion. They do not affect the classiÔ¨Åcation decision in the multinomial model;

but in the Bernoulli model the probability of nonoccurrence is factored in

when computing P(c|d) (Figure 13.3, APPLYBERNOULLINB, Line 7). This is

because only the Bernoulli NB model models absence of terms explicitly.



Example 13.2:

Applying the Bernoulli model to the example in Table 13.1, we

have the same estimates for the priors as before:

ÀÜP(c) = 3/4, ÀÜP(



c) = 1/4. The

conditional probabilities are:

ÀÜP(Chinese|c)

=

(3 + 1)/(3 + 2) = 4/5

ÀÜP(Japan|c) = ÀÜP(Tokyo|c)

=

(0 + 1)/(3 + 2) = 1/5

ÀÜP(Beijing|c) = ÀÜP(Macao|c) = ÀÜP(Shanghai|c)

=

(1 + 1)/(3 + 2) = 2/5

ÀÜP(Chinese|



c)

=

(1 + 1)/(1 + 2) = 2/3

ÀÜP(Japan|



c) = ÀÜP(Tokyo|



c)

=

(1 + 1)/(1 + 2) = 2/3

ÀÜP(Beijing|



c) = ÀÜP(Macao|



c) = ÀÜP(Shanghai|



c)

=

(0 + 1)/(1 + 2) = 1/3

The denominators are (3 + 2) and (1 + 2) because there are three documents in c

and one document in



c and because the constant B in Equation (13.7) is 2 ‚Äì there are

two cases to consider for each term, occurrence and nonoccurrence.

The scores of the test document for the two classes are

ÀÜP(c|d5)

‚àù

ÀÜP(c) ¬∑ ÀÜP(Chinese|c) ¬∑ ÀÜP(Japan|c) ¬∑ ÀÜP(Tokyo|c)

¬∑ (1 ‚àí ÀÜP(Beijing|c)) ¬∑ (1 ‚àí ÀÜP(Shanghai|c)) ¬∑ (1 ‚àí ÀÜP(Macao|c))

=

3/4 ¬∑ 4/5 ¬∑ 1/5 ¬∑ 1/5 ¬∑ (1‚àí2/5) ¬∑ (1‚àí2/5) ¬∑ (1‚àí2/5)

‚âà

0.005

and, analogously,

ÀÜP(



c|d5)

‚àù

1/4 ¬∑ 2/3 ¬∑ 2/3 ¬∑ 2/3 ¬∑ (1‚àí1/3) ¬∑ (1‚àí1/3) ¬∑ (1‚àí1/3)

‚âà

0.022

Thus, the classiÔ¨Åer assigns the test document to



c = not-China. When looking only

at binary occurrence and not at term frequency, Japan and Tokyo are indicators for



c

(2/3 &gt; 1/5) and the conditional probabilities of Chinese for c and



c are not different

enough (4/5 vs. 2/3) to affect the classiÔ¨Åcation decision.


Online edition (c)ÔøΩ2009 Cambridge UP

13.4

Properties of Naive Bayes

265

13.4

Properties of Naive Bayes

To gain a better understanding of the two models and the assumptions they

make, let us go back and examine how we derived their classiÔ¨Åcation rules in

Chapters 11 and 12. We decide class membership of a document by assigning

it to the class with the maximum a posteriori probability (cf. Section 11.3.2,

page 226), which we compute as follows:

cmap

=

arg max

c‚ààC

P(c|d)

=

arg max

c‚ààC

P(d|c)P(c)



P(d)

(13.9)

=

arg max

c‚ààC

P(d|c)P(c),

(13.10)

where Bayes‚Äô rule (Equation (11.4), page 220) is applied in (13.9) and we drop

the denominator in the last step because P(d) is the same for all classes and

does not affect the argmax.

We can interpret Equation (13.10) as a description of the generative process

we assume in Bayesian text classiÔ¨Åcation. To generate a document, we Ô¨Årst

choose class c with probability P(c) (top nodes in Figures 13.4 and 13.5). The

two models differ in the formalization of the second step, the generation of

the document given the class, corresponding to the conditional distribution

P(d|c):

Multinomial

P(d|c)

=

P(‚ü®t1, . . . , tk, . . . , tnd‚ü©|c)

(13.11)

Bernoulli

P(d|c)

=

P(‚ü®e1, . . . , ei, . . . , eM‚ü©|c),

(13.12)

where ‚ü®t1, . . . , tnd‚ü© is the sequence of terms as it occurs in d (minus terms

that were excluded from the vocabulary) and ‚ü®e1, . . . , ei, . . . , eM‚ü© is a binary

vector of dimensionality M that indicates for each term whether it occurs in

d or not.

It should now be clearer why we introduced the document space X in

Equation (13.1) when we deÔ¨Åned the classiÔ¨Åcation problem. A critical step

in solving a text classiÔ¨Åcation problem is to choose the document represen-

tation. ‚ü®t1, . . . , tnd‚ü© and ‚ü®e1, . . . , eM‚ü© are two different document representa-

tions. In the Ô¨Årst case, X is the set of all term sequences (or, more precisely,

sequences of term tokens). In the second case, X is {0, 1}M.

We cannot use Equations (13.11) and (13.12) for text classiÔ¨Åcation directly.

For the Bernoulli model, we would have to estimate 2M|C| different param-

eters, one for each possible combination of M values ei and a class. The

number of parameters in the multinomial case has the same order of magni-


Online edition (c)ÔøΩ2009 Cambridge UP

266

13

Text classiÔ¨Åcation and Naive Bayes

C=China

X1=Beijing

X2=and

X3=Taipei

X4=join

X5=WTO

‚óÆ Figure 13.4

The multinomial NB model.



tude.3 This being a very large quantity, estimating these parameters reliably

is infeasible.

To reduce the number of parameters, we make the Naive Bayes conditional

CONDITIONAL

INDEPENDENCE

ASSUMPTION

independence assumption. We assume that attribute values are independent of

each other given the class:

Multinomial

P(d|c)

=

P(‚ü®t1, . . . , tnd‚ü©|c) = ‚àè

1‚â§k‚â§nd

P(Xk = tk|c)

(13.13)

Bernoulli

P(d|c)

=

P(‚ü®e1, . . . , eM‚ü©|c) = ‚àè

1‚â§i‚â§M

P(Ui = ei|c).

(13.14)

We have introduced two random variables here to make the two different

generative models explicit. Xk is the random variable for position k in the

RANDOM VARIABLE X

document and takes as values terms from the vocabulary. P(Xk = t|c) is the

probability that in a document of class c the term t will occur in position k. Ui

RANDOM VARIABLE U

is the random variable for vocabulary term i and takes as values 0 (absence)

and 1 (presence). ÀÜP(Ui = 1|c) is the probability that in a document of class c

the term ti will occur ‚Äì in any position and possibly multiple times.

We illustrate the conditional independence assumption in Figures 13.4 and 13.5.

The class China generates values for each of the Ô¨Åve term attributes (multi-

nomial) or six binary attributes (Bernoulli) with a certain probability, inde-

pendent of the values of the other attributes. The fact that a document in the

class China contains the term Taipei does not make it more likely or less likely

that it also contains Beijing.

In reality, the conditional independence assumption does not hold for text

data. Terms are conditionally dependent on each other. But as we will dis-

cuss shortly, NB models perform well despite the conditional independence

assumption.



3. In fact, if the length of documents is not bounded, the number of parameters in the multino-

mial case is inÔ¨Ånite.


Online edition (c)ÔøΩ2009 Cambridge UP

13.4

Properties of Naive Bayes

267

UAlaska=0

UBeijing=1

UIndia=0

Ujoin=1

UTaipei=1

UWTO=1

C=China

‚óÆ Figure 13.5

The Bernoulli NB model.



Even when assuming conditional independence, we still have too many

parameters for the multinomial model if we assume a different probability

distribution for each position k in the document. The position of a term in a

document by itself does not carry information about the class. Although

there is a difference between China sues France and France sues China, the

occurrence of China in position 1 versus position 3 of the document is not

useful in NB classiÔ¨Åcation because we look at each term separately. The con-

ditional independence assumption commits us to this way of processing the

evidence.

Also, if we assumed different term distributions for each position k, we

would have to estimate a different set of parameters for each k. The probabil-

ity of bean appearing as the Ô¨Årst term of a coffee document could be different

from it appearing as the second term, and so on. This again causes problems

in estimation owing to data sparseness.

For these reasons, we make a second independence assumption for the

multinomial model, positional independence: The conditional probabilities for

POSITIONAL

INDEPENDENCE

a term are the same independent of position in the document.

P(Xk1 = t|c) = P(Xk2 = t|c)

for all positions k1, k2, terms t and classes c. Thus, we have a single dis-

tribution of terms that is valid for all positions ki and we can use X as its

symbol.4 Positional independence is equivalent to adopting the bag of words

model, which we introduced in the context of ad hoc retrieval in Chapter 6

(page 117).

With conditional and positional independence assumptions, we only need

to estimate Œò(M|C|) parameters P(tk|c) (multinomial model) or P(ei|c) (Bernoulli



4. Our terminology is nonstandard. The random variable X is a categorical variable, not a multi-

nomial variable, and the corresponding NB model should perhaps be called a sequence model. We

have chosen to present this sequence model and the multinomial model in Section 13.4.1 as the

same model because they are computationally identical.


Online edition (c)ÔøΩ2009 Cambridge UP

268

13

Text classiÔ¨Åcation and Naive Bayes

‚óÆ Table 13.3

Multinomial versus Bernoulli model.

multinomial model

Bernoulli model



event model

generation of token

generation of document

random variable(s)

X = t iff t occurs at given pos

Ut = 1 iff t occurs in doc

document representation

d = ‚ü®t1, . . . , tk, . . . , tnd‚ü©, tk ‚àà V

d = ‚ü®e1, . . . , ei, . . . , eM‚ü©,

ei ‚àà {0, 1}

parameter estimation

ÀÜP(X = t|c)

ÀÜP(Ui = e|c)

decision rule: maximize

ÀÜP(c) ‚àè1‚â§k‚â§nd ÀÜP(X = tk|c)

ÀÜP(c) ‚àèti‚ààV ÀÜP(Ui = ei|c)

multiple occurrences

taken into account

ignored

length of docs

can handle longer docs

works best for short docs

# features

can handle more

works best with fewer

estimate for term the

ÀÜP(X = the|c) ‚âà 0.05

ÀÜP(Uthe = 1|c) ‚âà 1.0



model), one for each term‚Äìclass combination, rather than a number that is

at least exponential in M, the size of the vocabulary.

The independence

assumptions reduce the number of parameters to be estimated by several

orders of magnitude.

To summarize, we generate a document in the multinomial model (Fig-

ure 13.4) by Ô¨Årst picking a class C = c with P(c) where C is a random variable

RANDOM VARIABLE C

taking values from C as values. Next we generate term tk in position k with

P(Xk = tk|c) for each of the nd positions of the document. The Xk all have

the same distribution over terms for a given c. In the example in Figure 13.4,

we show the generation of ‚ü®t1, t2, t3, t4, t5‚ü© = ‚ü®Beijing, and, Taipei, join, WTO‚ü©,

corresponding to the one-sentence document Beijing and Taipei join WTO.

For a completely speciÔ¨Åed document generation model, we would also

have to deÔ¨Åne a distribution P(nd|c) over lengths. Without it, the multino-

mial model is a token generation model rather than a document generation

model.

We generate a document in the Bernoulli model (Figure 13.5) by Ô¨Årst pick-

ing a class C = c with P(c) and then generating a binary indicator ei for each

term ti of the vocabulary (1 ‚â§ i ‚â§ M). In the example in Figure 13.5, we

show the generation of ‚ü®e1, e2, e3, e4, e5, e6‚ü© = ‚ü®0, 1, 0, 1, 1, 1‚ü©, corresponding,

again, to the one-sentence document Beijing and Taipei join WTO where we

have assumed that and is a stop word.

We compare the two models in Table 13.3, including estimation equations

and decision rules.

Naive Bayes is so called because the independence assumptions we have

just made are indeed very naive for a model of natural language. The condi-

tional independence assumption states that features are independent of each

other given the class. This is hardly ever true for terms in documents. In

many cases, the opposite is true. The pairs hong and kong or london and en-


Online edition (c)ÔøΩ2009 Cambridge UP

13.4

Properties of Naive Bayes

269

‚óÆ Table 13.4

Correct estimation implies accurate prediction, but accurate predic-

tion does not imply correct estimation.

c1

c2

class selected



true probability P(c|d)

0.6

0.4

c1

ÀÜP(c) ‚àè1‚â§k‚â§nd ÀÜP(tk|c) (Equation (13.13))

0.00099

0.00001

NB estimate ÀÜP(c|d)

0.99

0.01

c1



glish in Figure 13.7 are examples of highly dependent terms. In addition, the

multinomial model makes an assumption of positional independence. The

Bernoulli model ignores positions in documents altogether because it only

cares about absence or presence. This bag-of-words model discards all in-

formation that is communicated by the order of words in natural language

sentences. How can NB be a good text classiÔ¨Åer when its model of natural

language is so oversimpliÔ¨Åed?

The answer is that even though the probability estimates of NB are of low

quality, its classiÔ¨Åcation decisions are surprisingly good. Consider a document

d with true probabilities P(c1|d) = 0.6 and P(c2|d) = 0.4 as shown in Ta-

ble 13.4. Assume that d contains many terms that are positive indicators for

c1 and many terms that are negative indicators for c2. Thus, when using the

multinomial model in Equation (13.13), ÀÜP(c1) ‚àè1‚â§k‚â§nd ÀÜP(tk|c1) will be much

larger than ÀÜP(c2) ‚àè1‚â§k‚â§nd ÀÜP(tk|c2) (0.00099 vs. 0.00001 in the table). After di-

vision by 0.001 to get well-formed probabilities for P(c|d), we end up with

one estimate that is close to 1.0 and one that is close to 0.0. This is common:

The winning class in NB classiÔ¨Åcation usually has a much larger probabil-

ity than the other classes and the estimates diverge very signiÔ¨Åcantly from

the true probabilities. But the classiÔ¨Åcation decision is based on which class

gets the highest score. It does not matter how accurate the estimates are. De-

spite the bad estimates, NB estimates a higher probability for c1 and therefore

assigns d to the correct class in Table 13.4. Correct estimation implies accurate

prediction, but accurate prediction does not imply correct estimation. NB classiÔ¨Åers

estimate badly, but often classify well.

Even if it is not the method with the highest accuracy for text, NB has many

virtues that make it a strong contender for text classiÔ¨Åcation. It excels if there

are many equally important features that jointly contribute to the classiÔ¨Å-

cation decision. It is also somewhat robust to noise features (as deÔ¨Åned in

the next section) and concept drift ‚Äì the gradual change over time of the con-

CONCEPT DRIFT

cept underlying a class like US president from Bill Clinton to George W. Bush

(see Section 13.7). ClassiÔ¨Åers like kNN (Section 14.3, page 297) can be care-

fully tuned to idiosyncratic properties of a particular time period. This will

then hurt them when documents in the following time period have slightly


Online edition (c)ÔøΩ2009 Cambridge UP

270

13

Text classiÔ¨Åcation and Naive Bayes

‚óÆ Table 13.5

A set of documents for which the NB independence assumptions are

problematic.

(1)

He moved from London, Ontario, to London, England.

(2)

He moved from London, England, to London, Ontario.

(3)

He moved from England to London, Ontario.



different properties.

The Bernoulli model is particularly robust with respect to concept drift.

We will see in Figure 13.8 that it can have decent performance when using

fewer than a dozen terms. The most important indicators for a class are less

likely to change. Thus, a model that only relies on these features is more

likely to maintain a certain level of accuracy in concept drift.

NB‚Äôs main strength is its efÔ¨Åciency: Training and classiÔ¨Åcation can be ac-

complished with one pass over the data. Because it combines efÔ¨Åciency with

good accuracy it is often used as a baseline in text classiÔ¨Åcation research.

It is often the method of choice if (i) squeezing out a few extra percentage

points of accuracy is not worth the trouble in a text classiÔ¨Åcation application,

(ii) a very large amount of training data is available and there is more to be

gained from training on a lot of data than using a better classiÔ¨Åer on a smaller

training set, or (iii) if its robustness to concept drift can be exploited.

In this book, we discuss NB as a classiÔ¨Åer for text. The independence as-

sumptions do not hold for text. However, it can be shown that NB is an

optimal classiÔ¨Åer (in the sense of minimal error rate on new data) for data

OPTIMAL CLASSIFIER

where the independence assumptions do hold.

13.4.1

A variant of the multinomial model

An alternative formalization of the multinomial model represents each doc-

ument d as an M-dimensional vector of counts ‚ü®tft1,d, . . . , tftM,d‚ü© where tfti,d

is the term frequency of ti in d. P(d|c) is then computed as follows (cf. Equa-

tion (12.8), page 243);

P(d|c) = P(‚ü®tft1,d, . . . , tftM,d‚ü©|c) ‚àù ‚àè

1‚â§i‚â§M

P(X = ti|c)tfti,d

(13.15)

Note that we have omitted the multinomial factor. See Equation (12.8) (page 243).

Equation (13.15) is equivalent to the sequence model in Equation (13.2) as

P(X = ti|c)tfti,d = 1 for terms that do not occur in d (tfti,d = 0) and a term

that occurs tfti,d ‚â• 1 times will contribute tfti,d factors both in Equation (13.2)

and in Equation (13.15).


Online edition (c)ÔøΩ2009 Cambridge UP

13.5

Feature selection

271

SELECTFEATURES(D, c, k)

1

V ‚Üê EXTRACTVOCABULARY(D)

2

L ‚Üê []

3

for each t ‚àà V

4

do A(t, c) ‚Üê COMPUTEFEATUREUTILITY(D, t, c)

5

APPEND(L, ‚ü®A(t, c), t‚ü©)

6

return FEATURESWITHLARGESTVALUES(L, k)

‚óÆ Figure 13.6

Basic feature selection algorithm for selecting the k best features.



?

Exercise 13.2

[‚ãÜ]

Which of the documents in Table 13.5 have identical and different bag of words rep-

resentations for (i) the Bernoulli model (ii) the multinomial model? If there are differ-

ences, describe them.

Exercise 13.3

The rationale for the positional independence assumption is that there is no useful

information in the fact that a term occurs in position k of a document. Find exceptions.

Consider formulaic documents with a Ô¨Åxed document structure.

Exercise 13.4

Table 13.3 gives Bernoulli and multinomial estimates for the word the. Explain the

difference.

13.5

Feature selection

Feature selection is the process of selecting a subset of the terms occurring

FEATURE SELECTION

in the training set and using only this subset as features in text classiÔ¨Åca-

tion. Feature selection serves two main purposes. First, it makes training

and applying a classiÔ¨Åer more efÔ¨Åcient by decreasing the size of the effective

vocabulary. This is of particular importance for classiÔ¨Åers that, unlike NB,

are expensive to train. Second, feature selection often increases classiÔ¨Åca-

tion accuracy by eliminating noise features. A noise feature is one that, when

NOISE FEATURE

added to the document representation, increases the classiÔ¨Åcation error on

new data. Suppose a rare term, say arachnocentric, has no information about

a class, say China, but all instances of arachnocentric happen to occur in China

documents in our training set. Then the learning method might produce a

classiÔ¨Åer that misassigns test documents containing arachnocentric to China.

Such an incorrect generalization from an accidental property of the training

set is called overÔ¨Åtting.

OVERFITTING

We can view feature selection as a method for replacing a complex clas-

siÔ¨Åer (using all features) with a simpler one (using a subset of the features).


Online edition (c)ÔøΩ2009 Cambridge UP

272

13

Text classiÔ¨Åcation and Naive Bayes

It may appear counterintuitive at Ô¨Årst that a seemingly weaker classiÔ¨Åer is

advantageous in statistical text classiÔ¨Åcation, but when discussing the bias-

variance tradeoff in Section 14.6 (page 308), we will see that weaker models

are often preferable when limited training data are available.

The basic feature selection algorithm is shown in Figure 13.6. For a given

class c, we compute a utility measure A(t, c) for each term of the vocabulary

and select the k terms that have the highest values of A(t, c). All other terms

are discarded and not used in classiÔ¨Åcation. We will introduce three different

utility measures in this section: mutual information, A(t, c) = I(Ut; Cc); the

œá2 test, A(t, c) = X2(t, c); and frequency, A(t, c) = N(t, c).

Of the two NB models, the Bernoulli model is particularly sensitive to

noise features. A Bernoulli NB classiÔ¨Åer requires some form of feature se-

lection or else its accuracy will be low.

This section mainly addresses feature selection for two-class classiÔ¨Åcation

tasks like China versus not-China. Section 13.5.5 brieÔ¨Çy discusses optimiza-

tions for systems with more than two classes.

13.5.1

Mutual information

A common feature selection method is to compute A(t, c) as the expected

mutual information (MI) of term t and class c.5 MI measures how much in-

MUTUAL INFORMATION

formation the presence/absence of a term contributes to making the correct

classiÔ¨Åcation decision on c. Formally:

I(U; C)

=

‚àë

et‚àà{1,0} ‚àë

ec‚àà{1,0}

P(U = et, C = ec) log2

P(U = et, C = ec)



P(U = et)P(C = ec),

(13.16)

where U is a random variable that takes values et = 1 (the document contains

term t) and et = 0 (the document does not contain t), as deÔ¨Åned on page 266,

and C is a random variable that takes values ec = 1 (the document is in class

c) and ec = 0 (the document is not in class c). We write Ut and Cc if it is not

clear from context which term t and class c we are referring to.

ForMLEs of the probabilities, Equation (13.16) is equivalent to Equation (13.17):

I(U; C)

=

N11



N log2

NN11



N1.N.1

+ N01



N log2

NN01



N0.N.1

(13.17)

+ N10



N log2

NN10



N1.N.0

+ N00



N log2

NN00



N0.N.0

where the Ns are counts of documents that have the values of et and ec that

are indicated by the two subscripts. For example, N10 is the number of doc-



5. Take care not to confuse expected mutual information with pointwise mutual information,

which is deÔ¨Åned as log N11/E11 where N11 and E11 are deÔ¨Åned as in Equation (13.18). The

two measures have different properties. See Section 13.7.


Online edition (c)ÔøΩ2009 Cambridge UP

13.5

Feature selection

273

uments that contain t (et = 1) and are not in c (ec = 0). N1. = N10 + N11 is

the number of documents that contain t (et = 1) and we count documents

independent of class membership (ec ‚àà {0, 1}). N = N00 + N01 + N10 + N11

is the total number of documents. An example of one of the MLE estimates

that transform Equation (13.16) into Equation (13.17) is P(U = 1, C = 1) =

N11/N.



Example 13.3:

Consider the class poultry and the term export in Reuters-RCV1.

The counts of the number of documents with the four possible combinations of indi-

cator values are as follows:

ec = epoultry = 1

ec = epoultry = 0



et = eexport = 1



N11 = 49



N10 = 27,652





et = eexport = 0



N01 = 141



N00 = 774,106





After plugging these values into Equation (13.17) we get:

I(U; C)

=

49



801,948 log2

801,948 ¬∑ 49



(49+27,652)(49+141)

+

141



801,948 log2

801,948 ¬∑ 141



(141+774,106)(49+141)

+ 27,652



801,948 log2

801,948 ¬∑ 27,652



(49+27,652)(27,652+774,106)

+ 774,106



801,948 log2

801,948 ¬∑ 774,106



(141+774,106)(27,652+774,106)

‚âà

0.0001105

To select k terms t1, . . . , tk for a given class, we use the feature selection al-

gorithm in Figure 13.6: We compute the utility measure as A(t, c) = I(Ut, Cc)

and select the k terms with the largest values.

Mutual information measures how much information ‚Äì in the information-

theoretic sense ‚Äì a term contains about the class. If a term‚Äôs distribution is

the same in the class as it is in the collection as a whole, then I(U; C) =

0. MI reaches its maximum value if the term is a perfect indicator for class

membership, that is, if the term is present in a document if and only if the

document is in the class.

Figure 13.7 shows terms with high mutual information scores for the six

classes in Figure 13.1.6 The selected terms (e.g., london, uk, british for the class

UK) are of obvious utility for making classiÔ¨Åcation decisions for their respec-

tive classes. At the bottom of the list for UK we Ô¨Ånd terms like peripherals

and tonight (not shown in the Ô¨Ågure) that are clearly not helpful in deciding



6. Feature scores were computed on the Ô¨Årst 100,000 documents, except for poultry, a rare class,

for which 800,000 documents were used. We have omitted numbers and other special words

from the top ten lists.


Online edition (c)ÔøΩ2009 Cambridge UP

274

13

Text classiÔ¨Åcation and Naive Bayes

UK





london



0.1925





uk



0.0755





british



0.0596





stg



0.0555





britain



0.0469





plc



0.0357





england



0.0238





pence



0.0212





pounds



0.0149





english



0.0126





China





china



0.0997





chinese



0.0523





beijing



0.0444





yuan



0.0344





shanghai



0.0292





hong



0.0198





kong



0.0195





xinhua



0.0155





province



0.0117





taiwan



0.0108





poultry





poultry



0.0013





meat



0.0008





chicken



0.0006





agriculture



0.0005





avian



0.0004





broiler



0.0003





veterinary



0.0003





birds



0.0003





inspection



0.0003





pathogenic



0.0003





coffee





coffee



0.0111





bags



0.0042





growers



0.0025





kg



0.0019





colombia



0.0018





brazil



0.0016





export



0.0014





exporters



0.0013





exports



0.0013





crop



0.0012





elections





election



0.0519





elections



0.0342





polls



0.0339





voters



0.0315





party



0.0303





vote



0.0299





poll



0.0225





candidate



0.0202





campaign



0.0202





democratic



0.0198





sports





soccer



0.0681





cup



0.0515





match



0.0441





matches



0.0408





played



0.0388





league



0.0386





beat



0.0301





game



0.0299





games



0.0284





team



0.0264





‚óÆ Figure 13.7

Features with high mutual information scores for six Reuters-RCV1

classes.



whether the document is in the class. As you might expect, keeping the in-

formative terms and eliminating the non-informative ones tends to reduce

noise and improve the classiÔ¨Åer‚Äôs accuracy.

Such an accuracy increase can be observed in Figure 13.8, which shows

F1 as a function of vocabulary size after feature selection for Reuters-RCV1.7

Comparing F1 at 132,776 features (corresponding to selection of all features)

and at 10‚Äì100 features, we see that MI feature selection increases F1 by about

0.1 for the multinomial model and by more than 0.2 for the Bernoulli model.

For the Bernoulli model, F1 peaks early, at ten features selected. At that point,

the Bernoulli model is better than the multinomial model. When basing a

classiÔ¨Åcation decision on only a few features, it is more robust to consider bi-

nary occurrence only. For the multinomial model (MI feature selection), the

peak occurs later, at 100 features, and its effectiveness recovers somewhat at



7. We trained the classiÔ¨Åers on the Ô¨Årst 100,000 documents and computed F1 on the next 100,000.

The graphs are averages over Ô¨Åve classes.


Online edition (c)ÔøΩ2009 Cambridge UP

13.5

Feature selection

275

#

# #

#

#

#

#

#

#

#

#

#

#

#

1

10

100

1000

10000

0.0

0.2

0.4

0.6

0.8

number of features selected

F1 measure

o

o o oo

o

o

o

o

o

o

o

o

o

x

x

x x

x

x

x

x

x

x

x

x

x

xx

b

b

b

bb

b

b

b

b

b

b

b

b

b

#

o

x

b

multinomial, MI

multinomial, chisquare

multinomial, frequency

binomial, MI

‚óÆ Figure 13.8

Effect of feature set size on accuracy for multinomial and Bernoulli

models.



the end when we use all features. The reason is that the multinomial takes

the number of occurrences into account in parameter estimation and clas-

siÔ¨Åcation and therefore better exploits a larger number of features than the

Bernoulli model. Regardless of the differences between the two methods,

using a carefully selected subset of the features results in better effectiveness

than using all features.

13.5.2

œá2 Feature selection

Another popular feature selection method is œá2. In statistics, the œá2 test is

œá2 FEATURE SELECTION

applied to test the independence of two events, where two events A and B are

deÔ¨Åned to be independent if P(AB) = P(A)P(B) or, equivalently, P(A|B) =

INDEPENDENCE

P(A) and P(B|A) = P(B). In feature selection, the two events are occurrence

of the term and occurrence of the class. We then rank terms with respect to

the following quantity:

X2(D, t, c) =

‚àë

et‚àà{0,1} ‚àë

ec‚àà{0,1}

(Netec ‚àí Eetec)2



Eetec

(13.18)


Online edition (c)ÔøΩ2009 Cambridge UP

276

13

Text classiÔ¨Åcation and Naive Bayes

where et and ec are deÔ¨Åned as in Equation (13.16). N is the observed frequency

in D and E the expected frequency. For example, E11 is the expected frequency

of t and c occurring together in a document assuming that term and class are

independent.



Example 13.4:

We Ô¨Årst compute E11 for the data in Example 13.3:

E11

=

N √ó P(t) √ó P(c) = N √ó N11 + N10



N

√ó N11 + N01



N

=

N √ó 49 + 141



N

√ó 49 + 27652



N

‚âà 6.6

where N is the total number of documents as before.

We compute the other Eetec in the same way:

epoultry = 1

epoultry = 0



eexport = 1



N11 = 49

E11 ‚âà 6.6



N10 = 27,652

E10 ‚âà 27,694.4





eexport = 0



N01 = 141

E01 ‚âà 183.4



N00 = 774,106

E00 ‚âà 774,063.6





Plugging these values into Equation (13.18), we get a X2 value of 284:

X2(D, t, c) =

‚àë

et‚àà{0,1}

‚àë

ec‚àà{0,1}

(Netec ‚àí Eetec)2



Eetec

‚âà 284

X2 is a measure of how much expected counts E and observed counts N

deviate from each other. A high value of X2 indicates that the hypothesis of

independence, which implies that expected and observed counts are similar,

is incorrect. In our example, X2 ‚âà 284 &gt; 10.83. Based on Table 13.6, we

can reject the hypothesis that poultry and export are independent with only a

0.001 chance of being wrong.8 Equivalently, we say that the outcome X2 ‚âà

284 &gt; 10.83 is statistically signiÔ¨Åcant at the 0.001 level. If the two events are

STATISTICAL

SIGNIFICANCE

dependent, then the occurrence of the term makes the occurrence of the class

more likely (or less likely), so it should be helpful as a feature. This is the

rationale of œá2 feature selection.

An arithmetically simpler way of computing X2 is the following:

X2(D, t, c) =

(N11 + N10 + N01 + N00) √ó (N11N00 ‚àí N10N01)2



(N11 + N01) √ó (N11 + N10) √ó (N10 + N00) √ó (N01 + N00)

(13.19)

This is equivalent to Equation (13.18) (Exercise 13.14).



8. We can make this inference because, if the two events are independent, then X2 ‚àº œá2, where

œá2 is the œá2 distribution. See, for example, Rice (2006).


Online edition (c)ÔøΩ2009 Cambridge UP

13.5

Feature selection

277

‚óÆ Table 13.6

Critical values of the œá2 distribution with one degree of freedom. For

example, if the two events are independent, then P(X2 &gt; 6.63) &lt; 0.01. So for X2 &gt;

6.63 the assumption of independence can be rejected with 99% conÔ¨Ådence.

p

œá2 critical value



0.1

2.71

0.05

3.84

0.01

6.63

0.005

7.88

0.001

10.83



ÔøΩ

Assessing œá2 as a feature selection method

From a statistical point of view, œá2 feature selection is problematic. For a

test with one degree of freedom, the so-called Yates correction should be

used (see Section 13.7), which makes it harder to reach statistical signiÔ¨Åcance.

Also, whenever a statistical test is used multiple times, then the probability

of getting at least one error increases. If 1,000 hypotheses are rejected, each

with 0.05 error probability, then 0.05 √ó 1000 = 50 calls of the test will be

wrong on average. However, in text classiÔ¨Åcation it rarely matters whether a

few additional terms are added to the feature set or removed from it. Rather,

the relative importance of features is important. As long as œá2 feature selec-

tion only ranks features with respect to their usefulness and is not used to

make statements about statistical dependence or independence of variables,

we need not be overly concerned that it does not adhere strictly to statistical

theory.

13.5.3

Frequency-based feature selection

A third feature selection method is frequency-based feature selection, that is,

selecting the terms that are most common in the class. Frequency can be

either deÔ¨Åned as document frequency (the number of documents in the class

c that contain the term t) or as collection frequency (the number of tokens of

t that occur in documents in c). Document frequency is more appropriate for

the Bernoulli model, collection frequency for the multinomial model.

Frequency-based feature selection selects some frequent terms that have

no speciÔ¨Åc information about the class, for example, the days of the week

(Monday, Tuesday, ...), which are frequent across classes in newswire text.

When many thousands of features are selected, then frequency-based fea-

ture selection often does well. Thus, if somewhat suboptimal accuracy is

acceptable, then frequency-based feature selection can be a good alternative

to more complex methods. However, Figure 13.8 is a case where frequency-


Online edition (c)ÔøΩ2009 Cambridge UP

278

13

Text classiÔ¨Åcation and Naive Bayes

based feature selection performs a lot worse than MI and œá2 and should not

be used.

13.5.4

Feature selection for multiple classiÔ¨Åers

In an operational system with a large number of classiÔ¨Åers, it is desirable

to select a single set of features instead of a different one for each classiÔ¨Åer.

One way of doing this is to compute the X2 statistic for an n √ó 2 table where

the columns are occurrence and nonoccurrence of the term and each row

corresponds to one of the classes. We can then select the k terms with the

highest X2 statistic as before.

More commonly, feature selection statistics are Ô¨Årst computed separately

for each class on the two-class classiÔ¨Åcation task c versus



c and then com-

bined. One combination method computes a single Ô¨Ågure of merit for each

feature, for example, by averaging the values A(t, c) for feature t, and then

selects the k features with highest Ô¨Ågures of merit. Another frequently used

combination method selects the top k/n features for each of n classiÔ¨Åers and

then combines these n sets into one global feature set.

ClassiÔ¨Åcation accuracy often decreases when selecting k common features

for a system with n classiÔ¨Åers as opposed to n different sets of size k. But even

if it does, the gain in efÔ¨Åciency owing to a common document representation

may be worth the loss in accuracy.

13.5.5

Comparison of feature selection methods

Mutual information and œá2 represent rather different feature selection meth-

ods. The independence of term t and class c can sometimes be rejected with

high conÔ¨Ådence even if t carries little information about membership of a

document in c. This is particularly true for rare terms. If a term occurs once

in a large collection and that one occurrence is in the poultry class, then this

is statistically signiÔ¨Åcant. But a single occurrence is not very informative

according to the information-theoretic deÔ¨Ånition of information.

Because

its criterion is signiÔ¨Åcance, œá2 selects more rare terms (which are often less

reliable indicators) than mutual information. But the selection criterion of

mutual information also does not necessarily select the terms that maximize

classiÔ¨Åcation accuracy.

Despite the differences between the two methods, the classiÔ¨Åcation accu-

racy of feature sets selected with œá2 and MI does not seem to differ systemat-

ically. In most text classiÔ¨Åcation problems, there are a few strong indicators

and many weak indicators. As long as all strong indicators and a large num-

ber of weak indicators are selected, accuracy is expected to be good. Both

methods do this.

Figure 13.8 compares MI and œá2 feature selection for the multinomial model.


Online edition (c)ÔøΩ2009 Cambridge UP

13.6

Evaluation of text classiÔ¨Åcation

279

Peak effectiveness is virtually the same for both methods. œá2 reaches this

peak later, at 300 features, probably because the rare, but highly signiÔ¨Åcant

features it selects initially do not cover all documents in the class. However,

features selected later (in the range of 100‚Äì300)are of better quality than those

selected by MI.

All three methods ‚Äì MI, œá2 and frequency based ‚Äì are greedy methods.

GREEDY FEATURE

SELECTION

They may select features that contribute no incremental information over

previously selected features. In Figure 13.7, kong is selected as the seventh

term even though it is highly correlated with previously selected hong and

therefore redundant. Although such redundancy can negatively impact ac-

curacy, non-greedy methods (see Section 13.7 for references) are rarely used

in text classiÔ¨Åcation due to their computational cost.

?

Exercise 13.5

Consider the following frequencies for the class coffee for four terms in the Ô¨Årst 100,000

documents of Reuters-RCV1:

term



N00

N01

N10

N11



brazil



98,012

102

1835

51

council



96,322

133

3525

20

producers



98,524

119

1118

34

roasted



99,824

143

23

10

Select two of these four terms based on (i) œá2, (ii) mutual information, (iii) frequency.

13.6

Evaluation of text classiÔ¨Åcation

] Historically, the classic Reuters-21578 collection was the main benchmark

for text classiÔ¨Åcation evaluation. This is a collection of 21,578 newswire ar-

ticles, originally collected and labeled by Carnegie Group, Inc. and Reuters,

Ltd. in the course of developing the CONSTRUE text classiÔ¨Åcation system.

It is much smaller than and predates the Reuters-RCV1 collection discussed

in Chapter 4 (page 69). The articles are assigned classes from a set of 118

topic categories. A document may be assigned several classes or none, but

the commonest case is single assignment (documents with at least one class

received an average of 1.24 classes). The standard approach to this any-of

problem (Chapter 14, page 306) is to learn 118 two-class classiÔ¨Åers, one for

each class, where the two-class classiÔ¨Åer for class c is the classiÔ¨Åer for the two

TWO-CLASS CLASSIFIER

classes c and its complement



c.

For each of these classiÔ¨Åers, we can measure recall, precision, and accu-

racy. In recent work, people almost invariably use the ModApte split, which

MODAPTE SPLIT

includes only documents that were viewed and assessed by a human indexer,


Online edition (c)ÔøΩ2009 Cambridge UP

280

13

Text classiÔ¨Åcation and Naive Bayes

‚óÆ Table 13.7

The ten largest classes in the Reuters-21578 collection with number of

documents in training and test sets.

class

# train

# testclass

# train

# test



earn

2877

1087 trade

369

119

acquisitions

1650

179 interest

347

131

money-fx

538

179 ship

197

89

grain

433

149 wheat

212

71

crude

389

189 corn

182

56



and comprises 9,603 training documents and 3,299 test documents. The dis-

tribution of documents in classes is very uneven, and some work evaluates

systems on only documents in the ten largest classes. They are listed in Ta-

ble 13.7. A typical document with topics is shown in Figure 13.9.

In Section 13.1, we stated as our goal in text classiÔ¨Åcation the minimization

of classiÔ¨Åcation error on test data. ClassiÔ¨Åcation error is 1.0 minus classiÔ¨Åca-

tion accuracy, the proportion of correct decisions, a measure we introduced

in Section 8.3 (page 155). This measure is appropriate if the percentage of

documents in the class is high, perhaps 10% to 20% and higher. But as we

discussed in Section 8.3, accuracy is not a good measure for ‚Äúsmall‚Äù classes

because always saying no, a strategy that defeats the purpose of building a

classiÔ¨Åer, will achieve high accuracy. The always-no classiÔ¨Åer is 99% accurate

for a class with relative frequency 1%. For small classes, precision, recall and

F1 are better measures.

We will use effectiveness as a generic term for measures that evaluate the

EFFECTIVENESS

quality of classiÔ¨Åcation decisions, including precision, recall, F1, and accu-

racy. Performance refers to the computational efÔ¨Åciency of classiÔ¨Åcation and

PERFORMANCE

EFFICIENCY

IR systems in this book. However, many researchers mean effectiveness, not

efÔ¨Åciency of text classiÔ¨Åcation when they use the term performance.

When we process a collection with several two-class classiÔ¨Åers (such as

Reuters-21578 with its 118 classes), we often want to compute a single ag-

gregate measure that combines the measures for individual classiÔ¨Åers. There

are two methods for doing this. Macroaveraging computes a simple aver-

MACROAVERAGING

age over classes. Microaveraging pools per-document decisions across classes,

MICROAVERAGING

and then computes an effectiveness measure on the pooled contingency ta-

ble. Table 13.8 gives an example.

The differences between the two methods can be large. Macroaveraging

gives equal weight to each class, whereas microaveraging gives equal weight

to each per-document classiÔ¨Åcation decision. Because the F1 measure ignores

true negatives and its magnitude is mostly determined by the number of

true positives, large classes dominate small classes in microaveraging. In the

example, microaveraged precision (0.83) is much closer to the precision of


Online edition (c)ÔøΩ2009 Cambridge UP

13.6

Evaluation of text classiÔ¨Åcation

281

&lt;REUTERS TOPICS=‚Äô‚ÄôYES‚Äô‚Äô LEWISSPLIT=‚Äô‚ÄôTRAIN‚Äô‚Äô

CGISPLIT=‚Äô‚ÄôTRAINING-SET‚Äô‚Äô OLDID=‚Äô‚Äô12981‚Äô‚Äô NEWID=‚Äô‚Äô798‚Äô‚Äô&gt;

&lt;DATE&gt; 2-MAR-1987 16:51:43.42&lt;/DATE&gt;

&lt;TOPICS&gt;&lt;D&gt;livestock&lt;/D&gt;&lt;D&gt;hog&lt;/D&gt;&lt;/TOPICS&gt;

&lt;TITLE&gt;AMERICAN PORK CONGRESS KICKS OFF TOMORROW&lt;/TITLE&gt;

&lt;DATELINE&gt; CHICAGO, March 2 - &lt;/DATELINE&gt;&lt;BODY&gt;The American Pork

Congress kicks off tomorrow, March 3, in Indianapolis with 160

of the nations pork producers from 44 member states determining

industry positions on a number of issues, according to the

National Pork Producers Council, NPPC.

Delegates to the three day Congress will be considering 26

resolutions concerning various issues, including the future

direction of farm policy and the tax law as it applies to the

agriculture sector. The delegates will also debate whether to

endorse concepts of a national PRV (pseudorabies virus) control

and eradication program, the NPPC said. A large

trade show, in conjunction with the congress, will feature

the latest in technology in all areas of the industry, the NPPC

added. Reuter

\&amp;\#3;&lt;/BODY&gt;&lt;/TEXT&gt;&lt;/REUTERS&gt;

‚óÆ Figure 13.9

A sample document from the Reuters-21578 collection.



c2 (0.9) than to the precision of c1 (0.5) because c2 is Ô¨Åve times larger than

c1. Microaveraged results are therefore really a measure of effectiveness on

the large classes in a test collection. To get a sense of effectiveness on small

classes, you should compute macroaveraged results.

In one-of classiÔ¨Åcation (Section 14.5, page 306), microaveraged F1 is the

same as accuracy (Exercise 13.6).

Table 13.9 gives microaveraged and macroaveraged effectiveness of Naive

Bayes for the ModApte split of Reuters-21578. To give a sense of the relative

effectiveness of NB, we compare it with linear SVMs (rightmost column; see

Chapter 15), one of the most effective classiÔ¨Åers, but also one that is more

expensive to train than NB. NB has a microaveraged F1 of 80%, which is

9% less than the SVM (89%), a 10% relative decrease (row ‚Äúmicro-avg-L (90

classes)‚Äù). So there is a surprisingly small effectiveness penalty for its sim-

plicity and efÔ¨Åciency. However, on small classes, some of which only have on

the order of ten positive examples in the training set, NB does much worse.

Its macroaveraged F1 is 13% below the SVM, a 22% relative decrease (row

‚Äúmacro-avg (90 classes)‚Äù).

The table also compares NB with the other classiÔ¨Åers we cover in this book:


Online edition (c)ÔøΩ2009 Cambridge UP

282

13

Text classiÔ¨Åcation and Naive Bayes

‚óÆ Table 13.8

Macro- and microaveraging. ‚ÄúTruth‚Äù is the true class and ‚Äúcall‚Äù the

decision of the classiÔ¨Åer. In this example, macroaveraged precision is [10/(10 + 10) +

90/(10 + 90)]/2 = (0.5 + 0.9)/2 = 0.7. Microaveraged precision is 100/(100 + 20) ‚âà

0.83.

class 1



truth:



truth:



yes



no



call:

yes



10



10



call:

no



10



970

class 2



truth:



truth:



yes



no



call:

yes



90



10



call:

no



10



890

pooled table



truth:



truth:



yes



no



call:

yes



100



20



call:

no



20



1860

‚óÆ Table 13.9

Text classiÔ¨Åcation effectiveness numbers on Reuters-21578 for F1 (in

percent). Results from Li and Yang (2003) (a), Joachims (1998) (b: kNN) and Dumais

et al. (1998) (b: NB, Rocchio, trees, SVM).

(a)

NB

Rocchio

kNN

SVM



micro-avg-L (90 classes)

80

85

86

89

macro-avg (90 classes)

47

59

60

60

(b)

NB

Rocchio

kNN

trees

SVM



earn

96

93

97

98

98

acq

88

65

92

90

94

money-fx

57

47

78

66

75

grain

79

68

82

85

95

crude

80

70

86

85

89

trade

64

65

77

73

76

interest

65

63

74

67

78

ship

85

49

79

74

86

wheat

70

69

77

93

92

corn

65

48

78

92

90



micro-avg (top 10)

82

65

82

88

92

micro-avg-D (118 classes)

75

62

n/a

n/a

87



Rocchio and kNN. In addition, we give numbers for decision trees, an impor-

DECISION TREES

tant classiÔ¨Åcation method we do not cover. The bottom part of the table

shows that there is considerable variation from class to class. For instance,

NB beats kNN on ship, but is much worse on money-fx.

Comparing parts (a) and (b) of the table, one is struck by the degree to

which the cited papers‚Äô results differ. This is partly due to the fact that the

numbers in (b) are break-even scores (cf. page 161) averaged over 118 classes,

whereas the numbers in (a) are true F1 scores (computed without any know-


Online edition (c)ÔøΩ2009 Cambridge UP

13.6

Evaluation of text classiÔ¨Åcation

283

ledge of the test set) averaged over ninety classes. This is unfortunately typ-

ical of what happens when comparing different results in text classiÔ¨Åcation:

There are often differences in the experimental setup or the evaluation that

complicate the interpretation of the results.

These and other results have shown that the average effectiveness of NB

is uncompetitive with classiÔ¨Åers like SVMs when trained and tested on inde-

pendent and identically distributed (i.i.d.) data, that is, uniform data with all the

good properties of statistical sampling. However, these differences may of-

ten be invisible or even reverse themselves when working in the real world

where, usually, the training sample is drawn from a subset of the data to

which the classiÔ¨Åer will be applied, the nature of the data drifts over time

rather than being stationary (the problem of concept drift we mentioned on

page 269), and there may well be errors in the data (among other problems).

Many practitioners have had the experience of being unable to build a fancy

classiÔ¨Åer for a certain problem that consistently performs better than NB.

Our conclusion from the results in Table 13.9 is that, although most re-

searchers believe that an SVM is better than kNN and kNN better than NB,

the ranking of classiÔ¨Åers ultimately depends on the class, the document col-

lection, and the experimental setup. In text classiÔ¨Åcation, there is always

more to know than simply which machine learning algorithm was used, as

we further discuss in Section 15.3 (page 334).

When performing evaluations like the one in Table 13.9, it is important to

maintain a strict separation between the training set and the test set. We can

easily make correct classiÔ¨Åcation decisions on the test set by using informa-

tion we have gleaned from the test set, such as the fact that a particular term

is a good predictor in the test set (even though this is not the case in the train-

ing set). A more subtle example of using knowledge about the test set is to

try a large number of values of a parameter (e.g., the number of selected fea-

tures) and select the value that is best for the test set. As a rule, accuracy on

new data ‚Äì the type of data we will encounter when we use the classiÔ¨Åer in

an application ‚Äì will be much lower than accuracy on a test set that the clas-

siÔ¨Åer has been tuned for. We discussed the same problem in ad hoc retrieval

in Section 8.1 (page 153).

In a clean statistical text classiÔ¨Åcation experiment, you should never run

any program on or even look at the test set while developing a text classiÔ¨Åca-

tion system. Instead, set aside a development set for testing while you develop

DEVELOPMENT SET

your method. When such a set serves the primary purpose of Ô¨Ånding a good

value for a parameter, for example, the number of selected features, then it

is also called held-out data. Train the classiÔ¨Åer on the rest of the training set

HELD-OUT DATA

with different parameter values, and then select the value that gives best re-

sults on the held-out part of the training set. Ideally, at the very end, when

all parameters have been set and the method is fully speciÔ¨Åed, you run one

Ô¨Ånal experiment on the test set and publish the results. Because no informa-


Online edition (c)ÔøΩ2009 Cambridge UP

284

13

Text classiÔ¨Åcation and Naive Bayes

‚óÆ Table 13.10

Data for parameter estimation exercise.

docID

words in document

in c = China?



training set

1

Taipei Taiwan

yes

2

Macao Taiwan Shanghai

yes

3

Japan Sapporo

no

4

Sapporo Osaka Taiwan

no



test set

5

Taiwan Taiwan Sapporo

?



tion about the test set was used in developing the classiÔ¨Åer, the results of this

experiment should be indicative of actual performance in practice.

This ideal often cannot be met; researchers tend to evaluate several sys-

tems on the same test set over a period of several years. But it is neverthe-

less highly important to not look at the test data and to run systems on it as

sparingly as possible. Beginners often violate this rule, and their results lose

validity because they have implicitly tuned their system to the test data sim-

ply by running many variant systems and keeping the tweaks to the system

that worked best on the test set.

?

Exercise 13.6

[‚ãÜ‚ãÜ]

Assume a situation where every document in the test collection has been assigned

exactly one class, and that a classiÔ¨Åer also assigns exactly one class to each document.

This setup is called one-of classiÔ¨Åcation (Section 14.5, page 306). Show that in one-of

classiÔ¨Åcation (i) the total number of false positive decisions equals the total number

of false negative decisions and (ii) microaveraged F1 and accuracy are identical.

Exercise 13.7

The class priors in Figure 13.2 are computed as the fraction of documents in the class

as opposed to the fraction of tokens in the class. Why?

Exercise 13.8

The function APPLYMULTINOMIALNB in Figure 13.2 has time complexity Œò(La +

|C|La). How would you modify the function so that its time complexity is Œò(La +

|C|Ma)?

Exercise 13.9

Based on the data in Table 13.10, (i) estimate a multinomial Naive Bayes classiÔ¨Åer, (ii)

apply the classiÔ¨Åer to the test document, (iii) estimate a Bernoulli NB classiÔ¨Åer, (iv)

apply the classiÔ¨Åer to the test document. You need not estimate parameters that you

don‚Äôt need for classifying the test document.

Exercise 13.10

Your task is to classify words as English or not English. Words are generated by a

source with the following distribution:


Online edition (c)ÔøΩ2009 Cambridge UP

13.6

Evaluation of text classiÔ¨Åcation

285

event

word

English?

probability



1

ozb

no

4/9

2

uzu

no

4/9

3

zoo

yes

1/18

4

bun

yes

1/18

(i) Compute the parameters (priors and conditionals) of a multinomial NB classi-

Ô¨Åer that uses the letters b, n, o, u, and z as features. Assume a training set that

reÔ¨Çects the probability distribution of the source perfectly. Make the same indepen-

dence assumptions that are usually made for a multinomial classiÔ¨Åer that uses terms

as features for text classiÔ¨Åcation. Compute parameters using smoothing, in which

computed-zero probabilities are smoothed into probability 0.01, and computed-nonzero

probabilities are untouched. (This simplistic smoothing may cause P(A) + P(



A) &gt; 1.

Solutions are not required to correct this.) (ii) How does the classiÔ¨Åer classify the

word zoo? (iii) Classify the word zoo using a multinomial classiÔ¨Åer as in part (i), but

do not make the assumption of positional independence. That is, estimate separate

parameters for each position in a word. You only need to compute the parameters

you need for classifying zoo.

Exercise 13.11

What are the values of I(Ut; Cc) and X2(D, t, c) if term and class are completely inde-

pendent? What are the values if they are completely dependent?

Exercise 13.12

The feature selection method in Equation (13.16) is most appropriate for the Bernoulli

model. Why? How could one modify it for the multinomial model?

Exercise 13.13

Features can also be selected according toinformation gain (IG), which is deÔ¨Åned as:

INFORMATION GAIN

IG(D, t, c) = H(pD) ‚àí

‚àë

x‚àà{Dt+,Dt‚àí}

|x|



|D| H(px)

where H is entropy, D is the training set, and Dt+, and Dt‚àí are the subset of D with

term t, and the subset of D without term t, respectively. pA is the class distribution

in (sub)collection A, e.g., pA(c) = 0.25, pA(



c) = 0.75 if a quarter of the documents in

A are in class c.

Show that mutual information and information gain are equivalent.

Exercise 13.14

Show that the two X2 formulas (Equations (13.18) and (13.19)) are equivalent.

Exercise 13.15

In the œá2 example on page 276 we have |N11 ‚àí E11| = |N10 ‚àí E10| = |N01 ‚àí E01| =

|N00 ‚àí E00|. Show that this holds in general.

Exercise 13.16

œá2 and mutual information do not distinguish between positively and negatively cor-

related features. Because most good text classiÔ¨Åcation features are positively corre-

lated (i.e., they occur more often in c than in



c), one may want to explicitly rule out

the selection of negative indicators. How would you do this?


Online edition (c)ÔøΩ2009 Cambridge UP

286

13

Text classiÔ¨Åcation and Naive Bayes

13.7

References and further reading

General introductions to statistical classiÔ¨Åcation and machine learning can be

found in (Hastie et al. 2001), (Mitchell 1997), and (Duda et al. 2000), including

many important methods (e.g., decision trees and boosting) that we do not

cover. A comprehensive review of text classiÔ¨Åcation methods and results is

(Sebastiani 2002). Manning and Sch√ºtze (1999, Chapter 16) give an accessible

introduction to text classiÔ¨Åcation with coverage of decision trees, perceptrons

and maximum entropy models. More information on the superlinear time

complexity of learning methods that are more accurate than Naive Bayes can

be found in (Perkins et al. 2003) and (Joachims 2006a).

Maron and Kuhns (1960) described one of the Ô¨Årst NB text classiÔ¨Åers. Lewis

(1998) focuses on the history of NB classiÔ¨Åcation. Bernoulli and multinomial

models and their accuracy for different collections are discussed by McCal-

lum and Nigam (1998). Eyheramendy et al. (2003) present additional NB

models. Domingos and Pazzani (1997), Friedman (1997), and Hand and Yu

(2001) analyze why NB performs well although its probability estimates are

poor. The Ô¨Årst paper also discusses NB‚Äôs optimality when the independence

assumptions are true of the data. Pavlov et al. (2004) propose a modiÔ¨Åed

document representation that partially addresses the inappropriateness of

the independence assumptions. Bennett (2000) attributes the tendency of NB

probability estimates to be close to either 0 or 1 to the effect of document

length. Ng and Jordan (2001) show that NB is sometimes (although rarely)

superior to discriminative methods because it more quickly reaches its opti-

mal error rate. The basic NB model presented in this chapter can be tuned for

better effectiveness (Rennie et al. 2003;Ko≈Çcz and Yih 2007). The problem of

concept drift and other reasons why state-of-the-art classiÔ¨Åers do not always

excel in practice are discussed by Forman (2006) and Hand (2006).

Early uses of mutual information and œá2 for feature selection in text clas-

siÔ¨Åcation are Lewis and Ringuette (1994) and Sch√ºtze et al. (1995), respec-

tively. Yang and Pedersen (1997) review feature selection methods and their

impact on classiÔ¨Åcation effectiveness. They Ô¨Ånd that pointwise mutual infor-

POINTWISE MUTUAL

INFORMATION

mation is not competitive with other methods. Yang and Pedersen refer to

expected mutual information (Equation (13.16)) as information gain (see Ex-

ercise 13.13, page 285). (Snedecor and Cochran 1989) is a good reference for

the œá2 test in statistics, including the Yates‚Äô correction for continuity for 2 √ó 2

tables. Dunning (1993) discusses problems of the œá2 test when counts are

small. Nongreedy feature selection techniques are described by Hastie et al.

(2001). Cohen (1995) discusses the pitfalls of using multiple signiÔ¨Åcance tests

and methods to avoid them. Forman (2004) evaluates different methods for

feature selection for multiple classiÔ¨Åers.

David D. Lewis deÔ¨Ånes the ModApte split at www.daviddlewis.com/resources/testcollections/reuters215

based on Apt√© et al. (1994). Lewis (1995) describes utility measures for the

UTILITY MEASURE


Online edition (c)ÔøΩ2009 Cambridge UP

13.7

References and further reading

287

evaluation of text classiÔ¨Åcation systems. Yang and Liu (1999) employ signif-

icance tests in the evaluation of text classiÔ¨Åcation methods.

Lewis et al. (2004) Ô¨Ånd that SVMs (Chapter 15) perform better on Reuters-

RCV1 than kNN and Rocchio (Chapter 14).

