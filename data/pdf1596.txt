
Structural Language Models of Code

Uri Alon 1 Roy Sadaka 1 Omer Levy 2 3 Eran Yahav 1

Abstract

We address the problem of any-code comple-

tion – generating a missing piece of source

code in a given program without any restric-

tion on the vocabulary or structure. We intro-

duce a new approach to any-code completion

that leverages the strict syntax of programming

languages to model a code snippet as a tree –

structural language modeling (SLM). SLM es-

timates the probability of the program’s abstract

syntax tree (AST) by decomposing it into a prod-

uct of conditional probabilities over its nodes.

We present a neural model that computes these

conditional probabilities by considering all AST

paths leading to a target node.

Unlike previ-

ous techniques that have severely restricted the

kinds of expressions that can be generated in this

task, our approach can generate arbitrary code in

any programming language. Our model signif-

icantly outperforms both seq2seq and a variety

of structured approaches in generating Java and

C# code. Our code, data, and trained models are

available at http://github.com/tech-srl/

slm-code-generation/. An online demo is

available at http://AnyCodeGen.org.

1. Introduction

Code completion is the problem of generating code given

its surrounding code as context. In its most general form,

this problem is extremely challenging as it requires reason-

ing over an unbounded number of syntactic structures and

user-deﬁned symbols. Previous approaches have avoided

this issue by limiting the generation problem: program

synthesis approaches are often tailored to domain-speciﬁc

languages (Gulwani, 2011; Polozov &amp; Gulwani, 2015; De-

vlin et al., 2017; Ellis et al., 2019), while other recent ap-

proaches generate code in general languages like Java and

1Technion,

Israel

2Tel

Aviv

University

3Facebook

AI

Research.

Correspondence

to:

Uri

Alon

&lt;urialon@cs.technion.ac.il&gt;,

Roy

Sadaka

&lt;roysadaka@gmail.com&gt;,

Omer

Levy

&lt;omer-

levy@gmail.com&gt;, Eran Yahav &lt;yahave@cs.technion.ac.il&gt;.

C#, but severely restrict the syntax, vocabulary, domain,

or nature of the generated programs (Murali et al., 2018;

Brockschmidt et al., 2019; Young et al., 2019).

We introduce the task of any-code completion – generating

code in a general-purpose programming language without

any restriction on its vocabulary or structure. Speciﬁcally,

we focus on generating code in context: given a program

P and some part of the program p, the task is to predict

p from the rest of the program P−=P\p. Any-code com-

pletion thus generalizes the restricted completion task of

Brockschmidt et al. (2019), in which the target code con-

tained only primitive types (e.g., int and string) and ex-

cluded user-deﬁned functions. Figure 1 shows two any-

code completion examples.

In related tasks such as semantic parsing (Dong &amp; Lapata,

2018; Yu et al., 2018; Iyer et al., 2019), natural-language-

to-code (Allamanis et al., 2015; Iyer et al., 2018), and edit-

to-code (Yin et al., 2019; Zhao et al., 2019), models must

use separate encoders and decoders because of the different

modalities of the input (e.g. natural language text) and the

output (code). In contrast, we leverage the fact that our in-

put and output are of the same modality (code), and pursue

better generalization by modeling them jointly.

We present a new approach that explicitly models the

source and the target code as the same tree – structural

language modeling (SLM). SLM estimates the probability

of the program’s abstract syntax tree (AST) by decompos-

ing it into a product of conditional probabilities over its

nodes. We present a neural model that computes these con-

ditional probabilities by considering all AST paths lead-

ing to a target node, generalizing over traditional language

models that consider sequences of words. While prior work

uses AST paths to read programs (Alon et al., 2019b), we

generate code by predicting the next node along the set of

paths, generating the target AST node-by-node.

We evaluate SLMs on Java any-code completion, achieving

a new state of the art: exact-match accuracy@1 of 18.04%

and accuracy@5 of 24.83% (previous SOTA: 16.93% and

23.17%). SLMs also outperform existing models in the re-

stricted completion task of Brockschmidt et al. (2019) in

C# by a wide margin, 37.61% accuracy@1 compared to

26.42%. Our ablation study reveals the importance of joint

modeling of the source and target code, rather than sep-

arXiv:1910.00577v4  [cs.LG]  29 Jul 2020


Structural Language Models of Code

public static Path[] stat2Paths(

FileStatus[] stats) {

if (stats == null) return null;

Path[] ret = new Path[stats.length];

for (int i = 0; i &lt; stats.length; ++i){

ret[i] =

;

}

return ret;

}

public static string Camelize(

this string input)

{

var word = input.Pascalize();

return word.Length &gt; 0 ?

.ToLower()

+ word.Substring(1)

: word;

}

True ref (Java):

stats[i].getPath()

SLM

top-5:

(25.2%)

stats[i].getPath()

(3.3%)

Path(stats[i])

(2.5%)

new Path(stats[i], charset)

(1.7%)

stat(stats[i], ret)

(0.8%)

new Path(stats[i])

(a)

True ref (C#):

word.Substring(0, 1)

SLM

top-5:

(14.1%)

word.Substring(0, 1)

(8.2%)

word.trim()

(5.8%)

word.Substring(1)

(2.4%)

input.Substring(0, 1)

(1.9%)

wordValue.Substring(0, 1)

(b)

Figure 1. Examples from the Java (left) and C# (right) test sets. The highlighted expression in each example is the target p, which our

models correctly generated from the rest of the snippet. Additional and larger examples can be found in Appendices F and G.

arating encoders from decoders. Finally, we discuss the

theoretical advantages of SLMs, and show how they gen-

eralize many previous structural approaches for code gen-

eration. An interactive demo of our model is presented at

http://AnyCodeGen.org.

2. Code Generation as Structural Language

Modeling

We model the task of any-code completion by comput-

ing the probability of a program Pr (P), similar to how a

language model computes the probability of a natural lan-

guage sentence. While language models typically assume a

sequence as their input, our input is an abstract syntax tree

AP. We thus introduce a structural language modeling ap-

proach (SLM).

The intuition behind this idea is that a language model

could generalize better by modeling the tree rather than the

sequential form of the program. Further, learning from the

AST allows a model to save learning capacity, instead of

having to re-learn known syntactic patterns from the text.

We ﬁrst show a chain-rule decomposition of the tree’s prob-

ability Pr (AP) into a product of conditional node proba-

bilities, and then describe our path-based model for com-

puting the individual conditional probabilities. We explain

how to construct a tree from local node predictions, and ﬁ-

nally discuss how our approach differs from previous work

on production-based tree generation.

Representing Code as a Tree A program P is a sequence

of tokens that can be unambiguously mapped to an abstract

syntax tree (AST) AP, where every node represents an el-

ement in the language (e.g. conditions, loops, variable dec-

larations) from a set T . Each AST leaf (terminal) has an

associated user-deﬁned value v ∈ V. Nonterminal nodes

can have a varying number of children nodes.

Decomposing the Probability of a Tree Given a tree AP,

we ﬁrst traverse the tree, depth-ﬁrst,1 to induce an ordering

over its nodes a0, . . . , a|AP| ∈ AP. We decompose the

probability of a tree Pr (AP) using the chain rule, akin to

the standard approach in language modeling:

Pr (AP) =

�

t

Pr (at|a&lt;t)

(1)

where a&lt;t are all the nodes that were traversed before at.

In any-code completion, part of the tree (AP−) is already

observed. Therefore, we order the nodes of AP− to be be-

fore the nodes of the target p, and compute only the condi-

tional probabilities over the nodes in p, essentially condi-

tioning on the observed tree AP−.

Representing Partial Trees via Paths How can we rep-

resent the partial tree composed of a&lt;t when computing

Pr (at|a&lt;t)? In standard language modeling, the structure

is linear, and a&lt;t is a sequence. One way to represent a

partial tree is to linearize it according to the traversal order

(Xiao et al., 2016); however, this creates artiﬁcially long

distances between the current node at and ancestor nodes

(e.g., the root a0). Another option is to use only the path

from the root node to at (Rabinovich et al., 2017), but this

ignores a lot of contextual information (e.g., sibling nodes).

We follow Alon et al. (2018) and use the set of paths from

every leaf to at together with the path from the root to

1Depth-ﬁrst ordering is a common practice in tree generation

(Maddison &amp; Tarlow, 2014; Raychev et al., 2016), but in principle

our framework also allows for other orderings.


Structural Language Models of Code

IfExpr

Method

Root





?



(a)

Greater

IfExpr

Method

Root



?





(b)

Greater

Name

IfExpr

Method

Root



?





(c)

Greater

Name

IfExpr

x

Method

Root



?







(d)

Greater

Name

IntExp

IfExpr

x

Method

Root



?







(e)

...

if(

x &gt; 1

) {

...

}

...

(f)

Figure 2. The subtree representing x &gt; 1 is generated given its surrounding tree. At each step, the model generates the next node

(denoted by ? ) of path1, path2 and path3 using the root path R. Dashed lines denote the AST structure; solid lines denote AST paths.

Most AST paths are omitted from the ﬁgure, for clarity.

at. Intuitively, each path captures the effect of a differ-

ent, possibly distant, program element on at, along with

the syntactic relationship between them. For example, in

Figure 1 (left) the three paths originating from Path[]

ret inform the model about the existence of ret which

is an array of type Path. Thus, when completing ret[i]

= ... – the completion should be a Path object. Other

paths inform the model that the target is inside a For loop,

iterated stats.length times. Considering the informa-

tion ﬂowing from all paths, our model correctly generates

stats[i].getPath().

We denote the (candidate) node at time t as at, its (given)

parent, which is currently expanded, by π (at), and the set

of all paths as St:

St = {ℓ � π (at) |ℓ ∈ leaves (a&lt;t)}

where ℓ � π (at) is the (only) path in the tree between a

leaf ℓ and the current node to expand π (at). We denote the

path from the root of the program as Rt = a0 � π (at),

which represents the current, relative position of π (at) in

the program (marked as R in Figure 2). Whereas prior

work used whole paths (between two leaf nodes) to encode

ASTs (Alon et al., 2019a;b), our model observes partial

paths (between a leaf and any other node) and learns to

extend them by predicting their next node.

Figure 2 illustrates the traversal order of a subtree that rep-

resents the expression x &gt; 1 and some of the paths used

to compute the probability at each step. At each step, the

probability of the next node is computed given the paths St

from the root and every given leaf up to the current node

to expand. Figure 2(d) shows how after the terminal node

with the value x is given, path3 originating from this leaf

is also used to compute the probability of the next nodes.

Our path-based approach generalizes previous approaches

such as “parent feeding” and “previous action” encoding

(Yin &amp; Neubig, 2017), context nodes (Bielik et al., 2016),

and some of the graph-edges of Brockschmidt et al. (2019).

See Section 8 for further discussion.

Greater

Name

IntExp

x

EOStok

1

EOStok

EOSnode

Figure 3. Augmenting the

AST with EOSnode and

EOStok nodes.

Generating

Trees

In

se-

quence generation, the length

of the target sequence is con-

trolled by generating an EOS

token to stop. When generat-

ing trees, we require a more

sophisticated

mechanism

to control arity and depth.

We

augment

AP

in

two

ways to allow node-by-node

generation.

First, we add a special EOSnode node to every nontermi-

nal to control for arity. Generating this node indicates that

the parent node has no more children nodes. Second, we

end each subtoken sequence with a special EOStok node to

control for depth during generation; we decompose each

terminal node nv into a sequence of terminal nodes Tv

by splitting up the node’s value v into subtokens based on


Structural Language Models of Code

camel notation. For example, if v = toLowerCase, then

Tv = to → lower → case → EOStok. Figure 3 shows

an example of both EOSnode and EOStok in action.

Node Trees vs. Production Trees While we predict a sin-

gle node at each step, previous work (Iyer et al., 2018;

2019) predicts a grammar production rule. This represen-

tation decomposes the code in a way that often forces the

model to predict with partial information. For instance,

consider generating the expression str.Substring(3).

The model of Brockschmidt et al. (2019) would ﬁrst predict

the rule Expr→Expr.Substring(Expr), and only then

expand Expr→str and Expr→3. That is, the model needs

to predict the method name (Substring) before the invok-

ing object (str). Further, the Substring method can get

either one or two arguments, forcing the model to choose

whether to use the one- or two-argument rule in advance.

Node generation, however, allows us to predict the pres-

ence of a function call and only then to predict its object

and method name, rather than predicting these a priori.

3. Model Architecture

In the previous section, we described how we can gener-

ate code given the probabilities Pr (at|a&lt;t), where a&lt;t is

represented by the set of partial AST paths St. Here, we

present a neural model that estimates Pr (at|St). We ﬁrst

encode each path in St as a vector (Section 3.1); then, we

contextualize and aggregate the entire set. Finally, we pre-

dict the target node at by combining a subtoken vocabulary

with a syntactic copy mechanism (Section 3.3).

3.1. Encoding AST Paths

Given a partial AST path, i.e., a sequence of nodes

n1, . . . , nk, our goal is to create a vector representation.

We ﬁrst represent each node ni using embeddings.

A

subtoken node is represented by the index of its subtoken

w in the embedding matrix Esubtoken; AST nodes are rep-

resented as a pair ni = (τ, κ) where τ is the node type, e.g.

IfStatement, and κ is the node index among its sibling

nodes. We represent node types using a learned embedding

matrix Etype and the child indices using a learned matrix

Eindex. The node’s vector representation is the concatena-

tion of the type and index vectors.

e (ni) =

�

Esubtoken

w

ni is the subtoken w

�

Etype

τ

; Eindex

κ

�

ni is the AST node (τ, κ)

We encode the entire path using a uni-directional LSTM

stack, and take the ﬁnal states:2

�f (n1, . . . , nk) = LSTM (e (n1) , . . . , e (nk))

2Replacing the LSTMs with transformers yielded similar re-

sults in preliminary experiments.

Given a set of partial paths S (omitting the itera-

tor t for simplicity),

we denote their encodings as

H = {

�f (n1, . . . , nk) | (n1, . . . , nk) ∈ S}.

Efﬁcient Computation When modeling a subtree, there

are large overlaps between paths from different time steps.

In particular, paths that originate from the same leaf share

the same preﬁx. We therefore apply the LSTM on the pre-

ﬁx once and cache the intermediate state across sufﬁxes,

speeding up both training and inference signiﬁcantly. An

example is shown in Figure 7 (supplementary material).

3.2. Aggregating Multiple Paths

Given the set of paths S leading up to the parent π(a) of

the target node a, our goal is to represent S in the context

of predicting a. To do so, we introduce the aggregation

function g (H, r, i). As its input, g takes the set of encoded

paths H, the encoded root path r, and the child index i of

the currently predicted child node a relative to its parent.

We ﬁrst contextualize the path encodings H using a trans-

former encoder (Vaswani et al., 2017).3 In parallel, we ap-

ply a non-linear transformation to the encoding of the root

path r =

�f (R), in order to inform it that we wish to predict

the i-th child of π(a):

Z = Transformer (H)

�r = Wr · ReLU (Ci · r)

In this formulation, the parameter matrix Ci is used when

the child index is i, while the parameter matrix Wr is used

for every instance.

We then compute attention over the set of contextualized

path encodings Z using the index-informed root-path en-

coding �r as the query; we pass the weighted average �z and

the root-path encoding �r through another fully-connected

layer; we denote the resulting vector representation as �h:

α = softmax (Z · �r)

�z =

�

j

αj · Zj

�h = g (H, r, i) = ReLU (Wg [�z; �r])

where semicolons (;) denote vector concatenation.

3.3. Predicting with a Syntactic Copy Mechanism

We can now predict a from the representation �h. If the

target node’s parent π(a) is a nonterminal AST node, then

a must be an AST node; otherwise, a is a subtoken.

Predicting AST Nodes If a is an AST node, we predict a

using a softmax over the node type embeddings Etype:

Pr (a|S) = softmax

�

Etype · �h

�

(π(a) is a nonterminal)

3Since H is a set, we do not use positional embeddings.


Structural Language Models of Code

Predicting Subtokens Programs repeatedly refer to previ-

ously declared symbols, resulting in highly repetitive us-

age of identiﬁers.

We therefore use a copy mechanism

(Gu et al., 2016) to allow our model to predict either en-

tire tokens or individual subtokens that exist in the context.

As we show in Section 6, copying greatly improves our

model’s performance. For brevity, we describe how entire

tokens are copied, and elaborate on the copy of subtokens

in Appendix D. We score each leaf ℓ using a bilinear func-

tion (Wc) between its path’s encoding Hℓ and �h. At the

same time, we score the token w, which is the token as-

sociated with ℓ, from a limited vocabulary using the inner

product between its representation in the subtoken embed-

ding matrix Esubtoken and �h.

scopy (ℓ) = Hℓ · Wc · �h

sgen (w) = Esubtoken

w

· �h

The scores scopy and sgen are then summed over all oc-

currences that correspond to the same symbol and subse-

quently normalized via softmax. A key difference from

most previous work (Ling et al., 2016; Yin &amp; Neubig,

2017) is that our copy mechanism uses the syntactic rela-

tion to the source (the path Hℓ), rather than the sequential

relation or the graph-node representation (Yin et al., 2019).

4. Experimental Setup

4.1. Benchmarks

Any-Code Completion:

Java We take the Java-small

dataset of Alon et al. (2019a), which is a re-split of the

dataset of Allamanis et al. (2016). It contains 11 GitHub

projects, broken down into a single method per example,

and split to train/dev/test by project to reduce code overlap.

This dataset was found to contain the least code duplica-

tion by Allamanis (2019). We create any-code completion

examples by selecting every expression larger than a single

AST node as the target, using the remainder of the method

as the context. We remove methods containing the word

“test” in their body or ﬁle name, and omit 10% of the exam-

ples by ﬁltering out methods longer than 20 lines to avoid

conﬁgurations, initializations, and auto-generated code. To

make the task even harder, we remove examples where the

target appears as-is in the context. Ultimately, this dataset

contains 1.3M/10k/20k train/dev/test examples.

Restricted Completion: C# To provide a fair compari-

son to Brockschmidt et al. (2019), we create an additional

benchmark where the missing code is more limited. We

use the code of Brockschmidt et al. (2019) which ﬁlters

out examples where the targets contain non-primitive types

or user-deﬁned functions. We extract the exact same types

of limited expressions. Since the dataset of Brockschmidt

et al. (2019) is not publicly available, we consulted with

Brockschmidt et al. directly and extracted examples from

the raw dataset of Allamanis et al. (2018) using their “un-

seen projects test” set. This dataset contains 30 GitHub

projects broken down to one method per example. This

dataset contains 16k/8k/3k train/dev/test examples.

Our datasets are available at:

http://github.com/

tech-srl/slm-code-generation/. Detailed statistics

are provided in Figure 6 in Appendix A.

Metrics Following Brockschmidt et al. (2019), we report

exact match accuracy at 1 and 5. We also introduce a new

tree@k metric which counts a prediction as correct if the

entire tree structures, ignoring leaf values, are identical.

For example, x &gt; 1 and y &gt; 2 would not count as identi-

cal in exact match, but would count as “tree-match identi-

cal” because both express that an identiﬁer is greater than

an integer (NAME &gt; INT). The tree@k metric is interest-

ing because it allows us to tease apart the model’s syntactic

errors from incorrect subtoken predictions.

4.2. Baselines

We compare our model to a variety of original implemen-

tations and adaptations of existing models. We put signiﬁ-

cant effort to perform a fair comparison, including adding a

copy mechanism to the NMT baselines and subtokenization

as in our model. We adapt strong baselines from the liter-

ature to our task, even if they were designed to different

tasks such as NL→code and code→NL. We re-train all the

following baselines on the same datasets as our models.

NMT

We

use

standard

autoregressive

sequence-to-

sequence NMT baselines, in which we subtokenize the

given code snippet, replace the target in the source with

a special PRED symbol, and train the network to predict the

target as a sequence of subtokens. Transformerbase+copy

(Vaswani et al., 2017) uses the implementation of Open-

NMT (Klein et al., 2017) with a copy mechanism (Gu et al.,

2016). Transformersmall+copy uses dmodel=256, dff=1024,

and 4 self attention heads per layer. BiLSTM→LSTM+copy

is a 2-layer bidirectional LSTM encoder-decoder with

d=512 and attention. seq2tree+copy follows Aharoni &amp;

Goldberg (2017) and learns to generate the linearized,

subtokenized target AST.

Java-speciﬁc Baselines We use the original implementa-

tion of Iyer et al. (2018), and also their seq2prod base-

line which is a re-implementation of Yin &amp; Neubig (2017);

these are designed for NL→code tasks, in which we feed

the code context as the NL input. The model of Iyer et al.

(2018) is designed to get additional input of the available

variables and their types, for which we do not feed types.

While these models could also be applied to other lan-

guages, their implementation only supports Java.

C#-speciﬁc Baselines We compare our model to the graph-

based GNN→NAG model using the implementation of

Brockschmidt et al. (2019).

Bielik et al. (2016) kindly


Structural Language Models of Code

Model

acc@1

acc@5

tree@1

tree@5

code2seq (Alon et al., 2019a)

10.68

15.56

30.46

43.94

Iyer et al. (2018)

5.94

9.19

25.54

36.75

seq2prod (Yin &amp; Neubig, 2017)

8.05

11.82

30.77

41.73

Transformersmall (Vaswani et al., 2017)+copy

14.23

21.35

31.83

47.40

Transformerbase (Vaswani et al., 2017)+copy

16.65

24.05

34.68

50.52

BiLSTM→LSTM (Luong et al., 2015)+copy

16.93

23.17

34.29

49.72

seq2tree (Aharoni &amp; Goldberg, 2017)+copy

16.81

23.04

38.14

52.36

SLM (this work)

18.04

24.83

39.10

55.32

Table 1. Results on any-code completion in Java.

trained and tested their non-neural PHOG model on our

C# dataset. We note that PHOG does not have an explicit

copy mechanism, and considers only context to the left of

the target code, while we consider also context to the right.

Extending PHOG could potentially improve its results.

In both Java and C#, we compare to code2seq (Alon et al.,

2019a), which is a strong code→NL model. We train it to

generate the target code as a sequence of subtokens.

4.3. Implementation and Hyperparameter Settings

Architecture We use embeddings of size 512, 2 layers of

LSTMs with 256 units, and 4 transformer layers with 8 at-

tention heads. We kept a small subtoken vocabulary of size

1000 to encourage the model to learn to copy; larger vo-

cabularies did not show an improvement. These resulted in

a very lightweight model of only 15M parameters, which is

close to Transformersmall (11.8M parameters). In compar-

ison, Transformerbase had more than 45M parameters (3×

more parameters than our model).

Training We train the model end-to-end on a single

V100 GPU, using cross entropy and the Adam optimizer

(Kingma &amp; Ba, 2015), an initial learning rate of 10−4 mul-

tiplied by 0.95 every 20k steps. We bucket examples based

on the number of predictions in the target subtree (nodes

+ subtokens + EOS), and vary the batch size such that each

batch contains about 512 targets. We train the model to pre-

fer copying entire tokens rather than copying subtokens, if

possible, by optimizing for the entire token as the true la-

bel. We apply dropout of 0.25 in the Transformer layers,

and a recurrent dropout of 0.5 in the LSTMs.

Inference We perform beam search with width of 5 and

optimize for accuracy@1.

5. Results

Any-Code Completion: Java Table 1 shows that our SLM

achieves over 1.1% and 0.78% better acc@1 and acc@5

(respectively) over the two strongest baselines. The im-

provement over Transformersmall, which is closer to our

model in the number of parameters, is even higher: over

Model

acc@1

acc@5

tree@1

tree@5

GNN→NAG

15.19

27.05

26.48

40.09

code2seq

6.20

10.05

21.97

30.89

seq2seq+copy

26.42

37.94

34.10

49.23

seq2tree+copy

22.29

35.86

31.85

48.53

PHOG

7.40

12.00

–

–

SLM (this work)

37.61

45.51

51.10

59.82

Table 2. Results on restricted completion in C#.

3.8% and 3.4% in acc@1 and acc@5.

The NMT baselines performed better than code-speciﬁc

baselines. We hypothesize that the reason is that the NMT

baselines are more generic, while the code-speciﬁc base-

lines are designed for different tasks: seq2prod is designed

for tasks which involve generating code given natural lan-

guage input; Iyer et al. (2018) additionally expects all

member methods, ﬁelds, and their types as input; code2seq

is designed to generate sequences rather than code, and

does not have a copy mechanism. An approximation of

code2seq with a copy mechanism is presented in Section 6.

Interestingly, the syntactically-informed seq2tree baseline

achieved the highest tree@k among the baselines, while our

model achieved higher acc@k and tree@k. This shows that

leveraging the syntax can beneﬁt NMT models as well.

Restricted Completion: C# Table 2 shows the results for

the restricted completion task in C#, where seq2seq+copy

is the BiLSTM→LSTM+copy model which performed the

best among the Java baselines. We ﬁrst observe that the

seq2seq+copy and the seq2tree+copy baselines outperform

the GNN→NAG of Brockschmidt et al. (2019), who intro-

duced this task. Although Brockschmidt et al. (2019) did

compare to a seq2seq baseline, their GNN→NAG model

could copy symbols from the context, but their baseline did

not. To conduct a fair comparison with our SLM model,

we equipped the seq2seq and seq2tree baselines with a

copy mechanism. Even though the seq2seq+copy and the

seq2tree+copy baselines perform substantially better than

the state of the art in this setting, our SLM model is able to

go beyond, achieving signiﬁcant gains over all models.


Structural Language Models of Code

Ablation

acc@1

acc@5

tree@1

tree@5

Paths→Seq

12.95

18.52

33.44

43.43

Seq→Path

12.12

17.12

28.68

43.99

Paths→Paths

17.63

24.62

37.78

53.98

No Root Att

14.43

18.48

28.20

35.65

No Copy

10.72

15.70

30.61

44.35

SLM (original)

18.04

24.83

39.10

55.32

Table 3. Ablations on any-code completion in Java.

The superiority of our model over GNN→NAG may also

be related to the GNN bottleneck (Alon &amp; Yahav, 2020),

which hinders GNNs from propagating long-range mes-

sages. In contrast, propagating long-range messages using

paths is natural for our model.

6. Ablation Study

To understand the importance of the various components

and design decisions in our model, we conducted an exten-

sive ablation study.

Paths→Seq follows code2seq (Alon et al., 2019a) and sep-

arates the model to an encoder and a decoder, where the de-

coder generates the target code as a sequence of subtokens.

The main difference from code2seq is that Paths→Seq in-

cludes a copy mechanism, as in our model.

Seq→Path follows Rabinovich et al. (2017) and separates

our model to an encoder and a decoder (including a copy

mechanism), where the encoder encodes the context as a

sequence of subtokens using a BiLSTM, and the decoder

generates the missing subtree using the root path and the

index of the generated child.

Paths→Paths is similar to our SLM model except that it

uses separate encoder and decoder. These encoder and de-

coder have untied weights, unlike our SLM model which

models the source and the target jointly.

No Root Attention uses max pooling instead of attention

in aggregating multiple paths (see Section 3.2). The index-

informed path from the root to the target’s parent (R in

Figure 2) is concatenated with the result, instead of being

used as attention query.

No Copy replaces copy mechanism with a much larger vo-

cabulary (25k subtokens instead of 1k).

Results Table 3 shows the results of these alternatives. As

our SLM model performs better than Paths→Paths, this ab-

lation shows the importance of joint modeling of the con-

text and the target subtree by parameter tying.

Each of Paths→Paths and the seq2seq baselines (Ta-

ble 1) performs better than Paths→Seq and Seq→Path; this

shows the importance of using the same type of encoder

and decoder for any-code completion, rather than combin-

ing “an optimal encoder” with “an optimal decoder”. While

this distinction between encoder and decoder types might

be necessary for semantic parsing (Rabinovich et al., 2017;

Dong &amp; Lapata, 2018), NL→code (Yin &amp; Neubig, 2017)

and code→NL (Alon et al., 2019a; Fernandes et al., 2019)

tasks because of the different modalities of the input and

the output, this discrepancy may hurt generalization when

the output is essentially a missing part of the input’s AST.

Paths→Paths performs better than the seq2seq baselines

(Table 1), showing the advantage of using paths over tex-

tual sequences, even without parameter tying.

No Root Attention degrades acc@1 and acc@5 by 3.6% to

6.3%. This shows that dynamically attending to the context

paths given the current root path is crucial.

Not using a copying mechanism results in a degradation of

7.3% to 9.1%. Programs use symbols and identiﬁers repet-

itively, thus the ability to copy symbols from the context is

crucial for this task. For this reason, we included a copying

mechanism in all NMT baselines in Section 4.

7. Qualitative Analysis

Our main results (Table 1 and Table 2) reveal a gap be-

tween acc@k and tree@k: when ignoring identiﬁer values

and comparing only the tree structure, accuracy is signif-

icantly higher across all models. While our SLM model

performs better than all baselines in acc@k, our model also

shows greater potential for improvement in its tree@k re-

sults, which are much higher than the baselines’. We thus

focus on studying the cases where the tree was predicted

correctly, but the model failed to generate the code exactly

including names.

Figure 4(a) shows an example of this case: the ground

truth has a structure of the form: NAME.NAME() &gt; INT.

Our model predicts value.length() &gt; 0 (a tree-match)

as its ﬁrst candidate and value.length() &gt; 55 (the

ground truth) as its second. Null-checking a string is of-

ten followed by checking that it is also not empty, making

the ﬁrst candidate a reasonable prediction as well.

Figure 4(b) shows another example: in this case, the ground

truth thisValue == thatValue ?

0 :

1 was pre-

dicted correctly only as the second candidate. Neverthe-

less, the top-3 candidates are tree-matches since all of them

are of the form: NAME == NAME ? INT : INT. Interest-

ingly, the ﬁfth candidate (thisValue == thatValue)

?

0 :

1 is logically-equivalent to the ground truth.

In both examples, our model’s top candidate differs from

the ground truth by a single identiﬁer or literal: in Fig-

ure 4(a) the model predicted 0 instead of 55; in Figure 4(b)


Structural Language Models of Code

private static void log(String value) {

if (value != null

&amp;&amp;

)

value = value.substring(0, 55)+"...";

LOG.info(value);

}

True ref:

value.length() &gt; 55

SLM

top-5:

(9.6%)

value.length() &gt; 0

(7.3%)

value.length() &gt; 55

✓

(1.8%)

value.startsWith("...")

(1.5%)

!value.startsWith("...")

(0.9%)

value.charAt(0) == '.'

(a)

public int compareTo(LongWritable o) {

long thisValue = this.value;

long thatValue = o.value;

return (thisValue &lt; thatValue ? -1 :

(

));

}

thisValue == thatValue ?

0 :

1

(16.3%)

thisValue == thisValue ?

0 :

1

(11.0%)

thisValue == thatValue ?

0 :

1

✓

(9.5%)

thisValue == value ?

0 :

1

(6.6%)

thisValue &gt; thatValue ?

0 :

1

(6.1%)

(thisValue == thatValue) ? 0 :

1

↔

(b)

Figure 4. Examples for cases where the top candidate is a “tree-match” (marked with

), but only the second candidate is an “exact

match” (marked with ✓ in bold). Predictions that are logically equivalent to the ground truth are marked with ↔. Additional (and larger)

examples along with the predictions of the baselines are shown in Appendices F and G.

the model predicted thisValue instead of thatValue.

Such single subtoken errors are responsible for 30% of the

cases where the model’s top prediction is a tree-match but

not an exact match. Single token (whole identiﬁer or literal)

mismatches are responsible for 74% of these cases. Thus,

improving our model’s ability to predict the right names

has the potential to enhance our gains furthermore. De-

tailed results of allowing such mistakes in our model and

in the baselines can be found in Appendix C.

Additional possible post-ﬁltering could ﬁlter out can-

didates that do not compile.

In Figure 5, the ﬁrst,

third and fourth candidates do not compile, because the

this.currentAttempt object does not have getCount,

get, nor getTime methods.

If the model’s predictions

would have been considered in the context of the entire

project including its dependencies, these candidates could

have been ﬁltered out, and the (correct) ﬁfth candidate

would be ranked second. We leave compiler-guided code

generation to future work.

Additional examples can be found in Appendices F and G,

and in our interactive demo at http://AnyCodeGen.org.

8. Related Work

Generalizing Previous Approaches Our approach frames

code generation as predicting the next node in all partial

AST paths. This simple framing generalizes most previous

work, without hand-crafted edges and special actions:

• Models that use information about ancestor nodes

only (Rabinovich et al., 2017), as well as the “Parent

Feeding” of Yin &amp; Neubig (2017), are generalized by

our model, since all paths that go into a node at pass

through its parent, and the path from the root is the

attention query.

• The “previous action encoding” of Yin &amp; Neubig

(2017) is also a special case of our approach, because

St contains the paths starting from the previously ex-

panded leaves of Ap into the currently expanded node

π (at), such as path3 in Figure 2(e).

• The “context node” of PHOG (Bielik et al., 2016) is

just one of the previously-traversed leaf nodes in a&lt;t.

Thus, not only that our model conditions on this con-

text node as well, our model also takes into account

the syntactic relation, i.e., the path, between the con-

text and π (at). Moreover, while PHOG conditions on

a single leaf, SLMs condition on every leaf in a&lt;t.

• Finally, Brockschmidt et al. (2019) deﬁne special

graph edges (e.g., “NextSib” and “Child”) to capture

relations on the AST. Allamanis et al. (2018) fur-

ther deﬁnes data-ﬂow and control-ﬂow graph edges

such as “ComputedFrom” and “GuardedByNegation”.

Most of these relations can be expressed as partial

AST paths without manually designing them.

Program Generation Learning to generate programs is

one of the oldest problems in machine learning (Waldinger

&amp; Lee, 1969) and has been considered by some as the “holy

grail of computer science” (Pnueli &amp; Rosner, 1989; Gul-

wani et al., 2017). Typically, the task is to generate a pro-

gram given some form of input or context, such as com-

plete formal speciﬁcations (Green, 1981; Si et al., 2019) or

input-output examples (Gulwani, 2011; Devlin et al., 2017;

Parisotto et al., 2017; Balog et al., 2017; Gaunt et al., 2017).

While these approaches work well in some cases, they are

often bounded to DSLs that prevent them from being ap-

plied to realistic, general-purpose code.

Bielik et al. (2016) learn a dynamic DSL expression that

points to a single context that guides the generation of


Structural Language Models of Code

public float getProgress() {

this.readLock.lock();

try {

if (this.currentAttempt != null) {

return

;

}

return 0;

} finally {

this.readLock.unlock();

}

}

True ref:

this.currentAttempt.getProgress()

SLM top-5:

(31.3%)

this.currentAttempt.getCount()

(30.6%)

-1

(1.5%)

this.currentAttempt.get()

(1.2%)

this.currentAttempt.getTime()

(0.9%)

this.currentAttempt.getProgress()

✓

Figure 5. An example from our test set in which a compiler-guided generation could ﬁlter out non-compiling candidates, and thus rank

the ground truth second instead of ﬁfth. Four out of the ﬁve candidates are “tree-match” (marked with

), the ﬁfth candidate is an

“exact match” (marked with ✓ in bold), and only the second and the ﬁfth candidate compile (marked with

).

a JavaScript program.

Maddison &amp; Tarlow (2014) and

Amodio et al. (2017) generate general-purpose uncondi-

tional code, and do not deal with the challenge of ﬁtting

the code to a given context.

Brockschmidt et al. (2019) addressed a similar code com-

pletion task as ours using a graph encoder and a neural at-

tribute grammar decoder. However, they limit their model

to generate only primitive types or arrays of these; use

a closed vocabulary; and omit user-deﬁned functions. In

this paper, we lift these constraints and allow any, general-

purpose, generation of code, of all types and containing any

names. As we show in Section 5, our model performs sig-

niﬁcantly better.

Murali et al. (2018) generate code given a set of APIs in

a ”Java-like” language; they state that their approach is

thus intrinsically limited to generate only API-heavy pro-

grams. Yin et al. (2019) generate general-purpose code by

applying a given edit to a given code snippet. Brody et al.

(2020) predict code edits directly given other edits that oc-

curred in the context. Yin &amp; Neubig (2017) and Rabinovich

et al. (2017) used a top-down syntactic approach for gen-

erating general-purpose code given a natural language de-

scription. Models that address APIs→code, edit→code, or

NL→code tasks must model the input separately and differ-

ently from the output code. As we show in Section 6, mod-

eling the source and the target differently perform poorly

in our task, in which the input is code as well.

Chen et al. (2018) addressed JavaScript↔CoffeeScript

translation with a tree-to-tree approach, which required a

strong alignment between the source and target trees.

9. Conclusion

We presented a novel approach for any-code completion:

joint modeling of an AST and its missing subtree using a

structural language model. Our approach generalizes most

previous work in this area while reaching state-of-the-art

performance on challenging benchmarks. We demonstrate

our approach in generating general-purpose code, in re-

stricted and unrestricted settings, in two languages. Our

model outperforms a variety of strong baselines, including

programming language-oriented models and strong NMT

models applied in our settings.

We believe that structural language modeling enables a

wide range of future applications, similarly to how lan-

guage modeling research has contributed to NLP in recent

years. Our approach also has a variety of direct applications

such as code completion, detecting and ﬁxing unlikely ex-

isting code, and re-ranking solutions produced by another

synthesizer or solver. To these ends, we make all our code,

datasets, and trained models publicly available.

Acknowledgments

We would like to thank Guy Waldman for developing the

AnyCodeGen.org website, Pavol Bielik and Martin Vechev

for training their PHOG model on our dataset, Srinivasan

Iyer for his useful advice, the guidance in training his

model and adapting it to our task, Marc Brockschmidt for

his useful implementation tips and guidance in training his

model, and Miltiadis Allamanis for the guidance in repro-

ducing his C# dataset.


Structural Language Models of Code

Supplementary Material

Java

C#

#projects - training

9

25

#projects - validation

1

2

#projects - test

1

3

#examples - training

1,309,842

16,295

#examples - validation

10,000

8,183

#examples - test

20,000

3,305

Avg. number of paths

27.8

131.1

Avg. source length - lines

10.4

57.5

Avg. source length - tokens

77.7

264.3

Avg. source length - subtokens

100.6

343.6

Avg. target length - tokens

5.4

3.9

Avg. target length - subtokens

7.8

5.0

Avg. target length - tree nodes

3.8

3.9

Avg. target length - tree targets

10.8

10.8

Figure 6. Statistics of our datasets. When not mentioned otherwise,

the statistic was measured on the training set.

Greater

Name

IntExp

IfExpr

x

1

time=3

time=4

Figure 7. Efﬁcient computation: partial paths for dif-

ferent time steps share the same preﬁx, allowing a

shared computation. In this example, the preﬁx is the

shared path from the leaf (not shown) to Greater,

and is much longer than either of the sufﬁxes.

A. Data Statistics

Figure 6 shows some statistics of our used datasets. In Java: for the validation set, we randomly sampled 10, 000 examples

from the raw validation set; for the test set, we randomly sampled 20, 000 examples from the raw test set.

We will release all datasets, raw and preprocessed, with the ﬁnal version.

B. Additional Evaluation Details

For both Java and C# models, we experimented with the following hyper-parameter values. We performed beam search

on the validation set after every training iteration, and we selected the best conﬁguration and checkpoint according to

accuracy@1 on the validation set. After the best conﬁguration was chosen, we ran a single evaluation run on the test set.

•

�f ∈ {LSTM, Transformer} – how to encode each path.

• LSTM #layers ∈ {1, 2}

• dsubtoken ∈ {256, 512} – embedding size.

• Transformer layers ∈ {0, 1, 2, 3, 4}

• lr ∈ {10−3, 10−4, 10−5} – learning rate

• Learning rate decay every {10000, 20000, 40000} steps.

C. Qualitative Analysis cont. - Correct Tree, Incorrect Names

In Section 7 we discussed the gap between acc@k and tree@k. We found that 30% of the examples in the gap could have

been exact match if a single subtoken prediction was ﬁxed; 74% of the examples in the gap could have been exact match

if a single identiﬁer prediction was ﬁxed. Table 4 shows the accuracy of our model and the leading baselines if a single

subtoken or a single token mismatches were counted as correct: One SubToken Diff and One Token Diff are similar to

exact match, except that they allow a single subtoken or a single token mistake, respectively. As Table 4 shows, not only

that our model performs better than the baselines in exact match, it also shows a greater potential for improvement.


Structural Language Models of Code

Model

Exact-match (acc@k)

One SubToken Diff

One Token Diff

Tree@k

@1

@5

@1

@5

@1

@5

@1

@5

Transformerbase +copy

16.65

24.05

23.08

34.06

29.39

43.46

34.68

50.52

BiLSTM→LSTM +copy

16.93

23.17

22.39

31.68

27.23

38.92

34.29

49.72

seq2tree +copy

16.81

23.04

24.02

33.89

32.67

43.75

38.14

52.36

SLM (this work)

18.04

24.83

24.40

35.19

33.68

46.57

39.10

55.32

Table 4. Examining the gap between acc@k and tree@k: the acc@k and tree@k results here are the same as in Table 1; One SubToken

Diff allows a single subtoken mismatch; One Token Diff allows a single token mismatch.

D. Copying Single Subtokens

In addition to scoring the entire token to be copied, we also score each of the subtokens composing it according to their

position. For each position i, we add a scoring function scopyi, such that scopyi (ℓ) produces the copying score of the i’th

subtoken of ℓ, which we denote as ℓi:

sw = sgen (w) +

�

val(ℓ)=w

scopy token (ℓ) +

�

i

�

val(ℓi)=w

scopyi (ℓ)

Pr (a|S) = softmax (s)

Where scopy token is the scoring function of copying the entire token, described in Section 3.3.

For example, a token of getX is scored entirely using scopy token; each of its subtokens, get and X, are scored using scopy1

and scopy2 respectively. That is, the model can either copy the entire token, or copy only some of its subtokens. This

ability is especially useful in generating a name like setX, where getX appears in the context, and X is any unknown,

user-deﬁned, subtoken; the model learns to generate set from the vocabulary, and copy only the subtoken X.

protected void checkRpcAdminAccess() throws IOException, AccessControlException {

UserGroupInformation ugi = UserGroupInformation.getCurrentUser();

UserGroupInformation zkfcUgi = UserGroupInformation.getLoginUser();

if (adminAcl.isUserAllowed(ugi)

|| ugi.getShortUserName().equals( zkfcUgi.getShortUserName() )) {

LOG.info("Allowed RPC access from " + ugi

+ " at " + Server.getRemoteAddress());

return;

}

String msg = "Disallowed RPC access from " + ugi

+ " at " + Server.getRemoteAddress() + ". Not listed in " + DFSConfigKeys.DFS_ADMIN;

LOG.warn(msg);

throw new AccessControlException(msg);

}

True ref:

zkfcUgi.getShortUserName()

SLM top-5 candidates:

zkfcUgi.getShortUserName()

(11.7%)

(exact match)

DFSConfigKeys.DFS

(4.5%)

zkfcUgi.getUserName()

(2.6%)

(tree-match)

zkfcUgi.getUser()

(1.7%)

(tree-match)

zkfcUgi.getUserId()

(0.6%)

(tree-match)

Entirely copied tokens are marked in brown; unknown copied subtokens are marked in blue; in-vocabulary subtokens are marked in

black; subtokens that are both in-vocabulary and copied from context are marked in purple.

Figure 8. A Java Any-Code Completion example from our test set along with the predictions of our model. The predictions of the

baselines are shown in Figure 12 below.


Structural Language Models of Code

E. Example: Usefulness of Copy Mechanism

As shown in Section 6, the ability to copy is crucial for the any-code completion task, because of the repetitive use of

identiﬁers and symbols in programs. Figure 8 shows a representative example for the necessity of the copy mechanism:

generating the ground truth zkfcUgi.getShortUserName() is feasible only thanks to the copy mechanism, since zkfc

is obviously an UNK subtoken which was not observed in the training data.

In this case, since both zkfcUgi and getShortUserName appear in context, both were copied as entire tokens, rather

than generated using subtokens. This example also shows how the ability to copy entire tokens ease the generation process

by reducing the number of target symbols (our SLM model is able to copy and combine single subtokens as well).

F. Java Examples

Figures 9 to 18 contain examples from our test set for the any-code completion task in Java, along with the prediction of our

model and some of the baselines. The highlighted expressions are the true references that should be generated. Indentation

and line breaks may have been altered for typesetting reasons.

G. C# Examples

Figures 19 to 26 contain examples from our test set for the restricted completion task in C# along with the prediction of

our model some of the baselines. The highlighted expressions are the true references that should be generated. Indentation

and line breaks may have been altered for typesetting reasons.


Structural Language Models of Code

private C findCounter(T key) {

int i = key.ordinal();

if (counters[i] == null) {

counters[i] = newCounter(key);

}

return

(C) counters[i] ;

}

Model

Prediction

True ref:

(C) counters[i]

SLM (this work)

(C) counters[i]

(71.6%)

(C) this

(6.3%)

counters[i]

(4.8%)

Transformerbase +copy

(C) this

(C) counters[i]

(C) counters

BiLSTM→LSTM +copy

(C) this

(C) counters[i]

counters[i]

Seq2tree +copy

(C) counters[i]

(C) counters[i].ordinal()

(C) counters.get(i)

private void handleTaskFinishedEvent(TaskFinishedEvent event) {

TaskInfo taskInfo = info.tasksMap.get( event.getTaskId() );

taskInfo.counters = event.getCounters();

taskInfo.finishTime = event.getFinishTime();

taskInfo.status = TaskStatus.State.SUCCEEDED.toString();

taskInfo.successfulAttemptId = event.getSuccessfulTaskAttemptId();

}

Model

Prediction

True ref:

event.getTaskId()

SLM (this work)

event.getTaskName()

(8.8%)

event.getId()

(8.2%)

event.getTask()

(3.4%)

event.getName()

(3.3%)

event.getTaskId()

(3.3%)

Transformerbase +copy

event.getTaskInfo()

event.getTaskId()

event.getId()

event.getTask()

taskInfo.getTaskId()()

BiLSTM→LSTM +copy

event.name

event.type

event.getId()

event.id

event.getKey()

Seq2tree +copy

event.getId()

event.getPath()

event.getDescription()

event.getTaskName()

event.getTaskName(

(Syntax error)

Figure 9. Java examples from our test set along with the predictions of our model and the baselines.


Structural Language Models of Code

private static void log(String value) {

if (value!= null &amp;&amp;

value.length() &gt; 55 )

value = value.substring(0, 55) + "...";

LOG.info(value);

}

Model

Prediction

True ref:

value.length() &gt; 55

SLM (this work)

value.length() &gt; 0

(9.6%)

value.length() &gt; 55

(7.3%)

value.startsWith("...")

(1.8%)

Transformerbase +copy

value.length() &gt; 55

value.length() &gt; 0)

value.length() &gt; 1

BiLSTM→LSTM +copy

value.length() &gt; 55

value.startsWith("")

value.startsWith("...")

Seq2tree +copy

value.length() 55

(Syntax error)

value.endsWith("info")

value.length() 55

(Syntax error)

private List&lt;INode&gt; initChildren() {

if (children == null) {

final ChildrenDiff combined = new ChildrenDiff();

for (DirectoryDiff d = DirectoryDiff.this; d != null;

d = d.getPosterior() ) {

combined.combinePosterior(d.diff, null);

}

children = combined.apply2Current(ReadOnlyList.Util.asList(

currentDir.getChildrenList(Snapshot.CURRENT_STATE_ID)));

}

return children;

}

Model

Prediction

True ref:

d = d.getPosterior()

SLM (this work)

d = d.getParent()

(18.8%)

d = d.getChildrenList()

(14.9%)

d = d

(4.5%)

d = combined

(2.5%)

d = d.getPosterior()

(1.8%)

Transformerbase +copy

d = d

d = d.diff

d = d.getChildren()

d = d.currentDir

d = d.currentStateId

BiLSTM→LSTM +copy

--d

d = d

d = d.getParent()

d = d.next

d = d.get()

Seq2tree +copy

d d.next

(Syntax error)

d d.parent

(Syntax error)

d d.getParent()

(Syntax error)

d d.getChildren()

(Syntax error)

d d.getRoot()

(Syntax error)

Figure 10. Java examples from our test set along with the predictions of our model and the baselines.


Structural Language Models of Code

public float getProgress() {

this.readLock.lock();

try {

if (this.currentAttempt != null) {

return

this.currentAttempt.getProgress() ;

}

return 0;

} finally {

this.readLock.unlock();

}

}

Model

Prediction

True ref:

this.currentAttempt.getProgress()

SLM (this work)

this.currentAttempt.getCount()

(31.3%)

-1

(30.6%)

this.currentAttempt.get()

(1.5%)

this.currentAttempt.getTime()

(1.2%)

this.currentAttempt.getProgress()

(0.9%)

Transformerbase +copy

this.currentAttempt.getProgress()

this.currentAttempt.floatValue()

this.currentAttempt.getFloat()

this.currentAttempt.get()

this.currentAttempt.getTime()

BiLSTM→LSTM +copy

this.currentAttempt.getProgress()

this.currentAttempt.float()

this.currentAttempt.get()

this.currentAttempt.size()

this.currentAttempt.compute()

Seq2tree +copy

this.currentAttempt.getProgress()

this.currentAttempt.floatValue()

this.currentAttempt.get()

this.currentAttempt.getValue()

(float)this.currentAttempt.size()

public int compareTo(LongWritable o) {

long thisValue = this.value;

long thatValue = o.value;

return (thisValue &lt; thatValue ? -1 : ( thisValue == thatValue ? 0 : 1 ));

}

Model

Prediction

True ref:

thisValue == thatValue ?

0 :

1

SLM (this work)

thisValue == thisValue ?

0 :

1

(16.3%)

thisValue == thatValue ?

0 :

1

(11.0%)

thisValue == value ?

0 :

1

(9.5%)

Transformerbase +copy

thatValue &gt;&gt; thatValue

thatValue &gt; thatValue ?

1 :

0

thatValue &gt; thatValue

BiLSTM→LSTM +copy

thisValue - thatValue

thatValue &amp; thatValue

thatValue ?

1 :

0

Seq2tree +copy

thisValue thatValue

(Syntax error)

thisValue thatValue 0 1

(Syntax error)

thisValue thatValue 1 0

(Syntax error)

Figure 11. Java examples from our test set along with the predictions of our model and the baselines.


Structural Language Models of Code

private static String getNameServiceId(

Configuration conf, String addressKey) {

String nameserviceId = conf.get(DFS_NAMESERVICE_ID);

if (nameserviceId != null) {

return nameserviceId;

}

Collection&lt;String&gt; nsIds = getNameServiceIds(conf);

if (1 ==

nsIds.size() ) {

return nsIds.toArray(new String[1])[0];

}

String nnId = conf.get(DFS_HA_NAMENODE_ID_KEY);

return

getSuffixIDs(conf, addressKey, null, nnId, LOCAL_ADDRESS_MATCHER)[0];

}

Model

Predictions

True ref:

nsIds.size()

SLM (this work)

nsIds.size()

(83.7%)

conf.size()

(3.0%)

getSuffixIDs(conf).length

(2.5%)

Transformerbase +copy

-1

ns.size()

conf.size()

BiLSTM→LSTM +copy

-1

Integer.MAX VALUE

conf.size()

Seq2tree +copy

1

nsIds.size()

stringPool.blank

protected void checkRpcAdminAccess() throws

IOException, AccessControlException {

UserGroupInformation ugi = UserGroupInformation.getCurrentUser();

UserGroupInformation zkfcUgi = UserGroupInformation.getLoginUser();

if (adminAcl.isUserAllowed(ugi) ||

ugi.getShortUserName().equals( zkfcUgi.getShortUserName() )) {

LOG.info("Allowed RPC access from " + ugi

+ " at " + Server.getRemoteAddress());

return;

}

String msg = "Disallowed RPC access from " + ugi

+ " at " + Server.getRemoteAddress()

+ ". Not listed in " + DFSConfigKeys.DFS_ADMIN;

LOG.warn(msg);

throw new AccessControlException(msg);

}

Model

Predictions

True ref:

zkfcUgi.getShortUserName()

SLM (this work)

zkfcUgi.getShortUserName()

(11.7%)

DFSConfigKeys.DFS

(4.5%)

zkfcUgi.getUserName()

(2.6%)

Transformerbase +copy

server.getRemoteAddress()

server.getRemoteUserName()

server.getShortUserName()

BiLSTM→LSTM +copy

server.getUserName()

zkfcUgi.getUserName()

ugiUgi.getUserName()

Seq2tree +copy

dfsConfigKeys.dfsAdmin

zkfc.getUserName()

zkfcUgi.getRemoteAddress()

Figure 12. Java examples from our test set along with the predictions of our model and the baselines.


Structural Language Models of Code

static String replaceSubstitution(

String base, Pattern from, String to, boolean repeat) {

Matcher match = from.matcher(base);

if (repeat) {

return

match.replaceAll(to) ;

} else {

return match.replaceFirst(to);

}

}

Model

Prediction

True ref:

match.replaceAll(to)

SLM (this work)

match.toString()

(9.0%)

match.replaceAll(to)

(8.2%)

match.replaceAll(to, from)

(6.5%)

Transformerbase +copy

match.replaceFirst(to)

replace.replaceFirst(to)

matcher.replaceFirst(to)

BiLSTM→LSTM +copy

match.getFirst()

match.replaceFirst(to)

match.replaceFirst(to, to)

Seq2tree +copy

match.replaceFirst(base)

match.replaceFirst(to)

match.replaceFirst(repeat)

public void responseReceived(ResponseReceivedEvent event) {

RequestResult result = event.getRequestResult();

Date startDate = result.getStartDate();

Date stopDate = result.getStopDate();

long elapsed = stopDate.getTime() - startDate.getTime();

synchronized (this) {

this.lastE2Elatency = elapsed;

}

if ( LOG.isDebugEnabled() ) {

int statusCode = result.getStatusCode();

String etag = result.getEtag();

HttpURLConnection urlConnection =

(HttpURLConnection) event.getConnectionObject();

int contentLength = urlConnection.getContentLength();

String requestMethod = urlConnection.getRequestMethod();

long threadId = Thread.currentThread().getId();

LOG.debug(String.format(

"SelfThrottlingIntercept:: ResponseReceived:

... threadId=%d, Status=%d, Elapsed(ms)=%d,

... ETAG=%s, contentLength=%d, requestMethod=%s",

threadId, statusCode, elapsed, etag, contentLength, requestMethod));

}

}

Model

Prediction

True ref:

LOG.isDebugEnabled()

SLM (this work)

elapsed != null

(32.1%)

LOG.isDebugEnabled()

(29.0%)

!LOG.isDebugEnabled()

(2.4%)

Transformerbase +copy

stopDate != null

result.hasStatusCode()

result.hasStatusCode() != elapsed

BiLSTM→LSTM +copy

result != null

elapsed &gt; 0

result.getStatusCode() == workflowConstants.STATUS

Seq2tree +copy

event.getConnectionObject() instanceof HttpUrlConnection

startDate != null

LOG.isDebugEnabled()

Figure 13. Java examples from our test set along with the predictions of our model and the baselines.


Structural Language Models of Code

private static boolean isNameResolved(InetAddress address) {

String hostname =

address.getHostName() ;

String ip = address.getHostAddress();

return !hostname.equals(ip) || NetUtils.isLocalAddress(address);

}

Model

Prediction

True ref:

address.getHostName()

SLM (this work)

address.getHostname()

(3.5%)

address.getHostName()

(2.0%)

inetAddress.getByName(address.getAddress())

(0.7%)

Transformerbase +copy

address.getHostAddress()

address.getLastElement().getValue()

address.getAddress()

BiLSTM→LSTM +copy

address.getHostAddress()

address.getPort()

address.getAddress()

Seq2tree +copy

address.getHostAddress()

address.getPort()

address.getAddress()

private synchronized void initJournals(List&lt;URI&gt; dirs) {

int minimumRedundantJournals = conf.getInt(

DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_KEY,

DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_DEFAULT);

journalSet = new JournalSet(minimumRedundantJournals);

for (URI u : dirs) {

boolean required =

FSNamesystem.getRequiredNamespaceEditsDirs(conf).contains(u);

if ( u.getScheme() .equals(NNStorage.LOCAL_URI_SCHEME)) {

StorageDirectory sd = storage.getStorageDirectory(u);

if (sd != null) {

journalSet.add(

new FileJournalManager(conf, sd, storage),

required, sharedEditsDirs.contains(u));

}

} else {

journalSet.add(createJournal(u),

required, sharedEditsDirs.contains(u));

}

}

if (journalSet.isEmpty()) {

LOG.error("No edits directories configured!");

}

}

Model

Prediction

True ref:

u.getScheme()

SLM (this work)

u.getName()

(27.4%)

u.getScheme()

(13.1%)

u.getVersion()

(8.2%)

Transformerbase +copy

journalSet.LOCAL URI SCHEME

u.getName()

Boolean.true

BiLSTM→LSTM +copy

u.toString()

Boolean.true

u.getURI()

Seq2tree +copy

u.getScheme()

u.getName()

storage.getLocalUriScheme()

Figure 14. Java examples from our test set along with the predictions of our model and the baselines.


Structural Language Models of Code

static EnumSet&lt;FileAttribute&gt; parse(String s) {

if (s == null || s.length() == 0) {

return EnumSet.allOf(FileAttribute.class);

}

EnumSet&lt;FileAttribute&gt; set = EnumSet.noneOf(FileAttribute.class);

FileAttribute[] attributes = values();

for (char c :

s.toCharArray() ) {

int i = 0;

for (; i &lt; attributes.length &amp;&amp; c != attributes[i].symbol; i++) ;

if (i &lt; attributes.length) {

if (!set.contains(attributes[i])) {

set.add(attributes[i]);

} else {

throw new IllegalArgumentException("There are more than one '"

+ attributes[i].symbol + "' in " + s);

}

} else {

throw new IllegalArgumentException("'" + c + "' in "

+ s + " is undefined.");

}

}

return set;

}

Model

Prediction

True ref:

s.toCharArray()

SLM (this work)

s.toCharArray()

(22.4%)

attributes[0].value

(18.5%)

attributes[undefined].length

(4.6%)

Transformerbase +copy

s.split(" "

set.split(" ")

attributes.keySet()

BiLSTM→LSTM +copy

attributes.length

attributes[0]

attributes[0].next

Seq2tree +copy

set.toArray()

s.toCharArray()

set.toCharArray()

public static Path[] stat2Paths(FileStatus[] stats) {

if (stats == null)

return null;

Path[] ret = new Path[stats.length];

for (int i = 0; i &lt; stats.length; ++i) {

ret[i] =

stats[i].getPath() ;

}

return ret;

}

Model

Prediction

True ref:

stats[i].getPath()

SLM (this work)

stats[i].getPath()

(25.2%)

Path(stats[i])

(3.3%)

new Path(stats[i], charset)

(2.5%)

Transformerbase +copy

stats[i]

stats[i].getPath()

new Path(stats[i])

BiLSTM→LSTM +copy

stats[i]

new Path(stats[i])

stats[i].toString()

Seq2tree +copy

stats[i]

new Path(stats[i])

stat(stats[i])

Figure 15. Java examples from our test set along with the predictions of our model and the baselines.


Structural Language Models of Code

void ensureCurrentDirExists() throws IOException {

for (

Iterator&lt;StorageDirectory&gt; it = storage.dirIterator();

it.hasNext(); ) {

StorageDirectory sd = it.next();

File curDir = sd.getCurrentDir();

if ( !curDir.exists()

&amp;&amp; !curDir.mkdirs()) {

throw new IOException("Could not create directory " + curDir);

}

}

}

Model

Prediction

True ref:

!curDir.exists()

SLM (this work)

!curDir.exists()

(29.0%)

curDir != null

(25.8%)

curDir.exists()

(24.4%)

Transformerbase +copy

curDir != null

!curDir.exists()

curDir.exists()

BiLSTM→LSTM +copy

curDir != null

curDir.exists()

sd != null

Seq2tree +copy

curDir != null

curDir.exists()

!curDir.exists()

public static byte[] getXAttr(final Map&lt;?, ?&gt; json, final String name)

throws IOException {

if (json == null) {

return null;

}

Map&lt;String, byte[]&gt; xAttrs = toXAttrs(json);

if (xAttrs != null) {

return

xAttrs.get(name) ;

}

return null;

}

Model

Prediction

True ref:

xAttrs.get(name)

SLM (this work)

xAttrs.get(name)

(28.2%)

xAttrs.get(xAttrs)

(5.8%)

xAttrs.toByteArray()

(4.4%)

Transformerbase +copy

xAttrs.get(name)

xAttrs.toByteArray()

new byte[0]

BiLSTM→LSTM +copy

xAttrs.getBytes()

new byte[0]

xAttrs.toByteArray()

Seq2tree +copy

xAttrs.get(name)

xAttrs.get()

xAttrs.get(0)

Figure 16. Java examples from our test set along with the predictions of our model and the baselines.


Structural Language Models of Code

private void setFlag(long flag) {

long prev;

do {

prev = unsafe.getLongVolatile(null, this.slotAddress);

if ( (prev &amp; flag)

!= 0) {

return;

}

} while (!unsafe.compareAndSwapLong(

null, this.slotAddress, prev, prev | flag));

}

Model

Prediction

True ref:

(prev &amp; flag)

SLM (this work)

(prev &amp; flag)

(8.9%)

(prev &amp; flagSlot)

(5.4%)

unsafe.get(prev)

(5.0%)

Transformerbase +copy

(prev &amp; flag)

(prev | flag)

unsafe.compareTo(prev)

BiLSTM→LSTM +copy

prev

prev + 1

prev - 1

Seq2tree +copy

unsafe prev flag

(Syntax error)

(volatile prev unsafe.get())

(Syntax error)

(volatile prev unsafe.getLongVolatile(null, prev))

(Syntax error)

public synchronized void setInput(byte[] b, int off, int len) {

if (b == null) {

throw new NullPointerException();

}

if (off &lt; 0 ||

len &lt; 0

|| off &gt; b.length - len) {

throw new ArrayIndexOutOfBoundsException();

}

finished = false;

if (len &gt; uncompressedDirectBuf.remaining()) {

this.userBuf = b;

this.userBufOff = off;

this.userBufLen = len;

} else {

((ByteBuffer) uncompressedDirectBuf).put(b, off, len);

uncompressedDirectBufLen = uncompressedDirectBuf.position();

}

bytesRead += len;

}

Model

Predictions

True ref:

len &lt; 0

SLM (this work)

len &lt; 0

(41.3%)

off &gt; b.length

(23.4%)

len &gt; b.length

(14.1%)

Transformerbase +copy

off &lt; 0

len &lt; 0

b == null

BiLSTM→LSTM +copy

off &lt; 0

len &lt; 0

b == null

Seq2tree +copy

off &lt; 0

len &lt; 0

0 &lt; off

Figure 17. Java examples from our test set along with the predictions of our model and the baselines.


Structural Language Models of Code

private int readData(byte[] buf, int off, int len) throws IOException {

int bytesRead = 0;

while (bytesRead &lt; len) {

int n = IOUtils.wrappedReadForCompressedData(

in, buf,

off + bytesRead , len - bytesRead);

if (n &lt; 0) {

return bytesRead;

}

bytesRead += n;

}

return len;

}

Model

Prediction

True ref:

off + bytesRead

SLM (this work)

bytesRead - bytesRead

(35.0%)

off + bytesRead

(14.1%)

off - bytesRead

(9.4%)

Transformerbase +copy

off - bytesRead

off + len

len - bytesRead

BiLSTM→LSTM +copy

-bytesRead

bytesRead++

bytesRead - bytesRead

Seq2tree +copy

compressed bytesRead

(Syntax error)

off + bytesRead

len - bytesRead

private Path getPath(int curId, int limitPerDir, Type type) {

if (curId &lt;= 0) {

return basePath;

}

String name = "";

switch(type) {

case FILE:

name = FILE_PREFIX + new Integer(curId % limitPerDir).toString();

break;

case DIRECTORY:

name = DIR_PREFIX + new Integer(curId % limitPerDir).toString();

break;

}

Path base = getPath((curId / limitPerDir), limitPerDir, Type.DIRECTORY);

return

new Path(base, name) ;

}

Model

Prediction

True ref:

new Path(base, name)

SLM (this work)

new Path(base, name)

(6.0%)

new Path(base, name, limitPerDir)

(2.9%)

new Path(base, name, type)

(2.8%)

Transformerbase +copy

new Path(base)

new Path(name)

getPath(base)

BiLSTM→LSTM +copy

new Path(base)

new File(base)

new Path(base.getPath())

Seq2tree +copy

new Path(base)

new File(base, name)

new Path(base, name)

Figure 18. Java examples from our test set along with the predictions of our model and the baselines.


Structural Language Models of Code

private static IEnumerable&lt;Token&gt; OfSequence(

this IEnumerable&lt;Token&gt; tokens, Token nameToken, TypeDescriptor info)

{

var nameIndex = tokens.IndexOf(t =&gt; t.Equals(nameToken));

if ( nameIndex &gt;= 0 )

{

return info.NextValue.MapValueOrDefault(

_ =&gt; info.MaxItems.MapValueOrDefault(

n =&gt; tokens.Skip(nameIndex + 1).Take(n),

tokens.Skip(nameIndex + 1).TakeWhile(v =&gt; v.IsValue())),

tokens.Skip(nameIndex + 1).TakeWhile(v =&gt; v.IsValue()));

}

return new Token[] { };

}

Model

Prediction

True ref:

nameIndex &gt;= 0

SLM (this work)

nameIndex &gt;= 0

(22.6%)

nameIndex == -1

(19.1%)

nameIndex &gt; -1

(13.9%)

BiLSTM→LSTM +copy

!nameIndex

nameIndex == -1

nameIndex &lt; 0

GNN→NAG (Brockschmidt et al., 2019)

nameIndex == 0

nameIndex &gt; 0

nameIndex &lt; 0

public static IEnumerable&lt;T[]&gt; Group&lt;T&gt;(

this IEnumerable&lt;T&gt; source, int groupSize)

{

if (groupSize &lt; 1)

{

throw new ArgumentOutOfRangeException(nameof(groupSize));

}

T[] group = new T[groupSize];

int groupIndex = 0;

foreach (var item in source)

{

group[groupIndex++] = item;

if ( groupIndex == groupSize )

{

yield return group;

group = new T[groupSize];

groupIndex = 0;

}

}

}

Model

Prediction

True ref:

groupIndex == groupSize

SLM (this work)

groupIndex &lt; 0

(21.4%)

groupIndex == -1

(10.3%)

groupIndex &lt; groupIndex

(5.3%)

BiLSTM→LSTM +copy

group.IsNullOrEmpty()

groupGroup[groupIndex++]

group.EndsWith(group)

GNN→NAG (Brockschmidt et al., 2019)

groupIndex == 0

groupIndex == 1

groupIndex == groupSize

Figure 19. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.


Structural Language Models of Code

internal static void AddLine(StringBuilder builder,

string value, int maximumLength)

{

if (builder == null)

{

throw new ArgumentNullException(nameof(builder));

}

if (value == null)

{

throw new ArgumentNullException(nameof(value));

}

if (maximumLength &lt; 1)

{

throw new ArgumentOutOfRangeException(nameof(value));

}

value =

value.Trim() ;

builder.AppendWhen(builder.Length &gt; 0, Environment.NewLine);

do

{

var wordBuffer = 0;

var words = value.Split(' ');

for (var i = 0; i &lt; words.Length; i++)

{

if (words[i].Length &lt; (maximumLength - wordBuffer))

{

builder.Append(words[i]);

wordBuffer += words[i].Length;

if ((maximumLength - wordBuffer) &gt; 1 &amp;&amp; i != words.Length - 1)

{

builder.Append(" ");

wordBuffer++;

}

}

else if (words[i].Length &gt;= maximumLength &amp;&amp; wordBuffer == 0)

{

builder.Append(words[i].Substring(0, maximumLength));

wordBuffer = maximumLength;

break;

}

else break;

}

value = value.Substring(Math.Min(wordBuffer, value.Length));

builder.AppendWhen(value.Length &gt; 0, Environment.NewLine);

}

while (value.Length &gt; maximumLength);

builder.Append(value);

}

Model

Prediction

True ref:

value.Trim()

SLM (this work)

value.Trim()

(16.0%)

value.Substring(0, maximumLength)

(10.9%)

value.Replace(maximumLength, maximumLength

(10.7%)

BiLSTM→LSTM +copy

maximumLength - 1

value.Trim()

valueLength++

GNN→NAG

value + &lt;UNK&gt;

value + maximumLength

value.Substring(0, maximumLength)

Figure 20. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.


Structural Language Models of Code

public static string[] TrimStringArray(this IEnumerable&lt;string&gt; array)

{

return array.Select(item =&gt;

item.Trim() ).ToArray();

}

Model

Prediction

True ref:

item.Trim()

SLM (this work)

item.Trim()

(20.1%)

item.ToUpperInvariant()

(3.5%)

item.ToUpper()

(1.6%)

BiLSTM→LSTM +copy

item.Trim()

item.ToTrim()

item.]

(Syntax error)

GNN→NAG (Brockschmidt et al., 2019)

item + &lt;UNK&gt;

item + item

item + 1

public static string Camelize(this string input)

{

var word = Pascalize(input);

return

word.Substring(0, 1) .ToLower() +

word.Substring(1) ;

}

Model

Prediction

True ref:

word.Substring(0, 1)

word.Substring(1)

SLM (this work)

word.Substring(0, 1)

word.Substring(1)

word.Trim()

wordData.Substring(1)

word.Substring(1)

word.Substring(0, 1)

BiLSTM→LSTM +copy

input.Replace("&amp;", " )

input.Replace("&amp;", " &lt;UNK&gt; )

input.Replace(1, ’’)

input + "." + input

input.Replace("&amp;", "")

input.Substring(0, 1)

GNN→NAG

word.CombineWith(&lt;UNK&gt;)

word.CombineWith(&lt;UNK&gt;)

word.Trim()

word + &lt;UNK&gt;

word.CombineWith(input)

word.Replace(&lt;UNK&gt;, &lt;UNK&gt;)

Figure 21. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.


Structural Language Models of Code

public string Truncate(string value, int length, string truncationString,

TruncateFrom truncateFrom = TruncateFrom.Right)

{

if (value == null)

return null;

if (value.Length == 0)

return value;

if (truncationString == null)

truncationString = string.Empty;

if (truncationString.Length &gt; length)

return truncateFrom == TruncateFrom.Right ?

value.Substring(0, length) : value.Substring(value.Length - length);

var alphaNumericalCharactersProcessed = 0;

if (value.ToCharArray().Count(char.IsLetterOrDigit) &lt;= length)

return value;

if (truncateFrom == TruncateFrom.Left)

{

for (var i = value.Length - 1; i &gt; 0; i--)

{

if (char.IsLetterOrDigit(value[i]))

alphaNumericalCharactersProcessed++;

if (alphaNumericalCharactersProcessed + truncationString.Length

== length)

return truncationString + value.Substring(i);

}

}

for (var i = 0; i &lt; value.Length - truncationString.Length; i++)

{

if (char.IsLetterOrDigit(value[i]))

alphaNumericalCharactersProcessed++ ;

if (alphaNumericalCharactersProcessed + truncationString.Length

== length)

return value.Substring(0, i + 1) + truncationString;

}

return value;

}

Model

Prediction

True ref:

alphaNumericalCharactersProcessed++

SLM (this work)

alphaNumericalCharactersProcessed++

(48.1%)

iCount++

(5.8%)

iIndex++

(1.6%)

BiLSTM→LSTM +copy

i++

truncation++

alpha--

GNN→NAG

alphaNumericalCharactersProcessed++

alphaNumericalCharactersProcessed--

--alphaNumericalCharactersProcessed

Figure 22. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.


Structural Language Models of Code

public static int BinarySearch&lt;TItem, TSearch&gt;(

this IList&lt;TItem&gt; list, TSearch value,

Func&lt;TSearch, TItem, int&gt; comparer)

{

if (list == null)

{

throw new ArgumentNullException("list");

}

if (comparer == null)

{

throw new ArgumentNullException("comparer");

}

var lower = 0;

var upper = list.Count - 1;

while (lower &lt;= upper)

{

var middle = lower + (upper - lower) / 2;

var comparisonResult = comparer(value, list[middle]);

if ( comparisonResult &lt; 0 )

{

upper = middle - 1;

}

else if ( comparisonResult &gt; 0 )

{

lower = middle + 1;

}

else

{

return middle;

}

}

return lower;

}

Model

Prediction

True ref:

comparisonResult &lt; 0

comparisonResult &gt; 0

SLM (this work)

comparisonResult &lt; 0

comparisonResult &gt; 0

comparisonResult &gt; 0

comparisonResult &lt; 0

middle == comparisonResult

comparisonResult == 0

BiLSTM→LSTM +copy

lowerResult == middle

lower &lt; 0

lowerResult == 0

lower + "."

lower != middle

lower != middle

GNN→NAG

comparisonResult == 0

comparisonResult == 0

comparisonResult &gt; 0

comparisonResult &gt; 0

comparisonResult &lt; 0

comparisonResult == middle

Figure 23. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.


Structural Language Models of Code

public override string ToString()

{

// use reflection to display all the properties that

// ... have non default values

StringBuilder result = new StringBuilder();

var props = this.GetType().GetTypeInfo().DeclaredProperties;

result.AppendLine("{");

foreach (var prop in props)

{

if (prop.Name != "Content" &amp;&amp; prop.Name != "Subtitle"

&amp;&amp; prop.Name != "Title" &amp;&amp; prop.Name != "UniqueId")

{

object value = prop.GetValue(this);

bool valueIsNull = value == null;

object defaultValue = Common.GetDefault(prop.PropertyType);

bool defaultValueIsNull = defaultValue == null;

if ((valueIsNull != defaultValueIsNull)

// one is null when the other isn't

|| ( !valueIsNull

&amp;&amp; (value.ToString() != defaultValue.ToString())))

// both aren't null, so compare as strings

{

result.AppendLine(prop.Name + " : " + prop.GetValue(this));

}

}

}

result.AppendLine("}");

return result.ToString();

}

Model

Prediction

True ref:

!valueIsNull

SLM (this work)

!valueIsNull

(52.4%)

!defaultValueIsNull

(9.0%)

!valueIsNull.IsNullOrEmpty()

(3.2%)

BiLSTM→LSTM +copy

!defaultValueIsNull

(defaultValueIsNull || value)

(defaultValueIsNull || defaultValue)

GNN→NAG

!valueIsNull

!defaultValueIsNull

!!valueIsNull

Figure 24. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.


Structural Language Models of Code

public TradierOrderResponse PlaceOrder(string accountId,

TradierOrderClass classification,

TradierOrderDirection direction,

string symbol,

decimal quantity,

decimal price = 0,

decimal stop = 0,

string optionSymbol = "",

TradierOrderType type = TradierOrderType.Market,

TradierOrderDuration duration = TradierOrderDuration.GTC)

{

//Compose the request:

var request = new RestRequest("accounts/{accountId}/orders");

request.AddUrlSegment("accountId", accountId.ToString());

//Add data:

request.AddParameter("class", GetEnumDescription(classification));

request.AddParameter("symbol", symbol);

request.AddParameter("duration", GetEnumDescription(duration));

request.AddParameter("type", GetEnumDescription(type));

request.AddParameter("quantity", quantity);

request.AddParameter("side", GetEnumDescription(direction));

//Add optionals:

if (price &gt; 0) request.AddParameter("price", Math.Round(price, 2));

if (stop &gt; 0) request.AddParameter("stop", Math.Round(stop, 2));

if ( optionSymbol != "" )

request.AddParameter("option_symbol", optionSymbol);

//Set Method:

request.Method = Method.POST;

return Execute&lt;TradierOrderResponse&gt;(request,

TradierApiRequestType.Orders);

}

Model

Prediction

True ref:

optionSymbol != ""

SLM (this work)

optionSymbol != ""

(5.5%)

optionSymbol == ""

(4.4%)

optionSymbol.IsNullOrEmpty()

(1.1%)

BiLSTM→LSTM +copy

!stopSymbol

stopSymbol != optionSymbol

(stopSymbol " &amp;&amp; optionSymbol)

(Syntax error)

GNN→NAG

optionSymbol == &lt;UNK&gt;

optionSymbol == symbol

optionSymbol != symbol

Figure 25. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.


Structural Language Models of Code

[Test, TestCaseSource("GetLeanDataLineTestParameters")]

public void GetSourceMatchesGenerateZipFilePath(

LeanDataLineTestParameters parameters)

{

var source = parameters.Data.GetSource(

parameters.Config, parameters.Data.Time.Date, false);

var normalizedSourcePath = new FileInfo(source.Source).FullName;

var zipFilePath = LeanData.GenerateZipFilePath(

Globals.DataFolder, parameters.Data.Symbol,

parameters.Data.Time.Date,

parameters.Resolution, parameters.TickType);

var normalizeZipFilePath = new FileInfo(zipFilePath).FullName;

var indexOfHash = normalizedSourcePath.LastIndexOf(

"#", StringComparison.Ordinal);

if (indexOfHash &gt; 0)

{

normalizedSourcePath =

normalizedSourcePath.Substring(0, indexOfHash) ;

}

Assert.AreEqual(normalizeZipFilePath, normalizedSourcePath);

}

Model

Prediction

True ref:

normalizedSourcePath.Substring(0, indexOfHash)

SLM (this work)

normalizedSourcePath.Substring(0, indexOfHash)

(28.3%)

normalizedSourcePath.Substring(1)

(8.8%)

normalizedSourcePath.Remove(indexOfHash)

(8.2%)

BiLSTM→LSTM +copy

indexOfHash + "&lt;UNK&gt;"

indexOfHash &gt; normalizedOfHash

indexOfHash &gt; 0

GNN→NAG

normalizedSourcePath + normalizeZipFilePath

normalizedSourcePath + normalizedSourcePath

normalizedSourcePath + normalizeZipFilePath + &lt;UNK&gt;

Figure 26. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.


Structural Language Models of Code

References

Aharoni, R. and Goldberg, Y. Towards string-to-tree neural machine translation. In Proceedings of the 55th Annual Meeting

of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 132–140, 2017.

Allamanis, M. The adverse effects of code duplication in machine learning models of code. In Proceedings of the 2019

ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reﬂections on Programming and Software,

pp. 143–153. ACM, 2019.

Allamanis, M., Tarlow, D., Gordon, A., and Wei, Y. Bimodal modelling of source code and natural language. In Interna-

tional conference on machine learning, pp. 2123–2132, 2015.

Allamanis, M., Peng, H., and Sutton, C. A convolutional attention network for extreme summarization of source code. In

International conference on machine learning, pp. 2091–2100, 2016.

Allamanis, M., Brockschmidt, M., and Khademi, M.

Learning to represent programs with graphs.

In International

Conference on Learning Representations, 2018.

Alon, U. and Yahav, E.

On the bottleneck of graph neural networks and its practical implications.

arXiv preprint

arXiv:2006.05205, 2020.

Alon, U., Zilberstein, M., Levy, O., and Yahav, E. A general path-based representation for predicting program properties.

In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation, pp.

404–419, 2018.

Alon, U., Brody, S., Levy, O., and Yahav, E. code2seq: Generating sequences from structured representations of code. In

International Conference on Learning Representations, 2019a.

Alon, U., Zilberstein, M., Levy, O., and Yahav, E. code2vec: Learning distributed representations of code. Proceedings of

the ACM on Programming Languages, 3(POPL):1–29, 2019b.

Amodio, M., Chaudhuri, S., and Reps, T.

Neural attribute machines for program generation.

arXiv preprint

arXiv:1705.09231, 2017.

Balog, M., Gaunt, A. L., Brockschmidt, M., Nowozin, S., and Tarlow, D. Deepcoder: Learning to write programs. In

International Conference on Learning Representations, 2017.

Bielik, P., Raychev, V., and Vechev, M. Phog: probabilistic model for code. In International Conference on Machine

Learning, pp. 2933–2942, 2016.

Brockschmidt, M., Allamanis, M., Gaunt, A. L., and Polozov, O. Generative code modeling with graphs. In International

Conference on Learning Representations, 2019.

Brody, S., Alon, U., and Yahav, E. Neural edit completion. arXiv preprint arXiv:2005.13209, 2020.

Chen, X., Liu, C., and Song, D. Tree-to-tree neural networks for program translation. In Advances in Neural Information

Processing Systems, pp. 2547–2557, 2018.

Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A.-r., and Kohli, P. Robustﬁll: Neural program learning under

noisy i/o. In International Conference on Machine Learning, pp. 990–998, 2017.

Dong, L. and Lapata, M. Coarse-to-ﬁne decoding for neural semantic parsing. In Proceedings of the 56th Annual Meeting

of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 731–742, 2018.

Ellis, K., Nye, M., Pu, Y., Sosa, F., Tenenbaum, J., and Solar-Lezama, A. Write, execute, assess: Program synthesis with a

repl. In Advances in Neural Information Processing Systems, pp. 9169–9178, 2019.

Fernandes, P., Allamanis, M., and Brockschmidt, M. Structured neural summarization. In International Conference on

Learning Representations, 2019.

Gaunt, A. L., Brockschmidt, M., Kushman, N., and Tarlow, D. Differentiable programs with neural libraries. In Interna-

tional Conference on Machine Learning, pp. 1213–1222, 2017.


Structural Language Models of Code

Green, C. Application of theorem proving to problem solving. In Readings in Artiﬁcial Intelligence, pp. 202–222. Elsevier,

1981.

Gu, J., Lu, Z., Li, H., and Li, V. O. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings

of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1631–1640,

2016.

Gulwani, S. Automating string processing in spreadsheets using input-output examples. In Proceedings of the 38th annual

ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pp. 317–330, 2011.

Gulwani, S., Polozov, O., Singh, R., et al. Program synthesis. Foundations and Trends® in Programming Languages, 4

(1-2):1–119, 2017.

Iyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L. Mapping language to code in programmatic context. In Proceedings

of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1643–1652, 2018.

Iyer, S., Cheung, A., and Zettlemoyer, L. Learning programmatic idioms for scalable semantic parsing. In Proceedings of

the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference

on Natural Language Processing (EMNLP-IJCNLP), pp. 5429–5438, 2019.

Kingma, D. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Represen-

tations, 2015.

Klein, G., Kim, Y., Deng, Y., Senellart, J., and Rush, A. M. Opennmt: Open-source toolkit for neural machine translation.

In Proceedings of ACL 2017, System Demonstrations, pp. 67–72, 2017.

Ling, W., Blunsom, P., Grefenstette, E., Hermann, K. M., Koˇcisk`y, T., Wang, F., and Senior, A. Latent predictor networks

for code generation.

In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics

(Volume 1: Long Papers), pp. 599–609, 2016.

Luong, M.-T., Pham, H., and Manning, C. D. Effective approaches to attention-based neural machine translation. In

Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1412–1421, 2015.

Maddison, C. and Tarlow, D. Structured generative models of natural source code. In International Conference on Machine

Learning, pp. 649–657, 2014.

Murali, V., Qi, L., Chaudhuri, S., and Jermaine, C. Neural sketch learning for conditional program generation. In Interna-

tional Conference on Learning Representations, 2018.

Parisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D., and Kohli, P. Neuro-symbolic program synthesis. In Interna-

tional Conference on Learning Representations, 2017.

Pnueli, A. and Rosner, R. On the synthesis of a reactive module. In Proceedings of the 16th ACM SIGPLAN-SIGACT

symposium on Principles of programming languages, pp. 179–190. ACM, 1989.

Polozov, O. and Gulwani, S. Flashmeta: a framework for inductive program synthesis. In Proceedings of the 2015 ACM

SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications, pp. 107–

126, 2015.

Rabinovich, M., Stern, M., and Klein, D. Abstract syntax networks for code generation and semantic parsing. In Pro-

ceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.

1139–1149, 2017.

Raychev, V., Bielik, P., Vechev, M., and Krause, A. Learning programs from noisy data. In Proceedings of the 43rd Annual

ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pp. 761–774, 2016.

Si, X., Yang, Y., Dai, H., Naik, M., and Song, L.

Learning a meta-solver for syntax-guided program synthesis.

In

International Conference on Learning Representations, 2019.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all

you need. In Advances in Neural Information Processing Systems, pp. 6000–6010, 2017.


Structural Language Models of Code

Waldinger, R. J. and Lee, R. C. Prow: A step toward automatic program writing. In Proceedings of the 1st international

joint conference on Artiﬁcial intelligence, pp. 241–252, 1969.

Xiao, C., Dymetman, M., and Gardent, C. Sequence-based structured prediction for semantic parsing. In Proceedings of

the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1341–1350,

2016.

Yin, P. and Neubig, G. A syntactic neural model for general-purpose code generation. In Proceedings of the 55th Annual

Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 440–450, 2017.

Yin, P., Neubig, G., Allamanis, M., Brockschmidt, M., and Gaunt, A. L. Learning to represent edits. In International

Conference on Learning Representations, 2019.

Young, H., Bastani, O., and Naik, M. Learning neurosymbolic generative models via program synthesis. In International

Conference on Machine Learning, pp. 7144–7153, 2019.

Yu, T., Li, Z., Zhang, Z., Zhang, R., and Radev, D. Typesql: Knowledge-based type-aware neural text-to-sql generation. In

Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:

Human Language Technologies, Volume 2 (Short Papers), pp. 588–594, 2018.

Zhao, R., Bieber, D., Swersky, K., and Tarlow, D. Neural networks for modeling source code edits. arXiv preprint

arXiv:1904.02818, 2019.

