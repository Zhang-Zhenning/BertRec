


Published in

Towards Data Science



Jun 14, 2017

·

7 min read

·

Save

Word2Vec (skip-gram model): PART 1 - Intuition.

CBOW

Skip-Gram

corpus

window size

Intuition








1 x 300

exp(x)

If two different words have very similar “contexts” (that is, what words are


likely to appear around them), then our model needs to output very

similar results for these two words. And one way for the network to

output similar context predictions for these two words is if the word

vectors are similar. So, if two words have similar contexts, then our

network is motivated to learn similar word vectors for these two words!

Ta da!

Subsampling:


Negative Sampling:

Machine Learning

Deep Learning

Word2vec




Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





NLP

Towards Data Science

