
Statistical Natural Language Processing

N-gram Language Models

Çağrı Çöltekin

University of Tübingen

Seminar für Sprachwissenschaft

Summer Semester 2018

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

N-gram language models

• A language model answers the question how likely is a

sequence of words in a given language?

• They assign scores, typically probabilities, to sequences (of

words, letters, …)

• n-gram language models are the ‘classical’ approach to

language modeling

• The main idea is to estimate probabilities of sequences,

using the probabilities of words given a limited history

• As a bonus we get the answer for what is the most likely word

given previous words?

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

1 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

N-grams in practice: spelling correction

• How would a spell checker know that there is a spelling

error in the following sentence?

I like pizza wit spinach

• Or this one?

Zoo animals on the lose

We want:

P(I like pizza with spinach)

&gt; P(I like pizza wit spinach)

P(Zoo animals on the loose)

&gt; P(Zoo animals on the lose)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

2 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

N-grams in practice: speech recognition

r

e

k

@

n

ai

s

b

ii

ch

her

and

I

s

be

a

aren’t

ice

bee

an

eye

beach

not

nice

an

aren’t

speech

in

ice

speech

wreck

on

reckon

recognize

We want:

P(recognize speech) &gt; P(wreck a nice beach)

* Reproduced from Shillcock (1995)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

3 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Speech recognition gone wrong



Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

4 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Speech recognition gone wrong



Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

4 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Speech recognition gone wrong



Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

4 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Speech recognition gone wrong



Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

4 / 81


Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

What went wrong?

Recap: noisy channel model

tell the truth

encoder

decoder

smell the soup

noisy

channel

• We want P(u | A), probability of the utterance given the

acoustic signal

• The model of the noisy channel gives us P(A | u)

• We can use Bayes’ formula

P(u | A) = P(A | u)P(u)

P(A)

• P(u), probabilities of utterances, come from a language

model

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

5 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

N-grams in practice: machine translation

German to English translation:

• Correct word choice

German

English

Der grosse Mann tanzt gerne

The big man likes to dance

Der grosse Mann weiß alles

The great man knows all

• Correct ordering / word choice

German

English alternatives

Er tanzt gerne

He dances with pleasure

He likes to dance

We want:

P(He likes to dance) &gt; P(He dances with pleasure)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

6 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

N-grams in practice: predictive text



Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

7 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

N-grams in practice: predictive text



• How many language models are there in the example

above?

• Screenshot from google.com - but predictive text is used

everywhere

• If you want examples of predictive text gone wrong, look

for ‘auto-correct mistakes’ on the Web.

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

7 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

More applications for language models

• Spelling correction

• Speech recognition

• Machine translation

• Predictive text

• Text recognition (OCR, handwritten)

• Information retrieval

• Question answering

• Text classification

• …

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

8 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Overview

of the overview

Why do we need n-gram language models?

What are they?

How do we build and use them?

What alternatives are out there?

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

9 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Overview

in a bit more detail

• Why do we need n-gram language models?

• How to assign probabilities to sequences?

• N-grams: what are they, how do we count them?

• MLE: how to assign probabilities to n-grams?

• Evaluation: how do we know our n-gram model works

well?

• Smoothing: how to handle unknown words?

• Some practical issues with implementing n-grams

• Extensions, alternative approaches

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

10 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Our aim

We want to solve two related problems:

• Given a sequence of words w = (w1w2 . . . wm),

what is the probability of the sequence

P(w)?

(machine translation, automatic speech recognition, spelling correction)

• Given a sequence of words w1w2 . . . wm−1,

what is the probability of the next word

P(wm | w1 . . . wm−1)?

(predictive text)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

11 / 81


Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Assigning probabilities to sentences

count and divide?

How do we calculate the probability a sen-

tence like P(I like pizza with spinach)

• Can we count the occurrences of the

sentence, and divide it by the total

number of sentences (in a large corpus)?

• Short answer: No.

– Many sentences are not observed even

in very large corpora

– For the ones observed in a corpus,

probabilities will not reflect our

intuition, or will not be useful in most

applications

P( ) = ?

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

12 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Assigning probabilities to sentences

applying the chain rule

• The solution is to decompose

We use probabilities of parts of the sentence (words) to

calculate the probability of the whole sentence

• Using the chain rule of probability (without loss of

generality), we can write

P(w1, w2, . . . , wm) =

P(w2 | w1)

× P(w3 | w1, w2)

× . . .

× P(wm | w1, w2, . . . wm−1)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

13 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Example: applying the chain rule

P(I like pizza with spinach) =

P(like | I)

× P(pizza | I like)

× P(with | I like pizza)

× P(spinach | I like pizza with)

• Did we solve the problem?

• Not really, the last term is equally difficult to estimate

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

14 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Assigning probabilities to sentences

the Markov assumption

We make a conditional independence assumption: probabilities of

words are independent, given n previous words

P(wi | w1, . . . , wi−1) = P(wi | wi−n+1, . . . , wi−1)

and

P(w1, . . . , wm) =

m

∏

i=1

P(wi | wi−n+1, . . . , wi−1)

For example, with n = 2 (bigram, first order Markov model):

P(w1, . . . , wm) =

m

∏

i=1

P(wi | wi−1)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

15 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Example: bigram probabilities of a sentence

P(I like pizza with spinach) =

P(like | I)

× P(pizza | like)

× P(with | pizza)

× P(spinach | with)

• Now, hopefully, we can count them in a corpus

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

16 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Maximum-likelihood estimation (MLE)

• Maximum-likelihood estimation of n-gram probabilities is

based on their frequencies in a corpus

• We are interested in conditional probabilities of the form:

P(wi | w1, . . . , wi−1), which we estimate using

P(wi | wi−n+1, . . . , wi−1) =

C(wi−n+1 . . . wi)

C(wi−n+1 . . . wi−1)

where, C() is the frequency (count) of the sequence in the

corpus.

• For example, the probability P(like | I) would be

P(like | I)

=

C(I like)

C(I)

=

number of times I like occurs in the corpus

number of times I occurs in the corpus

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

17 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

MLE estimation of an n-gram language model

An n-gram model conditioned on n − 1 previous words.

• In a 1-gram (unigram) model,

P(wi) = C(wi)

N

• In a 2-gram (bigram) model,

P(wi) = P(wi | wi−1) = C(wi−1wi)

C(wi−1)

• In a 3-gram (trigram) model,

P(wi) = P(wi | wi−2wi−1) = C(wi−2wi−1wi)

C(wi−2wi−1)

Training an n-gram model involves estimating these pa-

rameters (conditional probabilities).

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

18 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Unigrams

Unigrams are simply the single words (or tokens).

A small corpus

I ’m sorry , Dave .

I ’m afraid I can ’t do that .

When tokenized, we

have 15 tokens, and 11

types.





















































































Unigram counts

ngram

freq

ngram

freq

ngram

freq

ngram

freq

I

3

,

1

afraid

1

do

1

’m

2

Dave

1

can

1

that

1

sorry

1

.

2

’t

1

Traditionally, can’t is tokenized as ca␣n’t (similar to have␣n’t, is␣n’t etc.), but for our purposes can␣’t is more readable.

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

19 / 81


Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Unigram probability of a sentence

Unigram counts

ngram

freq

ngram

freq

ngram

freq

ngram

freq

I

3

,

1

afraid

1

do

1

’m

2

Dave

1

can

1

that

1

sorry

1

.

2

’t

1

P(I 'm sorry , Dave .)

=

P(I)

×

P('m)

×

P(sorry)

×

P(,)

×

P(Dave)

×

P(.)

=

3

15

×

2

15

×

1

15

×

1

15

×

1

15

×

2

15

=

0.000 001 05

• P(, 'm I . sorry Dave) = ?

• What is the most likely sentence according to this model?

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

20 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

N-gram models define probability distributions

• An n-gram model defines a probability

distribution over words

∑

w∈V

P(w) = 1

• They also define probability

distributions over word sequences of

equal size. For example (length 2),

∑

w∈V

∑

v∈V

P(w)P(v) = 1

• What about sentences?

word

prob

I

0.200

’m

0.133

.

0.133

’t

0.067

,

0.067

Dave

0.067

afraid

0.067

can

0.067

do

0.067

sorry

0.067

that

0.067

1.000

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

21 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Unigram probabilities

I

’m

.

’t

,

Dave

afraid

can

do

sorry

that

0

0.1

0.2

3

2

2

1

1

1

1

1

1

1

1

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

22 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Unigram probabilities in a (slightly) larger corpus

MLE probabilities in the Universal Declaration of Human Rights

0

50

100

150

200

250

0.00

0.02

0.04

0.06

a long tail follows … 536



rank

MLE probability

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

23 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Zipf’s law – a short divergence

The frequency of a word is inversely proportional to its rank:

rank × frequency = k

or

frequency ∝

1

rank

• This is a reoccurring theme in (computational) linguistics:

most linguistic units follow more-or-less a similar

distribution

• Important consequence for us (in this lecture):

– even very large corpora will not contain some of the words

(or n-grams)

– there will be many low-probability events (words/n-grams)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

24 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Bigrams

Bigrams are overlapping sequences of two tokens.

I

’m

sorry

,

Dave

.

I

’m

afraid

I

can

’t

do

that

.

Bigram counts

ngram

freq

ngram

freq

ngram

freq

ngram

freq

I ’m

2

, Dave

1

afraid I

1

n’t do

1

’m sorry

1

Dave .

1

I can

1

do that

1

sorry ,

1

’m afraid

1

can ’t

1

that .

1

• What about the bigram ‘ . I ’?

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

25 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Sentence boundary markers

If we want sentence probabilities, we need to mark them.

⟨s⟩ I ’m sorry , Dave . ⟨/s⟩

⟨s⟩ I ’m afraid I can ’t do that . ⟨/s⟩

• The bigram ‘ ⟨s⟩ I ’ is not the same as the unigram ‘ I ’

Including ⟨s⟩ allows us to predict likely words at the

beginning of a sentence

• Including ⟨/s⟩ allows us to assign a proper probability

distribution to sentences

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

26 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Calculating bigram probabilities

recap with some more detail

We want to calculate P(w2 | w1). From the chain rule:

P(w2 | w1) = P(w1, w2)

P(w1)

and, the MLE

P(w2 | w1) =

C(w1w2)

N

C(w1)

N

= C(w1w2)

C(w1)

P(w2 | w1) is the probability of w2 given the previous word is w1

P(w2, w1) is the probability of the sequence w1w2

P(w1) is the probability of w1 occurring as the first item in a bigram,

not its unigram probability

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

27 / 81


Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Bigram probabilities

w1w2

C(w1w2)

C(w1)

P(w1w2)

P(w1)

P(w2 | w1)

P(w2)

⟨s⟩ I

2

2

0.12

0.12

1.00

0.18

I ’m

2

3

0.12

0.18

0.67

0.12

’m sorry

1

2

0.06

0.12

0.50

0.06

sorry ,

1

1

0.06

0.06

1.00

0.06

, Dave

1

1

0.06

0.06

1.00

0.06

Dave .

1

1

0.06

0.06

1.00

0.12

’m afraid

1

2

0.06

0.12

0.50

0.06

afraid I

1

1

0.06

0.06

1.00

0.18

I can

1

3

0.06

0.18

0.33

0.06

can ’t

1

1

0.06

0.06

1.00

0.06

n’t do

1

1

0.06

0.06

1.00

0.06

do that

1

1

0.06

0.06

1.00

0.06

that .

1

1

0.06

0.06

1.00

0.12

. ⟨/s⟩

2

2

0.12

0.12

1.00

0.12

unigram probability!

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

28 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Sentence probability: bigram vs. unigram

I

’m

sorry

,

Dave

.

⟨/s⟩

0

0.5

1

Unigram

Bigram

Puni(⟨s⟩ I ’m sorry , Dave . ⟨/s⟩)

= 2.83 × 10−9

Pbi(⟨s⟩ I ’m sorry , Dave . ⟨/s⟩)

= 0.33

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

29 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Unigram vs. bigram probabilities

in sentences and non-sentences

w

I

’m

sorry

,

Dave

.

Puni

0.20

0.13

0.07

0.07

0.07

0.07

2.83 × 10−9

Pbi

1.00

0.67

0.50

1.00

1.00

1.00

0.33

w

,

’m

I

.

sorry

Dave

Puni

0.07

0.13

0.20

0.07

0.07

0.07

2.83 × 10−9

Pbi

0.00

0.00

0.00

0.00

0.00

1.00

0.00

w

I

’m

afraid

,

Dave

.

Puni

0.07

0.13

0.07

0.07

0.07

0.07

2.83 × 10−9

Pbi

1.00

0.67

0.50

0.00

0.50

1.00

0.00

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

30 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Bigram model as a finite-state automaton

⟨s⟩

I

’m

can

sorry

afraid

,

Dave

.

⟨/s⟩

’t

do

that

1.0

0.67

0.33

1.0

1.0

1.0

1.0

1.0

1.0

1.0

1.0

0.5

0.5

1.0

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

31 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Trigrams

⟨s⟩ ⟨s⟩ I ’m sorry , Dave . ⟨/s⟩

⟨s⟩ ⟨s⟩ I ’m afraid I can ’t do that . ⟨/s⟩

Trigram counts

ngram

freq

ngram

freq

ngram

freq

⟨s⟩ ⟨s⟩ I

2

do that .

1

that . ⟨/s⟩

1

⟨s⟩ I ’m

2

I ’m sorry

1

’m sorry ,

1

sorry , Dave

1

, Dave .

1

Dave . ⟨/s⟩

1

I ’m afraid

1

’m afraid I

1

afraid I can

1

I can ’t

1

can ’t do

1

’t do that

1

• How many n-grams are there in a sentence of length m?

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

32 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Trigram probabilities of a sentence

I

’m

sorry

,

Dave

.

⟨/s⟩

0

0.5

1

Unigram

Bigram

Trigram

Puni(I ’m sorry , Dave . ⟨/s⟩)

= 2.83 × 10−9

Pbi(I ’m sorry , Dave . ⟨/s⟩)

= 0.33

Ptri(I ’m sorry , Dave . ⟨/s⟩)

= 0.50

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

33 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Short detour: colorless green ideas

But it must be recognized that the notion ’probability of a

sentence’ is an entirely useless one, under any known

interpretation of this term. — Chomsky (1968)

• The following ‘sentences’ are categorically different:

– Furiously sleep ideas green colorless

– Colorless green ideas sleep furiously

• Can n-gram models model the difference?

• Should n-gram models model the difference?

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

34 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

What do n-gram models model?

• Some morphosyntax: the bigram ‘ideas are’ is (much

more) likely than ‘ideas is’

• Some semantics: ‘bright ideas’ is more likely than ‘green

ideas’

• Some cultural aspects of everyday language: ‘Chinese

food’ is more likely than ‘British food’

• more aspects of ‘usage’ of language

N-gram models are practical tools, and they have been

useful for many tasks.

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

35 / 81


Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

N-grams, so far …

• N-gram language models are one of the basic tools in NLP

• They capture some linguistic (and non-linguistic)

regularities that are useful in many applications

• The idea is to estimate the probability of a sentence based

on its parts (sequences of words)

• N-grams are n consecutive units in a sequence

• Typically, we use sequences of words to estimate sentence

probabilities, but other units are also possible: characters,

phonemes, phrases, …

• For most applications, we introduce sentence boundary

markers

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

36 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

How to test n-gram models?

Extrinsic: improvement of the target application due to the

language model:

• Speech recognition accuracy

• BLEU score for machine translation

• Keystroke savings in predictive text

applications

Intrinsic: the higher the probability assigned to a test set

better the model. A few measures:

• Likelihood

• (cross) entropy

• perplexity

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

37 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Training and test set division

• We (almost) never use a statistical (language) model on the

training data

• Testing a model on the training set is misleading: the

model may overfit the training set

• Always test your models on a separate test set

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

38 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Intrinsic evaluation metrics: likelihood

• Likelihood of a model M is the probability of the (test) set

w given the model

L(M | w) = P(w | M) =

∏

s∈w

P(s)

• The higher the likelihood (for a given test set), the better

the model

• Likelihood is sensitive to the test set size

• Practical note: (minus) log likelihood is used more

commonly, because of ease of numerical manipulation

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

39 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Intrinsic evaluation metrics: cross entropy

• Cross entropy of a language model on a test set w is

H(w) = − 1

N

∑

wi

log2 �P(wi)

• The lower the cross entropy, the better the model

• Cross entropy is not sensitive to the test-set size

Reminder: Cross entropy is the bits required to encode the

data coming from a P using an approximate distribution �P.

H(P, Q) = −

∑

x

P(x) log �P(x)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

40 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Intrinsic evaluation metrics: perplexity

• Perplexity is a more common measure for evaluating

language models

PP(w) = 2H(w) = P(w)− 1

N =

N

√

1

P(w)

• Perplexity is the average branching factor

• Similar to cross entropy

– lower better

– not sensitive to test set size

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

41 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

What do we do with unseen n-grams?

…and other issues with MLE estimates

• Words (and word sequences) are distributed according to

the Zipf’s law: many words are rare.

• MLE will assign 0 probabilities to unseen words, and

sequences containing unseen words

• Even with non-zero probabilities, MLE overfits the training

data

• One solution is smoothing: take some probability mass

from known words, and assign it to unknown words

seen

seen

unseen

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

42 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Smoothing: what is in the name?

samples from N(0, 1)

0

0.5

1

5 samples

0

0.2

0.4

0.6

0.8

10 samples

−4

−2

0

2

4

0

0.2

0.4

0.6

30 samples

−4

−2

0

2

4

0

0.2

0.4

1000 samples

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

43 / 81


Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Laplace smoothing

(Add-one smoothing)

• The idea (from 1790): add one to all counts

• The probability of a word is estimated by

P+1(w) = C(w)+1

N+V

N number of word tokens

V number of word types - the size of the vocabulary

• Then, probability of an unknown word is:

0 + 1

N + V

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

44 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Laplace smoothing

for n-grams

• The probability of a bigram becomes

P+1(wiwi−1) = C(wiwi−1)+1

N+V2

• and, the conditional probability

P+1(wi | wi−1) = C(wi−1wi)+1

C(wi−1)+V

• In general

P+1(wi

i−n+1) =

C(wi

i−n+1) + 1

N + Vn

P+1(wi

i−n+1 | wi−1

i−n+1) =

C(wi

i−n+1) + 1

C(wi−1

i−n+1) + V

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

45 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Bigram probabilities

non-smoothed vs. Laplace smoothing

w1w2

C+1 PMLE(w1w2) P+1(w1w2) PMLE(w2 | w1) P+1(w2 | w1)

⟨s⟩ I

3

0.118

0.019

1.000

0.188

I ’m

3

0.118

0.019

0.667

0.176

’m sorry 2

0.059

0.012

0.500

0.125

sorry ,

2

0.059

0.012

1.000

0.133

, Dave

2

0.059

0.012

1.000

0.133

Dave .

2

0.059

0.012

1.000

0.133

’m afraid 2

0.059

0.012

0.500

0.125

afraid I

2

0.059

0.012

1.000

0.133

I can

2

0.059

0.012

0.333

0.118

can ’t

2

0.059

0.012

1.000

0.133

n’t do

2

0.059

0.012

1.000

0.133

do that

2

0.059

0.012

1.000

0.133

that .

2

0.059

0.012

1.000

0.133

. ⟨/s⟩

3

0.118

0.019

1.000

0.188

∑

1.000

0.193

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

46 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

MLE vs. Laplace probabilities

bigram probabilities in sentences and non-sentences

w

I

’m

sorry

,

Dave

.

⟨/s⟩

PMLE

1.00

0.67

0.50

1.00

1.00

1.00

1.00

0.33

P+1

0.25

0.23

0.17

0.18

0.18

0.18

0.25

1.44 × 10−5

w

,

’m

I

.

sorry

Dave

⟨/s⟩

PMLE

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

P+1

0.08

0.09

0.08

0.08

0.08

0.09

0.09

3.34 × 10−8

w

I

’m

afraid

,

Dave

.

⟨/s⟩

Puni

1.00

0.67

0.50

0.00

1.00

1.00

1.00

0.00

Pbi

0.25

0.23

0.17

0.09

0.18

0.18

0.25

7.22 × 10−6

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

47 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

How much mass does +1 smoothing steal?

• Laplace smoothing

reserves probability mass

proportional to the size of

the vocabulary

• This is just too much for

large vocabularies and

higher order n-grams

• Note that only very few of

the higher level n-grams

(e.g., trigrams) are possible

unseen (3.33 %)

seen

Unigrams

unseen (83.33 %)

seen

Bigrams

unseen (98.55 %)

seen

Trigrams

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

48 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Lidstone correction

(Add-α smoothing)

• A simple improvement over Laplace smoothing is adding

0 &lt; α (and typically &lt; 1) instead of 1

P+α(wi

i−n+1 | wi−1

i−n+1) = C(wi

i−n+1) + α

C(wi−1

i−n+1) + αV

• With smaller α values, the model behaves similar to MLE,

it overfits: it has high variance

• Larger α values reduce overfitting/variance, but result in

large bias

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

49 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

How do we pick a good α value

setting smoothing parameters

• We want α value that works best outside the training data

• Peeking at your test data during training/development is

wrong

• This calls for another division of the available data: set

aside a development set for tuning hyperparameters

• Alternatively, we can use k-fold cross validation and take

the α with the best average score

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

50 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Absolute discounting

ϵ

• An alternative to the additive smoothing is to reserve an

explicit amount of probability mass, ϵ, for the unseen

events

• The probabilities of known events has to be re-normalized

• How do we decide what ϵ value to use?

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

51 / 81


Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Good-Turing smoothing

‘discounting’ view

• Estimate the probability mass to be reserved for the novel

n-grams using the observed n-grams

• Novel events in our training set is the ones that occur once

p0 = n1

n

where n1 is the number of distinct n-grams with frequency

1 in the training data

• Now we need to discount this mass from the higher counts

• The probability of an n-gram that occurred r times in the

corpus is

(r + 1)nr+1

nrn

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

52 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Some terminology

frequencies of frequencies and equivalence classes

I

’m

.

’t

,

Dave

afraid

can

do

sorry

that

0

1

2

3

n3 = 1

n2 = 2

n1 = 8

3

2

2

1

1

1

1

1

1

1

1

• We often put n-grams into equivalence classes

• Good-Turing forms the equivalence classes based on

frequency

Note:

n =

∑

r

r × nr

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

53 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Good-Turing estimation: leave-one-out justification

• Leave each n-gram out

• Count the number of times the left-out n-gram had

frequency r in the remaining data

– novel n-grams

n1

n

– n-grams with frequency 1 (singletons)

(1 + 1) n2

n1n

– n-grams with frequency 2 (doubletons)*

(2 + 1) n3

n2n

* Yes, this seems to be a word.

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

54 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Adjusted counts

Sometimes it is instructive to see the ‘effective count’ of an

n-gram under the smoothing method.

For Good-Turing smoothing, the updated count, r∗ is

r∗ = (r + 1)nr+1

nr

• novel items: n1

• singletons: 2×n2

n1

• doubletons: 3×n3

n2

• …

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

55 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Good-Turing example

I

’m

.

’t

,

Dave

afraid

can

do

sorry

that

0

1

2

3

n3 = 1

n2 = 2

n1 = 8

3

2

2

1

1

1

1

1

1

1

1

PGT(the) = PGT(a) = . . . = 8

15

PGT(that) = PGT(do) = . . . =2 × 2

15

PGT(’m) = PGT(.) =3 × 1

15

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

56 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Issues with Good-Turing discounting

With some solutions

• Zero counts: we cannot assign probabilities if nr+1 = 0

• The estimates of some of the frequencies of frequencies are

unreliable

• A solution is to replace nr with smoothed counts zr

• A well-known technique (simple Good-Turing) for

smoothing nr is to use linear interpolation

log zr = a + b log r

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

57 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

N-grams, so far …

• Two different ways of evaluating n-gram models:

Extrinsic success in an external application

Intrinsic likelihood, (cross) entropy, perplexity

• Intrinsic evaluation metrics often correlate well with the

extrinsic metrics

• Test your n-grams models on an ‘unseen’ test set

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

58 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

N-grams, so far …

• Smoothing methods solve the zero-count problem (also

reduce the variance)

• Smoothing takes away some probability mass from the

observed n-grams, and assigns it to unobserved ones

– Additive smoothing: add a constant α to all counts

• α = 1 (Laplace smoothing) simply adds one to all counts –

simple but often not very useful

• A simple correction is to add a smaller α, which requires

tuning over a development set

– Discounting removes a fixed amount of probability mass, ϵ,

from the observed n-grams

• We need to re-normalize the probability estimates

• Again, we need a development set to tune ϵ

– Good-Turing discounting reserves the probability mass to

the unobserved events based on the n-grams seen only

once: p0 = n1

n

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

59 / 81


Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Not all (unknown) n-grams are equal

• Let’s assume that black squirrel is an unknown bigram

• How do we calculate the smoothed probability

P+1(squirrel | black) =

0 + 1

C(black) + V

• How about black wug?

P+1(black wug) = P+1(wug | black) =

0 + 1

C(black) + V

• Would it make a difference if we used a better smoothing

method (e.g., Good-Turing?)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

60 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Back-off and interpolation

The general idea is to fall-back to lower order n-gram

when estimation is unreliable

• Even if,

C(black squirrel) = C(black wug) = 0

it is unlikely that

C(squirrel) = C(wug)

in a reasonably sized corpus

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

61 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Back-off

Back-off uses the estimate if it is available, ‘backs off’ to the lower

order n-gram(s) otherwise:

P(wi | wi−1) =

{

P∗(wi | wi−1)

if C(wi−1wi) &gt; 0

αP(wi)

otherwise

where,

• P∗(·) is the discounted probability

• α makes sure that ∑ P(w) is the discounted amount

• P(wi), typically, smoothed unigram probability

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

62 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Interpolation

Interpolation uses a linear combination:

Pint(wi | wi−1) = λP(wi | wi−1) + (1 − λ)P(wi)

In general (recursive definition),

Pint(wi | wi−1

i−n+1) = λP(wi | wi−1

i−n+1) + (1 − λ)Pint(wi | wi−1

i−n+2)

• ∑ λi = 1

• Recursion terminates with

– either smoothed unigram counts

– or uniform distribution 1

V

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

63 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Not all contexts are equal

• Back to our example: given both bigrams

– black squirrel

– wuggy squirrel

are unknown, the above formulations assign the same

probability to both bigrams

• To solve this, the back-off or interpolation parameters

(α or λ) are often conditioned on the context

• For example,

Pint(wi | wi−1

i−n+1) =

λwi−1

i−n+1 P(wi | wi−1

i−n+1)

+

(1 − λwi−1

i−n+1) Pint(wi | wi−1

i−n+2)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

64 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Katz back-off

A popular back-off method is Katz back-off:

PKatz(wi|wi−1

i−n+1) =

{

P∗(wi | wi−1

i−n+1)

if C(wi

i−n+1) &gt; 0

αwi−1

i−n+1Pkatz(wi | wi−1

i−n+2)

otherwise

• P∗(·) is the Good-Turing discounted probability estimate

(only for n-grams with small counts)

• αwi−1

i−n+1 makes sure that the back-off probabilities sum to

the discounted amount

• α is high for frequent contexts. So, hopefully,

αblackP(squirrel) &gt;

αwuggyP(squirrel)

P(squirrel | black) &gt;

P(squirrel | wuggy)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

65 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

A quick summary

Markov assumption

• Our aim is to assign probabilities to sentences

P(I ’m sorry , Dave .) = ?

Problem: We cannot just count &amp; divide

– Most sentences are rare: no (reliable) way to count their

occurrences

– Sentence-internal structure tells a lot about it’s probability

Solution: Divide up, simplify with a Markov assumption

P(I ’m sorry , Dave) =

P(I | ⟨s⟩)P(’m | I)P(sorry | ’m)P(, | sorry)P(Dave | ,)P(. | Dave)P(⟨/s⟩ | .)

Now we can count the parts (n-grams), and estimate their

probability with MLE.

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

66 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

A quick summary

Smoothing

Problem The MLE assigns 0 probabilities to unobserved n-grams,

and any sentence containing unobserved n-grams. In

general, it overfits

Solution Reserve some probability mass for unobserved n-grams

Additive smoothing add α to every count

P+α(wi

i−n+1 | wi−1

i−n+1) = C(wi

i−n+1) + α

C(wi−1

i−n+1) + αV

Discounting

– reserve a fixed amount of probability mass to

unobserved n-grams

– normalize the probabilities of observed

n-grams

(e.g., Good-Turing smoothing)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

67 / 81


Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

A quick summary

Back-off &amp; interpolation

Problem if unseen we assign the same probability for

– black squirrel

– black wug

Solution Fall back to lower-order n-grams when you cannot

estimate the higher-order n-gram

Back-off

P(wi | wi−1) =

{

P∗(wi | wi−1)

if C(wi−1wi) &gt; 0

αP(wi)

otherwise

Interpolation

Pint(wi | wi−1) = λP(wi | wi−1) + (1 − λ)P(wi)

Now P(squirrel) contributes to P(squirrel|black),

it should be higher than P(wug | black).

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

68 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

A quick summary

Problems with simple back-off / interpolation

Problem if unseen, we assign the same probability for

– black squirrel

– wuggy squirrel

Solution make normalizing constants (α, λ) context dependent,

higher for context n-grams that are more frequent

Back-off

P(wi | wi−1) =

{

P∗(wi | wi−1)

if C(wi−1wi) &gt; 0

αi−1P(wi)

otherwise

Interpolation

Pint(wi | wi−1) = P∗(wi | wi−1) + λwi−1P(wi)

Now P(black) contributes to P(squirrel | black), it

should be higher than P(wuggy | squirrel).

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

69 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Kneser-Ney interpolation: intuition

• Use absolute discounting for the higher order n-gram

• Estimate the lower order n-gram probabilities based on the

probability of the target word occurring in a new context

• Example:

I can't see without my reading

.

• It turns out the word Francisco is more frequent than

glasses (in the typical English corpus, PTB)

• But Francisco occurs only in the context San Francisco

• Assigning probabilities to unigrams based on the number

of unique contexts they appear makes glasses more likely

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

70 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Kneser-Ney interpolation

for bigrams

PKN(wi|wi−1) = C(wi−1wi) − D

C(wi)

+λwi−1

|{v | C(vwi) &gt; 0}|

∑

w | {v | C(vw) &gt; 0}|

Absolute discount

Unique contexts wi appears

All unique contexts

• λs make sure that the probabilities sum to 1

• The same idea can be applied to back-off as well

(interpolation seems to work better)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

71 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Some shortcomings of the n-gram language models

The n-gram language models are simple and successful, but …

• They are highly sensitive to the training data: you do not

want to use an n-gram model trained on business news for

medical texts

• They cannot handle long-distance dependencies:

In the last race, the horse he bought last year

finally

.

• The success often drops in morphologically complex

languages

• The smoothing methods are often ‘a bag of tricks’

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

72 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Cluster-based n-grams

• The idea is to cluster the words, and fall-back (back-off or

interpolate) to the cluster

• For example,

– a clustering algorithm is likely to form a cluster containing

words for food, e.g., {apple, pear, broccoli, spinach}

– if you have never seen eat your broccoli, estimate

P(broccoli|eat your) = P(FOOD|eat your)×P(broccoli|FOOD)

• Clustering can be

hard a word belongs to only one cluster (simplifies the model)

soft words can be assigned to clusters probabilistically (more

flexible)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

73 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Skipping

• The contexts

– boring | the lecture was

– boring | (the) lecture yesterday was

are completely different for an n-gram model

• A potential solution is to consider contexts with gaps,

‘skipping’ one or more words

• We would, for example model P(e | abcd) with a

combination (e.g., interpolation) of

– P(e | abc_)

– P(e | ab_d)

– P(e | a_cd)

– …

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

74 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Modeling sentence types

• Another way to improve a language model is to condition

on the sentence types

• The idea is different types of sentences (e.g., ones related to

different topics) have different behavior

• Sentence types are typically based on clustering

• We create multiple language models, one for each sentence

type

• Often a ‘general’ language model is used, as a fall-back

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

75 / 81


Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Caching

• If a word is used in a document, its probability of being

used again is high

• Caching models condition the probability of a word, to a

larger context (besides the immediate history), such as

– the words in the document (if document boundaries are

marked)

– a fixed window around the word

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

76 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Structured language models

• Another possibility is using a generative parser

• Parsers try to explicitly model (good) sentences

• Parser naturally capture long-distance dependencies

• Parsers require much more computational resources than

the n-gram models

• The improvements are often small (if any)

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

77 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Maximum entropy models

• We can fit a logistic regression ‘max-ent’ model predicting

P(w | context)

• Main advantage is to be able to condition on arbitrary

features

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

78 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Neural language models

• A neural network can be trained to predict a word from its

context

• Then we can use the network for estimating the

P(w | context)

• In the process, the hidden layer(s) of a network will learn

internal representations for the word

• These representations, known as embeddings, are

continuous representations that place similar words in the

same neighborhood in a high-dimensional space

• We will return to embeddings later in this course

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

79 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Some notes on implementation

• The typical use of n-gram models are on (very) large

corpora

• We often need care for numeric instability issues:

– For example, often it is more convenient to work with ‘log

probabilities’

– Sometimes (log) probabilities are ’binned’ into integers,

stored with small number of bits in memory

• Memory or storage may become a problem too

– Assuming words below a frequency are ‘unknown’ often

helps

– Choice of correct data structure becomes important,

– A common data structure is a trie or a suffix tree

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

80 / 81

Motivation

Estimation

Evaluation

Smoothing

Back-off &amp; Interpolation

Extensions

Summary

• We want to assign probabilities to sentences

• N-gram language models do this by

– estimating probabilities of parts of the sentence (n-grams)

– use the n-gram probability and a conditional independence

assumption to estimate the probability of the sentence

• MLE estimate for n-gram overfit

• Smoothing is a way to fight overfitting

• Back-off and interpolation yields better ‘smoothing’

• There are other ways to improve n-gram models, and

language models without (explicitly) use of n-grams

Next:

Today POS tagging

Mon/Fri Statistical parsing

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

81 / 81

Additional reading, references, credits

• Textbook reference: Jurafsky and Martin (2009, chapter 4)

(draft chapter for the 3rd version is also available). Some of

the examples in the slides come from this book.

• Chen and J. Goodman (1998) and Chen and J. Goodman

(1999) include a detailed comparison of smoothing

methods. The former (technical report) also includes a

tutorial introduction

• J. T. Goodman (2001) studies a number of improvements to

(n-gram) language models we have discussed. This

technical report also includes some introductory material

• Gale and Sampson (1995) introduce the ‘simple’

Good-Turing estimation noted on Slide 19. The article also

includes an introduction to the basic method.

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

A.1

Additional reading, references, credits (cont.)

• The quote from 2001: A Space Odyssey, ‘I’m sorry Dave. I’m

afraid I can’t do it.’ is probably one of the most frequent

quotes in the CL literature. It was also quoted, among

many others, by Jurafsky and Martin (2009).

• The HAL9000 camera image on page 19 is from Wikipedia,

(re)drawn by Wikipedia user Cryteria.

• The Herman comic used in slide 4 is also a popular

example in quite a few lecture slides posted online, it is

difficult to find out who was the first.

• The smoothing visualization on slide ?? inspired by Julia

Hockenmaier’s slides.







Chen, Stanley F and Joshua Goodman (1998). An empirical study of smoothing techniques for language modeling.

Tech. rep. TR-10-98. Harvard University, Computer Science Group. url:

https://dash.harvard.edu/handle/1/25104739.







—

(1999). “An empirical study of smoothing techniques for language modeling”. In: Computer speech &amp; language

13.4, pp. 359–394.

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

A.2


Additional reading, references, credits (cont.)







Chomsky, Noam (1968). “Quine’s empirical assumptions”. In: Synthese 19.1, pp. 53–68. doi: 10.1007/BF00568049.







Gale, William A and Geoffrey Sampson (1995). “Good-Turing frequency estimation without tears”. In: Journal of

Quantitative Linguistics 2.3, pp. 217–237.







Goodman, Joshua T (2001). A bit of progress in language modeling extended version. Tech. rep. MSR-TR-2001-72.

Microsoft Research.







Jurafsky, Daniel and James H. Martin (2009). Speech and Language Processing: An Introduction to Natural Language

Processing, Computational Linguistics, and Speech Recognition. second. Pearson Prentice Hall. isbn:

978-0-13-504196-3.







Shillcock, Richard (1995). “Lexical Hypotheses in Continuous Speech”. In: Cognitive Models of Speech Processing.

Ed. by Gerry T. M. Altmann. MIT Press.

Ç. Çöltekin,

SfS / University of Tübingen

Summer Semester 2018

A.3

