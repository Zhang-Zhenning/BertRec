
On the Convergence of the EM Algorithm:

A Data-Adaptive Analysis

Chong Wu1, Can Yang1, Hongyu Zhao2 and Ji Zhu3

1Department of Mathematics, Hong Kong Baptist University

2Department of Biostatistics, Yale School of Public Health, Yale University

3Department of Statistics, University of Michigan

{chongwu,eeyang}@hkbu.edu.hk

hongyu.zhao@yale.edu

jizhu@umich.edu

Abstract

The Expectation-Maximization (EM) algorithm is an iterative method to maximize the

log-likelihood function for parameter estimation. Previous works on the convergence analysis

of the EM algorithm have established results on the asymptotic (population level) convergence

rate of the algorithm. In this paper, we give a data-adaptive analysis of the sample level local

convergence rate of the EM algorithm. In particular, we show that the local convergence rate

of the EM algorithm is a random variable Kn derived from the data generating distribution,

which adaptively yields the convergence rate of the EM algorithm on each ﬁnite sample data

set from the same population distribution.

We then give a non-asymptotic concentration

bound of Kn on the population level optimal convergence rate κ of the EM algorithm, which

implies that Kn → κ in probability as the sample size n → ∞. Our theory identiﬁes the

eﬀect of sample size on the convergence behavior of sample EM sequence, and explains a

surprising phenomenon in applications of the EM algorithm, i.e. the ﬁnite sample version

of the algorithm sometimes converges faster even than the population version.

We apply

our theory to the EM algorithm on three canonical models and obtain speciﬁc forms of the

adaptive convergence theorem for each model.

1

Introduction

The iterative algorithm of expectation-maximization (EM) has been proposed in various special

forms by a number of authors as early as in the 1970s, notably [2, 24, 30, 29, 31, 32]. Since

the advent of its modern formulation by Dempster, Laird and Rubin [12], the EM algorithm has

received much attention in the statistical community. A vast literature on theoretical properties

and real applications of the EM algorithm has been accumulated thereafter (see e.g. [12, 5, 40, 28,

22, 23]). Classical work of Wu [40] established general convergence results for EM sequences to

the MLE or some stationary points of the log-likelihood function; Redner and Walker [28] proved

asymptotic results on the convergence of the EM algorithm for mixture of densities from the

exponential family; Meng and Rubin [22] analyzed both asymptotic componentwise and global

convergence rates of the EM algorithm; some variants or generalizations of the EM algorithm

1

arXiv:1611.00519v2  [math.ST]  30 May 2017


were also proposed: Meng and Rubin [21] developed ECM algorithm to replace a complicated

M-step by several simpler CM-steps (conditional maximization); Liu et al. [19] proposed PX-EM

to use the expanded complete-data model to accelerate the convergence of the EM algorithm.

The book of McLachlan and Krishnan [20] gave a comprehensive account on both theoretical and

practical aspects of the EM algorithm.

Recent work of Balakrishnan et al. [1] presented statistical guarantees for the local linear

convergence of the EM algorithm and ﬁrst-order EM algorithm to the true population parameter

θ∗ within statistical precision. Along this line, Wang et al. [39] considered extensions to high-

dimensional settings by introducing a truncation step; Yi and Caramanis [41] proved statistical

guarantees for generalizations to regularized EM algorithms in high-dimensional latent variable

models. In this paper, we give a data-adaptive analysis of the ﬁnite sample level convergence

behavior of the EM algorithm, especially the dynamics of the convergence rate when the EM

algorithm is performed on multiple ﬁnite random data sets (with possibly diﬀerent sample sizes)

sampled from the same population distribution.

1.1

Problem Setup

Suppose {Pθ | θ ∈ Ω ⊆ Rp} is a family of parametric distributions, and Pθ has density function

pθ(y) with respect to the Lebesgue measure on Rd. A set of i.i.d. samples {yk}n

k=1 of Y ∼ Pθ∗ is

observed, where θ∗ ∈ Ω is an unknown population true parameter.

In latent variable models, Y is the observed part of a pair (Y, Z) of random variables and

Z is a latent variable. Suppose fθ(y, z) is the joint density of (Y, Z) and for θ ∈ Ω, the density

pθ(y) =

�

Z fθ(y, z)dz is the marginalization of fθ(y, z) over z, then the EM algorithm can be

applied to estimate θ∗ from the samples {yk}n

k=1 of Y ∼ Pθ∗.

Speciﬁcally, one ﬁrst calculates the sample Q-function (see Deﬁnition 1 in [1]) by a conditional

Expectation (E-step):

Qn(θ′|θ; {yk}) = 1

n

n

�

k=1

�

Z(yk)

log (fθ′(yk, z)) kθ(z|yk)dz,

where kθ(z|y) :=

fθ(y,z)

pθ(y)

is the conditional density of Z given Y .

Then for an initial point

θ0

n ∈ Br(θ∗), the sample EM sequence {θt

n}t≥0 is constructed by Maximization (M-step):

θt+1

n

∈ arg max{Qn(θ′|θt

n; {yk}) | θ′ ∈ Ω},

and we refer to this procedure as the EM algorithm is performed on the samples {yk}n

k=1.

We notice that the sample Q-function Qn(θ′|θ; {yk}) depends on a speciﬁc set of samples

{yk}n

k=1, hence so does the sample EM sequence {θt

n}t≥0 deﬁned above. Since the samples are

i.i.d. realizations of Y ∼ Pθ∗, it is sensible to conjecture that the convergence rate of the sample

EM sequence depends on the data generating distribution Pθ∗.

When the EM algorithm is

performed on diﬀerent sets of samples from the same population, the corresponding sample EM

sequences constructed as in the above procedure ought to converge at diﬀerent rates. In the

subsequent numerical experiments, we have also conﬁrmed this phenomenon, e.g. see Figure 1.

This observation motivates us to characterize the convergence rate of the EM algorithm as a

data-adaptive quantity.

2


Iteration Count

0

10

20

30

40

50

60

70

80

Log of error

-40

-35

-30

-25

-20

-15

-10

-5

0

5

Typical EM Algorithm (fixed sample size)

(a)

Iteration Count

0

10

20

30

40

50

60

70

80

90

100

Log of error

-40

-35

-30

-25

-20

-15

-10

-5

0

5

Typical EM Algorithm (sample size n)

n=100

n=1000

n=10000

(b)

Figure 1: These plots are generated by applying the EM algorithm to simulated data from a ﬁxed Gaussian

Mixture Model (Section 4.1) with the dimension of θ∗ set to p = 5, and the SNR (Signal-to-Noise Ratio)

∥θ∗∥

σ

= 1. Each line in the plots represents an instance of the EM algorithm performed on a diﬀerent data

set sampled from the Gaussian mixture distribution; the convergence rates of the EM algorithm are the

slopes of these lines. (a) 20 instances of the EM algorithm each performed on a diﬀerent set of n = 300

random samples; the slopes of the lines vary from one instance to another. (b) 20 instances of the EM

algorithm performed on diﬀerent sets of n random samples for each n ∈ {100, 1000, 10000}; when the

sample size is small, the lines are more spread-out (blue lines), hence the ﬂuctuations in the convergence

rate are large; when the sample size is large, the lines are more clustered (red lines and green lines), hence

the ﬂuctuations in the convergence rate are small.

1.2

Main Results and Contributions

The main results of this paper are as follows: we characterize the convergence rate of the empirical

EM sequence as a derived random variable Kn of the data generating distribution Pθ∗ in Theorem

3.2, then we give the concentration bound of Kn in Theorem 3.4.

Optimal Empirical Convergence Theorem

The primary goal of Theorem 3.2 is to show

that the convergence rate of the EM algorithm is a random variable. To this end, we adopt a

novel data-adaptive viewpoint in the ﬁnite sample level analysis by considering the samples as

i.i.d. copies {Yk}n

k=1 of the random variable Y ∼ Pθ∗ and exploiting the concentration of measure

phenomenon to obtain non-asymptotic sample level convergence results.

The theorem states that if the EM algorithm is initialized as Θ0

n ∈ Br(θ∗) in the ball of

population contraction (to be deﬁned precisely), then with high probability, we have a convergence

inequality in the form

���Θt

n − θ∗��� ≤

�

Kn

�t ���Θ0

n − θ∗��� +

En

V n − Γn

,

(1)

where {Θt

n}t≥0 is the empirical EM sequence, deﬁned as

Θt+1

n

∈ arg max{Qn(Θ′|Θt

n; {Yk}) | Θ′ ∈ BR(θ∗)},

3


for a set of i.i.d. copies {Yk}n

k=1 of Y ∼ Pθ∗. The quantities Γ n, V n, En and Kn are measurable

functions of (Y1, · · · , Yn), hence are random variables derived from Pθ∗. Kn is called the optimal

empirical convergence rate (See Section 3.2.2 for the deﬁnitions), which holds the information

of how the data generating distribution Pθ∗ “propagates” the randomness in sample data to the

convergence rate of the empirical EM sequence.

This theorem characterizes the convergence behavior of sample EM sequence adaptively:

Given a set of i.i.d. realizations (or samples) {yk}n

k=1 of Y ∼ Pθ∗, we have corresponding realiza-

tions gn, vn, en and kn of Γ n, V n, En and Kn respectively, and a realization of the convergence

inequality (1) as

���θt

n − θ∗��� ≤ (kn)t ���θ0

n − θ∗��� +

en

vn − gn

,

(2)

where the sample EM sequence {θt

n}t≥0, as a realization of {Θt

n}t≥0, is constructed as

θt+1

n

∈ arg max{Qn(θ′|θt

n; {yk}) | θ′ ∈ BR(θ∗)}.

Hence this particular realization kn of Kn gives the convergence rate of the corresponding sample

EM sequence {θt

n}t≥0 constructed when the EM algorithm is performed on the samples {yk}n

k=1.

A diﬀerent set of i.i.d. samples {y′

k}n′

k=1 gives rise to a diﬀerent sample EM sequence {θ

′t

n′}t≥0,

a diﬀerent realization k′

n′ of Kn′ and a diﬀerent realization of (1) in a form similar to (2).

Thus given each sample data set, the random variable Kn adaptively yields the convergence

rate of the corresponding sample EM sequence, and Theorem 3.2 is precisely the mathematical

substantiation of our claim that the convergence rate of the EM algorithm is a random variable.

Optimal Rate Convergence Theorem

Given the data generating distribution Pθ∗, it is in

general diﬃcult to calculate the distribution or density function of the derived random variables

Γ n, V n, En or Kn. Nonetheless, in Theorem 3.4 we give a non-asymptotic concentration bound

of Kn on the optimal oracle convergence rate κ, which sheds some light on the stochastic behavior

of the derived random variable Kn.

The theorem states that if the EM algorithm is initialized within the ball of population

contraction, the optimal empirical convergence rate Kn satisﬁes

���Kn − κ

��� ≤ 2

ν (ε1(δ, r, n, p) + κε2(δ, r, R, n, p))

(3)

with probability at least 1−δ, where ε1(δ, r, n, p) and ε2(δ, r, R, n, p) are inﬁnitesimals as n → ∞.

It then follows that Kn → κ in probability as n → ∞. One of our contributions on the three

canonical models is the calculation of the inﬁnitesimals ε1(δ, r, n, p) and ε2(δ, r, R, n, p) in closed

forms and the concentration bound of the random variable Kn for each model (see Section 4).

On the Convergence of the EM Algorithm

The data-adaptive analysis in our paper oﬀers

some new insights and theoretical explanations to the convergence behavior of the EM algorithm.

1. The sample size does not directly aﬀect the convergence rate of the EM algorithm. Indeed,

as we observed in numerical experiments and real applications, the EM algorithm performed

on smaller sample sets can converge faster than performed on larger sample sets, even faster

than the population (with inﬁnite many samples) EM algorithm. Theorem 3.2 suggests a

4


theoretical explanation to this phenomenon: the sample EM sequence constructed from a

ﬁnite sample data set {yk}n

k=1 converges at the rate kn, which is a realization of the random

variable Kn given the sample data set. Since Kn randomly ﬂuctuates around κ, and in

view of the concentration bound (3), it is possible that the realization kn &lt; κ. When this

is the case, the sample EM sequence exhibits a faster convergence rate than the population

EM sequence.

The convergence rate of the sample EM sequence randomly ﬂuctuates around the optimal

population convergence rate, and it is not simply proportional to the sample size.

2. The convergence behavior displayed in Figure 1 is ubiquitous in numerical experiments

and real applications of the EM algorithm. Our theory provides a cogent explanation to

such phenomena. For Figure 1a, the EM algorithm is performed on 20 data sets with the

same sample size n = 300.

By Theorem 3.2, the convergence rates of the sample EM

sequences are 20 realizations of the random variable Kn, one for each sample data set. The

randomness of the sampling process causes random ﬂuctuations among the 20 realizations

of Kn, which accounts for the variations of the slopes of these blue lines. For Figure 1b,

the EM algorithm is performed on 20 data sets for each sample size n ∈ {100, 1000, 10000}.

In view of the concentration bound (3) in Theorem 3.4, when the sample size n is large,

the right-hand side of (3) is small and the realizations of Kn are more concentrated around

κ, hence the convergence rate is stable (i.e. the green lines cluster together). Conversely,

when the sample size n is smaller, the right-hand side of (3) is larger and the realizations

of Kn are more scattered, hence the convergence rate is unstable (i.e. the red lines, and

especially the blue lines fan out).

The sample size regulates the stability of the convergence rate of the sample EM sequence.

The convergence rate of the EM algorithm performed on larger sample sets is stabler than

on smaller sample sets.

3. In low-dimensional regime where p ≪ n, the convergence behavior of the sample EM

sequence is “concentrated” on the convergence behavior of the corresponding (initialized

at the same point θ0

n) population EM sequence. In particular, the ball of contraction for

the sample EM sequence is the same as that for the population EM sequence; and the

convergence rate of the sample EM sequence is also well approximated by the population

convergence rate with high probability. For concrete models, our theory gives quantitative

characterization of the low-dimensional regime with respect to approximation error ϵ &gt; 0

and tolerance δ &gt; 0 as Rℓ(ϵ, δ) = {(p, n) |

���Kn − κ

��� &lt; ϵ with probability at least 1 − δ}.

The study of the convergence behavior of the EM algorithm in low-dimensional regime can

basically be reduced to the study of the population EM sequence.

1.3

Related Works

Our work was inspired by an insightful Population-Sample based analysis in [1] and we built

upon many classical works on the EM algorithm. The major diﬀerences of our theory to previous

works are in the following respects:

1. We focus on the study of a diﬀerent problem in the convergence analysis of the EM algo-

rithm. Previous works studied the convergence rate of the EM algorithm on an arbitrary

5


but ﬁxed sample data set. We study the dynamics of the convergence rate when the EM

algorithm is performed on multiple data sets (with possibly diﬀerent sample sizes) from

the same population distribution, and quantify the intrinsic connection between the data

generating distribution and the convergence rate of the sample EM algorithm. The central

objects in our analysis are derived random variables (deﬁned in the sequel) from the data

generating distribution Pθ∗. As we shall see, the population means of these random vari-

ables characterize the convergence of the population EM sequence, while their empirical

means characterize the convergence of the empirical EM sequence.

2. Classical works on the EM algorithm (e.g.

[12, 28, 22, 23]) analyzed the convergence

rate of the EM algorithm asymptotically. Recent work of Balakrishnan et al. [1] proved

geometric convergence results for sample EM algorithm when initialized within the basin

of contraction. They directly leveraged the κ-contractivity of the population M-operator

to obtain the sample level convergence result, hence the convergence rate is essentially the

population level (asymptotic) rate. In this paper, we characterize the ﬁnite sample level

convergence rate of the EM algorithm as a random variable, which adaptively yields the

convergence rate of the EM algorithm for each ﬁnite sample set.

3. From the technical aspects, the main tools in classical analysis of the convergence of the

EM algorithm are information matrices and the rate matrix (i.e. Jacobian matrix of the

M-operator).

Balakrishnan et al.

[1] exploited the KKT conditions which characterize

the optimality of θ∗ and M(θ) to derive the κ-contractivity of the population M-operator.

In this paper, we do not follow the M-operator approach in previous works [1, 39, 41].

Instead, we directly leverage the optimality of the EM sequence in each M-step of the EM

iteration for both population and sample (empirical) EM sequences. This approach allows

us to prove a basic contraction inequality (19), which can be viewed as a generalization of

the inequality in Theorem 4 of [1]. Meanwhile, this approach overcomes the diﬃculty of

verifying conditions involving M-operators, e.g. the First-Order Stability or the (uniform)

deviation bounds of sample M-operators to population M-operator etc. Another technical

diﬀerence is that, under natural concentration assumptions, the quantity characterizing the

statistical error in our theory is guaranteed to converge to zero in probability as the sample

size n → ∞. This observation allows us to avoid the diﬃculty in bounding an empirical

process of M-operators, and prove the statistical consistency of the EM algorithm not only

for speciﬁc models, but also at a general theoretical level.

The remainder of this paper is organized as follows. Following Notations and Conventions,

we brieﬂy review the EM algorithm in Section 2. Then we formulate our convergence theory in

two parts: Section 3.1 contains the theory of oracle convergence; Section 3.2 contains the theory

of empirical convergence and the consistency of the EM algorithm. In Section 4, we apply our

theory to three canonical models: the Gaussian Mixture Model (Section 4.1), the Mixture of

Linear Regressions (Section 4.2); and Linear Regression with Missing Covariates (Section 4.3).

We conclude the paper with Discussion (Section 5) and defer the detailed proofs for the canonical

models to the Appendix.

6


Notations and Conventions

• For p ≥ 1 and x ∈ Rp, let ∥x∥ =

��p

j=1

��xj��2� 1

2 be the L2-norm of x.

• For r &gt; 0 and θ ∈ Rp, let Br(θ) := {x ∈ Rp | ∥x − θ∥ &lt; r} be the open ball; and Br(θ) :=

{x ∈ Rp | ∥x − θ∥ ≤ r} be the closed ball; and B×

r (θ) := Br(θ)\{θ} be the punctured open

ball; and Sp−1 := {x ∈ Rp | ∥x∥ = 1} be the standard unit sphere in Rp.

• A random variable Y is a real-valued Borel measurable function on a probability measure

space (S , E , P) (see e.g. [3, 13, 14]). For ϖ ∈ S , the function value y = Y (ϖ) ∈ R is

called a realization or sample of Y ∼ PY . Hence for any Borel set B ⊆ R, an expression like

“any realization y ∈ B with probability at least δ” is simply “Pr {Y ∈ B} ≥ δ” paraphrased.

• For a real valued Borel measurable function f on R, we follow the convention of abusing

the notation f(Y ) for both a function on the range (or realizations) of a random variable

Y and the random variable f ◦ Y (or as a functional of Y ).

2

Review of the EM Algorithm

In this section, we brieﬂy review the notations and indicate some extensions to the basic theory

of the EM algorithm.

2.1

Log-Likelihood Function and Maximum Likelihood Estimate

Suppose a set of independent and identically distributed (i.i.d.) random samples {yk}n

k=1 are

observed from a distribution Pθ∗ with an unknown parameter θ∗ ∈ Ω ⊆ Rp.

The goal is to

estimate θ∗ from these samples. In practice, we assume the parametric distribution Pθ has a

density function pθ(y) for θ ∈ Ω with respect to the Lebesgue measure on Rd.

We consider the samples {yk}n

k=1 as a realization of the i.i.d. copies {Yk}n

k=1 of Y ∼ Pθ∗. For

the random variable Y ∼ Pθ∗, we deﬁne the stochastic log-likelihood functional

L(θ; Y ) := log pθ(Y ),

for θ ∈ Ω. Deﬁne the empirical log-likelihood functional as the empirical mean of the stochastic

log-likelihood functionals of the i.i.d. copies Yk (k = 1, · · · , n) of Y , i.e.

Ln(θ; {Yk}) := 1

n

n

�

k=1

L(θ; Yk).

A maximum likelihood estimate (MLE) �θ is obtained by maximizing a realization of the

empirical log-likelihood functional, that is, �θ ∈ arg maxθ∈Ω Ln(θ; {yk}) where the maximizer of

Ln(θ; {yk}) may not be unique. The expected (oracle) log-likelihood function is the expectation

of the stochastic log-likelihood functional

L∗(θ) := Eθ∗L(θ; Y ) =

�

Rd log (pθ(y)) pθ∗(y)dy.

(4)

A fundamental property of the expected log-likelihood function is the following result.

7


Proposition 2.1. The true population parameter θ∗ is a global maximizer of L∗(θ) over Ω.

Namely,

θ∗ ∈ arg max

θ∈Ω L∗ (θ) .

Proof. See Section D.1.

Remark. The expected log-likelihood function and the above result are well-known in the statistics

literature (e.g. [8]) . A proof is given only for the completeness. Further, if θ∗ is an interior point

of Ω and the function L∗ : Ω → R is diﬀerentiable1 in a neighborhood of θ∗ then ∇1L∗(θ∗) = 0.

Further, if θ∗ is the unique maximizer in an open neighborhood in which L∗ is twice continuously

diﬀerentiable, then in addition to ∇1L∗(θ∗) = 0, the Hessian matrix of L∗ at θ∗ is negative deﬁnite

or ∇1∇⊺

1L∗(θ∗) ≺ 0. This is equivalent to the positive deﬁniteness of the Fisher information

matrix I(θ∗). See (83) and (86).

2.2

The EM Algorithm and Q-Functions

The EM Algorithm is often applied to maximum likelihood estimation in latent variable models.

The basic assumptions and formulation of the EM algorithm is brieﬂy summarized as follows.

In latent variable models, the random variable Y ∼ Pθ∗ is considered as the observed part of

a pair (Y, Z) in which Z is the latent or hidden variable. Suppose for θ ∈ Ω, the complete joint

density fθ(y, z) of (Y, Z) ∈ Y×Z ⊆ Rd is known, then the density pθ(y) of Y is the marginalization

pθ(y) =

�

Z fθ(y, z)dz. Deﬁne the conditional density of Z given y ∈ Y as kθ(z|y) := fθ(y,z)

pθ(y) for

z ∈ Z(y) := {z | (y, z) ∈ Y × Z}, then the stochastic log-likelihood function satisﬁes

L(θ′; y) = log pθ′(y) = log fθ′(y, z) − log kθ′(z|y).

(5)

Now taking conditional expectation of Z given y at parameter θ (i.e. multiplying kθ(z|y) on

both sides and integrating with respect to z), one has

L(θ′; y) = Q(θ′|θ; y) − H(θ′|θ; y)

where H(θ′|θ; y) := Eθ [log kθ′(Z|y) | y] and we deﬁne the stochastic Q-function

Q(θ′|θ; y) := Eθ [log fθ′(y, Z) | y] =

�

Z(y)

log (fθ′(y, z)) kθ(z|y)dz.

Balakrishnan et al. [1] introduced the sample and population Q-functions to study the EM

algorithm from a Population-Sample based perspective. They deﬁned the sample (empirical)

Q-function as

Qn(θ′|θ; {yk}) := 1

n

n

�

k=1

Q(θ′|θ; yk) = 1

n

n

�

k=1

�

Z(yk)

log (fθ′(yk, z)) kθ(z|yk)dz,

which is the empirical mean of the stochastic Q-functions of a set of i.i.d. realizations {yk}n

k=1 of

1In the sense of possessing ﬁrst order partial derivatives.

8


Y ∼ Pθ∗ and also the population (or oracle) Q-function

Q∗(θ′|θ) :=

�

Y

��

Z(y)

log (fθ′(y, z)) kθ(z|y)dz

�

pθ∗(y)dy

for θ′, θ ∈ Ω, which is the expectation (or population mean) of the stochastic Q-function.

A sequence {θt

n}t≥0 generated by maximizing a sample Q-function recursively is called a

sample EM sequence.

A sequence {θt}t≥0 generated by maximizing a population Q-function

recursively is called a population (or oracle) EM sequence.

Note that the sample Q-function is a quantity we can actually compute in real applications,

since its deﬁnition involves only a set of samples from the population distribution Pθ∗, while the

computation of the oracle Q-function requires knowledge of the true population parameter θ∗.

To develop a data-adaptive theory, we extend these important concepts in our analysis: First

we deﬁne the stochastic Q-functional

Q(θ′|θ; Y ) := Eθ [log fθ′(Y, Z) | Y ]

as the basic derived random variable of Y ∼ Pθ∗, then the stochastic Q-function is simply a

realization of Q(θ′|θ; Y ), the sample Q-function is a realization of the empirical Q-functional

Qn(θ′|θ; {Yk}) := 1

n

n

�

k=1

Q(θ′|θ; Yk),

which is the empirical mean of the stochastic Q-functionals of i.i.d. copies {Yk}n

k=1 of Y ∼ Pθ∗,

and the oracle Q-function is the population mean of Q(θ′|θ; Y ).

Then given an initial point

Θ0

n ∈ Br(θ∗), we deﬁne the empirical EM sequence {Θt

n}t≥0 as

Θt+1

n

∈ arg max{Qn(Θ′|Θt

n; {Yk}) | Θ′ ∈ BR(θ∗)} for t ≥ 0.

(6)

It is not diﬃcult to see that Θt

n is a measurable function of (Y1, · · · , Yn) for each t ≥ 0, hence

a random variable. Now the sample EM sequence {θt

n}t≥0 is a realization of the empirical EM

sequence {Θt

n}t≥0. For a concrete example, consider the Gaussian Mixture model (see Section

4.1): For Θ0

n ∈ Br(θ∗), we have

Θt+1

n

= 1

n

n

�

k=1

tanh

��Θt

n, Yk

�

σ2

�

Yk,

where each Yk is an i.i.d. copy of Y ∼ 1

2N(θ∗, σ2Ip) + 1

2N(−θ∗, σ2Ip) for 1 ≤ k ≤ n.

A fundamental property of the oracle Q-function, referred as self-consistency in [20, 1], is the

following well-known result.

Proposition 2.2. The true population parameter θ∗ is a global maximizer of the oracle Q-

function on Ω. Namely,

θ∗ ∈ arg max

θ′∈Ω Q∗

�θ′|θ∗� .

Proof. See Section D.2.

Remark. The proof is given only for the completeness. Note that if θ∗ is an interior point of Ω and

9


the function Q∗(·|θ∗) : Ω → R is diﬀerentiable in a neighborhood of θ∗, then the self-consistency

implies that,

∇1Q∗ (θ∗|θ∗) = 0

(7)

which holds true in our local analysis of the convergence of oracle EM sequences.

3

Theory for the Convergence of the EM Algorithm

In this section, we formulate our theoretical framework for the convergence of the EM algorithm.

The main results consist of the optimal oracle convergence theorem (Theorem 3.1), the opti-

mal empirical convergence theorem (Theorem 3.2) and the optimal rate convergence theorem

(Theorem 3.4). We also prove the consistency of the EM algorithm (Theorem 3.3).

3.1

The Oracle Convergence of the EM Algorithm

We analyze the convergence of oracle EM sequences in this section. We ﬁrst deﬁne the derived

random quantities whose population means characterize the oracle convergence, then we deﬁne

the set of contraction parameters and deduce the oracle contraction inequality which leads to the

main theorem.

3.1.1

Deﬁnitions

For Y ∼ Pθ∗, where θ∗ ∈ Ω is the unknown true population parameter, we deﬁne three derived

random quantities, the gradient diﬀerence random vector (GRV)

Γ(θ; Y ) := ∇1Q(θ∗|θ; Y ) − ∇1Q(θ∗|θ∗; Y ),

(8)

the concavity random variable (CRV)

V (θ′|θ; Y ) := Q(θ′|θ; Y ) − Q(θ∗|θ; Y ) −

�∇1Q(θ∗|θ; Y ), θ′ − θ∗� ,

(9)

and the statistical error vector (SEV)

E(Y ) := ∇1Q(θ∗|θ∗; Y ).

(10)

As we shall see, the convergence of an oracle EM sequence is characterized by their population

means

Eθ∗Γ(θ; Y ) = ∇1Q∗(θ∗|θ) − ∇1Q∗(θ∗|θ∗) and

Eθ∗V (θ′|θ; Y ) = Q∗(θ′|θ) − Q∗(θ∗|θ) −

�∇1Q∗(θ∗|θ), θ′ − θ∗� .

(11)

Note that

Eθ∗E(Y ) = Eθ∗∇1Q(θ∗|θ∗; Y ) = ∇1Q∗(θ∗|θ∗) = 0,

(12)

which follows from Proposition 2.2 and the remark on the self-consistency of the oracle Q-function.

10


3.1.2

The Contraction Parameters

In our theory, the convergence behavior of an oracle EM sequence in a given ball Br(θ∗) is

characterized by a pair of parameters (γ, ν) and we consider all possible parameters for any ball

centered at the true population parameter θ∗. Speciﬁcally, for 0 &lt; r ≤ R, we deﬁne the following

sets,

G(r) := {γ &gt; 0 | ∥Eθ∗Γ(θ; Y )∥ ≤ γ ∥θ − θ∗∥ for θ ∈ Br(θ∗)} and

(13)

V(r, R) := {ν &gt; 0 | Eθ∗V (θ′|θ; Y ) ≤ −ν

��θ′ − θ∗��2 for

�θ′, θ

� ∈ BR(θ∗) × Br(θ∗)}.

(14)

Thus in view of (11), for each γ ∈G(r), the oracle Q-function satisﬁes a gradient stability

(γ-GS) condition:

∥∇1Q∗(θ∗|θ) − ∇1Q∗(θ∗|θ∗)∥ ≤ γ ∥θ − θ∗∥ for θ ∈ Br(θ∗),

(15)

and for each ν ∈ V(r, R), the oracle Q-function satisﬁes a local uniform strong concavity (ν-LUSC)

condition:

Q∗(θ′|θ) − Q∗(θ∗|θ) −

�∇1Q∗(θ∗|θ), θ′ − θ∗� ≤ −ν

��θ′ − θ∗��2

(16)

for (θ′, θ) ∈ BR(θ∗) × Br(θ∗).

Note the gradient stability condition (15) is diﬀerent from the Gradient Smoothness Condition

and the First-order Stability Condition introduced in [1].

Our gradient stability condition is

equivalent to the gradient ∇1Q∗ (θ∗|·) being Lipschitz continuous at θ∗ with parameter γ.

The local uniform strong concavity condition is diﬀerent from the Strong Concavity condition

in [1, 39, 41]. It requires that the oracle Q-function Q∗(θ′|θ) is ν-strongly concave with respect

to θ′ at the point θ∗, and that it holds uniformly for all θ ∈ Br(θ∗). This condition is easily

veriﬁed when Q∗(θ′|θ) is a quadratic function in θ′ and independent of θ, which is the case

for Gaussian Mixture (Section 4.1) and Mixture of Linear Regression (Section 4.2). From the

theoretical perspective, the local uniform strong concavity condition is motivated by the following

proposition.

Proposition 3.1. If the Fisher information matrix I(θ) of the parametric density pθ(y) is positive

deﬁnite at θ∗, then there exist 0 &lt; r ≤ R such that V(r, R) ̸= ∅.

Proof. See the proof in Section D.4.

Intuitively, for r &gt; 0, the set G(r) consists of all γ &gt; 0, such that the oracle Q-function

satisﬁes a γ-GS condition in Br(θ∗); and for 0 &lt; r ≤ R, the set V(r, R) consists of all ν &gt; 0 such

that the oracle Q-function satisﬁes a ν-LUSC condition in BR(θ∗) × Br(θ∗). It is easy to see that

G(r1) ⊆ G(r2) if r1 ≥ r2 ≥ 0 and V(r, R1) ⊆ V(r, R2) if R1 ≥ R2 ≥ r ≥ 0.

There is no a priori guarantee that these sets are non-empty for given 0 &lt; r ≤ R, but if this

is the case, then the following lemma completely characterizes these sets.

Lemma 3.1. Let G(r) and V(r, R) be deﬁned as above.

(a) If G(r) ̸= ∅ for some r &gt; 0, then G(r) = [γ, +∞) where

γ := sup

�∥Eθ∗Γ(θ; Y )∥

∥θ − θ∗∥

| θ ∈ B×

r (θ∗)

�

∈ R;

(17)

11


(b) If V(r, R) ̸= ∅ for some R ≥ r &gt; 0, then V(r, R) = (0, ν] where

ν := inf

�

−Eθ∗V (θ′|θ; Y )

∥θ′ − θ∗∥2

|

�θ′, θ

� ∈ B×

R(θ∗) × Br(θ∗)

�

∈ R.

(18)

Proof. (a) For any γ ∈ G(r) ̸= ∅, we have by deﬁnition

∥Eθ∗Γ(θ; Y )∥ ≤ γ ∥θ − θ∗∥ for θ ∈ Br(θ∗),

and hence

∥Eθ∗Γ(θ; Y )∥

∥θ − θ∗∥

≤ γ for θ ∈ B×

r (θ∗).

It then follows that

γ = sup

�∥Eθ∗Γ(θ; Y )∥

∥θ − θ∗∥

| θ ∈ B×

r (θ∗)

�

≤ γ

and hence γ ≤ inf G(r), since γ ∈ G(r) is arbitrary. Now by deﬁnition

∥Eθ∗Γ(θ; Y )∥

∥θ − θ∗∥

≤ γ for θ ∈ B×

r (θ∗)

and in view of the fact that Eθ∗Γ(θ∗; Y ) = 0, we see γ ∈ G(r) and hence γ = min G(r). The result

follows by noticing that if γ ∈ G(r), then γ′ ∈ G(r) for any γ′ &gt; γ.

(b) For any ν ∈ V(r, R) ̸= ∅, we have by deﬁnition

Eθ∗V (θ′|θ; Y ) ≤ −ν

��θ′ − θ∗��2 for

�θ′, θ

� ∈ BR(θ∗) × Br(θ∗),

and hence

−Eθ∗V (θ′|θ; Y )

∥θ′ − θ∗∥2

≥ ν for

�θ′, θ

� ∈ B×

R(θ∗) × Br(θ∗).

It then follows that

ν = inf

�

−Eθ∗V (θ′|θ)

∥θ′ − θ∗∥2 |

�θ′, θ

� ∈ B×

R(θ∗) × Br(θ∗)

�

≥ ν

and hence ν ≥ sup V(r, R), since ν ∈ V(r, R) is arbitrary. Now by deﬁnition

−Eθ∗V (θ′|θ; Y )

∥θ′ − θ∗∥2

≥ ν for

�θ′, θ

� ∈ B×

R(θ∗) × Br(θ∗)

and in view of the fact that Eθ∗V (θ∗|θ; Y ) = 0, we see ν ∈ V(r, R) and hence ν = max V(r, R).

The result follows by noticing that if ν ∈ V(r, R), then ν′ ∈ V(r, R) for any 0 &lt; ν′ &lt; ν.

3.1.3

The Oracle Contraction Inequality

The pair of parameters (γ, ν) ∈ G(r) × V(r, R) gives rise to an inequality which lies in the core of

our oracle convergence theory.

Proposition 3.2. If G(r) × V(r, R) ̸= ∅ for some 0 &lt; r ≤ R, then for any θ ∈ Br(θ∗) and

12


θ′ ∈ BR(θ∗) such that

Q∗(θ′|θ) ≥ Q∗(θ∗|θ),

there holds the inequality

��θ′ − θ∗�� ≤ γ

ν ∥θ − θ∗∥

(19)

for each pair (γ, ν) ∈ G(r) × V(r, R).

Proof. By deﬁnitions (13) and (14), for any (γ, ν) ∈ G(r) × V(r, R), we have

∥Eθ∗Γ(θ; Y )∥ ≤ γ ∥θ − θ∗∥ for θ ∈ Br(θ∗) and

Eθ∗V (θ′|θ; Y ) ≤ −ν

��θ′ − θ∗��2 for

�θ′, θ

� ∈ B×

R(θ∗) × Br(θ∗).

Then it follows that

0 ≤ Q∗(θ′|θ) − Q∗(θ∗|θ)

(a)

= Eθ∗V (θ′|θ; Y ) +

�∇1Q∗(θ∗|θ), θ′ − θ∗�

(b)

≤ Eθ∗V (θ′|θ; Y ) + ∥∇1Q∗(θ∗|θ)∥ ·

��θ′ − θ∗��

(c)

= Eθ∗V (θ′|θ; Y ) + ∥Eθ∗Γ(θ; Y )∥ ·

��θ′ − θ∗��

≤ −ν

��θ′ − θ∗��2 + γ ∥θ − θ∗∥ ·

��θ′ − θ∗��

where (a) follows from the deﬁnition of Eθ∗V (θ′|θ; Y ); (b) follows from the Cauchy-Schwartz

inequality; and (c) follows from the deﬁnition of Eθ∗Γ(θ; Y ) and the self-consistency (7). Hence

the proposition follows.

3.1.4

The Optimal Oracle Convergence Theorem

We note (19) holds for any pair of (γ, ν) ∈ G(r) × V(r, R), not only those γ &lt; ν. However, we

are more interested in the case when it is indeed a contraction. For this purpose, let

T := {(x, y) ∈ R+ × R+ | x &lt; y}

be the open upper-triangle of the ﬁrst quadrant and deﬁne the set of contraction parameters

C(r, R) := (G(r) × V(r, R)) ∩ T = {(γ, ν) | γ ∈ G(r), ν ∈ V(r, R) such that γ &lt; ν},

and we say 0 &lt; r ≤ R are radii of contraction if C(r, R) ̸= ∅.

If 0 &lt; r ≤ R are radii of contraction, then in view of Lemma 3.1, we have

C(r, R) = ([γ, +∞) × (0, ν]) ∩ T ,

and we call (γ, ν) the optimal pair since the ratio γ

ν ≤ γ

ν &lt; 1 for any (γ, ν) ∈ C(r, R), which

then gives the optimal rate of oracle convergence with respect to the radii of contraction r ≤ R.

Indeed, this is the content of our main theorem of this section.

13


Theorem 3.1 (Optimal Oracle Convergence Theorem). Suppose 0 &lt; r ≤ R are radii of contrac-

tion, then given initial point θ0 ∈ Br(θ∗), any oracle EM sequence {θt}t≥0 such that

θt+1 ∈ arg max{Q∗(θ′|θt) | θ′ ∈ BR(θ∗)} for t ≥ 0,

(20)

satisﬁes the inequality

���θt − θ∗��� ≤ κt ���θ0 − θ∗���

(21)

where κ := γ

ν ≤ γ

ν &lt; 1 for any (γ, ν) ∈ C(r, R), is the optimal rate of oracle convergence with

respect to r ≤ R.

Proof. We only need to show that

���θt − θ∗��� ≤

�γ

ν

�t ���θ0 − θ∗���

(22)

holds for any (γ, ν) ∈ C(r, R) and t ∈ N, and from which the result follows. We proceed by

induction. It is clear that (22) holds for t = 0. Assume it holds for t ≥ 0 then θt ∈ Br(θ∗) since

γ

ν &lt; 1, and by deﬁnition

Q∗(θt+1|θt) ≥ Q∗(θ∗|θt)

and θt+1 ∈ BR(θ∗). It follows from Proposition 3.2 and induction hypothesis that

���θt+1 − θ∗��� ≤ γ

ν

���θt − θ∗��� ≤

�γ

ν

�t+1 ���θ0 − θ∗���

and hence (22) holds for t + 1 and the proof is complete.

Remark. The theorem above can be viewed as a stronger version of the population convergence

result in Theorem 4 of [1]. It gives a family of deterministic convergence inequalities for oracle

EM sequences, one for each pair of (γ, ν) ∈ C(r, R). It also asserts that the oracle EM sequence

converges geometrically at the optimal rate κ with respect to given radii of contraction 0 &lt; r ≤ R.

Although in speciﬁc models and real applications, we only calculate one or a class of convergence

rates κ for some ball of contraction, (see Sections 4.1.1, 4.2.1 and 4.3.1), the oracle EM sequence

always converges at the optimal rate κ ≤ κ with respect to that ball of contraction.

3.2

The Empirical Convergence and Consistency of the EM Algorithm

The empirical EM sequence is constructed iteratively by maximizing the empirical Q-functional

Qn (θ′|θ; {Yk}), which is the empirical approximation on a ﬁnite set {Yk}n

k=1 of i.i.d. copies of

Y ∼ Pθ∗, to the oracle (population) Q-function.

Our intuition is that, due to the concentration of measure phenomenon of random variables,

the convergence behavior of the empirical EM sequence ought to “concentrate” on the convergence

behavior of the corresponding oracle EM sequence, with high probability.

Hence the results established in the oracle convergence theorem, namely the radii of con-

traction and the set of contraction parameters are oracle information that we can exploit to help

illuminate the convergence behavior of the empirical EM sequence. To substantiate this idea with

mathematical rigor, we prove the optimal empirical convergence theorem in this section and as

14


a consequence, a theorem on the statistical consistency of the EM algorithm. We ﬁrst introduce

the empirical versions of GRV, CRV and SEV.

3.2.1

Basic Deﬁnitions and Assumptions

For a set {Yk}n

k=1 of i.i.d. copies of Y ∼ Pθ∗, we deﬁne the empirical gradient diﬀerence random

vector as

Γn(θ; {Yk}) := 1

n

n

�

k=1

Γ(θ; Yk) = ∇1Qn(θ∗|θ; {Yk}) − ∇1Qn(θ∗|θ∗; {Yk}),

the empirical concavity random variable as

Vn(θ′|θ; {Yk}) := 1

n

n

�

k=1

V (θ′|θ; Yk)

= Qn(θ′|θ; {Yk}) − Qn(θ∗|θ; {Yk}) −

�∇1Qn(θ∗|θ; {Yk}), θ′ − θ∗� ,

and also the empirical statistical error vector as

En({Yk}) := 1

n

n

�

k=1

E(Yk) = ∇1Qn(θ∗|θ∗; {Yk}).

In order to exploit the oracle information from the population version of these quantities, we

postulate the following assumptions on the empirical versions of the GRV, CRV and SEV.

Assumptions

For δ ∈ (0, 1) and a set {Yk}n

k=1 of i.i.d. copies of Y ∼ Pθ∗:

(A1) There exits ε1(δ, r, n, p) ≥ 0 such that

∥Γn(θ; {Yk}) − Eθ∗Γ(θ; Y )∥ ≤ ε1(δ, r, n, p) ∥θ − θ∗∥

for θ ∈ Br(θ∗) with probability at least 1 − δ.

(A2) There exits ε2(δ, r, R, n, p) ≥ 0 such that

��Vn(θ′|θ; {Yk}) − Eθ∗V (θ′|θ; Y )

�� ≤ ε2(δ, r, R, n, p)

��θ′ − θ∗��2

for (θ′, θ) ∈ BR(θ∗) × Br(θ∗) with probability at least 1 − δ.

(A3) There exists εs(δ, r, R, n, p) &gt; 0 such that

∥En({Yk})∥ ≤ εs(δ, r, R, n, p)

with probability at least 1 − δ.

Remark. For the measurability issue involved in the assumptions, see Section D.5. These as-

sumptions are natural concentration inequalities for random variables or vectors, they are readily

veriﬁed in canonical example models. And in view of the Law of Large Numbers, we have that

ε1(δ, r, n, p) → 0, ε2(δ, r, R, n, p) → 0 and εs(δ, r, R, n, p) → 0 as the sample size n → ∞.

15


3.2.2

The Optimal Empirical Convergence Rate

Now we proceed to deﬁne the central object of our data-adaptive analysis of the EM algorithm.

For 0 &lt; r ≤ R and i.i.d. copies {Yk}n

k=1 of Y ∼ Pθ∗, deﬁne the (possibly) extended real-valued

(with range R ∪ {±∞}) random variables2

Γ n := sup

�∥Γn(θ; {Yk})∥

∥θ − θ∗∥

| θ ∈ B×

r (θ∗)

�

,

En := ∥En({Yk})∥ and

V n := inf

�

−Vn(θ′|θ; {Yk})

∥θ′ − θ∗∥2

|

�θ′, θ

� ∈ B×

R(θ∗) × Br(θ∗)

�

(23)

The following lemma asserts that these random variables assume ﬁnite values and are properly

bounded with high probability under our assumptions.

Lemma 3.2. Suppose δ ∈ (0, 1) and 0 &lt; r ≤ R. If assumptions (A1), (A2) and (A3) hold

true, then for any (γ, ν) ∈ G(r) × V(r, R), with probability at least 1 − δ, these random variables

satisfy Γ n ≤ γn, V n ≥ νn and En ≤ εs(δ, r, R, n, p) where γn := γ + ε1(δ, r, n, p) and νn :=

ν − ε2(δ, r, R, n, p).

Proof. Since (γ, ν) ∈ G(r) × V(r, R) and by deﬁnitions (13) and (14), one has

∥Eθ∗Γ(θ; Y )∥ ≤ γ ∥θ − θ∗∥ for θ ∈ Br(θ∗) and

Eθ∗V (θ′|θ; Y ) ≤ −ν

��θ′ − θ∗��2 for

�θ′, θ

� ∈ BR(θ∗) × Br(θ∗).

Then by assumption (A1), for the set {Yk}n

k=1 of i.i.d. copies of Y ∼ Pθ∗ and θ ∈ Br(θ∗),

∥Γn(θ; {Yk})∥ ≤ ∥Eθ∗Γ(θ; Y )∥ + ∥Γn(θ; {Yk}) − Eθ∗Γ(θ; Y )∥

≤ γ ∥θ − θ∗∥ + ε1(δ, r, n, p) ∥θ − θ∗∥

= γn ∥θ − θ∗∥

with probability at least 1 − δ/3. It follows that

Γ n = sup

�∥Γn(θ; {Yk})∥

∥θ − θ∗∥

| θ ∈ B×

r (θ∗)

�

≤ γn.

Likewise, by assumption (A2) and for (θ′, θ) ∈ BR(θ∗) × Br(θ∗), we have

Vn(θ′|θ; {Yk}) ≤ Eθ∗V (θ′|θ; Y ) +

��Vn(θ′|θ; {Yk}) − Eθ∗V (θ′|θ; Y )

��

≤ −ν

��θ′ − θ∗��2 + ε2(δ, r, R, n, p)

��θ′ − θ∗��2

= −νn

��θ′ − θ∗��2

with probability at least 1 − δ/3. It follows that

V n = inf

�

−Vn(θ′|θ; {Yk})

∥θ′ − θ∗∥2

|

�θ′, θ

� ∈ B×

R(θ∗) × Br(θ∗)

�

≥ νn.

2For the measurability issue, see Section D.5

16


Moreover by assumption (A3), En = ∥En({Yk})∥ ≤ εs(δ, r, R, n, p) with probability at least

1 − δ/3. Then the lemma is proved by applying a union bound.

If δ ∈ (0, 1) and 0 &lt; r ≤ R are radii of contraction, then the above lemma holds for the

optimal pair (γ, ν) ∈ C(r, R) and set γn := γ + ε1(δ, r, n, p) and νn := ν − ε2(δ, r, R, n, p). We call

(γn, νn) the optimal pair of the empirical contraction parameters. Deﬁne the event

En :=

�

ϖ | Γ n ≤ γn, V n ≥ νn and En ≤ εs(δ, r, R, n, p)

�

,

(24)

then the above lemma implies that Pr En ≥ 1 − δ. Now we deﬁne the random variable

Kn :=







min

�

Γ n

V n , κn

�

if 0 &lt; V n &lt; ∞

κn

otherwise

(25)

as the optimal empirical convergence rate, where κn := γn

νn . Under the event En, we have that

0 &lt; Γ n

V n ≤ κn, hence Kn = Γ n

V n . And by deﬁnition 0 &lt; Kn ≤ κn.

3.2.3

The Optimal Empirical Convergence Theorem

The following proposition gives the optimal empirical contraction inequality, which lies in the

core of our empirical convergence theory.

Proposition 3.3. Let δ ∈ (0, 1) and {Yk}n

k=1 be a set of i.i.d. copies of Y ∼ Pθ∗. Suppose

0 &lt; r ≤ R are radii of contraction and (γ, ν) ∈ C(r, R) is the optimal pair. If assumptions (A1),

(A2) and (A3) hold true and the sample size n is suﬃciently large such that νn &gt; 0, then for any

θ ∈ Br(θ∗) and θ′ ∈ BR(θ∗) such that

Qn(θ′|θ; {Yk}) ≥ Qn(θ∗|θ; {Yk}),

there holds the inequality

��θ′ − θ∗�� ≤ Kn ∥θ − θ∗∥ + En

V n

(26)

with probability at least 1 − δ.

Proof. By a similar argument to that of Proposition 3.2, we have

0 ≤ Qn(θ′|θ; {Yk}) − Qn(θ∗|θ; {Yk})

(a)

= Vn(θ′|θ; {Yk}) +

�∇1Qn(θ∗|θ; {Yk}), θ′ − θ∗�

(b)

≤ Vn(θ′|θ; {Yk}) + ∥∇1Qn(θ∗|θ; {Yk})∥ ·

��θ′ − θ∗��

(c)

= Vn(θ′|θ; {Yk}) + ∥Γn(θ; {Yk}) + En({Yk})∥ ·

��θ′ − θ∗��

(d)

≤ −V n

��θ′ − θ∗��2 +

�

Γn ∥θ − θ∗∥ + En

�

·

��θ′ − θ∗��

where (a) follows from the deﬁnition of Vn(θ′|θ; {Yk}); (b) follows from the Cauchy-Schwartz

inequality; (c) follows from the deﬁnitions of Γn(θ; {Yk}) and En({Yk}); and (d) follows from

17


the deﬁnition of the random variables Γn, V n and En. Conditioning on the event En, we have

V n ≥ νn &gt; 0 , then we can perform the division by V n on both sides of above inequality, and

obtain the desired result.

Remark. Since ε2(δ, r, R, n, p) → 0 as n → ∞, we have νn = ν − ε2(δ, r, R, n, p) &gt; 0 when n is

suﬃciently large.

Before we state the main theorem, we prove one more technical lemma for an event bound.

Lemma 3.3. Let δ ∈ (0, 1) and {Yk}n

k=1 be a set of i.i.d. copies of Y ∼ Pθ∗. Suppose 0 &lt; r ≤ R

are radii of contraction and (γ, ν) ∈ C(r, R) is the optimal pair. If assumptions (A1), (A2) and

(A3) hold true and the sample size n is suﬃciently large such that

εs(δ, r, R, n, p) + rε1(δ, r, n, p) + rε2(δ, r, R, n, p) &lt; r (ν − γ) ,

(27)

then En ⊆

�

ϖ | En &lt; r

�

V n − Γn

��

.

Proof. By deﬁnition (24), under the event En, we have Γ n ≤ γn = γ + ε1(δ, r, n, p), V n ≥ νn =

ν − ε2(δ, r, R, n, p) and En ≤ εs(δ, r, R, n, p), then simple calculation yields that

En ≤ εs(δ, r, R, n, p)

(a)

&lt; r (ν − γ) − rε1(δ, r, n, p) − rε2(δ, r, R, n, p)

= r (ν − ε2(δ, r, R, n, p) − (γ + ε1(δ, r, n, p)))

= r (νn − γn) ≤ r

�

V n − Γ n

�

where (a) follows from assumption (27) and the result is proved.

Remark. We note (27) implies that νn = ν − ε2(δ, r, R, n, p) &gt; γn + 1

rεs(δ, r, R, n, p) &gt; 0.

Now we state and prove the main theorem.

Theorem 3.2 (Optimal Empirical Convergence Theorem). Let δ ∈ (0, 1) and {Yk}n

k=1 be a set

of i.i.d. copies of Y ∼ Pθ∗. Suppose 0 &lt; r ≤ R are radii of contraction and (γ, ν) ∈ C(r, R) is the

optimal pair. If assumptions (A1), (A2) and (A3) hold true and the sample size n is suﬃciently

large such that

εs(δ, r, R, n, p) + rε1(δ, r, n, p) + rε2(δ, r, R, n, p) &lt; r(ν − γ),

(28)

then given an initial point Θ0

n ∈ Br(θ∗), the empirical EM sequence {Θt

n}t≥0 such that

Θt+1

n

∈ arg max{Qn(Θ′|Θt

n; {Yk}) | Θ′ ∈ BR(θ∗)} for t ≥ 0

satisﬁes the inequality

���Θt

n − θ∗��� ≤

�

Kn

�t ���Θ0

n − θ∗��� +

En

V n − Γn

(29)

with probability at least 1 − δ.

Proof. By Lemma 3.2 we have Pr En ≥ 1 − δ, where

En =

�

ϖ | Γ n ≤ γn, V n ≥ νn and En ≤ εs(δ, r, R, n, p)

�

.

(30)

18


Conditioning on this event and by Lemma 3.3, we have Kn =

Γn

V n ≤ κn &lt; 1 and En &lt;

r

�

V n − Γn

�

. Now we claim that: for t ∈ N, the empirical EM sequence satisﬁes

���Θt+1

n

− θ∗��� ≤ Kn

���Θt

n − θ∗��� + En

V n

.

(31)

We prove this claim by induction. Note Θ0

n ∈ Br(θ∗) and Θ1

n ∈ BR(θ∗) by deﬁnition, and

since Qn(Θ1

n|Θ0

n; {Yk}) ≥ Qn(θ∗|Θ0

n; {Yk}) and νn &gt; 0, it follows from Proposition 3.3 that

���Θ1

n − θ∗��� ≤ Kn

���Θ0

n − θ∗��� + En

V n

and

���Θ1

n − θ∗��� &lt; Γn

V n

· r + En

V n

&lt; r

under the event En. Hence (31) holds for t = 0 and Θ1

n ∈ Br(θ∗).

Now assume (31) holds for t ≥ 0 and Θt+1

n

∈ Br(θ∗), then for

Θt+2

n

∈ arg max{Qn(Θ′|Θt+1

n

; {Yk}) | Θ′ ∈ BR(θ∗)},

we have Θt+2

n

∈ BR(θ∗) and Qn(Θt+2

n

|Θt+1

n

; {Yk}) ≥ Qn(θ∗|Θt+1

n

; {Yk}). Then by Proposition 3.3,

���Θt+2

n

− θ∗��� ≤ Kn

���Θt+1

n

− θ∗��� + En

V n

and

���Θt+2

n

− θ∗��� &lt; Γn

V n

· r + En

V n

&lt; r

under the event En. Hence (31) holds for t + 1 and Θt+2

n

∈ Br(θ∗). We conclude that (31) holds

for all t ∈ N and the claim is proved.

Now it remains to show (29). We proceed by induction again. It clearly holds for t = 0;

assume it holds for t ≥ 0, then by (31) and the induction hypothesis,

���Θt+1

n

− θ∗��� ≤ Kn

���Θt

n − θ∗��� + En

V n

≤ Kn

��

Kn

�t ���Θ0

n − θ∗��� +

En

V n − Γn

�

+ En

V n

=

�

Kn

�t+1 ���Θ0

n − θ∗��� +

En

V n − Γn

.

Hence it holds for t + 1 and by induction it holds for all t ∈ N and the proof is complete.

Remark. In view of deﬁnition (23), the random variables Γ n, V n and hence Kn are data-adaptive.

For each realization {yk}n

k=1 of i.i.d. copies {Yk}n

k=1 of Y ∼ Pθ∗, the above theorem produces a re-

alization kn of the optimal empirical convergence rate Kn. The sample EM sequence constructed

from the realization Qn(θ′|θ; {yk}) converges geometrically at the rate of kn. Hence Kn quan-

titatively characterizes the propagation of the randomness from the underlying data generating

distribution Pθ∗ to the convergence rate of the empirical EM sequence.

Remark. Under the event En, we have Γ n ≤ γn &lt; γn, V n ≥ νn &gt; νn and En ≤ εs(δ, r, R, n, p),

hence

En

V n−Γn ≤ εs(δ,r,R,n,p)

νn−γn

. Then (29) implies that

���Θt

n − θ∗��� ≤

�

Kn

�t ���Θ0

n − θ∗��� + εs(δ, r, R, n, p)

νn − γn

(32)

19


with probability at least 1 − δ. This inequality is sometimes more convenient when we apply the

optimal empirical convergence theorem.

We note that since νn → ν, γn → γ, εs(δ, r, R, n, p) → 0 and Kn ≤ κn � κ as the sample

size n → ∞, then intuitively, the empirical inequality (32) “converges” to the oracle inequality in

the form (21), hence the limit of an empirical EM sequence should give a consistent estimate to

θ∗. Indeed, as a consequence of the self-consistency of the oracle Q-function (12) and the above

observation, we have the following result.

Theorem 3.3 (Consistency of the EM algorithm). Suppose δ ∈ (0, 1) and 0 &lt; r ≤ R are radii

of contraction, (γ, ν) ∈ C(r, R). If assumptions (A1), (A2) and (A3) hold true, then there exists

an N ∈ N such that whenever the sample size n &gt; N, for each set {Yk}n

k=1 of i.i.d. copies of

Y ∼ Pθ∗ and the empirical EM sequence {Θt

n}t≥0 therefrom, if limt→∞ Θt

n =: �Θn ∈ Ω for each

n &gt; N, then

����Θn − θ∗��� ≤ εs(δ, r, R, n, p)

νn − γn

(33)

with probability at least 1 − δ. Hence limn→∞ �Θn = θ∗ in probability.

Proof. Let N be the smallest n such that condition (28) holds, then for each n &gt; N, by Theorem

3.2 and the fact that Kn ≤ κn &lt; 1, the empirical EM sequence {Θt

n}t≥0 constructed from {Yk}n

k=1

satisﬁes

���Θt

n − θ∗��� ≤ κt

n

���Θ0

n − θ∗��� + εs(δ, r, R, n, p)

νn − γn

for t ∈ N,

with probability at least 1 − δ. Then let t → ∞ in the above inequality and notice Θt

n → �Θn as

t → ∞, we obtain (33). Since for δ &gt; 0, εs(δ, r, R, n, p) → 0 and νn − γn → ν − γ &gt; 0 as n → ∞,

it follows from (33) that limn→∞ �Θn = θ∗ in probability.

Remark. The classical work of Wu [40] proved that, under the unimodal assumption and other

regularity conditions on the log-likelihood function, the sample EM sequence converges to the

MLE. In this case, the statistical consistency of the EM algorithm can be guaranteed by that of

the MLE. Balakrishnan et al. [1] obtained convergence results of the sample EM sequence to the

statistical error ball of the true population parameter θ∗ in canonical models, which implies the

statistical consistency of the sample EM sequences in these cases. In their work, the statistical

error is characterized by the following deviation bound

sup

θ∈Br(θ∗)

∥Mn(θ) − M(θ)∥ ≤ εunif

M (n, δ).

In the general case, we do not know whether this uniform deviation can be bounded by an

inﬁnitesimal εunif

M (n, δ) as the sample size n → ∞, since it may not necessarily be true that the

sample M-operator Mn(θ) is the empirical mean and the population M-operator M(θ) is the

corresponding population mean.

In our theory, the statistical error is characterized by the norm of the empirical mean En({Yk})

of E(Yk) = ∇1Q(θ∗|θ∗; Yk) for 1 ≤ k ≤ n.

Since Eθ∗E(Y ) = ∇1Q∗(θ∗|θ∗) = 0 by the self-

consistency (7), it is then guaranteed that ∥En({Yk})∥ ≤ εs(δ, r, R, n, p) → 0 as n → ∞. Hence

the above theorem gives a theoretical guarantee for the consistency of the limit point �Θn of

20


the empirical EM sequence not only for canonical models but also for the general case, and

εs(δ, r, R, n, p) is exactly the convergence rate of the statistical error of the empirical EM sequence.

Remark. We do not claim �Θn as an MLE or stationary point of a log-likelihood function. Instead,

we believe any point within the statistical error ball of θ∗ serves equivalently as a consistent

estimate.

In practical applications, we do not even need the well-deﬁned convergence of the

empirical EM sequence {Θt

n}t≥0 to some point �Θn ∈ Ω. Indeed, by (32) when the number of

iterations T is suﬃciently large, the optimization error would be so small that any point Θt

n for

t ≥ T is almost within the statistical precision to θ∗.

3.2.4

The Optimal Rate Convergence Theorem

Now we prove a non-asymptotic concentration bound for the optimal empirical convergence rate

Kn on the optimal oracle convergence rate κ, which then implies that Kn → κ in probability as

the sample size n → ∞.

We ﬁrst characterize the concentration property of the empirical contraction parameters on

their population versions, which is the following result on concentration of contraction parameters.

Proposition 3.4. Suppose assumptions (A1), (A2) and (A3) hold true, δ ∈ (0, 1) and that

G(r) × V(r, R) ̸= ∅, then

���Γ n − γ

��� ≤ ε1(δ, r, n, p) and

���V n − ν

��� ≤ ε2(δ, r, R, n, p)

with probability at least 1 − δ.

Proof. In view of Lemma 3.2, for given (γ, ν) ∈ G(r) × V(r, R) ̸= ∅, we have

Γ n ≤ γn &lt; +∞ and V n ≥ νn &gt; −∞

with probability at least 1 − δ. Conditioning on this event and by (17) and (23), we have

���Γ n − γ

���

(a)

≤ sup

�����

∥Γn(θ; {Yk})∥

∥θ − θ∗∥

− ∥Eθ∗Γ(θ; Y )∥

∥θ − θ∗∥

���� | θ ∈ B×

r (θ∗)

�

≤ sup

�∥Γn(θ; {Yk}) − Eθ∗Γ(θ; Y )∥

∥θ − θ∗∥

| θ ∈ B×

r (θ∗)

�

≤ ε1(δ, r, n, p),

where (a) follows from Lemma E.1(a). Similarly, by (18) and (23), we have

���V n − ν

���

(a)

≤ sup

������

Eθ∗V (θ′|θ)

∥θ′ − θ∗∥2 − Vn(θ′|θ)

∥θ′ − θ∗∥2

����� |

�θ′, θ

� ∈ B×

R(θ∗) × Br(θ∗)

�

= sup

�

|Eθ∗V (θ′|θ) − Vn(θ′|θ)|

∥θ′ − θ∗∥2

|

�θ′, θ

� ∈ B×

R(θ∗) × Br(θ∗)

�

≤ ε2(δ, r, R, n, p),

where (a) follows from Lemma E.1(b), and the proof is complete.

Now we state and prove the concentration theorem.

21


Theorem 3.4 (Optimal Rate Convergence Theorem). Suppose assumptions (A1), (A2) and (A3)

hold true, δ ∈ (0, 1) and 0 &lt; r ≤ R are radii of contraction, let (γ, ν) ∈ C(r, R) be the optimal pair

of the oracle convergence. If the sample size n is suﬃciently large such that ε2(δ, r, R, n, p) &lt; 1

2ν,

then

���Kn − κ

��� ≤ 2

ν (ε1(δ, r, n, p) + κε2(δ, r, R, n, p))

(34)

with probability at least 1 − δ. Hence Kn → κ in probability as n → ∞.

Proof. In view of Proposition 3.4, we have

���Γ n − γ

��� ≤ ε1(δ, r, n, p) and

���V n − ν

��� ≤ ε2(δ, r, R, n, p)

with probability at least 1 − δ. Conditioning on this event, we have

���Kn − κ

��� =

���

�

Γ n − γ

�

ν +

�

ν − V n

�

γ

���

V nν

≤ 1

V n

����Γ n − γ

��� + γ

ν

���ν − V n

���

�

≤ 1

V n

(ε1(δ, r, n, p) + κε2(δ, r, R, n, p)) ,

and since V n ≥ ν − ε2(δ, r, R, n, p) &gt; 1

2ν, the bound (34) follows. Moreover for δ &gt; 0, we have

ε1(δ, r, n, p) → 0 and ε2(δ, r, R, n, p) → 0 as n → ∞, it follows from (34) that limn→∞ Kn = κ in

probability.

In view of deﬁnition (25) and the theorem above, Kn is upper bounded by κn and concentrated

on the optimal oracle convergence rate κ &lt; κn. The relationship of the real numbers κn, κ and

the random variable Kn can be illustrated in the following schematic diagram,

1

0

κn

κ

[

κ − ε

κ + ε

]

Kn

Figure 2: Concentration and upper bound of Kn

where ε → 0 and κn � κ as n → ∞, hence the distribution of the random variable Kn collapses

on κ when the sample size n is suﬃciently large.

4

Applications to Canonical Models

In this section, we apply our theory to the EM algorithm on three canonical models: the Gaussian

Mixture Model, the Mixture of Linear Regressions and the Regression with Missing Covariates

to obtain speciﬁc results for these models.

Notations The following notations are used throughout this section:

• We use c, C, C1, C2, C3 · · · to denote a numerical constant.

• For θ∗ ̸= 0, let η := ∥θ∗∥

σ

be the signal to noise ratio (SNR). Let ω :=

r

∥θ∗∥ be the

relative contraction radius (RCR) and let K := σ + ∥θ∗∥ = σ (1 + η).

22


• Let L = N 1

2

�Sp−1� &lt; 5p be the 1

2-covering number of Sp−1. (see Section E.2)

• Let φ (x; µ, Σ) be the density function of the multivariate normal distribution N (µ, Σ),

where µ ∈ Rp and Σ ∈ Rp×p.

4.1

Gaussian Mixture Model

Consider the balanced symmetric Gaussian mixture model

Y = Z · θ∗ + W,

where Z is a Rademacher random variable, W ∼ N(0, σ2Ip) is the Gaussian noise with variance

σ2 and θ∗ ∈ Rp (p ≥ 1). Suppose Y is observed and Z is a latent variable, the complete joint

density of (Y, Z) is

fθ∗(y, z) = 1

2φ(y − z · θ∗; 0, σ2Ip),

and marginalization over Z gives the density of Y as a Gaussian mixture

gθ∗(y) = 1

2φ(y − θ∗; 0, σ2Ip) + 1

2φ(y + θ∗; 0, σ2Ip).

Suppose a set of i.i.d. realizations {yk}n

k=1 of Y are observed from the mixture density, the

goal is to estimate the unknown true population parameter θ∗ ∈ Ω ⊆ Rp, while the variance σ2

is assumed known.

Standard calculation of the EM algorithm yields the stochastic Q-function

Q(θ′|θ; y) = − 1

2σ2

�

wθ(y)

��y − θ′��2 + (1 − wθ(y))

��y + θ′��2�

− log 2

�√

2πσ

�p

where wθ(y) := ς

�

2θ⊺y

σ2

�

, and ς(t) :=

1

1+e−t is the logistic function. Then the gradient

∇1Q(θ′|θ; y) = 1

σ2

�(2wθ(y) − 1)y − θ′� ,

and hence the GRV

Γ(θ; Y ) = ∇1Q(θ∗|θ; Y ) − ∇1Q(θ∗|θ∗; Y )

= 2

σ2 [wθ(Y ) − wθ∗(Y )] Y.

(35)

Since Q(θ′|θ; y) is quadratic in θ′, the CRV can be computed as

V (θ′|θ; Y ) = − 1

2σ2

��θ′ − θ∗��2

(36)

by Lemma E.2. Then the SEV

E(Y ) = 1

σ2 [(2wθ∗(Y ) − 1)Y − θ∗] .

(37)

23


4.1.1

Oracle Convergence

We ﬁrst characterize the sets G(r) and V(r, R). It is clear from (36) that

Eθ∗V (θ′|θ; Y ) = − 1

2σ2

��θ′ − θ∗��2 ,

and hence V(r, R) = (0, ν], where ν =

1

2σ2 for any 0 &lt; r ≤ R. As for the set G(r) we need to

bound

Eθ∗Γ(θ; Y ) = 2

σ2 Eθ∗ [(wθ(Y ) − wθ∗(Y ))Y ] .

To this end, we cite the following technical result from [1] (Lemma 2).

Lemma 4.1. If θ∗ ̸= 0 and the signal to noise ratio η is suﬃciently large, then

����

2

σ2 Eθ∗ [(wθ(Y ) − wθ∗(Y ))Y ]

���� ≤ γ (η) ∥θ − θ∗∥ for θ ∈ Br(θ∗),

where r = ∥θ∗∥

4

and γ (η) :=

1

σ2 e−cη2.

Hence for r = ∥θ∗∥

4 , we have G(r) = [γ, +∞) where γ ≤ γ (η). It is clear that γ (η) &lt; ν =

1

2σ2

when η is suﬃciently large. In that case, 0 &lt; r &lt; +∞ are radii of contraction and (γ (η) , ν) ∈

C (r, +∞) ̸= ∅. We then apply the oracle convergence theorem to get the following result for the

Gaussian Mixture Model.

Corollary 4.1. For the Gaussian Mixture Model, if η is suﬃciently large such that κ := 2e−cη2 &lt;

1, then 0 &lt; r &lt; +∞ where r = ∥θ∗∥

4 , are radii of contraction. For each pair

�

γ (η) ,

1

2σ2

�

∈

C (r, +∞) ̸= ∅ and initial point θ0 ∈ Br(θ∗), any oracle EM sequence {θt}t≥0 such that

θt+1 ∈ arg max

θ′∈Ω Q∗(θ′|θt) for t ≥ 0

satisﬁes the inequality

���θt − θ∗��� ≤ κt ���θ0 − θ∗��� ,

(38)

where κ := γ

ν ≤ κ &lt; 1, is the optimal oracle convergence rate.

4.1.2

Empirical Convergence

For empirical convergence results, we need to ﬁnd speciﬁc forms of the ε-bounds in the Assump-

tions.

Lemma 4.2. For δ ∈ (0, 1) and r &gt; 0, if n &gt; c log (L/δ), then3

∥Γn(θ; {Yk}) − Eθ∗Γ(θ; Y )∥ ≤ C K2

σ2

�

log(L/δ)

n

∥θ − θ∗∥ for θ ∈ Br(θ∗)

(39)

with probability at least 1 − δ.

Proof. See Section A.2.

3Note log(L/δ) ≤ O(p), see Section E.2.

24


Secondly, in view of (36) we have

V (θ′|θ; Y ) = Vn(θ′|θ; {Yk}) = Eθ∗V (θ′|θ; Y ) = − 1

2σ2

��θ′ − θ∗��2 ,

(40)

and hence ε2(δ, r, R, n, p) = 0. Thirdly, for the ε-bound on statistical error, we have the following

result.

Lemma 4.3. For δ ∈ (0, 1), there holds the inequality

∥En({Yk})∥ ≤ C K

σ2

�

log(L/δ)

n

(41)

with probability at least 1 − δ.

Proof. See Section A.3.

From (39) and (41) in the above lemmas, it is clear that the Assumptions (A1∼A3) are

satisﬁed with the following

ε1(δ, r, n, p) = C1

K2

σ2

�

log(L/δ)

n

,

ε2(δ, r, R, n, p) = 0

and

εs(δ, r, R, n, p) = C2

K

σ2

�

log(L/δ)

n

.

(42)

We obtain the following data-adaptive empirical convergence result for the Gaussian Mixture

Model.

Corollary 4.2. Suppose {Yk}n

k=1 is a set of i.i.d. copies of the Gaussian mixture Y ∼ gθ∗(y)

and δ ∈ (0, 1). If η is suﬃciently large such that κ := 2e−cη2 &lt; 1 and the sample size n satisﬁes

n &gt; log(L/δ)

(1 − κ)2

�

C1K2 + C2

�

1 + 1

η

��2

,

(43)

then for r = ∥θ∗∥

4

and an initial point θ0 ∈ Br(θ∗), with probability at least 1 − δ, the empirical

EM sequence {Θt

n}t≥0 such that

Θt+1

n

∈ arg max{Qn(Θ′|Θt

n; {yk} | Θ′ ∈ Rp) for t ≥ 0,

satisﬁes the inequality

���Θt

n − θ∗��� ≤

�

Kn

�t ���Θ0

n − θ∗��� + C3K

1 − κn

�

log(L/δ)

n

,

(44)

where the optimal empirical convergence rate Kn satisﬁes that

Kn ≤ κ + C4K2

�

log(L/δ)

n

&lt; 1 and

���Kn − κ

��� ≤ CK2

�

log(L/δ)

n

.

25


Proof. We ﬁrst note condition (43) implies the lower bound of n in Lemma 4.2. Hence to apply

the empirical convergence theorem, we only need to verify that

εs(δ, r, R, n, p) + rε1(δ, r, n, p) + rε2(δ, r, R, n, p) &lt; r (ν − γ (η))

holds true whenever n satisﬁes (43), but this is trivial.

Then note ε2(δ, r, R, n, p) = 0 and νn =

1

2σ2 , hence the concentration bound of Kn follows

from the optimal rate convergence theorem.

4.2

Mixture of Linear Regressions

Consider the Mixture of Linear Regressions model with two balanced symmetric components in

which the covariate-response (Y, X) are linked via

Y = ⟨X, Z · θ∗⟩ + W,

(45)

where Z is a Rademacher variable, W ∼ N(0, σ2) is the Gaussian noise and X ∼ N(0, Ip) is a

Gaussian covariate. Given a set of i.i.d realizations {(yk, xk)}n

k=1 generated by (45), the goal is

to estimate the unknown population parameter θ∗ ∈ Rp.

In above setting, we observe the covariate-response pair (Y, X) while Z is a latent variable.

The complete joint density is

fθ(y, x, z) = 1

2φ(y − ⟨x, z · θ⟩ ; 0, σ2)φ(x; 0, Ip),

where (y, x) ∈ R × Rp and z ∈ {−1, 1}. Then it is a standard procedure to obtain the stochastic

Q-function

Q(θ′|θ; (y, x)) = − 1

2σ2

�

wθ(y, x)

�y −

�x, θ′��2 + (1 − wθ(y, x))

�y +

�x, θ′��2�

− 1

2 ∥x∥2 − log 2

�√

2πσ

�p+1 ,

where wθ(y, x) := ς

� 2y⟨x,θ⟩

σ2

�

and ς(t) :=

1

1+e−t is the logistic function. Then the gradient

∇1Q(θ′|θ; (y, x)) = 1

σ2

�(2wθ(y, x) − 1)y −

�x, θ′�� x,

and hence the GRV

Γ(θ; (Y, X)) = ∇1Q(θ∗|θ; (Y, X)) − ∇1Q(θ∗|θ∗; (Y, X))

= 2

σ2 [wθ(Y, X) − wθ∗(Y, X)] Y X.

(46)

Since Q(θ′|θ; (y, x)) is quadratic in θ′, the CRV can be computed as

V (θ′|θ; (Y, X)) = − 1

2σ2 (θ′ − θ∗)⊺XX⊺(θ′ − θ∗)

(47)

26


by Lemma E.2. Then the SEV

E(Y, X) = 1

σ2 [(2wθ∗(Y, X) − 1)Y − ⟨X, θ∗⟩] X.

(48)

4.2.1

Oracle Convergence

We ﬁrst characterize the sets G(r) and V(r, R). Since X ∼ N(0, Ip) and by (47),

Eθ∗V (θ′|θ; (Y, X)) = − 1

2σ2

��θ′ − θ∗��2 ,

(49)

hence V(r, R) = (0, ν], where ν =

1

2σ2 for any 0 &lt; r ≤ R. As for the set G(r) we need to bound

Eθ∗Γ(θ; (Y, X)) = 2

σ2 Eθ∗ [(wθ(Y, X) − wθ∗(Y, X)) Y X] .

To this end, we cite a technical result from [41] (Lemma 7 in the Supplement), see also Lemma

3 in [1] for an alternative.

Lemma 4.4. If ω ∈

�

0, 1

4

�

and r = ω ∥θ∗∥, then

∥Eθ∗Γ(θ; (Y, X))∥ ≤ γ (ω, η) ∥θ − θ∗∥ for θ ∈ Br(θ∗),

where γ (ω, η) :=

1

σ2

�

7.3ω + 17

η

�

.

Hence G(r) = [γ, +∞), where γ ≤ γ (ω, η) for r = ω ∥θ∗∥ and ω ∈

�

0, 1

4

�

.

It is clear

that γ (ω, η) &lt; ν =

1

2σ2 , when η is suﬃciently large and ω is suﬃciently small. In that case,

0 &lt; ω ∥θ∗∥ &lt; +∞ are radii of contraction and (γ (ω, η) , ν) ∈ C(ω ∥θ∗∥ , +∞) ̸= ∅. We then apply

the oracle convergence theorem to get the following corollary.

Corollary 4.3. For the Mixture of Linear Regressions model, if η is suﬃciently large and ω is

suﬃciently small such that 7.3ω + 17

η &lt; 1

2, then 0 &lt; r &lt; +∞ where r = ω ∥θ∗∥ are radii of

contraction. For each pair

�

γ (ω, η) ,

1

2σ2

�

∈ C(r, +∞) and initial point θ0 ∈ Br(θ∗), any oracle

EM sequence {θt}t≥0 such that

θt+1 ∈ arg max

θ′∈Ω Q∗(θ′|θt) for t ≥ 0,

satisﬁes the inequality

���θt − θ∗��� ≤ κt ���θ0 − θ∗��� ,

(50)

where κ := γ

ν ≤ 2

�

7.3ω + 17

η

�

&lt; 1, is the optimal oracle convergence rate.

4.2.2

Empirical Convergence

For empirical convergence results, we need to ﬁnd speciﬁc forms of the ε-bounds in the Assump-

tions.

Lemma 4.5. For δ ∈ (0, 1) and r &gt; 0, there holds the inequality

∥Γn(θ; {(Yk, Xk)}) − Eθ∗Γ(θ; (Y, X))∥ ≤ C log(L/δ)

σ2n

1

2 −ϵ ∥θ − θ∗∥ for θ ∈ Br(θ∗)

(51)

27


with probability at least 1 − δ.

Proof. See Section B.2.

Lemma 4.6. For δ ∈ (0, 1) and r &gt; 0, if n &gt; c log (1/δ) then

��Vn(θ′|θ; {(Yk, Xk)}) − Eθ∗V (θ′|θ; (Y, X))

�� ≤ C

σ2

�

log(1/δ)

n

��θ′ − θ∗��2 for θ′, θ ∈ Br(θ∗)

(52)

with probability at least 1 − δ.

Proof. See Section B.3.

Lemma 4.7. For δ ∈ (0, 1) and n &gt; c log (L/δ), there holds the inequality

∥En({(Yk, Xk)})∥ ≤ C

σ (1 + 2η)

�

log(L/δ)

n

(53)

with probability at least 1 − δ.

Proof. See Section B.4.

From (51), (52) and (53) in above lemmas, it is clear that the Assumptions (A1∼A3) are

satisﬁed with the following

ε1(δ, r, n, p) = C1

σ2

log(L/δ)

n

1

2 −ϵ

,

ε2(δ, r, R, n, p) = C2

σ2

�

log(1/δ)

n

and

εs(δ, r, R, n, p) = C3

σ (1 + 2η)

�

log(L/δ)

n

.

(54)

We obtain the following data-adaptive empirical convergence result for the Mixture of Linear

Regressions model.

Corollary 4.4. Suppose {(Yk, Xk)}n

k=1 is a set of i.i.d. copies of (Y, X) ∼ Pθ∗ and δ ∈ (0, 1). If

η is suﬃciently large and ω is suﬃciently small such that 7.3ω + 17

η &lt; 1

2 and the sample size n

satisﬁes

n &gt; log(1/δ)

(1 − κ)2

�

C1

�

log(1/δ) · pnϵ + C2 + C3

(1 + 2η)

ωη

√p

�2

(55)

where κ := γ(ω,η)

ν

= 2

�

7.3ω + 17

η

�

and r = ω ∥θ∗∥. Then given an initial point Θ0

n ∈ Br(θ∗), with

probability at least 1 − δ, any empirical EM sequence {Θt

n}t≥0 such that

Θt+1

n

∈ arg max

θ′∈Ω Qn(Θ′|Θt

n; {(Yk, Xk)}) for t ≥ 0,

satisﬁes the inequality

���Θt

n − θ∗��� ≤

�

Kn

�t ���Θ0

n − θ∗��� + C3 (1 + 2η)

σ (νn − γn)

�

log(L/δ)

n

(56)

28


where

γn := γ (ω, η) + C1

σ2

log(L/δ)

n

1

2 −ϵ

,

νn :=

1

2σ2 − C2

σ2

�

log(1/δ)

n

,

and the optimal empirical convergence rate Kn satisﬁes that

Kn ≤ γn

νn

&lt; 1 and

���Kn − κ

��� ≤

�

C1

�

log(1/δ) · pnϵ + C2κ

� �

log(1/δ)

n

.

Proof. Note condition (55) implies the lower bounds of n in Lemma 4.6 and Lemma 4.7. Hence

to apply the empirical convergence theorem, we only need to verify that

εs(δ, r, R, n, p) + rε1(δ, r, n, p) + rε2(δ, r, R, n, p) &lt; r (ν − γ (ω, η))

holds true whenever n satisﬁes (55), which is not diﬃcult noticing that log(L/δ) &lt; Cp log(1/δ).

Moreover, (55) also implies that ε2(δ, r, R, n, p) = C2

σ2

�

log(1/δ)

n

&lt; 1

2ν =

1

4σ2 , hence the concen-

tration bound of Kn follows from the optimal rate convergence theorem.

Remark. In view of (47) and the deﬁnitions in (23), we ﬁnd V n =

1

2σ2 λmin

n

, where λmin

n

is the

smallest eigenvalue of the empirical mean of the Gaussian covariance matrix XX⊺, while we do

not know a closed form for Γn or Kn yet in this model.

4.3

Linear Regression with Missing Covariates

Consider the linear regression in which the covariate-response (Y, X) are linked via

Y = ⟨θ∗, X⟩ + W,

(57)

where W ∼ N(0, σ2) is the observational noise and the covariate X ∼ N(0, Ip) under Gaussian

design. Instead of observing the complete data (Y, X), we have each coordinate Xj (j = 1, · · · , p)

of the covariate missing completely at random with a probability ϵ ∈ [0, 1).

It is not diﬃcult to see that there is a 1-to-1 correspondence between the set of missing

patterns and the set of binary vectors {◦, 1}p, where ◦ is just 0 written diﬀerently and hence

◦ · a = ◦ and ◦ + a = a for a ∈ R.4 Indeed, given τ ∈ {◦, 1}p we say Xj is missing iﬀ τ j = ◦. The

missing pattern τ is a discrete random variable with distribution

ψ(τ) = ϵp−|τ|(1 − ϵ)|τ| for τ ∈ {◦, 1}p,

(58)

where |·| denotes the number of 1’s in τ. Let s := 1 − τ be the complement of τ in {◦, 1}p, where

1 denotes the vector with all coordinates 1, and we also introduce the following notation for

convenience: for a (random) vector x ∈ Rp and τ ∈ {◦, 1}p, denote by xτ = x ⊙ τ the Hadamard

product of x and τ, hence x = xs + xτ.

Then in the missing covariates regression model, the observed variable is (Y, Xs). Note the

missing pattern τ = 1 − s is determined by the observed Xs by checking the coordinates marked

as ◦.

4This notational variation is necessary to distinguish missing coordinates from those having value 0.

29


Suppose a set of i.i.d samples {(yk, xk)}n

k=1 and {τk}n

k=1 are generated by (57) and (58)

respectively, while we only observe {(yk, (xk)sk)}n

k=1 where sk = 1 − τk, and wish to estimate the

true population parameter θ∗.

It is not hard to write down the complete joint density

fθ(y, xs, xτ) = φ(y − θ⊺

sxs − θ⊺

τxτ; 0, σ2)φ(xs + xτ; 0, Ip)ψ(s),

where (y, xs, xτ) ∈ R × Rp × Rp.

The density of the observed pair (Y, Xs) is the marginalization

gθ(y, xs)

=

�

Rp fθ(y, xs, xτ)dxτ

=

φ(y − θ⊺

sxs; 0, σ2 + ∥θτ∥2)φ(xs; 0, diag{s})ψ(s),

where the integration is over coordinates of xτ not marked as ◦.

The conditional density of the latent variable Xτ is

kθ(xτ|y, xs) = fθ(y, xs, xτ)/gθ(y, xs) = φ (xτ; bθ, Aθ) ,

which is Gaussian with mean vector

bθ(y, xs) := Eθ [Xτ|y, xs] =

y − θ⊺

sxs

σ2 + ∥θτ∥2 θτ,

and covariance matrix

Aθ(τ) := Eθ [(Xτ − bθ(y, xs)) (Xτ − bθ(y, xs))⊺ |y, xs] = diag{τ} −

1

σ2 + ∥θτ∥2 θτθ⊺

τ,

(59)

which is also the conditional covariance matrix of the vector X given xs and y, since

Xτ − bθ(y, xs) = X − Eθ [X|y, xs] .

Denote the conditional mean of the covariate X by

µθ(y, xs) := Eθ [X|y, xs] = Eθ [xs + Xτ|y, xs] = xs + bθ(y, xs),

(60)

and the conditional mean of the matrix XX⊺ by

Σθ(y, xs) := Eθ [XX⊺|y, xs] = µθ(y, xs)µθ(y, xs)⊺ + Aθ(τ).

(61)

30


It is a routine procedure to calculate the stochastic Q-function

Q(θ′|θ; (y, xs))

=

Eθ [log fθ′(y, xs, Xτ)|y, xs]

=

− 1

2σ2 Eθ

�

y2 − 2yθ′⊺X + θ′⊺XX⊺θ′|y, xs

�

−1

2Eθ [X⊺X|y, xs] − p log

√

2π + log ψ(s)

=

− 1

2σ2

�

y2 − 2θ′⊺µθ(y, xs)y + θ′⊺Σθ(y, xs)θ′�

−1

2trΣθ(y, xs) − p log

√

2π + log ψ(s)

and the gradient

∇1Q(θ′|θ; (y, xs)) = 1

σ2

�yµθ(y, xs) − Σθ(y, xs)θ′� .

Hence the GRV

Γ(θ; (Y, Xs)) = 1

σ2 [Y (µθ(Y, Xs) − µθ∗(Y, Xs)) − (Σθ(Y, Xs) − Σθ∗(Y, Xs)) θ∗] .

(62)

Since Q(θ′|θ; (y, xs)) is quadratic in θ′, the CRV is

V (θ′|θ; (Y, Xs)) = − 1

2σ2 (θ′ − θ∗)⊺Σθ(Y, Xs)(θ′ − θ∗)

(63)

by Lemma E.2. Then the SEV

E(Y, Xs) = 1

σ2 [Y µθ∗(Y, Xs) − Σθ∗(Y, Xs)θ∗] .

(64)

4.3.1

Oracle Convergence

For r &gt; 0 and θ∗ ̸= 0, let ξ := (1 + ω) η2, where the RCR ω and SNR η are deﬁned in the notations.

To characterize the sets G(r) and V(r, R), we have the following bounds for the population mean

of GRV and CRV.

Lemma 4.8. For the Linear Regression with Missing Covariates model,

∥EΓ(θ; (Y, Xs))∥ ≤ γ (ω, η) ∥θ − θ∗∥ for θ ∈ Br (θ∗)

where

γ (ω, η) := 1

σ2

��

ωξ2 + (3ω + 2) ξ + 1

�

ϵ + ξ

�

ϵ (1 − ϵ)

�

,

(65)

and the expectation is taken over (Y, Xs) and the missing pattern τ = 1 − s.

Proof. See Section C.1.2.

Remark. It follows that γ (ω, η) ∈ G(r) ̸= ∅ for r &gt; 0.

Lemma 4.9. For the Linear Regression with Missing Covariates model,

EV (θ′|θ; (Y, Xs)) ≤ −ν (ω, η)

��θ′ − θ∗��2 for

�θ′, θ

� ∈ Rp × Br (θ∗)

31


where

ν (ω, η) :=

1

2σ2

�

1 − 2ωξ

�

ϵ (1 − ϵ) − (1 + ω) ξϵ

�

,

(66)

and the expectation is taken over (Y, Xs) and the missing pattern τ = 1 − s.

Proof. See Section C.1.3.

Remark. It follows that ν (ω, η) ∈ V(r, +∞) ̸= ∅ for r &gt; 0.

In view of above lemmas, for θ∗ ̸= 0, if the probability of missingness ϵ and the RCR ω are

suﬃciently small and the SNR η is bounded above, then γ (ω, η) ≪

1

2σ2 and ν (ω, η) ≈

1

2σ2 and

in that case, 0 &lt; r &lt; +∞, where r = ω ∥θ∗∥ are radii of contraction and (γ (ω, η) , ν (ω, η)) ∈

C (r, +∞) ̸= ∅ is a pair of contraction parameters. By imposing conditions that ensure γ (ω, η) &lt;

ν (ω, η), we can obtain various forms of oracle convergence results via the oracle convergence

theorem. Among them we state and prove the following corollary.

Corollary 4.5. For the Linear Regression with Missing Covariates model, if θ∗ ̸= 0 and

1

√1 + ω &lt; η &lt;

1

3 (1 + ω)

4√ϵ,

(67)

then 0 &lt; r &lt; +∞ where r = ω ∥θ∗∥ are radii of contraction and (γ (ω, η) , ν (ω, η)) ∈ C(r, +∞).

Then given initial point θ0 ∈ Br(θ∗), any oracle EM sequence {θt}t≥0 such that

θt+1 ∈ arg max

θ′∈Ω Q∗(θ′|θt) for t ≥ 0,

satisﬁes the inequality

���θt − θ∗��� ≤ κt ���θ0 − θ∗���

(68)

where κ := γ

ν ≤ γ(ω,η)

ν(ω,η) &lt; 1, is the optimal oracle convergence rate with respect to r &lt; +∞.

Proof. We only need to verify that (67) implies γ (ω, η) &lt; ν (ω, η), which is trivial, and the

corollary follows from the oracle convergence theorem.

Remark. The condition (67) imposes an upper bound on the probability of missingness ϵ, namely

√ϵ &lt;

1

9(1+ω) &lt; 1

9, hence ϵ &lt; 1

81.

4.3.2

Empirical Convergence

For empirical convergence results, we need to ﬁnd the speciﬁc forms of the ε-bounds in the

Assumptions. To ease notations, we use Zk := (Yk, (Xk)sk) to denote an i.i.d. copy of (Y, Xs)

throughout this section.

Lemma 4.10. For δ ∈ (0, 1) and r &gt; 0, if n &gt; c log (L/δ) then

∥Γn (θ; {Zk}) − EΓ (θ; (Y, Xs))∥ ≤ C (ω, η)

σ2

�

log(L/δ)

n

∥θ − θ∗∥ for θ ∈ Br(θ∗)

(69)

32


with probability at least 1 − δ, where

C (ω, η) = C1 (η (1 + η) (2 + ω) + 1) η (1 + η) (1 + ω)

+ C2 (η (1 + η) (2 + ω) + 1) (1 + ω) η2

+ C3

�

(1 + ω) η2 + 1

�

(2 + ω) η2

= O

�

(1 + ω)2 (1 + η)4�

.

Proof. See Section C.2.2.

Lemma 4.11. For δ ∈ (0, 1) and r &gt; 0, if n &gt; c log (L/δ), then

��Vn

�θ′|θ; {Zk}

� − EV (θ′|θ; (Y, Xs))

�� ≤ C

σ2

�

log(L/δ)

n

��θ′ − θ∗��2 for θ′, θ ∈ Br(θ∗)

(70)

with probability at least 1 − δ.

Proof. See Section C.2.3.

Lemma 4.12. For δ ∈ (0, 1) and n &gt; c log (L/δ), then

∥En ({Zk})∥ ≤ C (1 + η)

σ

�

log(L/δ)

n

(71)

with probability at least 1 − δ.

Proof. See Section C.2.4.

From (69), (70) and (71) in above lemmas, it is clear that the Assumptions (A1∼A3) are

satisﬁed with the following

ε1(δ, r, n, p) = C (ω, η)

σ2

�

log(L/δ)

n

,

ε2(δ, r, R, n, p) = C1

σ2

�

log(L/δ)

n

and

εs(δ, r, R, n, p) = C2

(1 + η)

σ

�

log(L/δ)

n

.

We obtain the following data-adaptive empirical convergence result for the Linear Regression

with Missing Covariates model.

Corollary 4.6. Suppose {(Yk, (Xk)sk)}n

k=1 is a set of i.i.d. copies of (Y, Xs) and δ ∈ (0, 1). If

θ∗ ̸= 0,

1

√1 + ω &lt; η &lt;

1

3 (1 + ω)

4√ϵ,

(72)

and the sample size n is suﬃciently large such that

n &gt;

�

C1 + C2

1 + η

ωη

+ C (ω, η)

�2

log(L/δ).

(73)

33


Then given an initial point Θ0

n ∈ Br(θ∗), with probability at least 1−δ, the empirical EM sequence

{Θt

n}t≥0 such that

Θt+1

n

∈ arg max

Θ′∈Ω Qn(Θ′|Θt

n; {(Yk, (Xk)sk)}) for t ≥ 0,

satisﬁes the inequality

���Θt

n − θ∗��� ≤

�

Kn

�t ���Θ0

n − θ∗��� +

C2K

σ2 (νn − γn)

�

log(L/δ)

n

(74)

where

γn := γ (ω, η) + C (ω, η)

σ2

�

log(L/δ)

n

and νn := ν (ω, η) − C1

σ2

�

log(L/δ)

n

and the optimal empirical convergence rate Kn satisﬁes that

Kn ≤ γn

νn

&lt; 1 and

���Kn − κ

��� ≤ (C (ω, η) + C1κ (ω, η))

�

log(L/δ)

n

.

Proof. Note condition (73) implies the lower bounds of n in Lemma 4.10, Lemma 4.11 and Lemma

4.12. Hence to apply the empirical convergence theorem, we only need to verify that

εs(δ, r, R, n, p) + rε1(δ, r, n, p) + rε2(δ, r, R, n, p) &lt; r (ν (ω, η) − γ (ω, η))

holds true whenever n satisﬁes (73), but this is trivial.

Moreover, it is not diﬃcult to see that ν (ω, η) &gt;

1

3σ2 under the condition (72), and condition

(73) implies that ε2(δ, r, R, n, p) = C1

σ2

�

log(L/δ)

n

&lt;

1

6σ2 &lt; 1

2ν (ω, η) ≤ 1

2ν, hence the concentration

bound of Kn follows from the optimal rate convergence theorem.

5

Discussion

In this paper, we have proved that for given radii of contraction 0 &lt; r ≤ R, the oracle EM

sequence {θt}t≥0 converges geometrically to the true population parameter θ∗ at the optimal rate

κ with respect to r ≤ R. This is a deterministic result.

As illustrated in Section 4, we can often obtain some upper bounds κ for the optimal rate

in concrete models. Although we may not be able to calculate κ in closed form, the oracle EM

sequence is smart enough to converge optimally.

Similar remarks apply to the empirical convergence, where we showed that given oracle con-

vergence with respect to radii of contraction r ≤ R, an empirical EM sequence converges geomet-

rically at the rate kn as a realization of the optimal empirical convergence rate Kn, which is a

random variable upper bounded by κn and concentrated on κ, see Figure 2. This is a probabilistic

result.

The concentration inequality (34) is how we ﬁnd a reconciliation of our theory with the

classical theories on the asymptotic convergence rate of the EM algorithm, i.e. when the sample

size n is suﬃciently large, the random ﬂuctuations of Kn are so small that it behaves almost like

the constant κ.

34


The idea of considering an MLE as a maximizer of a realization of the empirical log-likelihood

functional of i.i.d. random variables and the EM algorithm as a realization of an iterative process

for approximating the true population parameter θ∗ can be further applied in optimization prob-

lems involving iterative procedures, in which the data generative model is probabilistic. In such

a scenario, by exploiting the oracle deterministic convergence results and the concentration of

measure phenomena of random variables, it is foreseeable that one can obtain similar convergence

results as in this paper.

Acknowledgments

This work was partially supported by grant NO. 61501389 from National Natural Science Founda-

tion of China (NSFC), grants HKBU-22302815 and HKBU-12316116 from Hong Kong Research

Grant Council, and grant FRG2/15-16/011 from Hong Kong Baptist University.

A

Proofs for the Gaussian Mixture Model

We give proofs for the Gaussian Mixture model in this section.

A.1

Preliminaries

We ﬁrst prove the sub-gaussianity of the random vector Y deﬁned in the model.

Lemma A.1. Let W ∼ N(0, σ2Ip) and Y = Z ·θ∗+W be deﬁned in the Gaussian Mixture model,

then for any u ∈ Rp the random variable u⊺Y is sub-gaussian with Orlicz norm ∥u⊺Y ∥ψ2 ≤ K ∥u∥.

Proof. It is clear that u⊺W is a zero-mean Gaussian random variable with variance

Var (u⊺W) =

p

�

j=1

���uj���

2 σ2 = ∥u∥2 σ2.

Hence ∥u⊺W∥ψ2 ≤ σ ∥u∥ and since ∥u⊺Y ∥ψ2 ≤ ∥Z · u⊺θ∗∥ψ2 + ∥u⊺W∥ψ2 ≤ ∥θ∗∥ ∥u∥ + σ ∥u∥, the

lemma follows.

A.2

Proof of Lemma 4.2

Proof. By the Mean Value Theorem, we have

wθ(Y ) − wθ∗(Y ) = ς

�2θ⊺Y

σ2

�

− ς

�2θ∗⊺Y

σ2

�

= 2

σ2 ς′

�2ϑ⊺Y

σ2

�

Y ⊺ (θ − θ∗)

where ϑ is a point on the line segment joining θ and θ∗. In view of (35), for any u ∈ Sp−1,

|u⊺Γ(θ; Y )| = 2

σ2

����ς′

�2ϑ⊺Y

σ2

�

(θ − θ∗)⊺ Y Y ⊺u

����

(a)

≤

1

2σ2 |(θ − θ∗)⊺ Y · u⊺Y | ,

35


where (a) follows from the fact that ς′(t) = ς(t)(1 − ς(t)) ≤ 1

4 for t ∈ R. Then by Lemma A.1

and Lemma E.6(b), u⊺Γ(θ; Y ) is sub-exponential with Orlicz norm

∥u⊺Γ(θ; Y )∥ψ1 ≤

1

2σ2 ∥(θ − θ∗)⊺ Y · u⊺Y ∥ψ1

≤ C

2σ2 ∥(θ − θ∗)⊺ Y ∥ψ2 · ∥u⊺Y ∥ψ2

≤ CK2

2σ2 ∥θ − θ∗∥

and the result follows from Lemma E.8 on the concentration of sub-exponential random vectors.

A.3

Proof of Lemma 4.3

Proof. Let A =

1

σ2 (2wθ∗(Y ) − 1)Y , then simple calculation yields that Eθ∗A =

1

σ2 θ∗. Hence

E(Y ) = A − Eθ∗A is the centered random vector and En({Yk}) is the empirical mean of E(Y ).

Since 0 &lt; wθ∗(Y ) &lt; 1, for u ∈ Sp−1, we have

|u⊺A| =

����

1

σ2 (2wθ∗(Y ) − 1)u⊺Y

���� ≤ 1

σ2 |u⊺Y | .

Hence u⊺A is sub-gaussian with ∥u⊺A∥ψ2 ≤ K

σ2 by Lemma A.1, and the result follows from Lemma

E.7 on the concentration of sub-gaussian random vectors.

B

Proofs for Mixture of Linear Regressions

We give proofs for the Mixture of Linear Regressions model in this section.

B.1

Preliminaries

We ﬁrst prove some properties of the random variates deﬁned in the model.

Lemma B.1. Let X ∼ N(0, Ip) and Y = Z · X⊺θ∗ + W be deﬁned in the Mixture of Linear

Regressions model, then

(a)

u⊺X is Gaussian with Orlicz norm ∥u⊺X∥ψ2 ≤ ∥u∥ for u ∈ Rp;

(b)

∥X∥ is sub-gaussian with Orlicz norm at most √2p;

(c)

Y is sub-gaussian with Orlicz norm ∥Y ∥ψ2 ≤ K where K := ∥θ∗∥ + σ.

Proof. (a) It is easy to see that Var (u⊺X) = Var

��p

j=1 ujXj�

= �p

j=1

��uj��2 = ∥u∥2 and the

result follows. (b) Denote A = ∥X∥, then we have

∥A∥2

ψ2 ≤

���A2���

ψ1 =

������

p

�

j=1

�

Xj�2

������

ψ1

≤

p

�

j=1

����

�

Xj�2����

ψ1

≤

p

�

j=1

2

���Xj���

2

ψ2 ≤ 2p.

(c) By deﬁnition, ∥Y ∥ψ2 ≤ ∥Z · X⊺θ∗∥ψ2 +∥W∥ψ2 = ∥X⊺θ∗∥ψ2 +σ and since X⊺θ∗ is a zero-mean

Gaussian with variance Var (X⊺θ∗) = ∥θ∗∥2, the result follows.

36


B.2

Proof of Lemma 4.5

Proof. By the Mean Value Theorem, we have

wθ(Y, X) − wθ∗(Y, X) = ς

�2θ⊺XY

σ2

�

− ς

�2θ∗⊺XY

σ2

�

= 2

σ2 ς′

�2ϑ⊺XY

σ2

�

(θ − θ∗)⊺ XY,

where ϑ is a point on the line segment joining θ and θ∗. Then in view of (46), we have for any

u ∈ Sp−1,

|u⊺Γ(θ; (Y, X))| = 2

σ2 |wθ(Y, X) − wθ∗(Y, X)| · |u⊺XY |

(a)

≤

1

2σ2 ∥θ − θ∗∥ · ∥Y X∥2 ,

(75)

where (a) follows from the Cauchy-Schwartz inequality and the fact that ς′(t) = ς(t)(1−ς(t)) ≤ 1

4

for t ∈ R.

Deﬁne the random vector A = 2σ2Γ(θ;(Y,X))

∥θ−θ∗∥

for θ ∈ B×

r (θ∗); let Ak be the i.i.d. copy of A

corresponding to (Yk, Xk) and let Bn = 1

n

�n

k=1 Ak − Eθ∗A.

In view of (75), for u ∈ Sp−1 and t &gt; 0, we have

Pr {|u⊺A| ≥ t} ≤ Pr

�

∥Y X∥2 ≥ t

�

= Pr

�

∥Y X∥ ≥

√

t

�

≤ C exp

�

−ct

1

2

�

,

since ∥Y X∥ is sub-exponential with Orlicz norm at most CK√2p by Lemma B.1. It follows from

Proposition 2.1.9 and its extensions in [36] that

Pr {|u⊺Bn| ≥ t} ≤ C exp

�

−ctn

1

2 −ϵ�

,

where 0 &lt; ϵ ≪ 1

2 is a small constant. Then by discretization of norm, for a 1

2-net {ui}L

i=1 of Sp−1,

∥Bn∥ ≤ 2 max

1≤i≤L u⊺

i Bn,

and by the union bound and pigeonhole principle, we have

Pr {∥Bn∥ ≥ t} ≤

L

�

i=1

Pr

�

|u⊺

i Bn| ≥ t

2

�

≤ CL exp

�

−1

2ctn

1

2 −ϵ

�

.

Then by equating the right hand side to δ and solving for t, we obtain

∥Γn(θ; {(Yk, Xk)}) − Eθ∗Γ(θ; (Y, X))∥ ≤ C log(L/δ)

σ2n

1

2 −ϵ ∥θ − θ∗∥

for θ ∈ Br(θ∗) with probability at least 1 − δ.

B.3

Proof of Lemma 4.6

Proof. In view of (47), we have

V (θ′|θ; {(Y, X)}) = − 1

2σ2

��θ′ − θ∗�⊺ X

�2 .

37


By Lemma B.1, the random variable (θ′ − θ∗)⊺ X is Gaussian with Orlicz norm ∥(θ′ − θ∗)⊺ X∥ψ2 ≤

∥θ′ − θ∗∥ and hence V (θ; {(Y, X)}) is sub-exponential with Orlicz norm

��V (θ′|θ; {(Y, X)})

��

ψ1 ≤ C

σ2

��θ′ − θ∗��2 .

The result follows from Lemma E.8 on concentration of sub-exponential random variables.

B.4

Proof of Lemma 4.7

Proof. In view of (48), for u ∈ Sp−1, we have

|u⊺E(Y, X)| ≤ 1

σ2 (|u⊺X| |Y | + |X⊺θ∗| |X⊺u|) ,

since |2wθ∗(Y, X) − 1| &lt; 1. Then by Lemma B.1, u⊺E(Y, X) is sub-exponential with Orlicz norm

∥u⊺E(Y, X)∥ψ1 ≤ C

σ2 (K + ∥θ∗∥) = C

σ (1 + 2η) .

The result follows from Lemma E.8 on concentration of sub-exponential random variables.

C

Proofs for Linear Regression with Missing Covariates

We give proofs for the Linear Regression with Missing Covariates model in this section.

C.1

Proofs for Oracle Convergence

We prove lemmas for oracle convergence of the model, and we start with some basic facts.

C.1.1

Preliminaries

We denote the expectation with respect to the random vector τ and its measurable functions by

Eϵ [·]. It is easy to see that Eϵ [τ] = ϵ1, Eϵ [s] = (1−ϵ)1 and more generally, we have the following

lemma.

Lemma C.1. Eϵ [xτ] = ϵx, Eϵ

�

∥xτ∥2�

≤ ϵ ∥x∥2 and Eϵ

�

|x⊺

τyτ|2�

≤ ϵ |x⊺y|2 for x, y ∈ Rp.

Proof. These results follow from simple calculations.

Now for a ﬁxed missing pattern τ ∈ {◦, 1}p and s = 1 − τ, taking expectation with respect to

(Y, Xs) and by some calculation of multivariate Gaussian distribution, we have

Eθ∗ [Y µθ(Y, Xs)] = θ∗

s + σ2 + ∥θ∗

τ∥2 + θ∗⊺

s (θ∗

s − θs)

σ2 + ∥θτ∥2

θτ

and

38


Eθ∗ [µθ(Y, Xs)µθ(Y, Xs)⊺] = diag{s}

+

1

σ2 + ∥θτ∥2 (θτ(θ∗

s − θs)⊺ + (θ∗

s − θs)θ⊺

τ)

+ σ2 + ∥θ∗

τ∥2 + ∥θ∗

s − θs∥2

�

σ2 + ∥θτ∥2�2

θτθ⊺

τ,

and it follows that

Eθ∗Σθ(Y, Xs) = Ip

+

1

σ2 + ∥θτ∥2 (θτ(θ∗

s − θs)⊺ + (θ∗

s − θs)θ⊺

τ)

+ ∥θ∗

τ∥2 − ∥θτ∥2 + ∥θ∗

s − θs∥2

�

σ2 + ∥θτ∥2�2

θτθ⊺

τ.

(76)

Since Eθ∗ [Y µθ∗(Y, Xs)] = θ∗ and Eθ∗Σθ∗(Y, Xs) = Ip, we have

σ2 · Eθ∗Γ(θ; (Y, Xs)) = Eθ∗ [Y µθ(Y, Xs) − Σθ(Y, Xs)θ∗]

= θτ − θ∗

τ +

θ⊺

τθ∗

τ

σ2 + ∥θτ∥2 (θs − θ∗

s) +

ζ · θτ

�

σ2 + ∥θτ∥2�2

(77)

where

ζ :=

�

σ2 + ∥θτ∥2 − θ⊺

τθ∗

τ

� �

∥θ∗

τ∥2 − ∥θτ∥2�

− θ⊺

τθ∗

τ ∥θ∗

s − θs∥2 .

(78)

Further, we have the following bound for ζ.

Lemma C.2. |ζ| ≤

�

σ2 + ∥θτ∥2�

∥θτ − θ∗

τ∥2 + 2σ2 ∥θτ∥ ∥θτ − θ∗

τ∥ + ∥θ∗

τ∥ ∥θτ∥ ∥θ − θ∗∥2.

Proof. By the triangle inequality and Cauchy-Schwartz inequality, we have

|ζ| ≤

�

σ2 + ∥θτ∥ ∥θτ − θ∗

τ∥

�

(∥θ∗

τ∥ + ∥θτ∥) ∥θτ − θ∗

τ∥ + ∥θτ∥ ∥θ∗

τ∥ ∥θ∗

s − θs∥2

= σ2 ∥θτ − θ∗

τ∥ (∥θτ∥ + ∥θ∗

τ∥) + ∥θτ∥2 ∥θτ − θ∗

τ∥2 + ∥θτ∥ ∥θ∗

τ∥ ∥θ∗ − θ∥2

≤ σ2 ∥θτ − θ∗

τ∥ (2 ∥θτ∥ + ∥θ∗

τ − θτ∥) + ∥θτ∥2 ∥θτ − θ∗

τ∥2 + ∥θτ∥ ∥θ∗

τ∥ ∥θ∗ − θ∥2

=

�

σ2 + ∥θτ∥2�

∥θτ − θ∗

τ∥2 + 2σ2 ∥θτ∥ ∥θτ − θ∗

τ∥ + ∥θ∗

τ∥ ∥θτ∥ ∥θ − θ∗∥2 ,

and the result follows.

C.1.2

Proof of Lemma 4.8

Proof. In view of (77) and Lemma C.1, we have

σ2 · EΓ(θ; (Y, Xs)) = σ2 · Eϵ [Eθ∗Γ(θ; (Y, Xs))] = ϵ (θ − θ∗) + T1 + T2,

where

T1 := Eϵ

�

θ⊺

τθ∗

τ

σ2 + ∥θτ∥2 (θs − θ∗

s)

�

and

T2 := Eϵ





ζ · θτ

�

σ2 + ∥θτ∥2�2



 ,

39


and we can bound T1 as

∥T1∥ ≤ 1

σ2 Eϵ [|θ⊺

τθ∗

τ| ∥θs − θ∗

s∥] ≤ 1

σ2 Eϵ

�

|θ⊺

τθ∗

τ|2� 1

2 Eϵ

�

∥θs − θ∗

s∥2� 1

2

≤ 1

σ2

�

ϵ (1 − ϵ) |θ⊺θ∗| ∥θ − θ∗∥ ≤ (1 + ω) η2�

ϵ (1 − ϵ) ∥θ − θ∗∥ ,

(79)

and for T2 we have

∥T2∥ ≤ Eϵ





|ζ| ∥θτ∥

�

σ2 + ∥θτ∥2�2



 ≤ S1 + S2 + S3 where S1 := Eϵ

�

∥θτ − θ∗

τ∥2 ∥θτ∥

σ2 + ∥θτ∥2

�

S2 := Eϵ



2σ2 ∥θτ∥2 ∥θτ − θ∗

τ∥

�

σ2 + ∥θτ∥2�2



 ,

S3 := Eϵ



∥θ∗

τ∥ ∥θτ∥2 ∥θ − θ∗∥2

�

σ2 + ∥θτ∥2�2





and in view of Lemma C.1 and Cauchy-Schwartz inequality, we have

S1 ≤ 1

σ2 Eϵ [∥θτ − θ∗

τ∥ ∥θτ∥] ∥θ − θ∗∥ ≤ ϵ

σ2 ∥θ∥ ∥θ − θ∗∥2 ≤ ω (1 + ω) η2ϵ ∥θ − θ∗∥ ,

S2 ≤ 2

σ2 Eϵ [∥θτ − θ∗

τ∥ ∥θτ∥] ∥θ∥ ≤ 2ϵ

σ2 ∥θ∥2 ∥θ − θ∗∥ ≤ 2 (1 + ω)2 η2ϵ ∥θ − θ∗∥ and

S3 ≤ 1

σ4 Eϵ [∥θ∗

τ∥ ∥θτ∥] ∥θ∥ ∥θ − θ∗∥2 ≤ ϵ

σ4 ∥θ∗∥ ∥θ∥2 ∥θ − θ∗∥2 ≤ ω (1 + ω)2 η4ϵ ∥θ − θ∗∥ .

Hence we have

∥T2∥ ≤

�

ω + 2 (1 + ω) + ω (1 + ω) η2�

(1 + ω) η2ϵ ∥θ − θ∗∥ ,

(80)

and therefore for θ ∈ Br (θ∗), we have

∥EΓ(θ; (Y, Xs))∥ ≤ 1

σ2 (ϵ ∥θ − θ∗∥ + ∥T1∥ + ∥T2∥) ≤ γ (ω, η) ∥θ − θ∗∥

where by (79) and (80),

γ (ω, η) = 1

σ2

�

ϵ

�

ωξ2 + (3ω + 2) ξ + 1

�

+ ξ

�

ϵ (1 − ϵ)

�

,

and the result follows.

C.1.3

Proof of Lemma 4.9

Proof. In view of (63), we have

EV (θ′|θ; (Y, Xs)) = − 1

2σ2 (θ′ − θ∗)⊺EΣθ(Y, Xs)(θ′ − θ∗)

and by (76), it follows that

EΣθ(Y, Xs) = Eϵ [Eθ∗Σθ(Y, Xs)] = Ip + Σ1 + Σ2 − Σ3,

40


where

Σ1 := Eϵ

�

1

σ2 + ∥θτ∥2 (θτ(θ∗

s − θs)⊺ + (θ∗

s − θs)θ⊺

τ)

�

,

Σ2 := Eϵ



σ2 + ∥θ∗

τ∥2 + ∥θ∗

s − θs∥2

�

σ2 + ∥θτ∥2�2

θτθ⊺

τ



 and

Σ3 := Eϵ

�

1

σ2 + ∥θτ∥2 θτθ⊺

τ

�

.

For u ∈ Rp and θ ∈ Br (θ∗), by Lemma C.1 and Cauchy-Schwartz inequality, we have

|u⊺Σ1u| ≤ 2

σ2 Eϵ [|u⊺

τθτ| |(θ∗

s − θs)⊺ us|] ≤ 2

σ2 Eϵ

�

|u⊺

τθτ|2� 1

2 Eϵ

�

|(θ∗

s − θs)⊺ us|2� 1

2

≤ 2

σ2

�

ϵ (1 − ϵ) |u⊺θ| |(θ∗ − θ)⊺ u| ≤ 2ω (1 + ω) η2�

ϵ (1 − ϵ) ∥u∥2 ,

|u⊺Σ3u| ≤ 1

σ2 Eϵ

�

|u⊺

τθτ|2�

≤ ϵ

σ2 |u⊺θ|2 ≤ (1 + ω)2 η2ϵ ∥u∥2 ,

and u⊺Σ2u ≥ 0, since Σ2 is positive semi-deﬁnite. Then

u⊺EΣθ(Y, Xs)u = u⊺ (Ip + Σ1 + Σ2 − Σ3) u

≥

�

1 − 2ω (1 + ω) η2�

ϵ (1 − ϵ) − (1 + ω)2 η2ϵ

�

∥u∥2 ,

and it follows that

EV (θ′|θ; (Y, Xs)) ≤ −ν (ω, η)

��θ′ − θ∗��2 for θ ∈ Br (θ∗) and θ′ ∈ Rp,

where

ν (ω, η) =

1

2σ2

�

1 − 2ω (1 + ω) η2�

ϵ (1 − ϵ) − (1 + ω)2 η2ϵ

�

,

and the lemma is proved.

C.2

Proofs for Empirical Convergence

We prove lemmas for empirical convergence of the model, we begin with some basic facts.

C.2.1

Preliminaries

To ease notations, we omit the dependence of (Y, Xs) in µθ, bθ and τ in Aθ etc. in this section.

For τ ∈ {◦, 1}p, denote [τ] :=

�j ∈ N | τ j = 1

�. We ﬁrst prove some technical results for related

random variables. Recall in this model, Y = ⟨θ∗, X⟩+W with X ∼ N (0, Ip) and W ∼ N

�0, σ2�.

Lemma C.3. For u ∈ Rp, missing pattern τ ∈ {◦, 1}p and s = 1 − τ, the random variable u⊺

sXs

is Gaussian with Orlicz norm ∥u⊺

sXs∥ψ2 = ∥us∥ and Y is Gaussian with Orlicz norm ∥Y ∥ψ2 ≤

�

∥θ∗∥2 + σ2, while Y − u⊺

sXs is sub-gaussian with Orlicz norm ∥Y − u⊺

sXs∥ψ2 ≤ ∥uτ∥ + σ.

41


Proof. By rotation invariance of Gaussian variables, we have

Var (u⊺

sXs) = Var



 �

j∈[s]

ujXj



 =

�

j∈[s]

Var

�

ujXj�

=

�

j∈[s]

���uj���

2 = ∥us∥2 ,

hence ∥u⊺

sXs∥ψ2 = ∥us∥. Since Y = θ∗⊺X + W, it is clearly Gaussian and

Var (Y ) = Var (θ∗⊺X) + Var (W) = ∥θ∗∥2 + σ2,

hence ∥Y ∥ψ2 =

�

∥θ∗∥2 + σ2. Now since Y − θ∗⊺

s Xs = θ∗⊺

τ Xτ + W, then

∥Y − u⊺

sXs∥ψ2 ≤ ∥u⊺

τXτ∥ψ2 + ∥W∥ψ2 = ∥uτ∥ + σ.

Lemma C.4. For u ∈ Rp, missing pattern τ ∈ {◦, 1}p and s = 1 − τ, the random variable u⊺µθ

is sub-gaussian with Orlicz norm ∥u⊺µθ∥ψ2 ≤

√

3 ∥u∥ and u⊺µθY is sub-exponential with Orlicz

norm ∥u⊺µθY ∥ψ1 ≤ CK ∥u∥ where K := ∥θ∗∥ + σ.

Proof. It follows from Lemma C.3 that u⊺bθ =

u⊺

τθτ

σ2+∥θτ∥2 (Y − θ⊺

sXs) is sub-gaussian with Orlicz

norm ∥u⊺bθ∥ψ2 ≤

∥θτ∥+σ

σ2+∥θτ∥2 |u⊺

τθτ| and hence u⊺µθ = u⊺ (Xs + bθ) is sub-gaussian with Orlicz norm

∥u⊺µθ∥ψ2 ≤ ∥us∥ + ∥θτ∥ + σ

σ2 + ∥θτ∥2 |u⊺

τθτ|

≤

1

σ2 + ∥θτ∥2

��

σ2 + ∥θτ∥2�

∥us∥ +

�

∥θτ∥2 + σ ∥θτ∥

�

∥uτ∥

�

≤

1

σ2 + ∥θτ∥2

��

σ2 + ∥θτ∥2�2 + ∥θτ∥2 (∥θτ∥ + σ)2

� 1

2 �

∥us∥2 + ∥uτ∥2� 1

2

≤

1

σ2 + ∥θτ∥2

��

σ2 + ∥θτ∥2� �

σ2 + 3 ∥θτ∥2�� 1

2 ∥u∥ ≤

√

3 ∥u∥ .

(81)

Hence u⊺µθY is sub-exponential with Orlicz norm

∥u⊺µθY ∥ψ1 ≤ C1 ∥u⊺µθ∥ψ2 ∥Y ∥ψ2 ≤

√

3C1 ∥u∥

�

∥θ∗∥2 + σ2 ≤ CK ∥u∥ ,

(82)

where K := ∥θ∗∥ + σ, since

�

K

√

2 ≤

� �

∥θ∗∥2 + σ2 ≤ K.

Lemma C.5. For u, v ∈ Rp, missing pattern τ ∈ {◦, 1}p and s = 1 − τ, the random variable

u⊺Aθv is bounded hence sub-gaussian with Orlicz norm ∥u⊺Aθv∥ψ2 ≤ 2 ∥u∥ ∥v∥ and u⊺Σθv is

sub-exponential with Orlicz norm ∥u⊺Σθv∥ψ1 ≤ C ∥u∥ ∥v∥.

Proof. In view of deﬁnition (59), we have

|u⊺Aθv| ≤ |u⊺

τvτ| + |u⊺

τθτ| |v⊺

τθτ|

σ2 + ∥θτ∥2 ≤

�

1 +

∥θτ∥2

σ2 + ∥θτ∥2

�

∥u∥ ∥v∥ &lt; 2 ∥u∥ ∥v∥ ,

and since u⊺Σθv = (u⊺µθ) (v⊺µθ) + u⊺Aθv, the result follows from Lemma C.4.

42


Lemma C.6. For u ∈ Rp, missing pattern τ ∈ {◦, 1}p and s = 1 − τ, the random variable

u⊺ (µθ − µθ∗) is sub-gaussian with Orlicz norm

∥u⊺ (µθ − µθ∗)∥ψ2 ≤ (1 + ω) η

σ

(η (1 + η) (2 + ω) + 1) ∥θ − θ∗∥ ∥u∥ ,

and u⊺ (µθ − µθ∗) Y is sub-exponential with Orlicz norm

∥u⊺ (µθ − µθ∗) Y ∥ψ1 ≤ C (η (1 + η) (2 + ω) + 1) η (1 + η) (1 + ω) ∥θ − θ∗∥ ∥u∥ .

Proof. In view of deﬁnition (60), we have

µθ − µθ∗ =

�

Y − θ⊺

sXs

σ2 + ∥θτ∥2 − Y − θ∗⊺

s Xs

σ2 + ∥θ∗τ∥2

�

θτ + Y − θ∗⊺

s Xs

σ2 + ∥θ∗τ∥2 (θτ − θ∗

τ) .

Note the ﬁrst summand can be rewritten as

1

σ2 + ∥θτ∥2

�

Y − θ∗⊺

s Xs

σ2 + ∥θ∗τ∥2

�

∥θ∗

τ∥2 − ∥θτ∥2�

+ (θs − θ∗

s)⊺ Xs

�

θτ,

and it follows that,

|u⊺ (µθ − µθ∗)| ≤ ∥θτ∥

σ2

� 1

σ2 (∥θ∗

τ∥ + ∥θτ∥) |Y − θ∗⊺

s Xs| ∥θ∗

τ − θτ∥ + |(θs − θ∗

s)⊺ Xs|

�

∥u∥

≤ (1 + ω) η

σ2

((2 + ω) η |Y − θ∗⊺

s Xs| ∥θ∗

τ − θτ∥ + σ |(θs − θ∗

s)⊺ Xs|) ∥u∥ .

Hence in view of Lemma C.3, u⊺ (µθ − µθ∗) is sub-gaussian with Orlicz norm

∥u⊺ (µθ − µθ∗)∥ψ2 ≤ (1 + ω) η

σ2

((2 + ω) η (∥θ∗

τ∥ + σ) ∥θ∗

τ − θτ∥ + σ ∥θs − θ∗

s∥) ∥u∥

≤ (1 + ω) η

σ

(η (1 + η) (2 + ω) + 1) ∥θ − θ∗∥ ∥u∥ .

Therefore u⊺ (µθ − µθ∗) Y is sub-exponential with Orlicz norm

∥u⊺ (µθ − µθ∗) Y ∥ψ1 ≤ C ∥u⊺ (µθ − µθ∗)∥ψ2 ∥Y ∥ψ2

≤ C (η (1 + η) (2 + ω) + 1) η (1 + η) (1 + ω) ∥θ − θ∗∥ ∥u∥ .

Lemma C.7. For u, v ∈ Rp, missing pattern τ ∈ {◦, 1}p and s = 1 − τ, the random variable

u⊺ (Aθ − Aθ∗) v is bounded hence sub-gaussian with Orlicz norm

∥u⊺ (Aθ − Aθ∗) v∥ψ2 ≤ 1

σ

�

(1 + ω) η2 + 1

�

(2 + ω) η ∥θ∗ − θ∥ ∥u∥ ∥v∥ ,

and u⊺ �µθµ⊺

θ − µθ∗µ⊺

θ∗

� v is sub-exponential with Orlicz norm

��u⊺ �µθµ⊺

θ − µθ∗µ⊺

θ∗

� v

��

ψ1 ≤

√

3C (1 + ω) η

σ

(η (1 + η) (2 + ω) + 1) ∥θ − θ∗∥ ∥u∥ ∥v∥ .

43


Proof. Since we can write

Aθ − Aθ∗ =

1

σ2 + ∥θ∗τ∥2 θ∗

τθ∗⊺

τ −

1

σ2 + ∥θτ∥2 θτθ⊺

τ

=

∥θτ∥2 − ∥θ∗

τ∥2

�

σ2 + ∥θ∗τ∥2� �

σ2 + ∥θτ∥2�θ∗

τθ∗⊺

τ +

1

σ2 + ∥θτ∥2 (θ∗

τθ∗⊺

τ − θτθ⊺

τ)

and

θ∗

τθ∗⊺

τ − θτθ⊺

τ = (θ∗

τ − θτ) θ∗⊺

τ + θτ (θ∗

τ − θτ)⊺ ,

it follows that

|u⊺ (θ∗

τθ∗⊺

τ − θτθ⊺

τ) v| ≤ (∥θ∗

τ∥ + ∥θτ∥) ∥θ∗

τ − θτ∥ ∥u∥ ∥v∥ ,

and hence

|u⊺ (Aθ − Aθ∗) v| ≤

�

∥θ∗

τ∥ ∥θτ∥

σ2 + ∥θ∗τ∥2 + 1

�

∥θ∗

τ∥ + ∥θτ∥

σ2 + ∥θτ∥2 ∥θ∗

τ − θτ∥ ∥u∥ ∥v∥

≤ 1

σ

�

(1 + ω) η2 + 1

�

(2 + ω) η ∥θ∗

τ − θτ∥ ∥u∥ ∥v∥ .

Similarly, since

µθµ⊺

θ − µθ∗µ⊺

θ∗ = (µθ − µθ∗) µ⊺

θ + µθ∗ (µθ − µθ∗)⊺

and in view of Lemma C.4 and Lemma C.6, u⊺ �µθµ⊺

θ − µθ∗µ⊺

θ∗

� v is sub-exponential with Orlicz

norm

��u⊺ �µθµ⊺

θ − µθ∗µ⊺

θ∗

� v

��

ψ1 ≤

��u⊺ (µθ − µθ∗) µ⊺

θv

��

ψ1 + ∥u⊺µθ∗ (µθ − µθ∗)⊺ v∥ψ1

≤ C

�

∥u⊺ (µθ − µθ∗)∥ψ2 ∥v⊺µθ∥ψ2 + ∥u⊺µθ∥ψ2 ∥v⊺ (µθ − µθ∗)∥ψ2

�

≤

√

3C (1 + ω) η

σ

(η (1 + η) (2 + ω) + 1) ∥θ − θ∗∥ ∥u∥ ∥v∥ .

C.2.2

Proof of Lemma 4.10

Proof. By (62), for u ∈ Sp−1, we have

u⊺Γ(θ; (Y, Xs)) = 1

σ2 [u⊺ (µθ − µθ∗) Y − u⊺ (Σθ − Σθ∗) θ∗] = 1

σ2 [D1 − D2 − D3] ,

where D1 := u⊺ (µθ − µθ∗) Y , D2 := u⊺ �µθµ⊺

θ − µθ∗µ⊺

θ∗

� θ∗ and D3 := u⊺ (Aθ − Aθ∗) θ∗, and it

follows from Lemma C.6 and Lemma C.7 that D1 and D2 are sub-exponential with Orlicz norms

∥D1∥ψ1 ≤ C1 (η (1 + η) (2 + ω) + 1) η (1 + η) (1 + ω) ∥θ − θ∗∥ and

∥D2∥ψ1 ≤ C2 (η (1 + η) (2 + ω) + 1) (1 + ω) η2 ∥θ − θ∗∥ ,

while D3, which is independent of D1 and D2, is sub-gaussian with Orlicz norm

∥D3∥ψ2 ≤

�

(1 + ω) η2 + 1

�

(2 + ω) η2 ∥θ∗ − θ∥ .

44


It follows that u⊺Γ(θ; (Y, Xs)) is sub-exponential with Orlicz norm

∥u⊺Γ(θ; (Y, Xs))∥ψ1 ≤ C(η, ω)

σ2

∥θ∗ − θ∥ ,

where C(η, ω) = O

�

(1 + ω)2 (1 + η)4�

. Now the result follows from Lemma E.8 on the concen-

tration of sub-exponential random vectors.

C.2.3

Proof of Lemma 4.11

Proof. In view of (63) and by Lemma C.5, V (θ′|θ; (Y, Xs)) is sub-exponential with Orlicz norm

��V (θ′|θ; (Y, Xs))

��

ψ1 ≤ C

σ2

��θ′ − θ∗��2 ,

and the result follows from Lemma E.8 on the concentration of sub-exponential random variables.

C.2.4

Proof of Lemma 4.12

Proof. In view of (64), for u ∈ Sp−1, we have

u⊺E(Y, Xs) = 1

σ2 [u⊺µθ∗Y − u⊺Σθ∗θ∗] .

Then u⊺µθ∗Y is sub-exponential with Orlicz norm ∥u⊺µθ∗Y ∥ψ1 ≤ C1K by Lemma C.4, and

u⊺Σθ∗θ∗ is sub-exponential with Orlicz norm ∥u⊺Σθ∗θ∗∥ψ1 ≤ C2 ∥θ∗∥ by Lemma C.5.

Hence

u⊺E(Y, Xs) is sub-exponential with Orlicz norm

∥u⊺E(Y, Xs)∥ψ1 ≤ 1

σ2 (C1K + C2 ∥θ∗∥) ≤ C

σ (1 + η) ,

and the result follows from Lemma E.8 on the concentration of sub-exponential random vectors.

D

Miscellaneous Results and Proofs

We collect various results and proofs in this section.

D.1

Proof of Proposition 2.1

Proof. We show that L∗ (θ) ≤ L∗ (θ∗) for θ ∈ Ω. By deﬁnition

L∗(θ) =

�

Y

(log pθ(y)) pθ∗(y)dy ≤

�

Y

(log pθ∗(y)) pθ∗(y)dy = L∗(θ∗),

where the inequality follows from a version of the Jensen’s inequality in Lemma E.3.

45


D.2

Proof of Proposition 2.2

Proof. We show that Q∗ (θ′|θ∗) ≤ Q∗ (θ∗|θ∗) for θ′ ∈ Ω. By deﬁnition

Q∗(θ′|θ∗) =

�

Y

��

Z(y)

log (fθ′(y, z)) kθ∗(z|y)dz

�

pθ∗(y)dy

=

�

Y×Z

log (fθ′(y, z)) kθ∗(z|y)pθ∗(y)dzdy

=

�

Y×Z

log (fθ′(y, z)) fθ∗(y, z)dzdy,

then the result follows from a version of the Jensen’s inequality in Lemma E.3.

D.3

An Interpretation of the Convergence Inequality

Lemma D.1. Suppose θ∗ ∈ Rp, κ &lt; 1, ε &gt; 0 and a sequence {θt}T

t=0 such that

��θt − θ∗�� &gt; ε for

0 ≤ t ≤ T. Then it satisﬁes the inequality

���θt − θ∗��� ≤ κt ���θ0 − θ∗��� + ε for 0 ≤ t ≤ T,

if and only if there exists a sequence {ζt}T

t=0 such that ζt ∈ Sp−1

ε

(θ∗) and

���θt − ζt��� ≤ κt ���θ0 − θ∗��� for 0 ≤ t ≤ T,

where Sp−1

ε

(θ∗) := {u ∈ Rp | ∥u − θ∗∥ = ε} is the sphere centered at θ∗ with radius ε.

Proof. Suﬃciency. By triangle inequality

���θt − θ∗��� ≤

���θt − ζt��� +

���ζt − θ∗��� ≤ κt ���θ0 − θ∗��� + ε.

Necessity. Let ζt := (1 − λt)θ∗ + λtθt where λt :=

ε

∥θt−θ∗∥ &lt; 1 for 0 ≤ t ≤ T. Then since

��ζt − θ∗�� = ε and

���θt − ζt��� +

���ζt − θ∗��� =

���θt − θ∗��� ≤ κt ���θ0 − θ∗��� + ε,

the result follows.

Remark. It is clear from the proof that for each t, the point ζt is simply the intersection of the line

segment joining θ∗ and θt with the sphere Sp−1

ε

(θ∗), and hence

��θt − ζt�� is simply the distance

between θt and the ball Bε(θ∗). In the context of an empirical EM sequence, when θt lies outside

the ball Bε(θ∗) of statistical error, it “converges” geometrically onto it at the rate κ.

D.4

A Digression to the Theory of Information Matrices

The Fisher information matrix, the complete and missing information matrices as well as the

convergence rate matrix are classical objects for analyzing the asymptotic convergence of the

EM algorithm [12, 28, 22, 23].

In this section, we brieﬂy formulate and extend the classical

information matrix theory to make connections with our analysis of oracle convergence of the

EM algorithm.

46


On both sides of (5), diﬀerentiating twice with respect to θ′ and taking conditional expectation

of Z given y at parameter θ, we have

I(θ′; y) = Ic(θ′|θ; y) − Im(θ′|θ; y),

where we deﬁne I(θ′; y) := −∇1∇⊺

1L(θ′; y) as the negative of the Hessian matrix of the stochastic

log-likelihood function and deﬁne5

Ic(θ′|θ; y) := −

�

Z(y)

∇∇⊺ (log fθ′(y, z)) kθ(z|y)dz = −∇1∇⊺

1Q(θ′|θ; y),

Im(θ′|θ; y) := −

�

Z(y)

∇∇⊺ (log kθ′(z|y)) kθ(z|y)dz

for θ ∈ Ω, whenever these matrices are well-deﬁned. Note Im(θ|θ; y) is positive semi-deﬁnite for

y ∈ Y by Lemma E.4.

By taking expectations, we deﬁne the observed information matrix

I(θ′) :=

�

Y

I(θ′; y)pθ∗(y)dy = −∇1∇⊺

1L∗(θ′),

(83)

the complete information matrix

Ic(θ′|θ) :=

�

Y

Ic(θ′|θ; y)pθ∗(y)dy = −∇1∇⊺

1Q∗(θ′|θ),

(84)

and the missing information matrix

Im(θ′|θ) :=

�

Y

Im(θ′|θ; y)pθ∗(y)dy.

We obtain the oracle information equation or the missing information principle ([24, 20]) at the

population level

I(θ′) = Ic(θ′|θ) − Im(θ′|θ).

(85)

At the true population parameter θ∗ and by Lemma E.4, we have

I(θ∗) =

�

Y

[∇ log pθ∗(y)] [∇⊺ log pθ∗(y)] pθ∗(y)dy = I(θ∗),

(86)

which is just the Fisher information matrix and is positive semi-deﬁnite. Similarly, we have

Ic(θ∗|θ) = −

�

X

∇∇⊺ (log fθ∗(y, z)) kθ(z|y)pθ∗(y)dydz,

Im(θ∗|θ) = −

�

X

∇∇⊺ (log kθ∗(z|y)) kθ(z|y)pθ∗(y)dzdy

and by Lemma E.4,

Ic(θ∗|θ∗) =

�

X

[∇ log fθ∗(y, z)] [∇⊺ log fθ∗(y, z)] dydz,

5In this section the diﬀerential operators ∇ and ∇∇⊺ are with respect to the parameter θ′.

47


Im(θ∗|θ∗) =

�

X

[∇ log kθ∗(z|y)] [∇⊺ log kθ∗(z|y)] pθ∗(y)dzdy,

which are positive semi-deﬁnite.

In classical analysis of parameter estimation by MLE, we usually require that the Fisher

information matrix of the parametric density be positive deﬁnite at θ∗. A connection of this

condition and our strong concavity condition of the oracle Q-function is made in Proposition 3.1

for which we give the following proof.

Proof of Proposition 3.1. In view of (85), we have

I(θ∗) = I(θ∗) = Ic(θ∗|θ) − Im(θ∗|θ) for θ ∈ Br1(θ∗),

for some r1 &gt; 0. Since Im(θ∗|θ∗) is positive semi-deﬁnite and I(θ∗) is positive deﬁnite by our

assumption, Ic(θ∗|θ) is positive deﬁnite at θ = θ∗ and hence its minimal eigenvalue λmin(θ∗) &gt; 0.

Then by continuity of λmin, there exists 0 &lt; r2 &lt; r1 such that

ν := 1

3 inf

�

λmin(θ) | θ ∈ Br2(θ∗)

�

&gt; 0.

Now since ∇1∇⊺

1Q∗(θ∗|θ) = −Ic(θ∗|θ) by (84), which implies that there exists 0 &lt; r &lt; r2 such

that

Q∗(θ′|θ) − Q∗(θ∗|θ) −

�∇1Q∗(θ∗|θ), θ′ − θ∗� ≤ −ν

��θ′ − θ∗��2

whenever θ′, θ ∈ Br(θ∗) and it follows that ν ∈ V(r, r) ̸= ∅.

D.5

A Note on the Measurability Issue

In the statement of some deﬁnitions and assumptions in this paper, we implicitly used the fact

that certain uncountable operations of a family of measurable functions preserve the measurability

of the resulting function. To be speciﬁc, let T ⊂ Rq be a (possibly uncountable) index set and

(S , E , P) be a probability measure space.

Lemma D.2. If g(y, θ) : R×T → R is a Borel measurable function, then for any random variable

Y on (S , E , P), the supremum Z := supθ∈T g(Y, θ) is an E -measurable function hence a random

variable.

Proof. See Appendix C of [27] for a proof.

Remark. This result can be readily generalized to random vectors Y = (Y1, · · · , Yn) on (S , E , P).

In all our cases, the index set T = Br(θ∗) or T = Br(θ∗) × BR(θ∗), and as a simple consequence,

the sets like

{ϖ | g(Y, θ) ≤ a for θ ∈ T} =

�

θ∈T

{ϖ | g(Y, θ) ≤ a} = {ϖ | Z ≤ a}

are indeed measurable. See [10, 27] for more detailed discussions on this topic.

E

Auxiliaries

We give auxiliary results used throughout the paper in this section.

48


E.1

Supporting Lemmas

Lemma E.1. For real valued functions f and g on a non-empty set X.

(a) If sup f(x) &lt; +∞ and sup g(x) &lt; +∞, then

|sup f(x) − sup g(x)| ≤ sup |f(x) − g(x)| ;

(b) If inf f(x) &gt; −∞ and inf g(x) &gt; −∞, then

|inf f(x) − inf g(x)| ≤ sup |f(x) − g(x)| .

Proof. (a) Since f(x) ≤ g(x) + |f(x) − g(x)|, we have

sup f(x) ≤ sup g(x) + sup |f(x) − g(x)| .

Then since sup g(x) &lt; +∞, subtracting it from both sides yields

sup f(x) − sup g(x) ≤ sup |f(x) − g(x)| .

By exchanging the roles of f and g, we have sup g(x) − sup f(x) ≤ sup |g(x) − f(x)|, which then

combines to give the desired result. (b) Apply (a) to −f and −g, the result follows.

Lemma E.2. If F(x) = x⊺Ax + b⊺x + c is a quadratic function of x ∈ Rp, where A ∈ Rp×p is

symmetric, then F(x) − F(x0) − ⟨∇F(x0), x − x0⟩ = (x − x0)⊺A(x − x0).

Proof. This result follows from simple calculation.

Lemma E.3. Suppose f(x) and g(x) are positive and Lebesgue integrable functions on Rp (p ≥ 1).

If

�

Rp f(x)dx =

�

Rp g(x)dx = 1, then

�

Rp g(x) log f(x)dx ≤

�

Rp g(x) log g(x)dx.

Proof. Let dµ(x) := g(x)dx, then (Rp, Bp, µ) is clearly a probability measure space.

Since

− log(x) is convex on R+ and f(x)

g(x) ∈ L1 (µ), by applying the Jensen’s Inequality [14, 16], one has

− log

��

Rp

f(x)

g(x) dµ(x)

�

≤

�

Rp − log

�f(x)

g(x)

�

dµ(x).

Since

�

Rp f(x)dx = 1, the left-hand side of the above inequality is zero, hence

0 ≥

�

Rp log

�f(x)

g(x)

�

dµ(x) =

�

Rp g(x) log f(x)dx −

�

Rp g(x) log g(x)dx,

and the lemma follows.

Lemma E.4. For a family of parametric densities {pθ(x)}θ∈Ω where Ω ⊆ Rd, there holds

−

�

X

∇∇⊺ (log pθ(x)) pθ(x)dx =

�

X

[∇ log pθ(x)] [∇⊺ log pθ(x)] pθ(x)dx

49


for θ ∈ Ω and this d × d matrix is positive semi-deﬁnite.

Proof. Direct calculation yields

∇∇⊺ (log pθ(x)) pθ(x) = ∇∇⊺pθ(x) − [∇ log pθ(x)] [∇⊺ log pθ(x)] pθ(x).

Then the result follows by integration on both sides and noting

�

X pθ(x)dx = 1 for θ ∈ Ω.

E.2

The ϵ-Net and Discretization of Norm

Suppose (X, d) is a compact metric space, a ﬁnite subset N ⊆ X is called an ϵ-net if for any x ∈ X

there exists y ∈ N such that d(x, y) &lt; ϵ.6 Then Nϵ(X) := min{card(N) | N is an ϵ-net of X} is

called the ϵ-covering number of X. For the unit sphere Sp−1 with induced Euclidean norm, we

have Nϵ(Sp−1) &lt;

�

1 + 2

ϵ

�p. See [38] for a proof.

Lemma E.5 (Discretization of Norm). There exists {ui ∈ Sp−1 | 1 ≤ i ≤ L} with L &lt; 5p such

that for any Z ∈ Rp, there holds the inequality

∥Z∥ ≤ 2 max

1≤i≤L u⊺

i Z.

Proof. Let {ui ∈ Sp−1 | 1 ≤ i ≤ L} be a 1

2-net of the Sp−1 ⊂ Rp such that L = N 1

2

�Sp−1� &lt; 5p,

then for any u ∈ Sp−1, there exists ui such that ∥u − ui∥ ≤ 1

2. For a vector Z ∈ Rp, we have

u⊺Z ≤ |u⊺Z − u⊺

i Z| + u⊺

i Z ≤ ∥u − ui∥ ∥Z∥ + max

1≤i≤L u⊺

i Z ≤ 1

2 ∥Z∥ + max

1≤i≤L u⊺

i Z.

Then we have

∥Z∥ =

sup

u∈Sp−1 u⊺Z ≤ 1

2 ∥Z∥ + max

1≤i≤L u⊺

i Z,

and the lemma follows.

E.3

Concentration of Random Vectors

In this section we prove some Concentration Inequalities for sub-gaussian and sub-exponential

random vectors. We exploit the Orlicz norm in the proofs. An exposition on Orlicz norm and

concentration of random variables can be found in [38]. Here, we mention the following facts.

Lemma E.6. Let X and Y be random variables.

(a) (Centering) If X has mean EX, then ∥X − EX∥ψi ≤ 2 ∥X∥ψi for i = 1, 2;

(b) (Product of Sub-gaussians) If X and Y are sub-gaussian, then XY is sub-exponential with

Orlicz norm ∥XY ∥ψ1 ≤ C ∥X∥ψ2 ∥Y ∥ψ2 .

Proof. See [38].

A concentration inequality for sub-gaussian random vectors.

6It follows from the compactness of X that there exists an ϵ-net for any ϵ &gt; 0.

50


Lemma E.7. Suppose Y is a centered random vector in Rp such that u⊺Y is sub-gaussian with

Orlicz norm ∥u⊺Y ∥ψ2 ≤ K for any u ∈ Sp−1. If Yk is an i.i.d. copy of Y for k = 1, · · · , n, then

for δ &gt; 0 there holds

�����

1

n

n

�

k=1

Yk

����� ≤ CK

�

log(L/δ)

n

with probability at least 1 − δ.

Proof. Let {ui}L

i=1 be a 1

2-net of the unit sphere Sp−1 ⊂ Rp and let Z := 1

n

�n

k=1 Yk, then by

Lemma E.5, ∥Z∥ ≤ 2 max1≤i≤L u⊺

i Z. By rotation invariance of sub-gaussian variables, we have

∥u⊺Z∥2

ψ2 = 1

n2

�����

n

�

i=1

u⊺Yk

�����

2

ψ2

≤ C1

n2

n

�

i=1

∥u⊺Yk∥2

ψ2 ≤ C2K2

n

.

Then the moment generating function of ∥Z∥ is bounded by

E exp (λ ∥Z∥) ≤ E exp

�

2λ max

1≤i≤L u⊺

i Z

�

= E

�

max

1≤i≤L exp (2λu⊺

i Z)

�

≤

L

�

i=1

E exp (2λu⊺

i Z)

(a)

≤

L

�

i=1

exp

�

C3

4λ2K2

n

�

= L exp

�

C4

λ2K2

n

�

for λ &gt; 0, where (a) follows from the sub-gaussianity of u⊺

i Z. Hence by Chernoﬀ bound, for any

t &gt; 0, we have

Pr {∥Z∥ ≥ t} ≤ inf

λ&gt;0 {exp (−λt) E exp (λ ∥Z∥)}

≤ inf

λ&gt;0

�

L exp

�

C4

λ2K2

n

− λt

��

= L exp

�

−

nt2

4C4K2

�

,

and the lemma follows by setting L exp

�

−

nt2

4C4K2

�

= δ and solving for t.

Remark. In view of Lemma E.6(a), the result above can be extended to non-centered random

vectors by simply replacing Y with Y − EY .

A concentration inequality for sub-exponential random vectors.

Lemma E.8. Suppose Y is a centered random vector in Rp such that u⊺Y is sub-exponential

with Orlicz norm ∥u⊺Y ∥ψ1 ≤ K for any u ∈ Sp−1. If Yk is an i.i.d. copy of Y for k = 1, · · · , n,

then for δ &gt; 0 and n &gt; c log (L/δ) there holds

�����

1

n

n

�

k=1

Yk

����� ≤ CK

�

log(L/δ)

n

with probability at least 1 − δ.

51


Proof. Let {ui}L

i=1 be a 1

2-net of the unit sphere Sp−1 ⊂ Rp and let Z := 1

n

�n

k=1 Yk, then by

Lemma E.5, ∥Z∥ ≤ 2 max1≤i≤L u⊺

i Z.

Then for 0 &lt; λ &lt; C1n/K, the moment generating function of ∥Z∥ exists and is bounded by

E exp (λ ∥Z∥) ≤ E exp

�

2λ max

1≤i≤L u⊺

i Z

�

= E

�

max

1≤i≤L exp (2λu⊺

i Z)

�

≤

L

�

i=1

E exp (2λu⊺

i Z) =

L

�

i=1

E exp

� n

�

k=1

2λ

n u⊺

i Yk

�

(a)

=

L

�

i=1

n

�

k=1

E exp

�2λ

n u⊺

i Yk

� (b)

≤

L

�

i=1

n

�

k=1

exp

�

C2

�2λK

n

�2�

=

L

�

i=1

exp

�

C2

4λ2K2

n

�

= L exp

�

C3

λ2K2

n

�

,

where (a) follows from the independence of Yk; (b) follows from the fact that u⊺

i Yk is sub-

exponential. Then by Chernoﬀ bound, for any t &gt; 0, we have

Pr {∥Z∥ ≥ t} ≤ inf {exp (−λt) E exp (λ ∥Z∥) | 0 &lt; λ &lt; C1n/K}

≤ inf

�

L exp

�

C3

λ2K2

n

− λt

�

| 0 &lt; λ &lt; C1n/K

�

= L exp

�

−

nt2

4C3K2

�

,

if

nt

2C3K2 &lt; C1n/K or t &lt; C4K. By setting L exp

�

−

nt2

4C3K2

�

= δ, we have t = CK

�

log(L/δ)

n

, and

the lemma follows whenever n &gt; c log (L/δ).

Remark. In view of Lemma E.6(a), the result above can be extended to non-centered random

vectors by simply replacing Y with Y − EY .

52


References

[1] Balakrishnan, S., Wainwright, M.J. and Yu, B., 2017. Statistical guarantees for the EM

algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1), pp.77-

120.

[2] Baum, L.E., Petrie, T., Soules, G. and Weiss, N., 1970. A maximization technique occur-

ring in the statistical analysis of probabilistic functions of Markov chains. The annals of

mathematical statistics, 41(1), pp.164-171.

[3] Billingsley, P., 2008. Probability and measure. John Wiley &amp; Sons.

[4] Boucheron, S., Lugosi, G. and Bousquet, O., 2004. Concentration inequalities. In Advanced

Lectures on Machine Learning (pp. 208-240). Springer Berlin Heidelberg.

[5] Boyles, R.A., 1983. On the convergence of the EM algorithm. Journal of the Royal Statistical

Society. Series B (Methodological), pp.47-50.

[6] Buldygin, V.V. and Kozachenko, Y.V., 1980. Sub-Gaussian random variables. Ukrainian

Mathematical Journal, 32(6), pp.483-489.

[7] Chrétien, S. and Hero, A. O. On EM algorithms and their proximal generalizations. ESAIM:

Probability and Statistics, 12:308–326, 2008.

[8] Conniﬀe, D., 1987. Expected maximum log likelihood estimation. The Statistician, pp.317-329.

[9] Dasgupta, S. and Schulman, L. J. A probabilistic analysis of EM for mixtures of separated,

spherical gaussians. Journal of Machine Learning Research, 8:203–226, 2007.

[10] Dellacherie, C. and Meyer, P.A., 1982. Probabilities and Potentials. B, volume 72 of North-

Holland Mathematics Studies.

[11] Dembo, A. and Zeitouni, O., 2009. Large deviations techniques and applications (Vol. 38).

Springer Science &amp; Business Media.

[12] Dempster, A.P., Laird, N.M. and Rubin, D.B. 1977, Maximum Likelihood from Incomplete

Data via the EM Algorithm, Journal of the Royal Statistical Society. Series B (Methodolog-

ical), vol. 39, no. 1, pp. 1-38.

[13] Dudley, R.M., 2002. Real analysis and probability (Vol. 74). Cambridge University Press.

[14] Folland, G. 1999 Real Analysis, Modern Techniques and Their Applications, 2nd edn, Wiley-

Interscience

[15] Giannopoulos, A.A. and Milman, V.D., 2000. Concentration property on probability spaces.

Advances in Mathematics, 156(1), pp.77-106.

[16] Kuczma, M., 2009. An introduction to the theory of functional equations and inequalities:

Cauchy’s equation and Jensen’s inequality. Springer Science &amp; Business Media.

53


[17] Ledoux, M., 2005. The concentration of measure phenomenon (No. 89). American Mathe-

matical Society

[18] Ledoux, M. and Talagrand, M., 2013. Probability in Banach Spaces: isoperimetry and pro-

cesses. Springer Science &amp; Business Media.

[19] Liu, C., Rubin, D.B. and Wu, Y.N., 1998. Parameter expansion to accelerate EM: The PX-

EM algorithm. Biometrika, pp.755-770.

[20] McLachlan, G.J. and Krishnan, T. 2008, The EM algorithm and extensions, 2nd edn, Wiley-

Interscience, Hoboken, N.J.

[21] Meng, X.L. and Rubin, D.B., 1993. Maximum likelihood estimation via the ECM algorithm:

A general framework. Biometrika, 80(2), pp.267-278.

[22] Meng, X. and Rubin, D.B. 1994, On the global and componentwise rates of convergence of

the EM algorithm, Linear Algebra and Its Applications, vol. 199, no. 1, pp. 413-425.

[23] Meng, X. 1994, On the Rate of Convergence of the ECM Algorithm, The Annals of Statistics,

vol. 22, no. 1, pp. 326-339.

[24] Orchard, T. and Woodbury, M.A., 1972. A missing information principle: theory and ap-

plications. In Proceedings of the 6th Berkeley Symposium on mathematical statistics and

probability (Vol. 1, pp. 697-715). Berkeley, CA: University of California Press.

[25] Petrov, V., 2012. Sums of independent random variables (Vol. 82). Springer Science &amp; Busi-

ness Media.

[26] Pisier, G., 1999. The volume of convex bodies and Banach space geometry (Vol. 94). Cam-

bridge University Press.

[27] Pollard, D. 1984, Convergence of Stochastic Processes, Springer New York.

[28] Redner, R.A., Walker, H.F. Mixture Densities, Maximum-Likelihood and the EM Algorithm,

SIAM Review, Vol. 26, No.2, April 1984, pp. 195-239

[29] Rubin, D. B. Characterizing the estimation of parameters in incomplete-data problems. Jour-

nal of the American Statistical Association, 69(346):pp. 467–474, 1974.

[30] Sundberg, R., 1972. Maximum likelihood theory and applications for distributions generated

when observing a function of an exponential variable (Doctoral dissertation, PhD Thesis).

[31] Sundberg, R., 1974. Maximum likelihood theory for incomplete data from an exponential

family. Scandinavian Journal of Statistics, pp.49-58.

[32] Sundberg, R., 1976. An iterative method for solution of the likelihood equations for incomplete

data from exponential families. Communication in Statistics-Simulation and Computation,

5(1), pp.55-64.

[33] Talagrand, M., 1994. The supremum of some canonical processes. American Journal of Math-

ematics, 116(2), pp.283-325.

54


[34] Talagrand, M., 1995. Concentration of measure and isoperimetric inequalities in product

spaces. Publications Mathématiques de l’Institut des Hautes Etudes Scientiﬁques, 81(1),

pp.73-205.

[35] Talagrand, M., 1996. A new look at independence. The Annals of probability, pp.1-34.

[36] Tao, T., 2012, Topics in random matrix theory, American Mathematical Society, Providence,

R.I.

[37] Tseng, P. An analysis of the EM algorithm and entropy-like proximal point methods. Math-

ematics of Operations Research, 29(1):pp. 27–44, 2004.

[38] Vershynin, R. 2010, Introduction to the non-asymptotic analysis of random matrices,

arXiv:1011.3027

[39] Wang, Z., Gu, Q., Ning, Y. and Liu, H., 2015. High dimensional em algorithm: Statisti-

cal optimization and asymptotic normality. In Advances in Neural Information Processing

Systems (pp. 2521-2529).

[40] Wu, C.F.J. 1983, On the Convergence Properties of the EM Algorithm, The Annals of Statis-

tics, vol. 11, no. 1, pp. 95-103.

[41] Yi, X. and Caramanis, C., 2015. Regularized em algorithms: A uniﬁed framework and sta-

tistical guarantees. In Advances in Neural Information Processing Systems (pp. 1567-1575).

55

