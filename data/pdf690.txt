
Published in

Nwamaka Imasogie’s Machine Learning and Artificial Intelligence Projects



Oct 11, 2019

·

18 min read

Neural Networks &amp; Word Embeddings

Natural Language

Processing with Deep Learning Course

Representing words by their context






[Lecture 1 — Stanford NLP with Deep Learning — Introduction and Word Vectors].

Distributional semantics

Word Embeddings (a.k.a. Word vectors)

distributed

Word2vec

we want a model that gives a reasonably high probability

estimate to all words that occur in the context (fairly often)

Main Idea/Summary:

More Detailed Steps:

vector

random

c

o

similarity of the word vectors

c

o

o

Keep adjusting the word vectors

https://youtu.be/8rXD5-xhemo?t=2871

NOTE


Word2vec the Skip-Gram Model

single-layer

apricot

c

(apricot, jam)

(apricot, aardvark)

The probability that c is a real context word.

t·c


In summary, skip-gram trains a probabilistic classifier that, given a test target word t and

its context window of k words c1:k , assigns a probability based on how similar this

context window is to the target word.

Learning skip-gram embeddings

Efficient Estimation of Word Representation in Vector Space [Paper]

DistBelief

Distributed Representations of Words and Phrases and their Compositionality

[Paper]

Negative Sampling




Negative sampling

Assignment 1 —Explore Word2Vec [GitHub][Code]

co-occurrence matrices

word2vec

Notebook on nbviewer

Word Vectors

Note on Terminology

Part 1: Count-Based Word vectors

Create a Co-occurrence Matrix

Dimensionality Reduction

sklearn.decomposition.TruncatedSVD

Part 2: Prediction-Based Word Vectors

Dimensionality Reduction

Synonyms &amp; Antonyms


Synonyms &amp; Antonyms

Solving Analogies with Word Vectors:

man

king

woman

x

x

Finding Analogies

GenSim’s

most_similar

Austin

Texas

Atlanta

Georgia

Explore Incorrect Analogies

Analyze Bias in Word Vectors

most_similar

low bias

low variability

Word Vectors “2" and Word Senses

Stanford CS224N: NLP with Deep Learning | Lecture 2 — Word Vectors and Word Senses


We have 2 matrices. For the center words, we have a matrix, V, where for each word in our vocabulary we have a

vector (represented as rows). So the V matrix has 6 words. For the outside matrix, U, we have a 2nd vector for each

word. So if the center word is word 4, we take are taking a dot product of v4 and each row of U. Which gives us a

vector of dot products between U and V4. After that we run softmax’s on each of those numbers (element-wise),

which gives us a probability distribution over words in the context.

CAUTION: These 2-dimensional pictures are exceedingly misleading. For example, “Nokia” is next to “Samsung” so

let’s say you expect it to be far away from words in the bottom side. Or you might actually want the effect that Nokia

is close to Finland, for some reason. But you cannot do that in 2D vector spaces. Most properties of high

dimensional vector spaces are very unintuitive, because in a high-dimensional vector space a word

can be close to lots of other words in different directions.

Skip-grams (SG) versus Continuous Bag of Words (CBOW)

Skip-grams

Continuous Bag of Words (CBOW)

Disadvantage

negative sampling

Negative Sampling

Probability of a word in the context (o), given a center word ( c ).

We want to give a

high probability to the word that was actually observed (center word)

GloVe — Global Vectors for Word Representations. Combining the best of both

worlds.

Why?


NOTE

Co-occurrence probabilities for target words ice and steam with selected context words from a 6 billion token

corpus. Only in the ratio does noise from non-discriminative words like water and fashion cancel out, so that large

values (much greater than 1) correlate well with properties specific to ice, and small values (much less than 1)

correlate well with properties specific of steam.

The interesting

thing lies in the difference between the two co-occurrence probabilities P(x|ice) versus

P(x|steam).

Question: How can we capture rations of co-occurrence probabilities as linear meaning components in a word

vector space? Answer: Make the dot products equal to the log of the co-occurrence probability. Which yields the

fact that a vector difference leads to a log of the co-occurrence probabilities.

The GloVe model:

GloVe model: J is the objective function with a squared loss. The dot product of wi and wj should be as similar as

possible to the log of the co-occurrence probability so they will be “lost” to the extent that they are not the same!

Then bias terms, bi and bj, are added because maybe the word is very common or very uncommon. The f-

function in front caps the effect that very common word-pairs can have on the performance of the system.

GloVe: Global Vectors for Word Representation [Paper]

Improving Distributional Similarity with Lessons Learned from Word Embeddings

[Paper]

performance gains of word embeddings are due to

certain system design choices and hyperparameter optimizations

traditional

distributional models


Evaluation methods for unsupervised word embeddings [Paper]

Intrinsic Evaluations

Extrinsic Tasks

Word Window Classification, Neural Networks, and Matrix Calculus


https://www.youtube.com/watch?v=8CWyBNX6eDo&amp;feature=youtu.be

Cross entropy loss

p

q

Cross entropy loss formula

Because of one-hot p , the only term left is the negative log probability of the true class.

Classification difference with word vectors


W

x

weights

word representations

x = Le

e

Non-linear activation functions and why they’re needed

Left: Linear decision boundaries. Right: Non-linear decision boundaries.

Maximum margin objective function

“Museums in Paris are amazing”

J = max(∆ + sc − s, 0)

−

∆

∆

s = score(museums in Paris are amazing)

sc = score(Not all museums

in Paris)


Assignment 2: Implementing Word2Vec [Jupyter Notebook][Github]

sigmoid

softmax

negative sampling loss

gradient

naiveSoftmaxLossAndGradient

getNegativeSamples

negSamplingLossAndGradient

skipgram

SGD

sgd

Written part of CS224n Assignment 2

partial derivatives

Backpropogation &amp; Computation Graphs + Stuff You Should Know: Regularization,

Vectorization, Nonlinearities, Learning Rates, Initialization, Optimizers


A node receives an upstream gradient.

upstream gradient

This is a single node example. The goal is to pass on the correct downstream gradient. Each node has a local

gradient. Downstream gradient = (upstream gradient) x (local gradient).

Downstream gradient = (upstream gradient) x (local gradient)

Multiple inputs example

Algorithm

Things You Should Know about Deep Learning — Regularization, Vectorization,

Nonlinearities, Learning Rates, Initialization, Optimizers

Regularization

The training error loss will keep going down until it approaches zero due to so many parameters. To the right of the

grey line, the training model is just learning to memorize whatever is in the training data, but not in a way that it will

perform well on other data.

Non-linearities

Logistic

Tanh


Hard tanh

ReLU

the default

feed-forward neural

networks

ReLU has slope 0 when you’re in the negatives, while slope = 1 when in the positive regime so it acts like an identity

function.

Parameter Initialization

Xavier

Optimizers

adaptive

Learning Rates

Dependency Parsing


https://www.youtube.com/watch?v=nC9_RfjYwqA&amp;feature=youtu.be

Why do we need sentence structure?

word part of speech tag

dependency labels

http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture05-dep-parsing.pdf#page=39

My machine learning and artificial intelligence projects. The ability to communicate is very important for a ML/AI engineer. I focus on

Deep Learning

NLP

Neural Networks

Word2vec

Stanford


communicating the what, why, when, where, and who of the insights in the simplest way possible.



Read more from Nwamaka Imasogie’s Machine Learning and Artificial Intelligence Projects





