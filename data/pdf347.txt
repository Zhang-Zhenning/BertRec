
CS229 Lecture Notes

Andrew Ng

Updated by Tengyu Ma


Contents

I

Supervised learning

5

1

Linear regression

8

1.1

LMS algorithm

. . . . . . . . . . . . . . . . . . . . . . . . . .

9

1.2

The normal equations . . . . . . . . . . . . . . . . . . . . . . .

13

1.2.1

Matrix derivatives . . . . . . . . . . . . . . . . . . . . .

13

1.2.2

Least squares revisited . . . . . . . . . . . . . . . . . .

14

1.3

Probabilistic interpretation . . . . . . . . . . . . . . . . . . . .

15

1.4

Locally weighted linear regression (optional reading) . . . . . .

17

2

Classiﬁcation and logistic regression

20

2.1

Logistic regression

. . . . . . . . . . . . . . . . . . . . . . . .

20

2.2

Digression: the perceptron learning algorithn . . . . . . . . . .

23

2.3

Another algorithm for maximizing ℓ(θ) . . . . . . . . . . . . .

24

3

Generalized linear models

26

3.1

The exponential family . . . . . . . . . . . . . . . . . . . . . .

26

3.2

Constructing GLMs . . . . . . . . . . . . . . . . . . . . . . . .

28

3.2.1

Ordinary least squares . . . . . . . . . . . . . . . . . .

29

3.2.2

Logistic regression

. . . . . . . . . . . . . . . . . . . .

30

3.2.3

Softmax regression . . . . . . . . . . . . . . . . . . . .

30

4

Generative learning algorithms

35

4.1

Gaussian discriminant analysis . . . . . . . . . . . . . . . . . .

36

4.1.1

The multivariate normal distribution . . . . . . . . . .

36

4.1.2

The Gaussian discriminant analysis model . . . . . . .

39

4.1.3

Discussion: GDA and logistic regression

. . . . . . . .

41

4.2

Naive bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . .

42

4.2.1

Laplace smoothing . . . . . . . . . . . . . . . . . . . .

45

4.2.2

Event models for text classiﬁcation . . . . . . . . . . .

47

1


CS229 Spring 2022

2

5

Kernel methods

49

5.1

Feature maps . . . . . . . . . . . . . . . . . . . . . . . . . . .

49

5.2

LMS (least mean squares) with features . . . . . . . . . . . . .

50

5.3

LMS with the kernel trick

. . . . . . . . . . . . . . . . . . . .

50

5.4

Properties of kernels

. . . . . . . . . . . . . . . . . . . . . . .

54

6

Support vector machines

60

6.1

Margins: intuition . . . . . . . . . . . . . . . . . . . . . . . . .

60

6.2

Notation (option reading)

. . . . . . . . . . . . . . . . . . . .

62

6.3

Functional and geometric margins (option reading)

. . . . . .

62

6.4

The optimal margin classiﬁer (option reading) . . . . . . . . .

64

6.5

Lagrange duality (optional reading) . . . . . . . . . . . . . . .

66

6.6

Optimal margin classiﬁers: the dual form (option reading)

. .

69

6.7

Regularization and the non-separable case (optional reading) .

73

6.8

The SMO algorithm (optional reading) . . . . . . . . . . . . .

74

6.8.1

Coordinate ascent . . . . . . . . . . . . . . . . . . . . .

75

6.8.2

SMO . . . . . . . . . . . . . . . . . . . . . . . . . . . .

76

II

Deep learning

80

7

Deep learning

81

7.1

Supervised learning with non-linear models . . . . . . . . . . .

81

7.2

Neural networks . . . . . . . . . . . . . . . . . . . . . . . . . .

83

7.3

Backpropagation

. . . . . . . . . . . . . . . . . . . . . . . . .

92

7.3.1

Preliminary: chain rule . . . . . . . . . . . . . . . . . .

93

7.3.2

One-neuron neural networks . . . . . . . . . . . . . . .

93

7.3.3

Two-layer neural networks: a low-level unpacked com-

putation . . . . . . . . . . . . . . . . . . . . . . . . . .

94

7.3.4

Two-layer neural network with vector notation . . . . .

97

7.3.5

Multi-layer neural networks

. . . . . . . . . . . . . . .

99

7.4

Vectorization over training examples

. . . . . . . . . . . . . .

99

III

Generalization and regularization

102

8

Generalization

103

8.1

Bias-variance tradeoﬀ . . . . . . . . . . . . . . . . . . . . . . . 105

8.1.1

A mathematical decomposition (for regression) . . . . . 110

8.2

The double descent phenomenon . . . . . . . . . . . . . . . . . 111


CS229 Spring 2022

3

8.3

Sample complexity bounds (optional readings) . . . . . . . . . 116

8.3.1

Preliminaries

. . . . . . . . . . . . . . . . . . . . . . . 116

8.3.2

The case of ﬁnite H . . . . . . . . . . . . . . . . . . . . 118

8.3.3

The case of inﬁnite H

. . . . . . . . . . . . . . . . . . 121

9

Regularization and model selection

125

9.1

Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . 125

9.2

Implicit regularization eﬀect . . . . . . . . . . . . . . . . . . . 127

9.3

Model selection via cross validation . . . . . . . . . . . . . . . 129

9.4

Bayesian statistics and regularization . . . . . . . . . . . . . . 132

IV

Unsupervised learning

134

10 Clustering and the k-means algorithm

135

11 EM algorithms

138

11.1 EM for mixture of Gaussians . . . . . . . . . . . . . . . . . . . 138

11.2 Jensen’s inequality

. . . . . . . . . . . . . . . . . . . . . . . . 141

11.3 General EM algorithms . . . . . . . . . . . . . . . . . . . . . . 142

11.3.1 Other interpretation of ELBO . . . . . . . . . . . . . . 148

11.4 Mixture of Gaussians revisited . . . . . . . . . . . . . . . . . . 148

11.5 Variational inference and variational auto-encoder (optional

reading) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150

12 Principal components analysis

155

13 Independent components analysis

161

13.1 ICA ambiguities . . . . . . . . . . . . . . . . . . . . . . . . . . 162

13.2 Densities and linear transformations . . . . . . . . . . . . . . . 163

13.3 ICA algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 164

14 Self-supervised learning and foundation models

167

14.1 Pretraining and adaptation . . . . . . . . . . . . . . . . . . . . 167

14.2 Pretraining methods in computer vision . . . . . . . . . . . . . 169

14.3 Pretrained large language models . . . . . . . . . . . . . . . . 171

14.3.1 Zero-shot learning and in-context learning

. . . . . . . 173


CS229 Spring 2022

4

V

Reinforcement Learning and Control

175

15 Reinforcement learning

176

15.1 Markov decision processes . . . . . . . . . . . . . . . . . . . . 177

15.2 Value iteration and policy iteration . . . . . . . . . . . . . . . 179

15.3 Learning a model for an MDP . . . . . . . . . . . . . . . . . . 181

15.4 Continuous state MDPs

. . . . . . . . . . . . . . . . . . . . . 183

15.4.1 Discretization . . . . . . . . . . . . . . . . . . . . . . . 183

15.4.2 Value function approximation . . . . . . . . . . . . . . 186

15.5 Connections between Policy and Value Iteration (Optional) . . 190

16 LQR, DDP and LQG

193

16.1 Finite-horizon MDPs . . . . . . . . . . . . . . . . . . . . . . . 193

16.2 Linear Quadratic Regulation (LQR) . . . . . . . . . . . . . . . 197

16.3 From non-linear dynamics to LQR

. . . . . . . . . . . . . . . 200

16.3.1 Linearization of dynamics

. . . . . . . . . . . . . . . . 201

16.3.2 Diﬀerential Dynamic Programming (DDP) . . . . . . . 201

16.4 Linear Quadratic Gaussian (LQG) . . . . . . . . . . . . . . . . 203

17 Policy Gradient (REINFORCE)

207


Part I

Supervised learning

5


6

Let’s start by talking about a few examples of supervised learning prob-

lems. Suppose we have a dataset giving the living areas and prices of 47

houses from Portland, Oregon:

Living area (feet2)

Price (1000$s)

2104

400

1600

330

2400

369

1416

232

3000

540

...

...

We can plot this data:

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

0

100

200

300

400

500

600

700

800

900

1000

housing prices

square feet

price (in $1000)

Given data like this, how can we learn to predict the prices of other houses

in Portland, as a function of the size of their living areas?

To establish notation for future use, we’ll use x(i) to denote the “input”

variables (living area in this example), also called input features, and y(i)

to denote the “output” or target variable that we are trying to predict

(price). A pair (x(i), y(i)) is called a training example, and the dataset

that we’ll be using to learn—a list of n training examples {(x(i), y(i)); i =

1, . . . , n}—is called a training set. Note that the superscript “(i)” in the

notation is simply an index into the training set, and has nothing to do with

exponentiation. We will also use X denote the space of input values, and Y

the space of output values. In this example, X = Y = R.

To describe the supervised learning problem slightly more formally, our

goal is, given a training set, to learn a function h : X �→ Y so that h(x) is a

“good” predictor for the corresponding value of y. For historical reasons, this


7

function h is called a hypothesis. Seen pictorially, the process is therefore

like this:

Training 

    set

 house.)

(living area of

Learning 

algorithm

h

predicted y

x

(predicted price)

of house)

When the target variable that we’re trying to predict is continuous, such

as in our housing example, we call the learning problem a regression prob-

lem. When y can take on only a small number of discrete values (such as

if, given the living area, we wanted to predict if a dwelling is a house or an

apartment, say), we call it a classiﬁcation problem.


Chapter 1

Linear regression

To make our housing example more interesting, let’s consider a slightly richer

dataset in which we also know the number of bedrooms in each house:

Living area (feet2)

#bedrooms

Price (1000$s)

2104

3

400

1600

3

330

2400

3

369

1416

2

232

3000

4

540

...

...

...

Here, the x’s are two-dimensional vectors in R2. For instance, x(i)

1

is the

living area of the i-th house in the training set, and x(i)

2

is its number of

bedrooms. (In general, when designing a learning problem, it will be up to

you to decide what features to choose, so if you are out in Portland gathering

housing data, you might also decide to include other features such as whether

each house has a ﬁreplace, the number of bathrooms, and so on. We’ll say

more about feature selection later, but for now let’s take the features as

given.)

To perform supervised learning, we must decide how we’re going to rep-

resent functions/hypotheses h in a computer. As an initial choice, let’s say

we decide to approximate y as a linear function of x:

hθ(x) = θ0 + θ1x1 + θ2x2

Here, the θi’s are the parameters (also called weights) parameterizing the

space of linear functions mapping from X to Y. When there is no risk of

8


9

confusion, we will drop the θ subscript in hθ(x), and write it more simply as

h(x). To simplify our notation, we also introduce the convention of letting

x0 = 1 (this is the intercept term), so that

h(x) =

d

�

i=0

θixi = θTx,

where on the right-hand side above we are viewing θ and x both as vectors,

and here d is the number of input variables (not counting x0).

Now, given a training set, how do we pick, or learn, the parameters θ?

One reasonable method seems to be to make h(x) close to y, at least for

the training examples we have. To formalize this, we will deﬁne a function

that measures, for each value of the θ’s, how close the h(x(i))’s are to the

corresponding y(i)’s. We deﬁne the cost function:

J(θ) = 1

2

n

�

i=1

(hθ(x(i)) − y(i))2.

If you’ve seen linear regression before, you may recognize this as the familiar

least-squares cost function that gives rise to the ordinary least squares

regression model.

Whether or not you have seen it previously, let’s keep

going, and we’ll eventually show this to be a special case of a much broader

family of algorithms.

1.1

LMS algorithm

We want to choose θ so as to minimize J(θ). To do so, let’s use a search

algorithm that starts with some “initial guess” for θ, and that repeatedly

changes θ to make J(θ) smaller, until hopefully we converge to a value of

θ that minimizes J(θ).

Speciﬁcally, let’s consider the gradient descent

algorithm, which starts with some initial θ, and repeatedly performs the

update:

θj := θj − α ∂

∂θj

J(θ).

(This update is simultaneously performed for all values of j = 0, . . . , d.)

Here, α is called the learning rate. This is a very natural algorithm that

repeatedly takes a step in the direction of steepest decrease of J.

In order to implement this algorithm, we have to work out what is the

partial derivative term on the right hand side. Let’s ﬁrst work it out for the


10

case of if we have only one training example (x, y), so that we can neglect

the sum in the deﬁnition of J. We have:

∂

∂θj

J(θ)

=

∂

∂θj

1

2 (hθ(x) − y)2

=

2 · 1

2 (hθ(x) − y) · ∂

∂θj

(hθ(x) − y)

=

(hθ(x) − y) · ∂

∂θj

�

d

�

i=0

θixi − y

�

=

(hθ(x) − y) xj

For a single training example, this gives the update rule:1

θj := θj + α

�

y(i) − hθ(x(i))

�

x(i)

j .

The rule is called the LMS update rule (LMS stands for “least mean squares”),

and is also known as the Widrow-Hoﬀ learning rule. This rule has several

properties that seem natural and intuitive. For instance, the magnitude of

the update is proportional to the error term (y(i) − hθ(x(i))); thus, for in-

stance, if we are encountering a training example on which our prediction

nearly matches the actual value of y(i), then we ﬁnd that there is little need

to change the parameters; in contrast, a larger change to the parameters will

be made if our prediction hθ(x(i)) has a large error (i.e., if it is very far from

y(i)).

We’d derived the LMS rule for when there was only a single training

example. There are two ways to modify this method for a training set of

more than one example. The ﬁrst is replace it with the following algorithm:

Repeat until convergence {

θj := θj + α

n

�

i=1

�

y(i) − hθ(x(i))

�

x(i)

j , (for every j)

(1.1)

}

1We use the notation “a := b” to denote an operation (in a computer program) in

which we set the value of a variable a to be equal to the value of b. In other words, this

operation overwrites a with the value of b. In contrast, we will write “a = b” when we are

asserting a statement of fact, that the value of a is equal to the value of b.


11

By grouping the updates of the coordinates into an update of the vector

θ, we can rewrite update (1.1) in a slightly more succinct way:

θ := θ + α

n

�

i=1

�

y(i) − hθ(x(i))

�

x(i)

The reader can easily verify that the quantity in the summation in the

update rule above is just ∂J(θ)/∂θj (for the original deﬁnition of J). So, this

is simply gradient descent on the original cost function J. This method looks

at every example in the entire training set on every step, and is called batch

gradient descent. Note that, while gradient descent can be susceptible

to local minima in general, the optimization problem we have posed here

for linear regression has only one global, and no other local, optima; thus

gradient descent always converges (assuming the learning rate α is not too

large) to the global minimum.

Indeed, J is a convex quadratic function.

Here is an example of gradient descent as it is run to minimize a quadratic

function.

5

10

15

20

25

30

35

40

45

50

5

10

15

20

25

30

35

40

45

50

The ellipses shown above are the contours of a quadratic function.

Also

shown is the trajectory taken by gradient descent, which was initialized at

(48,30). The x’s in the ﬁgure (joined by straight lines) mark the successive

values of θ that gradient descent went through.

When we run batch gradient descent to ﬁt θ on our previous dataset,

to learn to predict housing price as a function of living area, we obtain

θ0 = 71.27, θ1 = 0.1345. If we plot hθ(x) as a function of x (area), along

with the training data, we obtain the following ﬁgure:


12

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

0

100

200

300

400

500

600

700

800

900

1000

housing prices

square feet

price (in $1000)

If the number of bedrooms were included as one of the input features as well,

we get θ0 = 89.60, θ1 = 0.1392, θ2 = −8.738.

The above results were obtained with batch gradient descent. There is

an alternative to batch gradient descent that also works very well. Consider

the following algorithm:

Loop {

for i = 1 to n, {

θj := θj + α

�

y(i) − hθ(x(i))

�

x(i)

j ,

(for every j)

(1.2)

}

}

By grouping the updates of the coordinates into an update of the vector

θ, we can rewrite update (1.2) in a slightly more succinct way:

θ := θ + α

�

y(i) − hθ(x(i))

�

x(i)

In this algorithm, we repeatedly run through the training set, and each

time we encounter a training example, we update the parameters according

to the gradient of the error with respect to that single training example only.

This algorithm is called stochastic gradient descent (also incremental

gradient descent). Whereas batch gradient descent has to scan through

the entire training set before taking a single step—a costly operation if n is

large—stochastic gradient descent can start making progress right away, and


13

continues to make progress with each example it looks at. Often, stochastic

gradient descent gets θ “close” to the minimum much faster than batch gra-

dient descent. (Note however that it may never “converge” to the minimum,

and the parameters θ will keep oscillating around the minimum of J(θ); but

in practice most of the values near the minimum will be reasonably good

approximations to the true minimum.2) For these reasons, particularly when

the training set is large, stochastic gradient descent is often preferred over

batch gradient descent.

1.2

The normal equations

Gradient descent gives one way of minimizing J. Let’s discuss a second way

of doing so, this time performing the minimization explicitly and without

resorting to an iterative algorithm. In this method, we will minimize J by

explicitly taking its derivatives with respect to the θj’s, and setting them to

zero. To enable us to do this without having to write reams of algebra and

pages full of matrices of derivatives, let’s introduce some notation for doing

calculus with matrices.

1.2.1

Matrix derivatives

For a function f : Rn×d �→ R mapping from n-by-d matrices to the real

numbers, we deﬁne the derivative of f with respect to A to be:

∇Af(A) =





∂f

∂A11

· · ·

∂f

∂A1d

...

...

...

∂f

∂An1

· · ·

∂f

∂And





Thus, the gradient ∇Af(A) is itself an n-by-d matrix, whose (i, j)-element is

∂f/∂Aij. For example, suppose A =

�

A11

A12

A21

A22

�

is a 2-by-2 matrix, and

the function f : R2×2 �→ R is given by

f(A) = 3

2A11 + 5A2

12 + A21A22.

2By slowly letting the learning rate α decrease to zero as the algorithm runs, it is also

possible to ensure that the parameters will converge to the global minimum rather than

merely oscillate around the minimum.


14

Here, Aij denotes the (i, j) entry of the matrix A. We then have

∇Af(A) =

�

3

2

10A12

A22

A21

�

.

1.2.2

Least squares revisited

Armed with the tools of matrix derivatives, let us now proceed to ﬁnd in

closed-form the value of θ that minimizes J(θ). We begin by re-writing J in

matrix-vectorial notation.

Given a training set, deﬁne the design matrix X to be the n-by-d matrix

(actually n-by-d + 1, if we include the intercept term) that contains the

training examples’ input values in its rows:

X =





— (x(1))T —

— (x(2))T —

...

— (x(n))T —



 .

Also, let ⃗y be the n-dimensional vector containing all the target values from

the training set:

⃗y =





y(1)

y(2)

...

y(n)



 .

Now, since hθ(x(i)) = (x(i))Tθ, we can easily verify that

Xθ − ⃗y

=





(x(1))Tθ

...

(x(n))Tθ



 −





y(1)

...

y(n)





=





hθ(x(1)) − y(1)

...

hθ(x(n)) − y(n)



 .

Thus, using the fact that for a vector z, we have that zTz = �

i z2

i :

1

2(Xθ − ⃗y)T(Xθ − ⃗y)

=

1

2

n

�

i=1

(hθ(x(i)) − y(i))2

=

J(θ)


15

Finally, to minimize J, let’s ﬁnd its derivatives with respect to θ. Hence,

∇θJ(θ)

=

∇θ

1

2(Xθ − ⃗y)T(Xθ − ⃗y)

=

1

2∇θ

�

(Xθ)TXθ − (Xθ)T⃗y − ⃗yT(Xθ) + ⃗yT⃗y

�

=

1

2∇θ

�

θT(XTX)θ − ⃗yT(Xθ) − ⃗yT(Xθ)

�

=

1

2∇θ

�

θT(XTX)θ − 2(XT⃗y)Tθ

�

=

1

2

�

2XTXθ − 2XT⃗y

�

=

XTXθ − XT⃗y

In the third step, we used the fact that aTb = bTa, and in the ﬁfth step

used the facts ∇xbTx = b and ∇xxTAx = 2Ax for symmetric matrix A (for

more details, see Section 4.3 of “Linear Algebra Review and Reference”). To

minimize J, we set its derivatives to zero, and obtain the normal equations:

XTXθ = XT⃗y

Thus, the value of θ that minimizes J(θ) is given in closed form by the

equation

θ = (XTX)−1XT⃗y.3

1.3

Probabilistic interpretation

When faced with a regression problem, why might linear regression, and

speciﬁcally why might the least-squares cost function J, be a reasonable

choice? In this section, we will give a set of probabilistic assumptions, under

which least-squares regression is derived as a very natural algorithm.

Let us assume that the target variables and the inputs are related via the

equation

y(i) = θTx(i) + ϵ(i),

3Note that in the above step, we are implicitly assuming that XT X is an invertible

matrix.

This can be checked before calculating the inverse.

If either the number of

linearly independent examples is fewer than the number of features, or if the features

are not linearly independent, then XT X will not be invertible. Even in such cases, it is

possible to “ﬁx” the situation with additional techniques, which we skip here for the sake

of simplicty.


16

where ϵ(i) is an error term that captures either unmodeled eﬀects (such as

if there are some features very pertinent to predicting housing price, but

that we’d left out of the regression), or random noise. Let us further assume

that the ϵ(i) are distributed IID (independently and identically distributed)

according to a Gaussian distribution (also called a Normal distribution) with

mean zero and some variance σ2. We can write this assumption as “ϵ(i) ∼

N(0, σ2).” I.e., the density of ϵ(i) is given by

p(ϵ(i)) =

1

√

2πσ exp

�

−(ϵ(i))2

2σ2

�

.

This implies that

p(y(i)|x(i); θ) =

1

√

2πσ exp

�

−(y(i) − θTx(i))2

2σ2

�

.

The notation “p(y(i)|x(i); θ)” indicates that this is the distribution of y(i)

given x(i) and parameterized by θ. Note that we should not condition on θ

(“p(y(i)|x(i), θ)”), since θ is not a random variable. We can also write the

distribution of y(i) as y(i) | x(i); θ ∼ N(θTx(i), σ2).

Given X (the design matrix, which contains all the x(i)’s) and θ, what

is the distribution of the y(i)’s?

The probability of the data is given by

p(⃗y|X; θ). This quantity is typically viewed a function of ⃗y (and perhaps X),

for a ﬁxed value of θ. When we wish to explicitly view this as a function of

θ, we will instead call it the likelihood function:

L(θ) = L(θ; X, ⃗y) = p(⃗y|X; θ).

Note that by the independence assumption on the ϵ(i)’s (and hence also the

y(i)’s given the x(i)’s), this can also be written

L(θ)

=

n

�

i=1

p(y(i) | x(i); θ)

=

n

�

i=1

1

√

2πσ exp

�

−(y(i) − θTx(i))2

2σ2

�

.

Now, given this probabilistic model relating the y(i)’s and the x(i)’s, what

is a reasonable way of choosing our best guess of the parameters θ? The

principal of maximum likelihood says that we should choose θ so as to

make the data as high probability as possible. I.e., we should choose θ to

maximize L(θ).


17

Instead of maximizing L(θ), we can also maximize any strictly increasing

function of L(θ). In particular, the derivations will be a bit simpler if we

instead maximize the log likelihood ℓ(θ):

ℓ(θ)

=

log L(θ)

=

log

n

�

i=1

1

√

2πσ exp

�

−(y(i) − θTx(i))2

2σ2

�

=

n

�

i=1

log

1

√

2πσ exp

�

−(y(i) − θTx(i))2

2σ2

�

=

n log

1

√

2πσ − 1

σ2 · 1

2

n

�

i=1

(y(i) − θTx(i))2.

Hence, maximizing ℓ(θ) gives the same answer as minimizing

1

2

n

�

i=1

(y(i) − θTx(i))2,

which we recognize to be J(θ), our original least-squares cost function.

To summarize: Under the previous probabilistic assumptions on the data,

least-squares regression corresponds to ﬁnding the maximum likelihood esti-

mate of θ. This is thus one set of assumptions under which least-squares re-

gression can be justiﬁed as a very natural method that’s just doing maximum

likelihood estimation. (Note however that the probabilistic assumptions are

by no means necessary for least-squares to be a perfectly good and rational

procedure, and there may—and indeed there are—other natural assumptions

that can also be used to justify it.)

Note also that, in our previous discussion, our ﬁnal choice of θ did not

depend on what was σ2, and indeed we’d have arrived at the same result

even if σ2 were unknown. We will use this fact again later, when we talk

about the exponential family and generalized linear models.

1.4

Locally weighted linear regression (optional

reading)

Consider the problem of predicting y from x ∈ R. The leftmost ﬁgure below

shows the result of ﬁtting a y = θ0 + θ1x to a dataset. We see that the data

doesn’t really lie on straight line, and so the ﬁt is not very good.


18

0

1

2

3

4

5

6

7

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

x

y

0

1

2

3

4

5

6

7

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

x

y

0

1

2

3

4

5

6

7

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

x

y

Instead, if we had added an extra feature x2, and ﬁt y = θ0 + θ1x + θ2x2,

then we obtain a slightly better ﬁt to the data. (See middle ﬁgure) Naively, it

might seem that the more features we add, the better. However, there is also

a danger in adding too many features: The rightmost ﬁgure is the result of

ﬁtting a 5-th order polynomial y = �5

j=0 θjxj. We see that even though the

ﬁtted curve passes through the data perfectly, we would not expect this to

be a very good predictor of, say, housing prices (y) for diﬀerent living areas

(x). Without formally deﬁning what these terms mean, we’ll say the ﬁgure

on the left shows an instance of underﬁtting—in which the data clearly

shows structure not captured by the model—and the ﬁgure on the right is

an example of overﬁtting. (Later in this class, when we talk about learning

theory we’ll formalize some of these notions, and also deﬁne more carefully

just what it means for a hypothesis to be good or bad.)

As discussed previously, and as shown in the example above, the choice of

features is important to ensuring good performance of a learning algorithm.

(When we talk about model selection, we’ll also see algorithms for automat-

ically choosing a good set of features.) In this section, let us brieﬂy talk

about the locally weighted linear regression (LWR) algorithm which, assum-

ing there is suﬃcient training data, makes the choice of features less critical.

This treatment will be brief, since you’ll get a chance to explore some of the

properties of the LWR algorithm yourself in the homework.

In the original linear regression algorithm, to make a prediction at a query

point x (i.e., to evaluate h(x)), we would:

1. Fit θ to minimize �

i(y(i) − θTx(i))2.

2. Output θTx.

In contrast, the locally weighted linear regression algorithm does the fol-

lowing:

1. Fit θ to minimize �

i w(i)(y(i) − θTx(i))2.

2. Output θTx.


19

Here, the w(i)’s are non-negative valued weights. Intuitively, if w(i) is large

for a particular value of i, then in picking θ, we’ll try hard to make (y(i) −

θTx(i))2 small. If w(i) is small, then the (y(i) − θTx(i))2 error term will be

pretty much ignored in the ﬁt.

A fairly standard choice for the weights is4

w(i) = exp

�

−(x(i) − x)2

2τ 2

�

Note that the weights depend on the particular point x at which we’re trying

to evaluate x. Moreover, if |x(i) − x| is small, then w(i) is close to 1; and

if |x(i) − x| is large, then w(i) is small. Hence, θ is chosen giving a much

higher “weight” to the (errors on) training examples close to the query point

x. (Note also that while the formula for the weights takes a form that is

cosmetically similar to the density of a Gaussian distribution, the w(i)’s do

not directly have anything to do with Gaussians, and in particular the w(i)

are not random variables, normally distributed or otherwise.) The parameter

τ controls how quickly the weight of a training example falls oﬀ with distance

of its x(i) from the query point x; τ is called the bandwidth parameter, and

is also something that you’ll get to experiment with in your homework.

Locally weighted linear regression is the ﬁrst example we’re seeing of a

non-parametric algorithm. The (unweighted) linear regression algorithm

that we saw earlier is known as a parametric learning algorithm, because

it has a ﬁxed, ﬁnite number of parameters (the θi’s), which are ﬁt to the

data. Once we’ve ﬁt the θi’s and stored them away, we no longer need to

keep the training data around to make future predictions. In contrast, to

make predictions using locally weighted linear regression, we need to keep

the entire training set around. The term “non-parametric” (roughly) refers

to the fact that the amount of stuﬀ we need to keep in order to represent the

hypothesis h grows linearly with the size of the training set.

4If x is vector-valued, this is generalized to be w(i) = exp(−(x(i) −x)T (x(i) −x)/(2τ 2)),

or w(i) = exp(−(x(i) − x)T Σ−1(x(i) − x)/(2τ 2)), for an appropriate choice of τ or Σ.


Chapter 2

Classiﬁcation and logistic

regression

Let’s now talk about the classiﬁcation problem. This is just like the regression

problem, except that the values y we now want to predict take on only

a small number of discrete values. For now, we will focus on the binary

classiﬁcation problem in which y can take on only two values, 0 and 1.

(Most of what we say here will also generalize to the multiple-class case.)

For instance, if we are trying to build a spam classiﬁer for email, then x(i)

may be some features of a piece of email, and y may be 1 if it is a piece

of spam mail, and 0 otherwise. 0 is also called the negative class, and 1

the positive class, and they are sometimes also denoted by the symbols “-”

and “+.” Given x(i), the corresponding y(i) is also called the label for the

training example.

2.1

Logistic regression

We could approach the classiﬁcation problem ignoring the fact that y is

discrete-valued, and use our old linear regression algorithm to try to predict

y given x.

However, it is easy to construct examples where this method

performs very poorly. Intuitively, it also doesn’t make sense for hθ(x) to take

values larger than 1 or smaller than 0 when we know that y ∈ {0, 1}.

To ﬁx this, let’s change the form for our hypotheses hθ(x). We will choose

hθ(x) = g(θTx) =

1

1 + e−θT x,

where

g(z) =

1

1 + e−z

20


21

is called the logistic function or the sigmoid function. Here is a plot

showing g(z):

−5

−4

−3

−2

−1

0

1

2

3

4

5

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

z

g(z)

Notice that g(z) tends towards 1 as z → ∞, and g(z) tends towards 0 as

z → −∞. Moreover, g(z), and hence also h(x), is always bounded between

0 and 1. As before, we are keeping the convention of letting x0 = 1, so that

θTx = θ0 + �d

j=1 θjxj.

For now, let’s take the choice of g as given. Other functions that smoothly

increase from 0 to 1 can also be used, but for a couple of reasons that we’ll see

later (when we talk about GLMs, and when we talk about generative learning

algorithms), the choice of the logistic function is a fairly natural one. Before

moving on, here’s a useful property of the derivative of the sigmoid function,

which we write as g′:

g′(z)

=

d

dz

1

1 + e−z

=

1

(1 + e−z)2

�

e−z�

=

1

(1 + e−z) ·

�

1 −

1

(1 + e−z)

�

=

g(z)(1 − g(z)).

So, given the logistic regression model, how do we ﬁt θ for it? Following

how we saw least squares regression could be derived as the maximum like-

lihood estimator under a set of assumptions, let’s endow our classiﬁcation

model with a set of probabilistic assumptions, and then ﬁt the parameters

via maximum likelihood.


22

Let us assume that

P(y = 1 | x; θ)

=

hθ(x)

P(y = 0 | x; θ)

=

1 − hθ(x)

Note that this can be written more compactly as

p(y | x; θ) = (hθ(x))y (1 − hθ(x))1−y

Assuming that the n training examples were generated independently, we

can then write down the likelihood of the parameters as

L(θ)

=

p(⃗y | X; θ)

=

n

�

i=1

p(y(i) | x(i); θ)

=

n

�

i=1

�

hθ(x(i))

�y(i) �

1 − hθ(x(i))

�1−y(i)

As before, it will be easier to maximize the log likelihood:

ℓ(θ)

=

log L(θ)

=

n

�

i=1

y(i) log h(x(i)) + (1 − y(i)) log(1 − h(x(i)))

How do we maximize the likelihood? Similar to our derivation in the case

of linear regression, we can use gradient ascent. Written in vectorial notation,

our updates will therefore be given by θ := θ + α∇θℓ(θ). (Note the positive

rather than negative sign in the update formula, since we’re maximizing,

rather than minimizing, a function now.) Let’s start by working with just

one training example (x, y), and take derivatives to derive the stochastic

gradient ascent rule:

∂

∂θj

ℓ(θ)

=

�

y

1

g(θTx) − (1 − y)

1

1 − g(θTx)

� ∂

∂θj

g(θTx)

=

�

y

1

g(θTx) − (1 − y)

1

1 − g(θTx)

�

g(θTx)(1 − g(θTx)) ∂

∂θj

θTx

=

�

y(1 − g(θTx)) − (1 − y)g(θTx)

�

xj

=

(y − hθ(x)) xj


23

Above, we used the fact that g′(z) = g(z)(1 − g(z)). This therefore gives us

the stochastic gradient ascent rule

θj := θj + α

�

y(i) − hθ(x(i))

�

x(i)

j

If we compare this to the LMS update rule, we see that it looks identical; but

this is not the same algorithm, because hθ(x(i)) is now deﬁned as a non-linear

function of θTx(i). Nonetheless, it’s a little surprising that we end up with

the same update rule for a rather diﬀerent algorithm and learning problem.

Is this coincidence, or is there a deeper reason behind this? We’ll answer this

when we get to GLM models.

2.2

Digression: the perceptron learning algo-

rithn

We now digress to talk brieﬂy about an algorithm that’s of some historical

interest, and that we will also return to later when we talk about learning

theory. Consider modifying the logistic regression method to “force” it to

output values that are either 0 or 1 or exactly. To do so, it seems natural to

change the deﬁnition of g to be the threshold function:

g(z) =

� 1

if z ≥ 0

0

if z &lt; 0

If we then let hθ(x) = g(θTx) as before but using this modiﬁed deﬁnition of

g, and if we use the update rule

θj := θj + α

�

y(i) − hθ(x(i))

�

x(i)

j .

then we have the perceptron learning algorithn.

In the 1960s, this “perceptron” was argued to be a rough model for how

individual neurons in the brain work. Given how simple the algorithm is, it

will also provide a starting point for our analysis when we talk about learning

theory later in this class. Note however that even though the perceptron may

be cosmetically similar to the other algorithms we talked about, it is actually

a very diﬀerent type of algorithm than logistic regression and least squares

linear regression; in particular, it is diﬃcult to endow the perceptron’s predic-

tions with meaningful probabilistic interpretations, or derive the perceptron

as a maximum likelihood estimation algorithm.


24

1

1.5

2

2.5

3

3.5

4

4.5

5

−10

0

10

20

30

40

50

60

x

f(x)

1

1.5

2

2.5

3

3.5

4

4.5

5

−10

0

10

20

30

40

50

60

x

f(x)

1

1.5

2

2.5

3

3.5

4

4.5

5

−10

0

10

20

30

40

50

60

x

f(x)

2.3

Another algorithm for maximizing ℓ(θ)

Returning to logistic regression with g(z) being the sigmoid function, let’s

now talk about a diﬀerent algorithm for maximizing ℓ(θ).

To get us started, let’s consider Newton’s method for ﬁnding a zero of a

function. Speciﬁcally, suppose we have some function f : R �→ R, and we

wish to ﬁnd a value of θ so that f(θ) = 0. Here, θ ∈ R is a real number.

Newton’s method performs the following update:

θ := θ − f(θ)

f ′(θ).

This method has a natural interpretation in which we can think of it as

approximating the function f via a linear function that is tangent to f at

the current guess θ, solving for where that linear function equals to zero, and

letting the next guess for θ be where that linear function is zero.

Here’s a picture of the Newton’s method in action:

In the leftmost ﬁgure, we see the function f plotted along with the line

y = 0. We’re trying to ﬁnd θ so that f(θ) = 0; the value of θ that achieves this

is about 1.3. Suppose we initialized the algorithm with θ = 4.5. Newton’s

method then ﬁts a straight line tangent to f at θ = 4.5, and solves for the

where that line evaluates to 0. (Middle ﬁgure.) This give us the next guess

for θ, which is about 2.8. The rightmost ﬁgure shows the result of running

one more iteration, which the updates θ to about 1.8. After a few more

iterations, we rapidly approach θ = 1.3.

Newton’s method gives a way of getting to f(θ) = 0. What if we want to

use it to maximize some function ℓ? The maxima of ℓ correspond to points

where its ﬁrst derivative ℓ′(θ) is zero. So, by letting f(θ) = ℓ′(θ), we can use

the same algorithm to maximize ℓ, and we obtain update rule:

θ := θ − ℓ′(θ)

ℓ′′(θ).

(Something to think about: How would this change if we wanted to use

Newton’s method to minimize rather than maximize a function?)


25

Lastly, in our logistic regression setting, θ is vector-valued, so we need to

generalize Newton’s method to this setting. The generalization of Newton’s

method to this multidimensional setting (also called the Newton-Raphson

method) is given by

θ := θ − H−1∇θℓ(θ).

Here, ∇θℓ(θ) is, as usual, the vector of partial derivatives of ℓ(θ) with respect

to the θi’s; and H is an d-by-d matrix (actually, d+1−by−d+1, assuming that

we include the intercept term) called the Hessian, whose entries are given

by

Hij = ∂2ℓ(θ)

∂θi∂θj

.

Newton’s method typically enjoys faster convergence than (batch) gra-

dient descent, and requires many fewer iterations to get very close to the

minimum. One iteration of Newton’s can, however, be more expensive than

one iteration of gradient descent, since it requires ﬁnding and inverting an

d-by-d Hessian; but so long as d is not too large, it is usually much faster

overall. When Newton’s method is applied to maximize the logistic regres-

sion log likelihood function ℓ(θ), the resulting method is also called Fisher

scoring.


Chapter 3

Generalized linear models

So far, we’ve seen a regression example, and a classiﬁcation example. In the

regression example, we had y|x; θ ∼ N(µ, σ2), and in the classiﬁcation one,

y|x; θ ∼ Bernoulli(φ), for some appropriate deﬁnitions of µ and φ as functions

of x and θ. In this section, we will show that both of these methods are

special cases of a broader family of models, called Generalized Linear Models

(GLMs).1 We will also show how other models in the GLM family can be

derived and applied to other classiﬁcation and regression problems.

3.1

The exponential family

To work our way up to GLMs, we will begin by deﬁning exponential family

distributions. We say that a class of distributions is in the exponential family

if it can be written in the form

p(y; η) = b(y) exp(ηTT(y) − a(η))

(3.1)

Here, η is called the natural parameter (also called the canonical param-

eter) of the distribution; T(y) is the suﬃcient statistic (for the distribu-

tions we consider, it will often be the case that T(y) = y); and a(η) is the log

partition function. The quantity e−a(η) essentially plays the role of a nor-

malization constant, that makes sure the distribution p(y; η) sums/integrates

over y to 1.

A ﬁxed choice of T, a and b deﬁnes a family (or set) of distributions that

is parameterized by η; as we vary η, we then get diﬀerent distributions within

this family.

1The presentation of the material in this section takes inspiration from Michael I.

Jordan, Learning in graphical models (unpublished book draft), and also McCullagh and

Nelder, Generalized Linear Models (2nd ed.).

26


27

We now show that the Bernoulli and the Gaussian distributions are ex-

amples of exponential family distributions. The Bernoulli distribution with

mean φ, written Bernoulli(φ), speciﬁes a distribution over y ∈ {0, 1}, so that

p(y = 1; φ) = φ; p(y = 0; φ) = 1 − φ. As we vary φ, we obtain Bernoulli

distributions with diﬀerent means. We now show that this class of Bernoulli

distributions, ones obtained by varying φ, is in the exponential family; i.e.,

that there is a choice of T, a and b so that Equation (3.1) becomes exactly

the class of Bernoulli distributions.

We write the Bernoulli distribution as:

p(y; φ)

=

φy(1 − φ)1−y

=

exp(y log φ + (1 − y) log(1 − φ))

=

exp

��

log

�

φ

1 − φ

��

y + log(1 − φ)

�

.

Thus, the natural parameter is given by η = log(φ/(1 − φ)). Interestingly, if

we invert this deﬁnition for η by solving for φ in terms of η, we obtain φ =

1/(1 + e−η). This is the familiar sigmoid function! This will come up again

when we derive logistic regression as a GLM. To complete the formulation

of the Bernoulli distribution as an exponential family distribution, we also

have

T(y)

=

y

a(η)

=

− log(1 − φ)

=

log(1 + eη)

b(y)

=

1

This shows that the Bernoulli distribution can be written in the form of

Equation (3.1), using an appropriate choice of T, a and b.

Let’s now move on to consider the Gaussian distribution. Recall that,

when deriving linear regression, the value of σ2 had no eﬀect on our ﬁnal

choice of θ and hθ(x). Thus, we can choose an arbitrary value for σ2 without

changing anything. To simplify the derivation below, let’s set σ2 = 1.2 We

2If we leave σ2 as a variable, the Gaussian distribution can also be shown to be in the

exponential family, where η ∈ R2 is now a 2-dimension vector that depends on both µ and

σ. For the purposes of GLMs, however, the σ2 parameter can also be treated by considering

a more general deﬁnition of the exponential family: p(y; η, τ) = b(a, τ) exp((ηT T(y) −

a(η))/c(τ)). Here, τ is called the dispersion parameter, and for the Gaussian, c(τ) = σ2;

but given our simpliﬁcation above, we won’t need the more general deﬁnition for the

examples we will consider here.


28

then have:

p(y; µ)

=

1

√

2π exp

�

−1

2(y − µ)2

�

=

1

√

2π exp

�

−1

2y2

�

· exp

�

µy − 1

2µ2

�

Thus, we see that the Gaussian is in the exponential family, with

η

=

µ

T(y)

=

y

a(η)

=

µ2/2

=

η2/2

b(y)

=

(1/

√

2π) exp(−y2/2).

There’re many other distributions that are members of the exponen-

tial family: The multinomial (which we’ll see later), the Poisson (for mod-

elling count-data; also see the problem set); the gamma and the exponen-

tial (for modelling continuous, non-negative random variables, such as time-

intervals); the beta and the Dirichlet (for distributions over probabilities);

and many more. In the next section, we will describe a general “recipe”

for constructing models in which y (given x and θ) comes from any of these

distributions.

3.2

Constructing GLMs

Suppose you would like to build a model to estimate the number y of cus-

tomers arriving in your store (or number of page-views on your website) in

any given hour, based on certain features x such as store promotions, recent

advertising, weather, day-of-week, etc. We know that the Poisson distribu-

tion usually gives a good model for numbers of visitors. Knowing this, how

can we come up with a model for our problem? Fortunately, the Poisson is an

exponential family distribution, so we can apply a Generalized Linear Model

(GLM). In this section, we will we will describe a method for constructing

GLM models for problems such as these.

More generally, consider a classiﬁcation or regression problem where we

would like to predict the value of some random variable y as a function of

x.

To derive a GLM for this problem, we will make the following three

assumptions about the conditional distribution of y given x and about our

model:


29

1. y | x; θ ∼ ExponentialFamily(η). I.e., given x and θ, the distribution of

y follows some exponential family distribution, with parameter η.

2. Given x, our goal is to predict the expected value of T(y) given x.

In most of our examples, we will have T(y) = y, so this means we

would like the prediction h(x) output by our learned hypothesis h to

satisfy h(x) = E[y|x]. (Note that this assumption is satisﬁed in the

choices for hθ(x) for both logistic regression and linear regression. For

instance, in logistic regression, we had hθ(x) = p(y = 1|x; θ) = 0 · p(y =

0|x; θ) + 1 · p(y = 1|x; θ) = E[y|x; θ].)

3. The natural parameter η and the inputs x are related linearly: η = θTx.

(Or, if η is vector-valued, then ηi = θT

i x.)

The third of these assumptions might seem the least well justiﬁed of

the above, and it might be better thought of as a “design choice” in our

recipe for designing GLMs, rather than as an assumption per se.

These

three assumptions/design choices will allow us to derive a very elegant class

of learning algorithms, namely GLMs, that have many desirable properties

such as ease of learning. Furthermore, the resulting models are often very

eﬀective for modelling diﬀerent types of distributions over y; for example, we

will shortly show that both logistic regression and ordinary least squares can

both be derived as GLMs.

3.2.1

Ordinary least squares

To show that ordinary least squares is a special case of the GLM family

of models, consider the setting where the target variable y (also called the

response variable in GLM terminology) is continuous, and we model the

conditional distribution of y given x as a Gaussian N(µ, σ2). (Here, µ may

depend x.)

So, we let the ExponentialFamily(η) distribution above be

the Gaussian distribution. As we saw previously, in the formulation of the

Gaussian as an exponential family distribution, we had µ = η. So, we have

hθ(x)

=

E[y|x; θ]

=

µ

=

η

=

θTx.

The ﬁrst equality follows from Assumption 2, above; the second equality

follows from the fact that y|x; θ ∼ N(µ, σ2), and so its expected value is given


30

by µ; the third equality follows from Assumption 1 (and our earlier derivation

showing that µ = η in the formulation of the Gaussian as an exponential

family distribution); and the last equality follows from Assumption 3.

3.2.2

Logistic regression

We now consider logistic regression. Here we are interested in binary classiﬁ-

cation, so y ∈ {0, 1}. Given that y is binary-valued, it therefore seems natural

to choose the Bernoulli family of distributions to model the conditional dis-

tribution of y given x. In our formulation of the Bernoulli distribution as

an exponential family distribution, we had φ = 1/(1 + e−η). Furthermore,

note that if y|x; θ ∼ Bernoulli(φ), then E[y|x; θ] = φ. So, following a similar

derivation as the one for ordinary least squares, we get:

hθ(x)

=

E[y|x; θ]

=

φ

=

1/(1 + e−η)

=

1/(1 + e−θT x)

So, this gives us hypothesis functions of the form hθ(x) = 1/(1 + e−θT x). If

you are previously wondering how we came up with the form of the logistic

function 1/(1 + e−z), this gives one answer: Once we assume that y condi-

tioned on x is Bernoulli, it arises as a consequence of the deﬁnition of GLMs

and exponential family distributions.

To introduce a little more terminology, the function g giving the distri-

bution’s mean as a function of the natural parameter (g(η) = E[T(y); η])

is called the canonical response function. Its inverse, g−1, is called the

canonical link function. Thus, the canonical response function for the

Gaussian family is just the identify function; and the canonical response

function for the Bernoulli is the logistic function.3

3.2.3

Softmax regression

Let’s look at one more example of a GLM. Consider a classiﬁcation problem

in which the response variable y can take on any one of k values, so y ∈

{1, 2, . . . , k}. For example, rather than classifying email into the two classes

3Many texts use g to denote the link function, and g−1 to denote the response function;

but the notation we’re using here, inherited from the early machine learning literature,

will be more consistent with the notation used in the rest of the class.


31

spam or not-spam—which would have been a binary classiﬁcation problem—

we might want to classify it into three classes, such as spam, personal mail,

and work-related mail. The response variable is still discrete, but can now

take on more than two values. We will thus model it as distributed according

to a multinomial distribution.

Let’s derive a GLM for modelling this type of multinomial data. To do

so, we will begin by expressing the multinomial as an exponential family

distribution.

To parameterize a multinomial over k possible outcomes, one could use

k parameters φ1, . . . , φk specifying the probability of each of the outcomes.

However, these parameters would be redundant, or more formally, they would

not be independent (since knowing any k − 1 of the φi’s uniquely determines

the last one, as they must satisfy �k

i=1 φi = 1).

So, we will instead pa-

rameterize the multinomial with only k − 1 parameters, φ1, . . . , φk−1, where

φi = p(y = i; φ), and p(y = k; φ) = 1 − �k−1

i=1 φi. For notational convenience,

we will also let φk = 1 − �k−1

i=1 φi, but we should keep in mind that this is

not a parameter, and that it is fully speciﬁed by φ1, . . . , φk−1.

To express the multinomial as an exponential family distribution, we will

deﬁne T(y) ∈ Rk−1 as follows:

T(1) =





1

0

0

...

0





, T(2) =





0

1

0

...

0





, T(3) =





0

0

1

...

0





, · · · , T(k−1) =





0

0

0

...

1





, T(k) =





0

0

0

...

0





,

Unlike our previous examples, here we do not have T(y) = y; also, T(y) is

now a k − 1 dimensional vector, rather than a real number. We will write

(T(y))i to denote the i-th element of the vector T(y).

We introduce one more very useful piece of notation. An indicator func-

tion 1{·} takes on a value of 1 if its argument is true, and 0 otherwise

(1{True} = 1, 1{False} = 0).

For example, 1{2 = 3} = 0, and 1{3 =

5 − 2} = 1. So, we can also write the relationship between T(y) and y as

(T(y))i = 1{y = i}. (Before you continue reading, please make sure you un-

derstand why this is true!) Further, we have that E[(T(y))i] = P(y = i) = φi.

We are now ready to show that the multinomial is a member of the


32

exponential family. We have:

p(y; φ)

=

φ1{y=1}

1

φ1{y=2}

2

· · · φ1{y=k}

k

=

φ1{y=1}

1

φ1{y=2}

2

· · · φ

1−�k−1

i=1 1{y=i}

k

=

φ(T(y))1

1

φ(T(y))2

2

· · · φ

1−�k−1

i=1 (T(y))i

k

=

exp((T(y))1 log(φ1) + (T(y))2 log(φ2) +

· · · +

�

1 − �k−1

i=1 (T(y))i

�

log(φk))

=

exp((T(y))1 log(φ1/φk) + (T(y))2 log(φ2/φk) +

· · · + (T(y))k−1 log(φk−1/φk) + log(φk))

=

b(y) exp(ηTT(y) − a(η))

where

η

=





log(φ1/φk)

log(φ2/φk)

...

log(φk−1/φk)



 ,

a(η)

=

− log(φk)

b(y)

=

1.

This completes our formulation of the multinomial as an exponential family

distribution.

The link function is given (for i = 1, . . . , k) by

ηi = log φi

φk

.

For convenience, we have also deﬁned ηk = log(φk/φk) = 0. To invert the

link function and derive the response function, we therefore have that

eηi

=

φi

φk

φkeηi

=

φi

(3.2)

φk

k

�

i=1

eηi

=

k

�

i=1

φi = 1

This implies that φk = 1/ �k

i=1 eηi, which can be substituted back into Equa-

tion (3.2) to give the response function

φi =

eηi

�k

j=1 eηj


33

This function mapping from the η’s to the φ’s is called the softmax function.

To complete our model, we use Assumption 3, given earlier, that the ηi’s

are linearly related to the x’s. So, have ηi = θT

i x (for i = 1, . . . , k − 1),

where θ1, . . . , θk−1 ∈ Rd+1 are the parameters of our model. For notational

convenience, we can also deﬁne θk = 0, so that ηk = θT

k x = 0, as given

previously. Hence, our model assumes that the conditional distribution of y

given x is given by

p(y = i|x; θ)

=

φi

=

eηi

�k

j=1 eηj

=

eθT

i x

�k

j=1 eθT

j x

(3.3)

This model, which applies to classiﬁcation problems where y ∈ {1, . . . , k}, is

called softmax regression. It is a generalization of logistic regression.

Our hypothesis will output

hθ(x)

=

E[T(y)|x; θ]

=

E





1{y = 1}

1{y = 2}

...

1{y = k − 1}

���������

x; θ





=





φ1

φ2

...

φk−1





=





exp(θT

1 x)

�k

j=1 exp(θT

j x)

exp(θT

2 x)

�k

j=1 exp(θT

j x)

...

exp(θT

k−1x)

�k

j=1 exp(θT

j x)





.

In other words, our hypothesis will output the estimated probability that

p(y = i|x; θ), for every value of i = 1, . . . , k. (Even though hθ(x) as deﬁned

above is only k − 1 dimensional, clearly p(y = k|x; θ) can be obtained as

1 − �k−1

i=1 φi.)


34

Lastly, let’s discuss parameter ﬁtting. Similar to our original derivation

of ordinary least squares and logistic regression, if we have a training set of

n examples {(x(i), y(i)); i = 1, . . . , n} and would like to learn the parameters

θi of this model, we would begin by writing down the log-likelihood

ℓ(θ)

=

n

�

i=1

log p(y(i)|x(i); θ)

=

n

�

i=1

log

k

�

l=1

�

eθT

l x(i)

�k

j=1 eθT

j x(i)

�1{y(i)=l}

To obtain the second line above, we used the deﬁnition for p(y|x; θ) given

in Equation (3.3). We can now obtain the maximum likelihood estimate of

the parameters by maximizing ℓ(θ) in terms of θ, using a method such as

gradient ascent or Newton’s method.


Chapter 4

Generative learning algorithms

So far, we’ve mainly been talking about learning algorithms that model

p(y|x; θ), the conditional distribution of y given x.

For instance, logistic

regression modeled p(y|x; θ) as hθ(x) = g(θTx) where g is the sigmoid func-

tion. In these notes, we’ll talk about a diﬀerent type of learning algorithm.

Consider a classiﬁcation problem in which we want to learn to distinguish

between elephants (y = 1) and dogs (y = 0), based on some features of

an animal.

Given a training set, an algorithm like logistic regression or

the perceptron algorithm (basically) tries to ﬁnd a straight line—that is, a

decision boundary—that separates the elephants and dogs. Then, to classify

a new animal as either an elephant or a dog, it checks on which side of the

decision boundary it falls, and makes its prediction accordingly.

Here’s a diﬀerent approach. First, looking at elephants, we can build a

model of what elephants look like. Then, looking at dogs, we can build a

separate model of what dogs look like. Finally, to classify a new animal, we

can match the new animal against the elephant model, and match it against

the dog model, to see whether the new animal looks more like the elephants

or more like the dogs we had seen in the training set.

Algorithms that try to learn p(y|x) directly (such as logistic regression),

or algorithms that try to learn mappings directly from the space of inputs X

to the labels {0, 1}, (such as the perceptron algorithm) are called discrim-

inative learning algorithms. Here, we’ll talk about algorithms that instead

try to model p(x|y) (and p(y)).

These algorithms are called generative

learning algorithms. For instance, if y indicates whether an example is a

dog (0) or an elephant (1), then p(x|y = 0) models the distribution of dogs’

features, and p(x|y = 1) models the distribution of elephants’ features.

After modeling p(y) (called the class priors) and p(x|y), our algorithm

35


36

can then use Bayes rule to derive the posterior distribution on y given x:

p(y|x) = p(x|y)p(y)

p(x)

.

Here, the denominator is given by p(x) = p(x|y = 1)p(y = 1) + p(x|y =

0)p(y = 0) (you should be able to verify that this is true from the standard

properties of probabilities), and thus can also be expressed in terms of the

quantities p(x|y) and p(y) that we’ve learned. Actually, if were calculating

p(y|x) in order to make a prediction, then we don’t actually need to calculate

the denominator, since

arg max

y

p(y|x)

=

arg max

y

p(x|y)p(y)

p(x)

=

arg max

y

p(x|y)p(y).

4.1

Gaussian discriminant analysis

The ﬁrst generative learning algorithm that we’ll look at is Gaussian discrim-

inant analysis (GDA). In this model, we’ll assume that p(x|y) is distributed

according to a multivariate normal distribution. Let’s talk brieﬂy about the

properties of multivariate normal distributions before moving on to the GDA

model itself.

4.1.1

The multivariate normal distribution

The multivariate normal distribution in d-dimensions, also called the multi-

variate Gaussian distribution, is parameterized by a mean vector µ ∈ Rd

and a covariance matrix Σ ∈ Rd×d, where Σ ≥ 0 is symmetric and positive

semi-deﬁnite. Also written “N(µ, Σ)”, its density is given by:

p(x; µ, Σ) =

1

(2π)d/2|Σ|1/2 exp

�

−1

2(x − µ)TΣ−1(x − µ)

�

.

In the equation above, “|Σ|” denotes the determinant of the matrix Σ.

For a random variable X distributed N(µ, Σ), the mean is (unsurpris-

ingly) given by µ:

E[X] =

�

x

x p(x; µ, Σ)dx = µ

The covariance of a vector-valued random variable Z is deﬁned as Cov(Z) =

E[(Z − E[Z])(Z − E[Z])T]. This generalizes the notion of the variance of a


37

real-valued random variable. The covariance can also be deﬁned as Cov(Z) =

E[ZZT]−(E[Z])(E[Z])T. (You should be able to prove to yourself that these

two deﬁnitions are equivalent.) If X ∼ N(µ, Σ), then

Cov(X) = Σ.

Here are some examples of what the density of a Gaussian distribution

looks like:

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

0.05

0.1

0.15

0.2

0.25

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

0.05

0.1

0.15

0.2

0.25

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

0.05

0.1

0.15

0.2

0.25

The left-most ﬁgure shows a Gaussian with mean zero (that is, the 2x1

zero-vector) and covariance matrix Σ = I (the 2x2 identity matrix). A Gaus-

sian with zero mean and identity covariance is also called the standard nor-

mal distribution. The middle ﬁgure shows the density of a Gaussian with

zero mean and Σ = 0.6I; and in the rightmost ﬁgure shows one with , Σ = 2I.

We see that as Σ becomes larger, the Gaussian becomes more “spread-out,”

and as it becomes smaller, the distribution becomes more “compressed.”

Let’s look at some more examples.

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

0.05

0.1

0.15

0.2

0.25

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

0.05

0.1

0.15

0.2

0.25

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

0.05

0.1

0.15

0.2

0.25

The ﬁgures above show Gaussians with mean 0, and with covariance

matrices respectively

Σ =

� 1

0

0

1

�

; Σ =

�

1

0.5

0.5

1

�

; Σ =

�

1

0.8

0.8

1

�

.

The leftmost ﬁgure shows the familiar standard normal distribution, and we

see that as we increase the oﬀ-diagonal entry in Σ, the density becomes more


38

“compressed” towards the 45◦ line (given by x1 = x2). We can see this more

clearly when we look at the contours of the same three densities:

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

Here’s one last set of examples generated by varying Σ:

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

The plots above used, respectively,

Σ =

�

1

-0.5

-0.5

1

�

; Σ =

�

1

-0.8

-0.8

1

�

; Σ =

�

3

0.8

0.8

1

�

.

From the leftmost and middle ﬁgures, we see that by decreasing the oﬀ-

diagonal elements of the covariance matrix, the density now becomes “com-

pressed” again, but in the opposite direction. Lastly, as we vary the pa-

rameters, more generally the contours will form ellipses (the rightmost ﬁgure

showing an example).

As our last set of examples, ﬁxing Σ = I, by varying µ, we can also move

the mean of the density around.

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

0.05

0.1

0.15

0.2

0.25

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

0.05

0.1

0.15

0.2

0.25

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

0.05

0.1

0.15

0.2

0.25


39

The ﬁgures above were generated using Σ = I, and respectively

µ =

� 1

0

�

; µ =

� -0.5

0

�

; µ =

�

-1

-1.5

�

.

4.1.2

The Gaussian discriminant analysis model

When we have a classiﬁcation problem in which the input features x are

continuous-valued random variables, we can then use the Gaussian Discrim-

inant Analysis (GDA) model, which models p(x|y) using a multivariate nor-

mal distribution. The model is:

y

∼

Bernoulli(φ)

x|y = 0

∼

N(µ0, Σ)

x|y = 1

∼

N(µ1, Σ)

Writing out the distributions, this is:

p(y)

=

φy(1 − φ)1−y

p(x|y = 0)

=

1

(2π)d/2|Σ|1/2 exp

�

−1

2(x − µ0)TΣ−1(x − µ0)

�

p(x|y = 1)

=

1

(2π)d/2|Σ|1/2 exp

�

−1

2(x − µ1)TΣ−1(x − µ1)

�

Here, the parameters of our model are φ, Σ, µ0 and µ1. (Note that while

there’re two diﬀerent mean vectors µ0 and µ1, this model is usually applied

using only one covariance matrix Σ.) The log-likelihood of the data is given

by

ℓ(φ, µ0, µ1, Σ)

=

log

n

�

i=1

p(x(i), y(i); φ, µ0, µ1, Σ)

=

log

n

�

i=1

p(x(i)|y(i); µ0, µ1, Σ)p(y(i); φ).


40

By maximizing ℓ with respect to the parameters, we ﬁnd the maximum like-

lihood estimate of the parameters (see problem set 1) to be:

φ

=

1

n

n

�

i=1

1{y(i) = 1}

µ0

=

�n

i=1 1{y(i) = 0}x(i)

�n

i=1 1{y(i) = 0}

µ1

=

�n

i=1 1{y(i) = 1}x(i)

�n

i=1 1{y(i) = 1}

Σ

=

1

n

n

�

i=1

(x(i) − µy(i))(x(i) − µy(i))T.

Pictorially, what the algorithm is doing can be seen in as follows:

−2

−1

0

1

2

3

4

5

6

7

−7

−6

−5

−4

−3

−2

−1

0

1

Shown in the ﬁgure are the training set, as well as the contours of the

two Gaussian distributions that have been ﬁt to the data in each of the

two classes. Note that the two Gaussians have contours that are the same

shape and orientation, since they share a covariance matrix Σ, but they have

diﬀerent means µ0 and µ1.

Also shown in the ﬁgure is the straight line

giving the decision boundary at which p(y = 1|x) = 0.5. On one side of

the boundary, we’ll predict y = 1 to be the most likely outcome, and on the

other side, we’ll predict y = 0.


41

4.1.3

Discussion: GDA and logistic regression

The GDA model has an interesting relationship to logistic regression. If we

view the quantity p(y = 1|x; φ, µ0, µ1, Σ) as a function of x, we’ll ﬁnd that it

can be expressed in the form

p(y = 1|x; φ, Σ, µ0, µ1) =

1

1 + exp(−θTx),

where θ is some appropriate function of φ, Σ, µ0, µ1.1 This is exactly the form

that logistic regression—a discriminative algorithm—used to model p(y =

1|x).

When would we prefer one model over another? GDA and logistic regres-

sion will, in general, give diﬀerent decision boundaries when trained on the

same dataset. Which is better?

We just argued that if p(x|y) is multivariate gaussian (with shared Σ),

then p(y|x) necessarily follows a logistic function. The converse, however,

is not true; i.e., p(y|x) being a logistic function does not imply p(x|y) is

multivariate gaussian. This shows that GDA makes stronger modeling as-

sumptions about the data than does logistic regression. It turns out that

when these modeling assumptions are correct, then GDA will ﬁnd better ﬁts

to the data, and is a better model. Speciﬁcally, when p(x|y) is indeed gaus-

sian (with shared Σ), then GDA is asymptotically eﬃcient. Informally,

this means that in the limit of very large training sets (large n), there is no

algorithm that is strictly better than GDA (in terms of, say, how accurately

they estimate p(y|x)). In particular, it can be shown that in this setting,

GDA will be a better algorithm than logistic regression; and more generally,

even for small training set sizes, we would generally expect GDA to better.

In contrast, by making signiﬁcantly weaker assumptions, logistic regres-

sion is also more robust and less sensitive to incorrect modeling assumptions.

There are many diﬀerent sets of assumptions that would lead to p(y|x) taking

the form of a logistic function. For example, if x|y = 0 ∼ Poisson(λ0), and

x|y = 1 ∼ Poisson(λ1), then p(y|x) will be logistic. Logistic regression will

also work well on Poisson data like this. But if we were to use GDA on such

data—and ﬁt Gaussian distributions to such non-Gaussian data—then the

results will be less predictable, and GDA may (or may not) do well.

To summarize: GDA makes stronger modeling assumptions, and is more

data eﬃcient (i.e., requires less training data to learn “well”) when the mod-

eling assumptions are correct or at least approximately correct.

Logistic

1This uses the convention of redeﬁning the x(i)’s on the right-hand-side to be (d + 1)-

dimensional vectors by adding the extra coordinate x(i)

0

= 1; see problem set 1.


42

regression makes weaker assumptions, and is signiﬁcantly more robust to

deviations from modeling assumptions.

Speciﬁcally, when the data is in-

deed non-Gaussian, then in the limit of large datasets, logistic regression will

almost always do better than GDA. For this reason, in practice logistic re-

gression is used more often than GDA. (Some related considerations about

discriminative vs. generative models also apply for the Naive Bayes algo-

rithm that we discuss next, but the Naive Bayes algorithm is still considered

a very good, and is certainly also a very popular, classiﬁcation algorithm.)

4.2

Naive bayes

In GDA, the feature vectors x were continuous, real-valued vectors. Let’s

now talk about a diﬀerent learning algorithm in which the xj’s are discrete-

valued.

For our motivating example, consider building an email spam ﬁlter using

machine learning. Here, we wish to classify messages according to whether

they are unsolicited commercial (spam) email, or non-spam email.

After

learning to do this, we can then have our mail reader automatically ﬁlter

out the spam messages and perhaps place them in a separate mail folder.

Classifying emails is one example of a broader set of problems called text

classiﬁcation.

Let’s say we have a training set (a set of emails labeled as spam or non-

spam).

We’ll begin our construction of our spam ﬁlter by specifying the

features xj used to represent an email.

We will represent an email via a feature vector whose length is equal to

the number of words in the dictionary. Speciﬁcally, if an email contains the

j-th word of the dictionary, then we will set xj = 1; otherwise, we let xj = 0.

For instance, the vector

x =





1

0

0

...

1

...

0





a

aardvark

aardwolf

...

buy

...

zygmurgy

is used to represent an email that contains the words “a” and “buy,” but not


43

“aardvark,” “aardwolf” or “zygmurgy.”2 The set of words encoded into the

feature vector is called the vocabulary, so the dimension of x is equal to

the size of the vocabulary.

Having chosen our feature vector, we now want to build a generative

model. So, we have to model p(x|y). But if we have, say, a vocabulary of

50000 words, then x ∈ {0, 1}50000 (x is a 50000-dimensional vector of 0’s and

1’s), and if we were to model x explicitly with a multinomial distribution over

the 250000 possible outcomes, then we’d end up with a (250000−1)-dimensional

parameter vector. This is clearly too many parameters.

To model p(x|y), we will therefore make a very strong assumption. We will

assume that the xi’s are conditionally independent given y. This assumption

is called the Naive Bayes (NB) assumption, and the resulting algorithm is

called the Naive Bayes classiﬁer. For instance, if y = 1 means spam email;

“buy” is word 2087 and “price” is word 39831; then we are assuming that if

I tell you y = 1 (that a particular piece of email is spam), then knowledge

of x2087 (knowledge of whether “buy” appears in the message) will have no

eﬀect on your beliefs about the value of x39831 (whether “price” appears).

More formally, this can be written p(x2087|y) = p(x2087|y, x39831). (Note that

this is not the same as saying that x2087 and x39831 are independent, which

would have been written “p(x2087) = p(x2087|x39831)”; rather, we are only

assuming that x2087 and x39831 are conditionally independent given y.)

We now have:

p(x1, . . . , x50000|y)

=

p(x1|y)p(x2|y, x1)p(x3|y, x1, x2) · · · p(x50000|y, x1, . . . , x49999)

=

p(x1|y)p(x2|y)p(x3|y) · · · p(x50000|y)

=

d

�

j=1

p(xj|y)

The ﬁrst equality simply follows from the usual properties of probabilities,

and the second equality used the NB assumption. We note that even though

2Actually, rather than looking through an English dictionary for the list of all English

words, in practice it is more common to look through our training set and encode in our

feature vector only the words that occur at least once there. Apart from reducing the

number of words modeled and hence reducing our computational and space requirements,

this also has the advantage of allowing us to model/include as a feature many words

that may appear in your email (such as “cs229”) but that you won’t ﬁnd in a dictionary.

Sometimes (as in the homework), we also exclude the very high frequency words (which

will be words like “the,” “of,” “and”; these high frequency, “content free” words are called

stop words) since they occur in so many documents and do little to indicate whether an

email is spam or non-spam.


44

the Naive Bayes assumption is an extremely strong assumptions, the resulting

algorithm works well on many problems.

Our model is parameterized by φj|y=1 = p(xj = 1|y = 1), φj|y=0 = p(xj =

1|y = 0), and φy = p(y = 1). As usual, given a training set {(x(i), y(i)); i =

1, . . . , n}, we can write down the joint likelihood of the data:

L(φy, φj|y=0, φj|y=1) =

n

�

i=1

p(x(i), y(i)).

Maximizing this with respect to φy, φj|y=0 and φj|y=1 gives the maximum

likelihood estimates:

φj|y=1

=

�n

i=1 1{x(i)

j

= 1 ∧ y(i) = 1}

�n

i=1 1{y(i) = 1}

φj|y=0

=

�n

i=1 1{x(i)

j

= 1 ∧ y(i) = 0}

�n

i=1 1{y(i) = 0}

φy

=

�n

i=1 1{y(i) = 1}

n

In the equations above, the “∧” symbol means “and.” The parameters have

a very natural interpretation. For instance, φj|y=1 is just the fraction of the

spam (y = 1) emails in which word j does appear.

Having ﬁt all these parameters, to make a prediction on a new example

with features x, we then simply calculate

p(y = 1|x)

=

p(x|y = 1)p(y = 1)

p(x)

=

��d

j=1 p(xj|y = 1)

�

p(y = 1)

��d

j=1 p(xj|y = 1)

�

p(y = 1) +

��d

j=1 p(xj|y = 0)

�

p(y = 0)

,

and pick whichever class has the higher posterior probability.

Lastly, we note that while we have developed the Naive Bayes algorithm

mainly for the case of problems where the features xj are binary-valued, the

generalization to where xj can take values in {1, 2, . . . , kj} is straightforward.

Here, we would simply model p(xj|y) as multinomial rather than as Bernoulli.

Indeed, even if some original input attribute (say, the living area of a house,

as in our earlier example) were continuous valued, it is quite common to

discretize it—that is, turn it into a small set of discrete values—and apply

Naive Bayes. For instance, if we use some feature xj to represent living area,

we might discretize the continuous values as follows:


45

Living area (sq. feet)

&lt; 400

400-800

800-1200

1200-1600

&gt;1600

xi

1

2

3

4

5

Thus, for a house with living area 890 square feet, we would set the value

of the corresponding feature xj to 3. We can then apply the Naive Bayes

algorithm, and model p(xj|y) with a multinomial distribution, as described

previously.

When the original, continuous-valued attributes are not well-

modeled by a multivariate normal distribution, discretizing the features and

using Naive Bayes (instead of GDA) will often result in a better classiﬁer.

4.2.1

Laplace smoothing

The Naive Bayes algorithm as we have described it will work fairly well

for many problems, but there is a simple change that makes it work much

better, especially for text classiﬁcation. Let’s brieﬂy discuss a problem with

the algorithm in its current form, and then talk about how we can ﬁx it.

Consider spam/email classiﬁcation, and let’s suppose that, we are in the

year of 20xx, after completing CS229 and having done excellent work on the

project, you decide around May 20xx to submit work you did to the NeurIPS

conference for publication.3 Because you end up discussing the conference

in your emails, you also start getting messages with the word “neurips”

in it. But this is your ﬁrst NeurIPS paper, and until this time, you had

not previously seen any emails containing the word “neurips”; in particular

“neurips” did not ever appear in your training set of spam/non-spam emails.

Assuming that “neurips” was the 35000th word in the dictionary, your Naive

Bayes spam ﬁlter therefore had picked its maximum likelihood estimates of

the parameters φ35000|y to be

φ35000|y=1

=

�n

i=1 1{x(i)

35000 = 1 ∧ y(i) = 1}

�n

i=1 1{y(i) = 1}

= 0

φ35000|y=0

=

�n

i=1 1{x(i)

35000 = 1 ∧ y(i) = 0}

�n

i=1 1{y(i) = 0}

= 0

I.e., because it has never seen “neurips” before in either spam or non-spam

training examples, it thinks the probability of seeing it in either type of email

is zero. Hence, when trying to decide if one of these messages containing

3NeurIPS is one of the top machine learning conferences. The deadline for submitting

a paper is typically in May-June.


46

“neurips” is spam, it calculates the class posterior probabilities, and obtains

p(y = 1|x)

=

�d

j=1 p(xj|y = 1)p(y = 1)

�d

j=1 p(xj|y = 1)p(y = 1) + �d

j=1 p(xj|y = 0)p(y = 0)

=

0

0.

This is because each of the terms “�d

j=1 p(xj|y)” includes a term p(x35000|y) =

0 that is multiplied into it. Hence, our algorithm obtains 0/0, and doesn’t

know how to make a prediction.

Stating the problem more broadly, it is statistically a bad idea to esti-

mate the probability of some event to be zero just because you haven’t seen

it before in your ﬁnite training set. Take the problem of estimating the mean

of a multinomial random variable z taking values in {1, . . . , k}. We can pa-

rameterize our multinomial with φj = p(z = j). Given a set of n independent

observations {z(1), . . . , z(n)}, the maximum likelihood estimates are given by

φj =

�n

i=1 1{z(i) = j}

n

.

As we saw previously, if we were to use these maximum likelihood estimates,

then some of the φj’s might end up as zero, which was a problem. To avoid

this, we can use Laplace smoothing, which replaces the above estimate

with

φj = 1 + �n

i=1 1{z(i) = j}

k + n

.

Here, we’ve added 1 to the numerator, and k to the denominator. Note that

�k

j=1 φj = 1 still holds (check this yourself!), which is a desirable property

since the φj’s are estimates for probabilities that we know must sum to 1.

Also, φj ̸= 0 for all values of j, solving our problem of probabilities being

estimated as zero. Under certain (arguably quite strong) conditions, it can

be shown that the Laplace smoothing actually gives the optimal estimator

of the φj’s.

Returning to our Naive Bayes classiﬁer, with Laplace smoothing, we

therefore obtain the following estimates of the parameters:

φj|y=1

=

1 + �n

i=1 1{x(i)

j

= 1 ∧ y(i) = 1}

2 + �n

i=1 1{y(i) = 1}

φj|y=0

=

1 + �n

i=1 1{x(i)

j

= 1 ∧ y(i) = 0}

2 + �n

i=1 1{y(i) = 0}


47

(In practice, it usually doesn’t matter much whether we apply Laplace smooth-

ing to φy or not, since we will typically have a fair fraction each of spam and

non-spam messages, so φy will be a reasonable estimate of p(y = 1) and will

be quite far from 0 anyway.)

4.2.2

Event models for text classiﬁcation

To close oﬀ our discussion of generative learning algorithms, let’s talk about

one more model that is speciﬁcally for text classiﬁcation. While Naive Bayes

as we’ve presented it will work well for many classiﬁcation problems, for text

classiﬁcation, there is a related model that does even better.

In the speciﬁc context of text classiﬁcation, Naive Bayes as presented uses

the what’s called the Bernoulli event model (or sometimes multi-variate

Bernoulli event model). In this model, we assumed that the way an email

is generated is that ﬁrst it is randomly determined (according to the class

priors p(y)) whether a spammer or non-spammer will send you your next

message. Then, the person sending the email runs through the dictionary,

deciding whether to include each word j in that email independently and

according to the probabilities p(xj = 1|y) = φj|y. Thus, the probability of a

message was given by p(y) �d

j=1 p(xj|y).

Here’s a diﬀerent model, called the Multinomial event model.

To

describe this model, we will use a diﬀerent notation and set of features for

representing emails. We let xj denote the identity of the j-th word in the

email. Thus, xj is now an integer taking values in {1, . . . , |V |}, where |V |

is the size of our vocabulary (dictionary). An email of d words is now rep-

resented by a vector (x1, x2, . . . , xd) of length d; note that d can vary for

diﬀerent documents. For instance, if an email starts with “A NeurIPS . . . ,”

then x1 = 1 (“a” is the ﬁrst word in the dictionary), and x2 = 35000 (if

“neurips” is the 35000th word in the dictionary).

In the multinomial event model, we assume that the way an email is

generated is via a random process in which spam/non-spam is ﬁrst deter-

mined (according to p(y)) as before. Then, the sender of the email writes the

email by ﬁrst generating x1 from some multinomial distribution over words

(p(x1|y)). Next, the second word x2 is chosen independently of x1 but from

the same multinomial distribution, and similarly for x3, x4, and so on, until

all d words of the email have been generated. Thus, the overall probability of

a message is given by p(y) �d

j=1 p(xj|y). Note that this formula looks like the

one we had earlier for the probability of a message under the Bernoulli event

model, but that the terms in the formula now mean very diﬀerent things. In

particular xj|y is now a multinomial, rather than a Bernoulli distribution.


48

The parameters for our new model are φy = p(y) as before, φk|y=1 =

p(xj = k|y = 1) (for any j) and φk|y=0 = p(xj = k|y = 0). Note that we have

assumed that p(xj|y) is the same for all values of j (i.e., that the distribution

according to which a word is generated does not depend on its position j

within the email).

If we are given a training set {(x(i), y(i)); i = 1, . . . , n} where x(i) =

(x(i)

1 , x(i)

2 , . . . , x(i)

di ) (here, di is the number of words in the i-training example),

the likelihood of the data is given by

L(φy, φk|y=0, φk|y=1)

=

n

�

i=1

p(x(i), y(i))

=

n

�

i=1

� di

�

j=1

p(x(i)

j |y; φk|y=0, φk|y=1)

�

p(y(i); φy).

Maximizing this yields the maximum likelihood estimates of the parameters:

φk|y=1

=

�n

i=1

�di

j=1 1{x(i)

j

= k ∧ y(i) = 1}

�n

i=1 1{y(i) = 1}di

φk|y=0

=

�n

i=1

�di

j=1 1{x(i)

j

= k ∧ y(i) = 0}

�n

i=1 1{y(i) = 0}di

φy

=

�n

i=1 1{y(i) = 1}

n

.

If we were to apply Laplace smoothing (which is needed in practice for good

performance) when estimating φk|y=0 and φk|y=1, we add 1 to the numerators

and |V | to the denominators, and obtain:

φk|y=1

=

1 + �n

i=1

�di

j=1 1{x(i)

j

= k ∧ y(i) = 1}

|V | + �n

i=1 1{y(i) = 1}di

φk|y=0

=

1 + �n

i=1

�di

j=1 1{x(i)

j

= k ∧ y(i) = 0}

|V | + �n

i=1 1{y(i) = 0}di

.

While not necessarily the very best classiﬁcation algorithm, the Naive Bayes

classiﬁer often works surprisingly well. It is often also a very good “ﬁrst thing

to try,” given its simplicity and ease of implementation.


Chapter 5

Kernel methods

5.1

Feature maps

Recall that in our discussion about linear regression, we considered the prob-

lem of predicting the price of a house (denoted by y) from the living area of

the house (denoted by x), and we ﬁt a linear function of x to the training

data. What if the price y can be more accurately represented as a non-linear

function of x? In this case, we need a more expressive family of models than

linear models.

We start by considering ﬁtting cubic functions y = θ3x3 +θ2x2 +θ1x+θ0.

It turns out that we can view the cubic function as a linear function over

the a diﬀerent set of feature variables (deﬁned below). Concretely, let the

function φ : R → R4 be deﬁned as

φ(x) =





1

x

x2

x3



 ∈ R4.

(5.1)

Let θ ∈ R4 be the vector containing θ0, θ1, θ2, θ3 as entries. Then we can

rewrite the cubic function in x as:

θ3x3 + θ2x2 + θ1x + θ0 = θTφ(x)

Thus, a cubic function of the variable x can be viewed as a linear function

over the variables φ(x). To distinguish between these two sets of variables,

in the context of kernel methods, we will call the “original” input value the

input attributes of a problem (in this case, x, the living area). When the

49


50

original input is mapped to some new set of quantities φ(x), we will call those

new quantities the features variables. (Unfortunately, diﬀerent authors use

diﬀerent terms to describe these two things in diﬀerent contexts.) We will

call φ a feature map, which maps the attributes to the features.

5.2

LMS (least mean squares) with features

We will derive the gradient descent algorithm for ﬁtting the model θTφ(x).

First recall that for ordinary least square problem where we were to ﬁt θTx,

the batch gradient descent update is (see the ﬁrst lecture note for its deriva-

tion):

θ := θ + α

n

�

i=1

�

y(i) − hθ(x(i))

�

x(i)

:= θ + α

n

�

i=1

�

y(i) − θTx(i)�

x(i).

(5.2)

Let φ : Rd → Rp be a feature map that maps attribute x (in Rd) to the

features φ(x) in Rp. (In the motivating example in the previous subsection,

we have d = 1 and p = 4.) Now our goal is to ﬁt the function θTφ(x), with

θ being a vector in Rp instead of Rd. We can replace all the occurrences of

x(i) in the algorithm above by φ(x(i)) to obtain the new update:

θ := θ + α

n

�

i=1

�

y(i) − θTφ(x(i))

�

φ(x(i))

(5.3)

Similarly, the corresponding stochastic gradient descent update rule is

θ := θ + α

�

y(i) − θTφ(x(i))

�

φ(x(i))

(5.4)

5.3

LMS with the kernel trick

The gradient descent update, or stochastic gradient update above becomes

computationally expensive when the features φ(x) is high-dimensional. For

example, consider the direct extension of the feature map in equation (5.1)

to high-dimensional input x: suppose x ∈ Rd, and let φ(x) be the vector that


51

contains all the monomials of x with degree ≤ 3

φ(x) =





1

x1

x2

...

x2

1

x1x2

x1x3

...

x2x1

...

x3

1

x2

1x2

...





.

(5.5)

The dimension of the features φ(x) is on the order of d3.1 This is a pro-

hibitively long vector for computational purpose — when d = 1000, each

update requires at least computing and storing a 10003 = 109 dimensional

vector, which is 106 times slower than the update rule for for ordinary least

squares updates (5.2).

It may appear at ﬁrst that such d3 runtime per update and memory usage

are inevitable, because the vector θ itself is of dimension p ≈ d3, and we may

need to update every entry of θ and store it. However, we will introduce the

kernel trick with which we will not need to store θ explicitly, and the runtime

can be signiﬁcantly improved.

For simplicity, we assume the initialize the value θ = 0, and we focus

on the iterative update (5.3). The main observation is that at any time, θ

can be represented as a linear combination of the vectors φ(x(1)), . . . , φ(x(n)).

Indeed, we can show this inductively as follows. At initialization, θ = 0 =

�n

i=1 0 · φ(x(i)). Assume at some point, θ can be represented as

θ =

n

�

i=1

βiφ(x(i))

(5.6)

1Here, for simplicity, we include all the monomials with repetitions (so that, e.g., x1x2x3

and x2x3x1 both appear in φ(x)). Therefore, there are totally 1 + d + d2 + d3 entries in

φ(x).


52

for some β1, . . . , βn ∈ R. Then we claim that in the next round, θ is still a

linear combination of φ(x(1)), . . . , φ(x(n)) because

θ := θ + α

n

�

i=1

�

y(i) − θTφ(x(i))

�

φ(x(i))

=

n

�

i=1

βiφ(x(i)) + α

n

�

i=1

�

y(i) − θTφ(x(i))

�

φ(x(i))

=

n

�

i=1

(βi + α

�

y(i) − θTφ(x(i))

�

)

�

��

�

new βi

φ(x(i))

(5.7)

You may realize that our general strategy is to implicitly represent the p-

dimensional vector θ by a set of coeﬃcients β1, . . . , βn. Towards doing this,

we derive the update rule of the coeﬃcients β1, . . . , βn. Using the equation

above, we see that the new βi depends on the old one via

βi := βi + α

�

y(i) − θTφ(x(i))

�

(5.8)

Here we still have the old θ on the RHS of the equation. Replacing θ by

θ = �n

j=1 βjφ(x(j)) gives

∀i ∈ {1, . . . , n}, βi := βi + α

�

y(i) −

n

�

j=1

βjφ(x(j))

Tφ(x(i))

�

We often rewrite φ(x(j))

Tφ(x(i)) as ⟨φ(x(j)), φ(x(i))⟩ to emphasize that it’s the

inner product of the two feature vectors. Viewing βi’s as the new representa-

tion of θ, we have successfully translated the batch gradient descent algorithm

into an algorithm that updates the value of β iteratively. It may appear that

at every iteration, we still need to compute the values of ⟨φ(x(j)), φ(x(i))⟩ for

all pairs of i, j, each of which may take roughly O(p) operation. However,

two important properties come to rescue:

1. We can pre-compute the pairwise inner products ⟨φ(x(j)), φ(x(i))⟩ for all

pairs of i, j before the loop starts.

2. For the feature map φ deﬁned in (5.5) (or many other interesting fea-

ture maps), computing ⟨φ(x(j)), φ(x(i))⟩ can be eﬃcient and does not


53

necessarily require computing φ(x(i)) explicitly. This is because:

⟨φ(x), φ(z)⟩ = 1 +

d

�

i=1

xizi +

�

i,j∈{1,...,d}

xixjzizj +

�

i,j,k∈{1,...,d}

xixjxkzizjzk

= 1 +

d

�

i=1

xizi +

�

d

�

i=1

xizi

�2

+

�

d

�

i=1

xizi

�3

= 1 + ⟨x, z⟩ + ⟨x, z⟩2 + ⟨x, z⟩3

(5.9)

Therefore, to compute ⟨φ(x), φ(z)⟩, we can ﬁrst compute ⟨x, z⟩ with

O(d) time and then take another constant number of operations to com-

pute 1 + ⟨x, z⟩ + ⟨x, z⟩2 + ⟨x, z⟩3.

As you will see, the inner products between the features ⟨φ(x), φ(z)⟩ are

essential here. We deﬁne the Kernel corresponding to the feature map φ as

a function that maps X × X → R satisfying: 2

K(x, z) ≜ ⟨φ(x), φ(z)⟩

(5.10)

To wrap up the discussion, we write the down the ﬁnal algorithm as

follows:

1. Compute all the values K(x(i), x(j)) ≜ ⟨φ(x(i)), φ(x(j))⟩ using equa-

tion (5.9) for all i, j ∈ {1, . . . , n}. Set β := 0.

2. Loop:

∀i ∈ {1, . . . , n}, βi := βi + α

�

y(i) −

n

�

j=1

βjK(x(i), x(j))

�

(5.11)

Or in vector notation, letting K be the n × n matrix with Kij =

K(x(i), x(j)), we have

β := β + α(⃗y − Kβ)

With the algorithm above, we can update the representation β of the

vector θ eﬃciently with O(n) time per update. Finally, we need to show that

2Recall that X is the space of the input x. In our running example, X = Rd


54

the knowledge of the representation β suﬃces to compute the prediction

θTφ(x). Indeed, we have

θTφ(x) =

n

�

i=1

βiφ(x(i))

Tφ(x) =

n

�

i=1

βiK(x(i), x)

(5.12)

You may realize that fundamentally all we need to know about the feature

map φ(·) is encapsulated in the corresponding kernel function K(·, ·). We

will expand on this in the next section.

5.4

Properties of kernels

In the last subsection, we started with an explicitly deﬁned feature map φ,

which induces the kernel function K(x, z) ≜ ⟨φ(x), φ(z)⟩. Then we saw that

the kernel function is so intrinsic so that as long as the kernel function is

deﬁned, the whole training algorithm can be written entirely in the language

of the kernel without referring to the feature map φ, so can the prediction of

a test example x (equation (5.12).)

Therefore, it would be tempted to deﬁne other kernel function K(·, ·) and

run the algorithm (5.11). Note that the algorithm (5.11) does not need to

explicitly access the feature map φ, and therefore we only need to ensure the

existence of the feature map φ, but do not necessarily need to be able to

explicitly write φ down.

What kinds of functions K(·, ·) can correspond to some feature map φ? In

other words, can we tell if there is some feature mapping φ so that K(x, z) =

φ(x)Tφ(z) for all x, z?

If we can answer this question by giving a precise characterization of valid

kernel functions, then we can completely change the interface of selecting

feature maps φ to the interface of selecting kernel function K. Concretely,

we can pick a function K, verify that it satisﬁes the characterization (so

that there exists a feature map φ that K corresponds to), and then we can

run update rule (5.11). The beneﬁt here is that we don’t have to be able

to compute φ or write it down analytically, and we only need to know its

existence. We will answer this question at the end of this subsection after

we go through several concrete examples of kernels.

Suppose x, z ∈ Rd, and let’s ﬁrst consider the function K(·, ·) deﬁned as:

K(x, z) = (xTz)2.


55

We can also write this as

K(x, z)

=

�

d

�

i=1

xizi

� �

d

�

j=1

xjzj

�

=

d

�

i=1

d

�

j=1

xixjzizj

=

d

�

i,j=1

(xixj)(zizj)

Thus, we see that K(x, z) = ⟨φ(x), φ(z)⟩ is the kernel function that corre-

sponds to the the feature mapping φ given (shown here for the case of d = 3)

by

φ(x) =





x1x1

x1x2

x1x3

x2x1

x2x2

x2x3

x3x1

x3x2

x3x3





.

Revisiting the computational eﬃciency perspective of kernel, note that whereas

calculating the high-dimensional φ(x) requires O(d2) time, ﬁnding K(x, z)

takes only O(d) time—linear in the dimension of the input attributes.

For another related example, also consider K(·, ·) deﬁned by

K(x, z)

=

(xTz + c)2

=

d

�

i,j=1

(xixj)(zizj) +

d

�

i=1

(

√

2cxi)(

√

2czi) + c2.

(Check this yourself.) This function K is a kernel function that corresponds


56

to the feature mapping (again shown for d = 3)

φ(x) =





x1x1

x1x2

x1x3

x2x1

x2x2

x2x3

x3x1

x3x2

x3x3

√

2cx1

√

2cx2

√

2cx3

c





,

and the parameter c controls the relative weighting between the xi (ﬁrst

order) and the xixj (second order) terms.

More broadly, the kernel K(x, z) = (xTz + c)k corresponds to a feature

mapping to an

�d+k

k

�

feature space, corresponding of all monomials of the

form xi1xi2 . . . xik that are up to order k. However, despite working in this

O(dk)-dimensional space, computing K(x, z) still takes only O(d) time, and

hence we never need to explicitly represent feature vectors in this very high

dimensional feature space.

Kernels as similarity metrics.

Now, let’s talk about a slightly diﬀerent

view of kernels. Intuitively, (and there are things wrong with this intuition,

but nevermind), if φ(x) and φ(z) are close together, then we might expect

K(x, z) = φ(x)Tφ(z) to be large. Conversely, if φ(x) and φ(z) are far apart—

say nearly orthogonal to each other—then K(x, z) = φ(x)Tφ(z) will be small.

So, we can think of K(x, z) as some measurement of how similar are φ(x)

and φ(z), or of how similar are x and z.

Given this intuition, suppose that for some learning problem that you’re

working on, you’ve come up with some function K(x, z) that you think might

be a reasonable measure of how similar x and z are. For instance, perhaps

you chose

K(x, z) = exp

�

−||x − z||2

2σ2

�

.

This is a reasonable measure of x and z’s similarity, and is close to 1 when

x and z are close, and near 0 when x and z are far apart. Does there exist


57

a feature map φ such that the kernel K deﬁned above satisﬁes K(x, z) =

φ(x)Tφ(z)? In this particular example, the answer is yes. This kernel is called

the Gaussian kernel, and corresponds to an inﬁnite dimensional feature

mapping φ. We will give a precise characterization about what properties

a function K needs to satisfy so that it can be a valid kernel function that

corresponds to some feature map φ.

Necessary conditions for valid kernels.

Suppose for now that K is

indeed a valid kernel corresponding to some feature mapping φ, and we will

ﬁrst see what properties it satisﬁes. Now, consider some ﬁnite set of n points

(not necessarily the training set) {x(1), . . . , x(n)}, and let a square, n-by-n

matrix K be deﬁned so that its (i, j)-entry is given by Kij = K(x(i), x(j)).

This matrix is called the kernel matrix. Note that we’ve overloaded the

notation and used K to denote both the kernel function K(x, z) and the

kernel matrix K, due to their obvious close relationship.

Now, if K is a valid kernel, then Kij = K(x(i), x(j)) = φ(x(i))Tφ(x(j)) =

φ(x(j))Tφ(x(i)) = K(x(j), x(i)) = Kji, and hence K must be symmetric. More-

over, letting φk(x) denote the k-th coordinate of the vector φ(x), we ﬁnd that

for any vector z, we have

zTKz

=

�

i

�

j

ziKijzj

=

�

i

�

j

ziφ(x(i))Tφ(x(j))zj

=

�

i

�

j

zi

�

k

φk(x(i))φk(x(j))zj

=

�

k

�

i

�

j

ziφk(x(i))φk(x(j))zj

=

�

k

��

i

ziφk(x(i))

�2

≥

0.

The second-to-last step uses the fact that �

i,j aiaj = (�

i ai)2 for ai =

ziφk(x(i)). Since z was arbitrary, this shows that K is positive semi-deﬁnite

(K ≥ 0).

Hence, we’ve shown that if K is a valid kernel (i.e., if it corresponds to

some feature mapping φ), then the corresponding kernel matrix K ∈ Rn×n

is symmetric positive semideﬁnite.


58

Suﬃcient conditions for valid kernels.

More generally, the condition

above turns out to be not only a necessary, but also a suﬃcient, condition

for K to be a valid kernel (also called a Mercer kernel). The following result

is due to Mercer.3

Theorem (Mercer).

Let K : Rd × Rd �→ R be given.

Then for K

to be a valid (Mercer) kernel, it is necessary and suﬃcient that for any

{x(1), . . . , x(n)}, (n &lt; ∞), the corresponding kernel matrix is symmetric pos-

itive semi-deﬁnite.

Given a function K, apart from trying to ﬁnd a feature mapping φ that

corresponds to it, this theorem therefore gives another way of testing if it is

a valid kernel. You’ll also have a chance to play with these ideas more in

problem set 2.

In class, we also brieﬂy talked about a couple of other examples of ker-

nels. For instance, consider the digit recognition problem, in which given

an image (16x16 pixels) of a handwritten digit (0-9), we have to ﬁgure out

which digit it was. Using either a simple polynomial kernel K(x, z) = (xTz)k

or the Gaussian kernel, SVMs were able to obtain extremely good perfor-

mance on this problem.

This was particularly surprising since the input

attributes x were just 256-dimensional vectors of the image pixel intensity

values, and the system had no prior knowledge about vision, or even about

which pixels are adjacent to which other ones. Another example that we

brieﬂy talked about in lecture was that if the objects x that we are trying

to classify are strings (say, x is a list of amino acids, which strung together

form a protein), then it seems hard to construct a reasonable, “small” set of

features for most learning algorithms, especially if diﬀerent strings have dif-

ferent lengths. However, consider letting φ(x) be a feature vector that counts

the number of occurrences of each length-k substring in x. If we’re consid-

ering strings of English letters, then there are 26k such strings. Hence, φ(x)

is a 26k dimensional vector; even for moderate values of k, this is probably

too big for us to eﬃciently work with. (e.g., 264 ≈ 460000.) However, using

(dynamic programming-ish) string matching algorithms, it is possible to ef-

ﬁciently compute K(x, z) = φ(x)Tφ(z), so that we can now implicitly work

in this 26k-dimensional feature space, but without ever explicitly computing

feature vectors in this space.

3Many texts present Mercer’s theorem in a slightly more complicated form involving

L2 functions, but when the input attributes take values in Rd, the version given here is

equivalent.


59

Application of kernel methods: We’ve seen the application of kernels

to linear regression. In the next part, we will introduce the support vector

machines to which kernels can be directly applied. dwell too much longer on

it here. In fact, the idea of kernels has signiﬁcantly broader applicability than

linear regression and SVMs. Speciﬁcally, if you have any learning algorithm

that you can write in terms of only inner products ⟨x, z⟩ between input

attribute vectors, then by replacing this with K(x, z) where K is a kernel,

you can “magically” allow your algorithm to work eﬃciently in the high

dimensional feature space corresponding to K. For instance, this kernel trick

can be applied with the perceptron to derive a kernel perceptron algorithm.

Many of the algorithms that we’ll see later in this class will also be amenable

to this method, which has come to be known as the “kernel trick.”


Chapter 6

Support vector machines

This set of notes presents the Support Vector Machine (SVM) learning al-

gorithm. SVMs are among the best (and many believe are indeed the best)

“oﬀ-the-shelf” supervised learning algorithms. To tell the SVM story, we’ll

need to ﬁrst talk about margins and the idea of separating data with a large

“gap.” Next, we’ll talk about the optimal margin classiﬁer, which will lead

us into a digression on Lagrange duality. We’ll also see kernels, which give

a way to apply SVMs eﬃciently in very high dimensional (such as inﬁnite-

dimensional) feature spaces, and ﬁnally, we’ll close oﬀ the story with the

SMO algorithm, which gives an eﬃcient implementation of SVMs.

6.1

Margins: intuition

We’ll start our story on SVMs by talking about margins. This section will

give the intuitions about margins and about the “conﬁdence” of our predic-

tions; these ideas will be made formal in Section 6.3.

Consider logistic regression, where the probability p(y = 1|x; θ) is mod-

eled by hθ(x) = g(θTx). We then predict “1” on an input x if and only if

hθ(x) ≥ 0.5, or equivalently, if and only if θTx ≥ 0. Consider a positive

training example (y = 1). The larger θTx is, the larger also is hθ(x) = p(y =

1|x; θ), and thus also the higher our degree of “conﬁdence” that the label is 1.

Thus, informally we can think of our prediction as being very conﬁdent that

y = 1 if θTx ≫ 0. Similarly, we think of logistic regression as conﬁdently

predicting y = 0, if θTx ≪ 0. Given a training set, again informally it seems

that we’d have found a good ﬁt to the training data if we can ﬁnd θ so that

θTx(i) ≫ 0 whenever y(i) = 1, and θTx(i) ≪ 0 whenever y(i) = 0, since this

would reﬂect a very conﬁdent (and correct) set of classiﬁcations for all the

60


61

training examples. This seems to be a nice goal to aim for, and we’ll soon

formalize this idea using the notion of functional margins.

For a diﬀerent type of intuition, consider the following ﬁgure, in which x’s

represent positive training examples, o’s denote negative training examples,

a decision boundary (this is the line given by the equation θTx = 0, and

is also called the separating hyperplane) is also shown, and three points

have also been labeled A, B and C.

�



�



�



B

A

C

Notice that the point A is very far from the decision boundary. If we are

asked to make a prediction for the value of y at A, it seems we should be

quite conﬁdent that y = 1 there. Conversely, the point C is very close to

the decision boundary, and while it’s on the side of the decision boundary

on which we would predict y = 1, it seems likely that just a small change to

the decision boundary could easily have caused out prediction to be y = 0.

Hence, we’re much more conﬁdent about our prediction at A than at C. The

point B lies in-between these two cases, and more broadly, we see that if

a point is far from the separating hyperplane, then we may be signiﬁcantly

more conﬁdent in our predictions. Again, informally we think it would be

nice if, given a training set, we manage to ﬁnd a decision boundary that

allows us to make all correct and conﬁdent (meaning far from the decision

boundary) predictions on the training examples. We’ll formalize this later

using the notion of geometric margins.


62

6.2

Notation (option reading)

To make our discussion of SVMs easier, we’ll ﬁrst need to introduce a new

notation for talking about classiﬁcation.

We will be considering a linear

classiﬁer for a binary classiﬁcation problem with labels y and features x.

From now, we’ll use y ∈ {−1, 1} (instead of {0, 1}) to denote the class labels.

Also, rather than parameterizing our linear classiﬁer with the vector θ, we

will use parameters w, b, and write our classiﬁer as

hw,b(x) = g(wTx + b).

Here, g(z) = 1 if z ≥ 0, and g(z) = −1 otherwise. This “w, b” notation

allows us to explicitly treat the intercept term b separately from the other

parameters. (We also drop the convention we had previously of letting x0 = 1

be an extra coordinate in the input feature vector.) Thus, b takes the role of

what was previously θ0, and w takes the role of [θ1 . . . θd]T.

Note also that, from our deﬁnition of g above, our classiﬁer will directly

predict either 1 or −1 (cf. the perceptron algorithm), without ﬁrst going

through the intermediate step of estimating p(y = 1) (which is what logistic

regression does).

6.3

Functional and geometric margins (op-

tion reading)

Let’s formalize the notions of the functional and geometric margins. Given a

training example (x(i), y(i)), we deﬁne the functional margin of (w, b) with

respect to the training example as

ˆγ(i) = y(i)(wTx(i) + b).

Note that if y(i) = 1, then for the functional margin to be large (i.e., for

our prediction to be conﬁdent and correct), we need wTx(i) + b to be a large

positive number. Conversely, if y(i) = −1, then for the functional margin

to be large, we need wTx(i) + b to be a large negative number. Moreover, if

y(i)(wTx(i) + b) &gt; 0, then our prediction on this example is correct. (Check

this yourself.) Hence, a large functional margin represents a conﬁdent and a

correct prediction.

For a linear classiﬁer with the choice of g given above (taking values in

{−1, 1}), there’s one property of the functional margin that makes it not a

very good measure of conﬁdence, however. Given our choice of g, we note that


63

if we replace w with 2w and b with 2b, then since g(wTx+b) = g(2wTx+2b),

this would not change hw,b(x) at all. I.e., g, and hence also hw,b(x), depends

only on the sign, but not on the magnitude, of wTx + b. However, replacing

(w, b) with (2w, 2b) also results in multiplying our functional margin by a

factor of 2. Thus, it seems that by exploiting our freedom to scale w and b,

we can make the functional margin arbitrarily large without really changing

anything meaningful. Intuitively, it might therefore make sense to impose

some sort of normalization condition such as that ||w||2 = 1; i.e., we might

replace (w, b) with (w/||w||2, b/||w||2), and instead consider the functional

margin of (w/||w||2, b/||w||2). We’ll come back to this later.

Given a training set S = {(x(i), y(i)); i = 1, . . . , n}, we also deﬁne the

function margin of (w, b) with respect to S as the smallest of the functional

margins of the individual training examples. Denoted by ˆγ, this can therefore

be written:

ˆγ = min

i=1,...,n ˆγ(i).

Next, let’s talk about geometric margins. Consider the picture below:

w

A

γ

B

(i)

The decision boundary corresponding to (w, b) is shown, along with the

vector w. Note that w is orthogonal (at 90◦) to the separating hyperplane.

(You should convince yourself that this must be the case.)

Consider the

point at A, which represents the input x(i) of some training example with

label y(i) = 1. Its distance to the decision boundary, γ(i), is given by the line

segment AB.

How can we ﬁnd the value of γ(i)? Well, w/||w|| is a unit-length vector

pointing in the same direction as w. Since A represents x(i), we therefore


64

ﬁnd that the point B is given by x(i) − γ(i) · w/||w||. But this point lies on

the decision boundary, and all points x on the decision boundary satisfy the

equation wTx + b = 0. Hence,

wT

�

x(i) − γ(i) w

||w||

�

+ b = 0.

Solving for γ(i) yields

γ(i) = wTx(i) + b

||w||

=

� w

||w||

�T

x(i) +

b

||w||.

This was worked out for the case of a positive training example at A in the

ﬁgure, where being on the “positive” side of the decision boundary is good.

More generally, we deﬁne the geometric margin of (w, b) with respect to a

training example (x(i), y(i)) to be

γ(i) = y(i)

�� w

||w||

�T

x(i) +

b

||w||

�

.

Note that if ||w|| = 1, then the functional margin equals the geometric

margin—this thus gives us a way of relating these two diﬀerent notions of

margin. Also, the geometric margin is invariant to rescaling of the parame-

ters; i.e., if we replace w with 2w and b with 2b, then the geometric margin

does not change. This will in fact come in handy later. Speciﬁcally, because

of this invariance to the scaling of the parameters, when trying to ﬁt w and b

to training data, we can impose an arbitrary scaling constraint on w without

changing anything important; for instance, we can demand that ||w|| = 1, or

|w1| = 5, or |w1 + b| + |w2| = 2, and any of these can be satisﬁed simply by

rescaling w and b.

Finally, given a training set S = {(x(i), y(i)); i = 1, . . . , n}, we also deﬁne

the geometric margin of (w, b) with respect to S to be the smallest of the

geometric margins on the individual training examples:

γ = min

i=1,...,n γ(i).

6.4

The optimal margin classiﬁer (option read-

ing)

Given a training set, it seems from our previous discussion that a natural

desideratum is to try to ﬁnd a decision boundary that maximizes the (ge-

ometric) margin, since this would reﬂect a very conﬁdent set of predictions


65

on the training set and a good “ﬁt” to the training data. Speciﬁcally, this

will result in a classiﬁer that separates the positive and the negative training

examples with a “gap” (geometric margin).

For now, we will assume that we are given a training set that is linearly

separable; i.e., that it is possible to separate the positive and negative ex-

amples using some separating hyperplane. How will we ﬁnd the one that

achieves the maximum geometric margin? We can pose the following opti-

mization problem:

maxγ,w,b

γ

s.t.

y(i)(wTx(i) + b) ≥ γ, i = 1, . . . , n

||w|| = 1.

I.e., we want to maximize γ, subject to each training example having func-

tional margin at least γ. The ||w|| = 1 constraint moreover ensures that the

functional margin equals to the geometric margin, so we are also guaranteed

that all the geometric margins are at least γ. Thus, solving this problem will

result in (w, b) with the largest possible geometric margin with respect to the

training set.

If we could solve the optimization problem above, we’d be done. But the

“||w|| = 1” constraint is a nasty (non-convex) one, and this problem certainly

isn’t in any format that we can plug into standard optimization software to

solve. So, let’s try transforming the problem into a nicer one. Consider:

maxˆγ,w,b

ˆγ

||w||

s.t.

y(i)(wTx(i) + b) ≥ ˆγ, i = 1, . . . , n

Here, we’re going to maximize ˆγ/||w||, subject to the functional margins all

being at least ˆγ. Since the geometric and functional margins are related by

γ = ˆγ/||w|, this will give us the answer we want. Moreover, we’ve gotten rid

of the constraint ||w|| = 1 that we didn’t like. The downside is that we now

have a nasty (again, non-convex) objective

ˆγ

||w|| function; and, we still don’t

have any oﬀ-the-shelf software that can solve this form of an optimization

problem.

Let’s keep going. Recall our earlier discussion that we can add an arbi-

trary scaling constraint on w and b without changing anything. This is the

key idea we’ll use now. We will introduce the scaling constraint that the

functional margin of w, b with respect to the training set must be 1:

ˆγ = 1.


66

Since multiplying w and b by some constant results in the functional margin

being multiplied by that same constant, this is indeed a scaling constraint,

and can be satisﬁed by rescaling w, b. Plugging this into our problem above,

and noting that maximizing ˆγ/||w|| = 1/||w|| is the same thing as minimizing

||w||2, we now have the following optimization problem:

minw,b

1

2||w||2

s.t.

y(i)(wTx(i) + b) ≥ 1, i = 1, . . . , n

We’ve now transformed the problem into a form that can be eﬃciently

solved. The above is an optimization problem with a convex quadratic ob-

jective and only linear constraints. Its solution gives us the optimal mar-

gin classiﬁer. This optimization problem can be solved using commercial

quadratic programming (QP) code.1

While we could call the problem solved here, what we will instead do is

make a digression to talk about Lagrange duality. This will lead us to our

optimization problem’s dual form, which will play a key role in allowing us to

use kernels to get optimal margin classiﬁers to work eﬃciently in very high

dimensional spaces. The dual form will also allow us to derive an eﬃcient

algorithm for solving the above optimization problem that will typically do

much better than generic QP software.

6.5

Lagrange duality (optional reading)

Let’s temporarily put aside SVMs and maximum margin classiﬁers, and talk

about solving constrained optimization problems.

Consider a problem of the following form:

minw

f(w)

s.t.

hi(w) = 0, i = 1, . . . , l.

Some of you may recall how the method of Lagrange multipliers can be used

to solve it. (Don’t worry if you haven’t seen it before.) In this method, we

deﬁne the Lagrangian to be

L(w, β) = f(w) +

l

�

i=1

βihi(w)

1You may be familiar with linear programming, which solves optimization problems

that have linear objectives and linear constraints. QP software is also widely available,

which allows convex quadratic objectives and linear constraints.


67

Here, the βi’s are called the Lagrange multipliers. We would then ﬁnd

and set L’s partial derivatives to zero:

∂L

∂wi

= 0;

∂L

∂βi

= 0,

and solve for w and β.

In this section, we will generalize this to constrained optimization prob-

lems in which we may have inequality as well as equality constraints. Due to

time constraints, we won’t really be able to do the theory of Lagrange duality

justice in this class,2 but we will give the main ideas and results, which we

will then apply to our optimal margin classiﬁer’s optimization problem.

Consider the following, which we’ll call the primal optimization problem:

minw

f(w)

s.t.

gi(w) ≤ 0, i = 1, . . . , k

hi(w) = 0, i = 1, . . . , l.

To solve it, we start by deﬁning the generalized Lagrangian

L(w, α, β) = f(w) +

k

�

i=1

αigi(w) +

l

�

i=1

βihi(w).

Here, the αi’s and βi’s are the Lagrange multipliers. Consider the quantity

θP(w) =

max

α,β : αi≥0 L(w, α, β).

Here, the “P” subscript stands for “primal.” Let some w be given. If w

violates any of the primal constraints (i.e., if either gi(w) &gt; 0 or hi(w) ̸= 0

for some i), then you should be able to verify that

θP(w)

=

max

α,β : αi≥0 f(w) +

k

�

i=1

αigi(w) +

l

�

i=1

βihi(w)

(6.1)

=

∞.

(6.2)

Conversely, if the constraints are indeed satisﬁed for a particular value of w,

then θP(w) = f(w). Hence,

θP(w) =

� f(w)

if w satisﬁes primal constraints

∞

otherwise.

2Readers interested in learning more about this topic are encouraged to read, e.g., R.

T. Rockarfeller (1970), Convex Analysis, Princeton University Press.


68

Thus, θP takes the same value as the objective in our problem for all val-

ues of w that satisﬁes the primal constraints, and is positive inﬁnity if the

constraints are violated. Hence, if we consider the minimization problem

min

w θP(w) = min

w

max

α,β : αi≥0 L(w, α, β),

we see that it is the same problem (i.e., and has the same solutions as) our

original, primal problem. For later use, we also deﬁne the optimal value of

the objective to be p∗ = minw θP(w); we call this the value of the primal

problem.

Now, let’s look at a slightly diﬀerent problem. We deﬁne

θD(α, β) = min

w L(w, α, β).

Here, the “D” subscript stands for “dual.” Note also that whereas in the

deﬁnition of θP we were optimizing (maximizing) with respect to α, β, here

we are minimizing with respect to w.

We can now pose the dual optimization problem:

max

α,β : αi≥0 θD(α, β) =

max

α,β : αi≥0 min

w L(w, α, β).

This is exactly the same as our primal problem shown above, except that the

order of the “max” and the “min” are now exchanged. We also deﬁne the

optimal value of the dual problem’s objective to be d∗ = maxα,β : αi≥0 θD(w).

How are the primal and the dual problems related? It can easily be shown

that

d∗ =

max

α,β : αi≥0 min

w L(w, α, β) ≤ min

w

max

α,β : αi≥0 L(w, α, β) = p∗.

(You should convince yourself of this; this follows from the “max min” of a

function always being less than or equal to the “min max.”) However, under

certain conditions, we will have

d∗ = p∗,

so that we can solve the dual problem in lieu of the primal problem. Let’s

see what these conditions are.

Suppose f and the gi’s are convex,3 and the hi’s are aﬃne.4 Suppose

further that the constraints gi are (strictly) feasible; this means that there

exists some w so that gi(w) &lt; 0 for all i.

3When f has a Hessian, then it is convex if and only if the Hessian is positive semi-

deﬁnite. For instance, f(w) = wT w is convex; similarly, all linear (and aﬃne) functions

are also convex. (A function f can also be convex without being diﬀerentiable, but we

won’t need those more general deﬁnitions of convexity here.)

4I.e., there exists ai, bi, so that hi(w) = aT

i w + bi. “Aﬃne” means the same thing as

linear, except that we also allow the extra intercept term bi.


69

Under our above assumptions, there must exist w∗, α∗, β∗ so that w∗ is the

solution to the primal problem, α∗, β∗ are the solution to the dual problem,

and moreover p∗ = d∗ = L(w∗, α∗, β∗). Moreover, w∗, α∗ and β∗ satisfy the

Karush-Kuhn-Tucker (KKT) conditions, which are as follows:

∂

∂wi

L(w∗, α∗, β∗)

=

0, i = 1, . . . , d

(6.3)

∂

∂βi

L(w∗, α∗, β∗)

=

0, i = 1, . . . , l

(6.4)

α∗

i gi(w∗)

=

0, i = 1, . . . , k

(6.5)

gi(w∗)

≤

0, i = 1, . . . , k

(6.6)

α∗

≥

0, i = 1, . . . , k

(6.7)

Moreover, if some w∗, α∗, β∗ satisfy the KKT conditions, then it is also a solution to the primal and dual

problems.

We draw attention to Equation (6.5), which is called the KKT dual

complementarity condition. Speciﬁcally, it implies that if α∗

i &gt; 0, then

gi(w∗) = 0. (I.e., the “gi(w) ≤ 0” constraint is active, meaning it holds with

equality rather than with inequality.) Later on, this will be key for showing

that the SVM has only a small number of “support vectors”; the KKT dual

complementarity condition will also give us our convergence test when we

talk about the SMO algorithm.

6.6

Optimal margin classiﬁers: the dual form

(option reading)

Note: The equivalence of optimization problem (6.8) and the optimization

problem (6.12), and the relationship between the primary and dual variables

in equation (6.10) are the most important take home messages of this section.

Previously, we posed the following (primal) optimization problem for ﬁnd-

ing the optimal margin classiﬁer:

minw,b

1

2||w||2

(6.8)

s.t.

y(i)(wTx(i) + b) ≥ 1, i = 1, . . . , n

We can write the constraints as

gi(w) = −y(i)(wTx(i) + b) + 1 ≤ 0.


70

We have one such constraint for each training example. Note that from the

KKT dual complementarity condition, we will have αi &gt; 0 only for the train-

ing examples that have functional margin exactly equal to one (i.e., the ones

corresponding to constraints that hold with equality, gi(w) = 0). Consider

the ﬁgure below, in which a maximum margin separating hyperplane is shown

by the solid line.

The points with the smallest margins are exactly the ones closest to the

decision boundary; here, these are the three points (one negative and two pos-

itive examples) that lie on the dashed lines parallel to the decision boundary.

Thus, only three of the αi’s—namely, the ones corresponding to these three

training examples—will be non-zero at the optimal solution to our optimiza-

tion problem. These three points are called the support vectors in this

problem. The fact that the number of support vectors can be much smaller

than the size the training set will be useful later.

Let’s move on. Looking ahead, as we develop the dual form of the prob-

lem, one key idea to watch out for is that we’ll try to write our algorithm

in terms of only the inner product ⟨x(i), x(j)⟩ (think of this as (x(i))Tx(j))

between points in the input feature space. The fact that we can express our

algorithm in terms of these inner products will be key when we apply the

kernel trick.

When we construct the Lagrangian for our optimization problem we have:

L(w, b, α) = 1

2||w||2 −

n

�

i=1

αi

�

y(i)(wTx(i) + b) − 1

�

.

(6.9)

Note that there’re only “αi” but no “βi” Lagrange multipliers, since the

problem has only inequality constraints.


71

Let’s ﬁnd the dual form of the problem.

To do so, we need to ﬁrst

minimize L(w, b, α) with respect to w and b (for ﬁxed α), to get θD, which

we’ll do by setting the derivatives of L with respect to w and b to zero. We

have:

∇wL(w, b, α) = w −

n

�

i=1

αiy(i)x(i) = 0

This implies that

w =

n

�

i=1

αiy(i)x(i).

(6.10)

As for the derivative with respect to b, we obtain

∂

∂bL(w, b, α) =

n

�

i=1

αiy(i) = 0.

(6.11)

If we take the deﬁnition of w in Equation (6.10) and plug that back into

the Lagrangian (Equation 6.9), and simplify, we get

L(w, b, α) =

n

�

i=1

αi − 1

2

n

�

i,j=1

y(i)y(j)αiαj(x(i))Tx(j) − b

n

�

i=1

αiy(i).

But from Equation (6.11), the last term must be zero, so we obtain

L(w, b, α) =

n

�

i=1

αi − 1

2

n

�

i,j=1

y(i)y(j)αiαj(x(i))Tx(j).

Recall that we got to the equation above by minimizing L with respect to

w and b. Putting this together with the constraints αi ≥ 0 (that we always

had) and the constraint (6.11), we obtain the following dual optimization

problem:

maxα

W(α) =

n

�

i=1

αi − 1

2

n

�

i,j=1

y(i)y(j)αiαj⟨x(i), x(j)⟩.

(6.12)

s.t.

αi ≥ 0, i = 1, . . . , n

n

�

i=1

αiy(i) = 0,

You should also be able to verify that the conditions required for p∗ = d∗

and the KKT conditions (Equations 6.3–6.7) to hold are indeed satisﬁed in


72

our optimization problem. Hence, we can solve the dual in lieu of solving

the primal problem.

Speciﬁcally, in the dual problem above, we have a

maximization problem in which the parameters are the αi’s. We’ll talk later

about the speciﬁc algorithm that we’re going to use to solve the dual problem,

but if we are indeed able to solve it (i.e., ﬁnd the α’s that maximize W(α)

subject to the constraints), then we can use Equation (6.10) to go back and

ﬁnd the optimal w’s as a function of the α’s. Having found w∗, by considering

the primal problem, it is also straightforward to ﬁnd the optimal value for

the intercept term b as

b∗ = −maxi:y(i)=−1 w∗Tx(i) + mini:y(i)=1 w∗Tx(i)

2

.

(6.13)

(Check for yourself that this is correct.)

Before moving on, let’s also take a more careful look at Equation (6.10),

which gives the optimal value of w in terms of (the optimal value of) α.

Suppose we’ve ﬁt our model’s parameters to a training set, and now wish to

make a prediction at a new point input x. We would then calculate wTx + b,

and predict y = 1 if and only if this quantity is bigger than zero.

But

using (6.10), this quantity can also be written:

wTx + b

=

� n

�

i=1

αiy(i)x(i)

�T

x + b

(6.14)

=

n

�

i=1

αiy(i)⟨x(i), x⟩ + b.

(6.15)

Hence, if we’ve found the αi’s, in order to make a prediction, we have to

calculate a quantity that depends only on the inner product between x and

the points in the training set. Moreover, we saw earlier that the αi’s will all

be zero except for the support vectors. Thus, many of the terms in the sum

above will be zero, and we really need to ﬁnd only the inner products between

x and the support vectors (of which there is often only a small number) in

order calculate (6.15) and make our prediction.

By examining the dual form of the optimization problem, we gained sig-

niﬁcant insight into the structure of the problem, and were also able to write

the entire algorithm in terms of only inner products between input feature

vectors. In the next section, we will exploit this property to apply the ker-

nels to our classiﬁcation problem. The resulting algorithm, support vector

machines, will be able to eﬃciently learn in very high dimensional spaces.


73

6.7

Regularization and the non-separable case

(optional reading)

The derivation of the SVM as presented so far assumed that the data is

linearly separable. While mapping data to a high dimensional feature space

via φ does generally increase the likelihood that the data is separable, we

can’t guarantee that it always will be so. Also, in some cases it is not clear

that ﬁnding a separating hyperplane is exactly what we’d want to do, since

that might be susceptible to outliers.

For instance, the left ﬁgure below

shows an optimal margin classiﬁer, and when a single outlier is added in the

upper-left region (right ﬁgure), it causes the decision boundary to make a

dramatic swing, and the resulting classiﬁer has a much smaller margin.

To make the algorithm work for non-linearly separable datasets as well

as be less sensitive to outliers, we reformulate our optimization (using ℓ1

regularization) as follows:

minγ,w,b

1

2||w||2 + C

n

�

i=1

ξi

s.t.

y(i)(wTx(i) + b) ≥ 1 − ξi, i = 1, . . . , n

ξi ≥ 0, i = 1, . . . , n.

Thus, examples are now permitted to have (functional) margin less than 1,

and if an example has functional margin 1 − ξi (with ξ &gt; 0), we would pay

a cost of the objective function being increased by Cξi. The parameter C

controls the relative weighting between the twin goals of making the ||w||2

small (which we saw earlier makes the margin large) and of ensuring that

most examples have functional margin at least 1.


74

As before, we can form the Lagrangian:

L(w, b, ξ, α, r) = 1

2wTw + C

n

�

i=1

ξi −

n

�

i=1

αi

�

y(i)(xTw + b) − 1 + ξi

�

−

n

�

i=1

riξi.

Here, the αi’s and ri’s are our Lagrange multipliers (constrained to be ≥ 0).

We won’t go through the derivation of the dual again in detail, but after

setting the derivatives with respect to w and b to zero as before, substituting

them back in, and simplifying, we obtain the following dual form of the

problem:

maxα

W(α) =

n

�

i=1

αi − 1

2

n

�

i,j=1

y(i)y(j)αiαj⟨x(i), x(j)⟩

s.t.

0 ≤ αi ≤ C, i = 1, . . . , n

n

�

i=1

αiy(i) = 0,

As before, we also have that w can be expressed in terms of the αi’s as

given in Equation (6.10), so that after solving the dual problem, we can con-

tinue to use Equation (6.15) to make our predictions. Note that, somewhat

surprisingly, in adding ℓ1 regularization, the only change to the dual prob-

lem is that what was originally a constraint that 0 ≤ αi has now become

0 ≤ αi ≤ C. The calculation for b∗ also has to be modiﬁed (Equation 6.13 is

no longer valid); see the comments in the next section/Platt’s paper.

Also, the KKT dual-complementarity conditions (which in the next sec-

tion will be useful for testing for the convergence of the SMO algorithm)

are:

αi = 0

⇒

y(i)(wTx(i) + b) ≥ 1

(6.16)

αi = C

⇒

y(i)(wTx(i) + b) ≤ 1

(6.17)

0 &lt; αi &lt; C

⇒

y(i)(wTx(i) + b) = 1.

(6.18)

Now, all that remains is to give an algorithm for actually solving the dual

problem, which we will do in the next section.

6.8

The SMO algorithm (optional reading)

The SMO (sequential minimal optimization) algorithm, due to John Platt,

gives an eﬃcient way of solving the dual problem arising from the derivation


75

of the SVM. Partly to motivate the SMO algorithm, and partly because it’s

interesting in its own right, let’s ﬁrst take another digression to talk about

the coordinate ascent algorithm.

6.8.1

Coordinate ascent

Consider trying to solve the unconstrained optimization problem

max

α

W(α1, α2, . . . , αn).

Here, we think of W as just some function of the parameters αi’s, and for now

ignore any relationship between this problem and SVMs. We’ve already seen

two optimization algorithms, gradient ascent and Newton’s method. The

new algorithm we’re going to consider here is called coordinate ascent:

Loop until convergence: {

For i = 1, . . . , n, {

αi := arg maxˆαi W(α1, . . . , αi−1, ˆαi, αi+1, . . . , αn).

}

}

Thus, in the innermost loop of this algorithm, we will hold all the variables

except for some αi ﬁxed, and reoptimize W with respect to just the parameter

αi. In the version of this method presented here, the inner-loop reoptimizes

the variables in order α1, α2, . . . , αn, α1, α2, . . .. (A more sophisticated version

might choose other orderings; for instance, we may choose the next variable

to update according to which one we expect to allow us to make the largest

increase in W(α).)

When the function W happens to be of such a form that the “arg max”

in the inner loop can be performed eﬃciently, then coordinate ascent can be

a fairly eﬃcient algorithm. Here’s a picture of coordinate ascent in action:


76

−2

−1.5

−1

−0.5

0

0.5

1

1.5

2

2.5

−2

−1.5

−1

−0.5

0

0.5

1

1.5

2

2.5

The ellipses in the ﬁgure are the contours of a quadratic function that

we want to optimize. Coordinate ascent was initialized at (2, −2), and also

plotted in the ﬁgure is the path that it took on its way to the global maximum.

Notice that on each step, coordinate ascent takes a step that’s parallel to one

of the axes, since only one variable is being optimized at a time.

6.8.2

SMO

We close oﬀ the discussion of SVMs by sketching the derivation of the SMO

algorithm.

Here’s the (dual) optimization problem that we want to solve:

maxα

W(α) =

n

�

i=1

αi − 1

2

n

�

i,j=1

y(i)y(j)αiαj⟨x(i), x(j)⟩.

(6.19)

s.t.

0 ≤ αi ≤ C, i = 1, . . . , n

(6.20)

n

�

i=1

αiy(i) = 0.

(6.21)

Let’s say we have set of αi’s that satisfy the constraints (6.20-6.21). Now,

suppose we want to hold α2, . . . , αn ﬁxed, and take a coordinate ascent step

and reoptimize the objective with respect to α1. Can we make any progress?

The answer is no, because the constraint (6.21) ensures that

α1y(1) = −

n

�

i=2

αiy(i).


77

Or, by multiplying both sides by y(1), we equivalently have

α1 = −y(1)

n

�

i=2

αiy(i).

(This step used the fact that y(1) ∈ {−1, 1}, and hence (y(1))2 = 1.) Hence,

α1 is exactly determined by the other αi’s, and if we were to hold α2, . . . , αn

ﬁxed, then we can’t make any change to α1 without violating the con-

straint (6.21) in the optimization problem.

Thus, if we want to update some subject of the αi’s, we must update at

least two of them simultaneously in order to keep satisfying the constraints.

This motivates the SMO algorithm, which simply does the following:

Repeat till convergence {

1. Select some pair αi and αj to update next (using a heuristic that

tries to pick the two that will allow us to make the biggest progress

towards the global maximum).

2. Reoptimize W(α) with respect to αi and αj, while holding all the

other αk’s (k ̸= i, j) ﬁxed.

}

To test for convergence of this algorithm, we can check whether the KKT

conditions (Equations 6.16-6.18) are satisﬁed to within some tol. Here, tol is

the convergence tolerance parameter, and is typically set to around 0.01 to

0.001. (See the paper and pseudocode for details.)

The key reason that SMO is an eﬃcient algorithm is that the update to

αi, αj can be computed very eﬃciently. Let’s now brieﬂy sketch the main

ideas for deriving the eﬃcient update.

Let’s say we currently have some setting of the αi’s that satisfy the con-

straints (6.20-6.21), and suppose we’ve decided to hold α3, . . . , αn ﬁxed, and

want to reoptimize W(α1, α2, . . . , αn) with respect to α1 and α2 (subject to

the constraints). From (6.21), we require that

α1y(1) + α2y(2) = −

n

�

i=3

αiy(i).

Since the right hand side is ﬁxed (as we’ve ﬁxed α3, . . . αn), we can just let

it be denoted by some constant ζ:

α1y(1) + α2y(2) = ζ.

(6.22)

We can thus picture the constraints on α1 and α2 as follows:


78

α2

α1

α1

α2

C

C

(1)+

(2)

y

y =ζ

H

L

From the constraints (6.20), we know that α1 and α2 must lie within the box

[0, C]×[0, C] shown. Also plotted is the line α1y(1) +α2y(2) = ζ, on which we

know α1 and α2 must lie. Note also that, from these constraints, we know

L ≤ α2 ≤ H; otherwise, (α1, α2) can’t simultaneously satisfy both the box

and the straight line constraint. In this example, L = 0. But depending on

what the line α1y(1) + α2y(2) = ζ looks like, this won’t always necessarily be

the case; but more generally, there will be some lower-bound L and some

upper-bound H on the permissible values for α2 that will ensure that α1, α2

lie within the box [0, C] × [0, C].

Using Equation (6.22), we can also write α1 as a function of α2:

α1 = (ζ − α2y(2))y(1).

(Check this derivation yourself; we again used the fact that y(1) ∈ {−1, 1} so

that (y(1))2 = 1.) Hence, the objective W(α) can be written

W(α1, α2, . . . , αn) = W((ζ − α2y(2))y(1), α2, . . . , αn).

Treating α3, . . . , αn as constants, you should be able to verify that this is

just some quadratic function in α2. I.e., this can also be expressed in the

form aα2

2 + bα2 + c for some appropriate a, b, and c. If we ignore the “box”

constraints (6.20) (or, equivalently, that L ≤ α2 ≤ H), then we can easily

maximize this quadratic function by setting its derivative to zero and solving.

We’ll let αnew,unclipped

2

denote the resulting value of α2. You should also be

able to convince yourself that if we had instead wanted to maximize W with

respect to α2 but subject to the box constraint, then we can ﬁnd the resulting

value optimal simply by taking αnew,unclipped

2

and “clipping” it to lie in the


79

[L, H] interval, to get

αnew

2

=







H

if αnew,unclipped

2

&gt; H

αnew,unclipped

2

if L ≤ αnew,unclipped

2

≤ H

L

if αnew,unclipped

2

&lt; L

Finally, having found the αnew

2

, we can use Equation (6.22) to go back and

ﬁnd the optimal value of αnew

1

.

There’re a couple more details that are quite easy but that we’ll leave you

to read about yourself in Platt’s paper: One is the choice of the heuristics

used to select the next αi, αj to update; the other is how to update b as the

SMO algorithm is run.


Part II

Deep learning

80


Chapter 7

Deep learning

We now begin our study of deep learning. In this set of notes, we give an

overview of neural networks, discuss vectorization and discuss training neural

networks with backpropagation.

7.1

Supervised learning with non-linear mod-

els

In the supervised learning setting (predicting y from the input x), suppose

our model/hypothesis is hθ(x). In the past lectures, we have considered the

cases when hθ(x) = θ⊤x (in linear regression or logistic regression) or hθ(x) =

θ⊤φ(x) (where φ(x) is the feature map). A commonality of these two models

is that they are linear in the parameters θ. Next we will consider learning

general family of models that are non-linear in both the parameters θ

and the inputs x. The most common non-linear models are neural networks,

which we will deﬁne staring from the next section. For this section, it suﬃces

to think hθ(x) as an abstract non-linear model.1

Suppose {(x(i), y(i))}n

i=1 are the training examples. For simplicity, we start

with the case where y(i) ∈ R and hθ(x) ∈ R.

Cost/loss function.

We deﬁne the least square cost function for the i-th

example (x(i), y(i)) as

J(i)(θ) = 1

2(hθ(x(i)) − y(i))2

(7.1)

1If a concrete example is helpful, perhaps think about the model hθ(x) = θ2

1x2

1 +θ2

2x2

2 +

· · · + θ2

dx2

d in this subsection, even though it’s not a neural network.

81


82

and deﬁne the mean-square cost function for the dataset as

J(θ) = 1

n

n

�

i=1

J(i)(θ)

(7.2)

which is same as in linear regression except that we introduce a constant

1/n in front of the cost function to be consistent with the convention. Note

that multiplying the cost function with a scalar will not change the local

minima or global minima of the cost function. Also note that the underlying

parameterization for hθ(x) is diﬀerent from the case of linear regression,

even though the form of the cost function is the same mean-squared loss.

Throughout the notes, we use the words “loss” and “cost” interchangeably.

Optimizers (SGD).

Commonly, people use gradient descent (GD), stochas-

tic gradient (SGD), or their variants to optimize the loss function J(θ). GD’s

update rule can be written as2

θ := θ − α∇θJ(θ)

(7.3)

where α &gt; 0 is often referred to as the learning rate or step size. Next, we

introduce a version of the SGD (Algorithm 1), which is lightly diﬀerent from

that in the ﬁrst lecture notes.

Algorithm 1 Stochastic Gradient Descent

1: Hyperparameter: learning rate α, number of total iteration niter.

2: Initialize θ randomly.

3: for i = 1 to niter do

4:

Sample j uniformly from {1, . . . , n}, and update θ by

θ := θ − α∇θJ(j)(θ)

(7.4)

Oftentimes computing the gradient of B examples simultaneously for the

parameter θ can be faster than computing B gradients separately due to

hardware parallelization. Therefore, a mini-batch version of SGD is most

2Recall that, as deﬁned in the previous lecture notes, we use the notation “a := b” to

denote an operation (in a computer program) in which we set the value of a variable a to

be equal to the value of b. In other words, this operation overwrites a with the value of

b. In contrast, we will write “a = b” when we are asserting a statement of fact, that the

value of a is equal to the value of b.


83

commonly used in deep learning, as shown in Algorithm 2. There are also

other variants of the SGD or mini-batch SGD with slightly diﬀerent sampling

schemes.

Algorithm 2 Mini-batch Stochastic Gradient Descent

1: Hyperparameters: learning rate α, batch size B, # iterations niter.

2: Initialize θ randomly

3: for i = 1 to niter do

4:

Sample B examples j1, . . . , jB (without replacement) uniformly from

{1, . . . , n}, and update θ by

θ := θ − α

B

B

�

k=1

∇θJ(jk)(θ)

(7.5)

With these generic algorithms, a typical deep learning model is learned

with the following steps. 1. Deﬁne a neural network parametrization hθ(x),

which we will introduce in Section 7.2, and 2. write the backpropagation

algorithm to compute the gradient of the loss function J(j)(θ) eﬃciently,

which will be covered in Section 7.3, and 3. run SGD or mini-batch SGD (or

other gradient-based optimizers) with the loss function J(θ).

7.2

Neural networks

Neural networks refer to broad type of non-linear models/parametrizations

hθ(x) that involve combinations of matrix multiplications and other entry-

wise non-linear operations. We will start small and slowly build up a neural

network, step by step.

A Neural Network with a Single Neuron.

Recall the housing price

prediction problem from before: given the size of the house, we want to

predict the price. We will use it as a running example in this subsection.

Previously, we ﬁt a straight line to the graph of size vs. housing price.

Now, instead of ﬁtting a straight line, we wish to prevent negative housing

prices by setting the absolute minimum price as zero. This produces a “kink”

in the graph as shown in Figure 7.1. How do we represent such a function

with a single kink as hθ(x) with unknown parameter? (After doing so, we

can invoke the machinery in Section 7.1.)


84

We deﬁne a parameterized function hθ(x) with input x, parameterized by

θ, which outputs the price of the house y. Formally, hθ : x → y. Perhaps

one of the simplest parametrization would be

hθ(x) = max(wx + b, 0), where θ = (w, b) ∈ R2

(7.6)

Here hθ(x) returns a single value: (wx+b) or zero, whichever is greater. In

the context of neural networks, the function max{t, 0} is called a ReLU (pro-

nounced “ray-lu”), or rectiﬁed linear unit, and often denoted by ReLU(t) ≜

max{t, 0}.

Generally, a one-dimensional non-linear function that maps R to R such as

ReLU is often referred to as an activation function. The model hθ(x) is said

to have a single neuron partly because it has a single non-linear activation

function. (We will discuss more about why a non-linear activation is called

neuron.)

When the input x ∈ Rd has multiple dimensions, a neural network with

a single neuron can be written as

hθ(x) = ReLU(w⊤x + b), where w ∈ Rd, b ∈ R, and θ = (w, b)

(7.7)

The term b is often referred to as the “bias”, and the vector w is referred

to as the weight vector. Such a neural network has 1 layer. (We will deﬁne

what multiple layers mean in the sequel.)

Stacking Neurons.

A more complex neural network may take the single

neuron described above and “stack” them together such that one neuron

passes its output as input into the next neuron, resulting in a more complex

function.

Let us now deepen the housing prediction example. In addition to the size

of the house, suppose that you know the number of bedrooms, the zip code

and the wealth of the neighborhood. Building neural networks is analogous

to Lego bricks: you take individual bricks and stack them together to build

complex structures. The same applies to neural networks: we take individual

neurons and stack them together to create complex neural networks.

Given these features (size, number of bedrooms, zip code, and wealth),

we might then decide that the price of the house depends on the maximum

family size it can accommodate. Suppose the family size is a function of the

size of the house and number of bedrooms (see Figure 7.2). The zip code

may provide additional information such as how walkable the neighborhood

is (i.e., can you walk to the grocery store or do you need to drive everywhere).

Combining the zip code with the wealth of the neighborhood may predict


85

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

0

100

200

300

400

500

600

700

800

900

1000

housing prices

square feet

price (in $1000)

Figure 7.1: Housing prices with a “kink” in the graph.

the quality of the local elementary school. Given these three derived features

(family size, walkable, school quality), we may conclude that the price of the

home ultimately depends on these three features.

Family Size

School Quality

Walkable

Size

# Bedrooms

Zip Code

Wealth

Price

y

Figure 7.2: Diagram of a small neural network for predicting housing prices.

Formally, the input to a neural network is a set of input features

x1, x2, x3, x4. We denote the intermediate variables for “family size”, “walk-

able”, and “school quality” by a1, a2, a3 (these ai’s are often referred to as

“hidden units” or “hidden neurons”). We represent each of the ai’s as a neu-

ral network with a single neuron with a subset of x1, . . . , x4 as inputs. Then

as in Figure 7.1, we will have the parameterization:

a1 = ReLU(θ1x1 + θ2x2 + θ3)

a2 = ReLU(θ4x3 + θ5)

a3 = ReLU(θ6x3 + θ7x4 + θ8)


86

where (θ1, · · · , θ8) are parameters. Now we represent the ﬁnal output hθ(x)

as another linear function with a1, a2, a3 as inputs, and we get3

hθ(x) = θ9a1 + θ10a2 + θ11a3 + θ12

(7.8)

where θ contains all the parameters (θ1, · · · , θ12).

Now we represent the output as a quite complex function of x with pa-

rameters θ. Then you can use this parametrization hθ with the machinery of

Section 7.1 to learn the parameters θ.

Inspiration from Biological Neural Networks.

As the name suggests,

artiﬁcial neural networks were inspired by biological neural networks. The

hidden units a1, . . . , am correspond to the neurons in a biological neural net-

work, and the parameters θi’s correspond to the synapses.

However, it’s

unclear how similar the modern deep artiﬁcial neural networks are to the bi-

ological ones. For example, perhaps not many neuroscientists think biological

neural networks could have 1000 layers, while some modern artiﬁcial neural

networks do (we will elaborate more on the notion of layers.) Moreover, it’s

an open question whether human brains update their neural networks in a

way similar to the way that computer scientists learn artiﬁcial neural net-

works (using backpropagation, which we will introduce in the next section.).

Two-layer Fully-Connected Neural Networks.

We constructed the

neural network in equation (7.8) using a signiﬁcant amount of prior knowl-

edge/belief about how the “family size”, “walkable”, and “school quality” are

determined by the inputs. We implicitly assumed that we know the family

size is an important quantity to look at and that it can be determined by

only the “size” and “# bedrooms”. Such a prior knowledge might not be

available for other applications. It would be more ﬂexible and general to have

a generic parameterization. A simple way would be to write the intermediate

variable a1 as a function of all x1, . . . , x4:

a1 = ReLU(w⊤

1 x + b1), where w1 ∈ R4 and b1 ∈ R

(7.9)

a2 = ReLU(w⊤

2 x + b2), where w2 ∈ R4 and b2 ∈ R

a3 = ReLU(w⊤

3 x + b3), where w3 ∈ R4 and b3 ∈ R

We still deﬁne hθ(x) using equation (7.8) with a1, a2, a3 being deﬁned

as above. Thus we have a so-called fully-connected neural network as

3Typically, for multi-layer neural network, at the end, near the output, we don’t apply

ReLU, especially when the output is not necessarily a positive number.


87



Figure 7.3: Diagram of a two-layer fully connected neural network. Each

edge from node xi to node aj indicates that aj depends on xi. The edge from

xi to aj is associated with the weight (w[1]

j )i which denotes the i-th coordinate

of the vector w[1]

j . The activation aj can be computed by taking the ReLUof

the weighted sum of xi’s with the weights being the weights associated with

the incoming edges, that is, aj = ReLU(�d

i=1(w[1]

j )ixi).

visualized in the dependency graph in Figure 7.3 because all the intermediate

variables ai’s depend on all the inputs xi’s.

For full generality, a two-layer fully-connected neural network with m

hidden units and d dimensional input x ∈ Rd is deﬁned as

∀j ∈ [1, ..., m],

zj = w[1]

j

⊤x + b[1]

j

where w[1]

j

∈ Rd, b[1]

j ∈ R

(7.10)

aj = ReLU(zj),

a = [a1, . . . , am]⊤ ∈ Rm

hθ(x) = w[2]⊤a + b[2] where w[2] ∈ Rm, b[2] ∈ R,

(7.11)

Note that by default the vectors in Rd are viewed as column vectors, and

in particular a is a column vector with components a1, a2, ..., am. The indices

[1] and [2] are used to distinguish two sets of parameters: the w[1]

j ’s (each of

which is a vector in Rd) and w[2] (which is a vector in Rm). We will have

more of these later.

Vectorization.

Before we introduce neural networks with more layers and

more complex structures, we will simplify the expressions for neural networks

with more matrix and vector notations. Another important motivation of


88

vectorization is the speed perspective in the implementation. In order to

implement a neural network eﬃciently, one must be careful when using for

loops. The most natural way to implement equation (7.10) in code is perhaps

to use a for loop. In practice, the dimensionalities of the inputs and hidden

units are high. As a result, code will run very slowly if you use for loops.

Leveraging the parallelism in GPUs is/was crucial for the progress of deep

learning.

This gave rise to vectorization. Instead of using for loops, vectorization

takes advantage of matrix algebra and highly optimized numerical linear

algebra packages (e.g., BLAS) to make neural network computations run

quickly. Before the deep learning era, a for loop may have been suﬃcient

on smaller datasets, but modern deep networks and state-of-the-art datasets

will be infeasible to run with for loops.

We vectorize the two-layer fully-connected neural network as below. We

deﬁne a weight matrix W [1] in Rm×d as the concatenation of all the vectors

w[1]

j ’s in the following way:

W [1] =





— w[1]

1

⊤ —

— w[1]

2

⊤ —

...

— w[1]

m

⊤ —





∈ Rm×d

(7.12)

Now by the deﬁnition of matrix vector multiplication, we can write z =

[z1, . . . , zm]⊤ ∈ Rm as





z1

...

...

zm





� �� �

z ∈ Rm×1

=





— w[1]

1

⊤ —

— w[1]

2

⊤ —

...

— w[1]

m

⊤ —





�

��

�

W [1] ∈ Rm×d





x1

x2

...

xd





� �� �

x ∈ Rd×1

+





b[1]

1

b[1]

2...

b[1]

m





� �� �

b[1] ∈ Rm×1

(7.13)

Or succinctly,

z = W [1]x + b[1]

(7.14)

We remark again that a vector in Rd in this notes, following the conventions

previously established, is automatically viewed as a column vector, and can

also be viewed as a d × 1 dimensional matrix. (Note that this is diﬀerent

from numpy where a vector is viewed as a row vector in broadcasting.)


89

Computing the activations a ∈ Rm from z ∈ Rm involves an element-

wise non-linear application of the ReLU function, which can be computed in

parallel eﬃciently. Overloading ReLU for element-wise application of ReLU

(meaning, for a vector t ∈ Rd, ReLU(t) is a vector such that ReLU(t)i =

ReLU(ti)), we have

a = ReLU(z)

(7.15)

Deﬁne W [2] = [w[2]⊤] ∈ R1×m similarly.

Then, the model in equa-

tion (7.11) can be summarized as

a = ReLU(W [1]x + b[1])

hθ(x) = W [2]a + b[2]

(7.16)

Here θ consists of W [1], W [2] (often referred to as the weight matrices) and

b[1], b[2] (referred to as the biases). The collection of W [1], b[1] is referred to as

the ﬁrst layer, and W [2], b[2] the second layer. The activation a is referred to as

the hidden layer. A two-layer neural network is also called one-hidden-layer

neural network.

Multi-layer fully-connected neural networks.

With this succinct no-

tations, we can stack more layers to get a deeper fully-connected neu-

ral network.

Let r be the number of layers (weight matrices).

Let

W [1], . . . , W [r], b[1], . . . , b[r] be the weight matrices and biases of all the layers.

Then a multi-layer neural network can be written as

a[1] = ReLU(W [1]x + b[1])

a[2] = ReLU(W [2]a[1] + b[2])

· · ·

a[r−1] = ReLU(W [r−1]a[r−2] + b[r−1])

hθ(x) = W [r]a[r−1] + b[r]

(7.17)

We note that the weight matrices and biases need to have compatible

dimensions for the equations above to make sense. If a[k] has dimension mk,

then the weight matrix W [k] should be of dimension mk ×mk−1, and the bias

b[k] ∈ Rmk. Moreover, W [1] ∈ Rm1×d and W [r] ∈ R1×mr−1.

The total number of neurons in the network is m1 + · · · + mr, and the

total number of parameters in this network is (d+1)m1 +(m1 +1)m2 +· · ·+

(mr−1 + 1)mr.


90

Sometimes for notational consistency we also write a[0] = x, and a[r] =

hθ(x). Then we have simple recursion that

a[k] = ReLU(W [k]a[k−1] + b[k]), ∀k = 1, . . . , r − 1

(7.18)

Note that this would have be true for k = r if there were an additional

ReLU in equation (7.17), but often people like to make the last layer linear

(aka without a ReLU) so that negative outputs are possible and it’s easier

to interpret the last layer as a linear model. (More on the interpretability at

the “connection to kernel method” paragraph of this section.)

Other activation functions.

The activation function ReLU can be re-

placed by many other non-linear function σ(·) that maps R to R such as

σ(z) =

1

1 + e−z

(sigmoid)

(7.19)

σ(z) = ez − e−z

ez + e−z

(tanh)

(7.20)

Why do we not use the identity function for σ(z)?

That is, why

not use σ(z) = z? Assume for sake of argument that b[1] and b[2] are zeros.

Suppose σ(z) = z, then for two-layer neural network, we have that

hθ(x) = W [2]a[1]

(7.21)

= W [2]σ(z[1])

by deﬁnition

(7.22)

= W [2]z[1]

since σ(z) = z

(7.23)

= W [2]W [1]x

from Equation (7.13)

(7.24)

= ˜Wx

where ˜W = W [2]W [1]

(7.25)

Notice how W [2]W [1] collapsed into ˜W.

This is because applying a linear function to another linear function will

result in a linear function over the original input (i.e., you can construct a ˜W

such that ˜Wx = W [2]W [1]x). This loses much of the representational power

of the neural network as often times the output we are trying to predict

has a non-linear relationship with the inputs. Without non-linear activation

functions, the neural network will simply perform linear regression.

Connection to the Kernel Method.

In the previous lectures, we covered

the concept of feature maps. Recall that the main motivation for feature


91

maps is to represent functions that are non-linear in the input x by θ⊤φ(x),

where θ are the parameters and φ(x), the feature map, is a handcrafted

function non-linear in the raw input x.

The performance of the learning

algorithms can signiﬁcantly depends on the choice of the feature map φ(x).

Oftentimes people use domain knowledge to design the feature map φ(x) that

suits the particular applications. The process of choosing the feature maps

is often referred to as feature engineering.

We can view deep learning as a way to automatically learn the right

feature map (sometimes also referred to as “the representation”) as follows.

Suppose we denote by β the collection of the parameters in a fully-connected

neural networks (equation (7.17)) except those in the last layer. Then we

can abstract right a[r−1] as a function of the input x and the parameters in

β: a[r−1] = φβ(x). Now we can write the model as

hθ(x) = W [r]φβ(x) + b[r]

(7.26)

When β is ﬁxed, then φβ(·) can viewed as a feature map, and therefore hθ(x)

is just a linear model over the features φβ(x). However, we will train the

neural networks, both the parameters in β and the parameters W [r], b[r] are

optimized, and therefore we are not learning a linear model in the feature

space, but also learning a good feature map φβ(·) itself so that it’s possi-

ble to predict accurately with a linear model on top of the feature map.

Therefore, deep learning tends to depend less on the domain knowledge of

the particular applications and requires often less feature engineering. The

penultimate layer a[r] is often (informally) referred to as the learned features

or representations in the context of deep learning.

In the example of house price prediction, a fully-connected neural network

does not need us to specify the intermediate quantity such “family size”, and

may automatically discover some useful features in the last penultimate layer

(the activation a[r−1]), and use them to linearly predict the housing price.

Often the feature map / representation obtained from one datasets (that is,

the function φβ(·) can be also useful for other datasets, which indicates they

contain essential information about the data. However, oftentimes, the neural

network will discover complex features which are very useful for predicting

the output but may be diﬃcult for a human to understand or interpret. This

is why some people refer to neural networks as a black box, as it can be

diﬃcult to understand the features it has discovered.


92

7.3

Backpropagation

In this section, we introduce backpropgation or auto-diﬀerentiation, which

computes the gradient of the loss ∇J(j)(θ) eﬃciently. We will start with an

informal theorem that states that as long as a real-valued function f can be

eﬃciently computed/evaluated by a diﬀerentiable network or circuit, then its

gradient can be eﬃciently computed in a similar time. We will then show

how to do this concretely for fully-connected neural networks.

Because the formality of the general theorem is not the main focus here,

we will introduce the terms with informal deﬁnitions. By a diﬀerentiable

circuit or a diﬀerentiable network, we mean a composition of a sequence of

diﬀerentiable arithmetic operations (additions, subtraction, multiplication,

divisions, etc) and elementary diﬀerentiable functions (ReLU, exp, log, sin,

cos, etc.). Let the size of the circuit be the total number of such operations

and elementary functions. We assume that each of the operations and func-

tions, and their derivatives or partial derivatives ecan be computed in O(1)

time in the computer.

Theorem 7.3.1: [backpropagation or auto-diﬀerentiation, informally stated]

Suppose a diﬀerentiable circuit of size N computes a real-valued function

f : Rℓ → R. Then, the gradient ∇f can be computed in time O(N), by a

circuit of size O(N).4

We note that the loss function J(j)(θ) for j-th example can be indeed

computed by a sequence of operations and functions involving additions,

subtraction, multiplications, and non-linear activations. Thus the theorem

suggests that we should be able to compute the ∇J(j)(θ) in a similar time

to that for computing J(j)(θ) itself. This does not only apply to the fully-

connected neural network introduced in the Section 7.2, but also many other

types of neural networks.

In the rest of the section, we will showcase how to compute the gradient of

the loss eﬃciently for fully-connected neural networks using backpropagation.

Even though auto-diﬀerentiation or backpropagation is implemented in all

the deep learning packages such as tensorﬂow and pytorch, understanding it

is very helpful for gaining insights into the working of deep learning.

4We note if the output of the function f does not depend on some of the input co-

ordinates, then we set by default the gradient w.r.t that coordinate to zero. Setting to

zero does not count towards the total runtime here in our accounting scheme. This is why

when N ≤ ℓ, we can compute the gradient in O(N) time, which might be potentially even

less than ℓ.


93

7.3.1

Preliminary: chain rule

We ﬁrst recall the chain rule in calculus. Suppose the variable J depends on

the variables θ1, . . . , θp via the intermediate variable g1, . . . , gk:

gj = gj(θ1, . . . , θp), ∀j ∈ {1, · · · , k}

(7.27)

J = J(g1, . . . , gk)

(7.28)

Here we overload the meaning of gj’s: they denote both the intermediate

variables but also the functions used to compute the intermediate variables.

Then, by the chain rule, we have that ∀i,

∂J

∂θi

=

k

�

j=1

∂J

∂gj

∂gj

∂θi

(7.29)

For the ease of invoking the chain rule in the following subsections in various

ways, we will call J the output variable, g1, . . . , gk intermediate variables,

and θ1, . . . , θp the input variable in the chain rule.

7.3.2

One-neuron neural networks

Simplifying notations:

In the rest of the section, we will consider a

generic input x and compute the gradient of hθ(x) w.r.t θ. For simplicity,

we use o as a shorthand for hθ(x) (o stands for output). For simplicity, with

slight abuse of notation, we use J = 1

2(y − o)2 to denote the loss function.

(Note that this overrides the deﬁnition of J as the total loss in Section 7.1.)

Our goal is to compute the derivative of J w.r.t the parameter θ.

We ﬁrst consider the neural network with one neuron deﬁned in equa-

tion (7.7). Recall that we compute the loss function via the following se-

quential steps:

z = w⊤x + b

(7.30)

o = ReLU(z)

(7.31)

J = 1

2(y − o)2

(7.32)

By the chain rule with J as the output variable, o as the intermediate variable,

and wi the input variable, we have that

∂J

∂wi

= ∂J

∂o · ∂o

∂wi

(7.33)


94

Invoking the chain rule with o as the output variable, z as the intermediate

variable, and wi the input variable, we have that

∂o

∂wi

= ∂o

∂z · ∂z

∂wi

Combining the equation above with equation (7.33), we have

∂J

∂wi

= ∂J

∂o · ∂o

∂z · ∂z

∂wi

= (o − y) · ReLU′(z) · xi

(because ∂J

∂o = (o − y) and ∂o

∂z = ReLU′(z) and

∂z

∂wi = xi)

Here, the key is that we reduce the computation of

∂J

∂wi to the computa-

tion of three simpler more “local” objects ∂J

∂o , ∂o

∂z, and

∂z

∂wi, which are much

simpler to compute because J directly depends on o via equation (7.32), o

directly depends on a via equation (7.31), and z directly depends on wi via

equation (7.30). Note that in a vectorized form, we can also write

∇wJ = (o − y) · ReLU′(z) · x

Similarly, we compute the gradient w.r.t b by

∂J

∂b = ∂J

∂o · ∂o

∂z · ∂z

∂b = (o − y) · ReLU′(z)

(because ∂J

∂o = (o − y) and ∂o

∂z = ReLU′(z) and ∂z

∂b = 1)

7.3.3

Two-layer neural networks: a low-level unpacked

computation

Note: this subsection derives the derivatives with low-level notations to

help you build up intuition on backpropagation. If you are looking for a

clean formula, or you are familiar with matrix derivatives, then feel free to

jump to the next subsection directly.

Now we consider the two-layer neural network deﬁned in equation (7.11).

We compute the loss J by following sequence of operations

∀j ∈ [1, ..., m],

zj = w[1]

j

⊤x + b[1]

j

where w[1]

j

∈ Rd, b[1]

j

aj = ReLU(zj),

a = [a1, . . . , am]⊤ ∈ Rm

o = w[2]⊤a + b[2] where w[2] ∈ Rm, b[2] ∈ R

J = 1

2(y − o)2

(7.34)


95

We will use (w[2])ℓ to denote the ℓ-th coordinate of w[2], and (w[1]

j )ℓ to denote

the ℓ-coordinate of w[1]

j . (We will avoid using these cumbersome notations

once we ﬁgure out how to write everything in matrix and vector forms.)

By invoking chain rule with J as the output variable, o as intermediate

variable, and (w[2])ℓ as the input variable, we have

∂J

∂(w[2])ℓ

= ∂J

∂o

∂o

∂(w[2])ℓ

= (o − y)

∂o

∂(w[2])ℓ

= (o − y)aℓ

It’s more challenging to compute

∂J

∂(w[1]

j )ℓ.

Towards computing it, we ﬁrst

invoke the chain rule with J as the output variable, zj as the intermediate

variable, and (w[1]

j )ℓ as the input variable.

∂J

∂(w[1]

j )ℓ

= ∂J

∂zj

·

∂zj

∂(w[1]

j )ℓ

= ∂J

∂zj

· xℓ

(becaues

∂zj

∂(w[1]

j )ℓ = xℓ.)

Thus, it suﬃces to compute the ∂J

∂zj . We invoke the chain rule with J as the

output variable, aj as the intermediate variable, and zj as the input variable,

∂J

∂zj

= ∂J

∂aj

∂aj

∂zj

= ∂J

∂aj

ReLU′(zj)

Now it suﬃces to compute

∂J

∂aj , and we invoke the chain rule with J as the

output variable, o as the intermediate variable, and aj as the input variable,

∂J

∂aj

= ∂J

∂o

∂o

∂aj

= (o − y) · (w[2])j

Now combining the equations above, we obtain

∂J

∂(w[1]

j )ℓ

= (o − y) · (w[2])jReLU′(zj)xℓ


96

Next we gauge the runtime of computing these partial derivatives. Let p

denotes the total number of parameters in the network. We note that p ≥ md

where m is the number of hidden units and d is the input dimension. For

every j and ℓ, to compute

∂J

∂(w[1]

j )ℓ, apparently we need to compute at least

the output o, which takes at least p ≥ md operations. Therefore at the ﬁrst

glance computing a single gradient takes at least md time, and the total time

to compute the derivatives w.r.t to all the parameters is at least (md)2, which

is ineﬃcient.

However, the key of the backpropagation is that for diﬀerent choices of ℓ,

the formulas above for computing

∂J

∂(w[1]

j )ℓ share many terms, such as, (o − y),

(w[2])j and ReLU′(zj). This suggests that we can re-organize the computation

to leverage the shared computation.

It turns out the crucial shared quantities in these formulas are

∂J

∂o ,

∂J

∂z1, . . . , ∂J

∂zm. We now write the following formulas to compute the gradi-

ents eﬃciently in Algorithm 3.

Algorithm 3 Backpropagation for two-layer neural networks

1: Compute the values of z1, . . . , zm, a1, . . . , am and o as in the deﬁnition of

neural network (equation (7.34)).

2: Compute ∂J

∂o = (o − y).

3: Compute ∂J

∂zj for j = 1, . . . , m by

∂J

∂zj

= ∂J

∂o

∂o

∂aj

∂aj

∂zj

= ∂J

∂o · (w[2])j · ReLU′(zj)

(7.35)

4: Compute

∂J

∂(w[1]

j )ℓ,

∂J

∂b[1]

j ,

∂J

∂(w[2])j , and

∂J

∂b[2]by

∂J

∂(w[1]

j )ℓ

= ∂J

∂zj

·

∂zj

∂(w[1]

j )ℓ

= ∂J

∂zj

· xℓ

∂J

∂b[1]

j

= ∂J

∂zj

· ∂zj

∂b[1]

j

= ∂J

∂zj

∂J

∂(w[2])j

= ∂J

∂o

∂o

∂(w[2])j

= ∂J

∂o · aj

∂J

∂b[2] = ∂J

∂o

∂o

∂b[2] = ∂J

∂o


97

7.3.4

Two-layer neural network with vector notation

As we have done before in the deﬁnition of neural networks, the equations for

backpropagation becomes much cleaner with proper matrix notation. Here

we state the algorithm ﬁrst and also provide a cleaner proof via matrix cal-

culus.

Let

δ[2] ≜ ∂J

∂o ∈ R

δ[1] ≜ ∂J

∂z ∈ Rm

(7.36)

Here we note that when A is a real-valued variable,5 and B is a vector or

matrix variable, then ∂A

∂B denotes the collection of the partial derivatives with

the same shape as B.6 In other words, if B is a matrix of dimension m × d,

then ∂A

∂B is a matrix in Rm×d with

∂A

∂Bij as the ijth-entry. Let v ⊙ w denote

the entry-wise product of two vectors v and w of the same dimension. Now

we are ready to describe backpropagation in Algorithm 4.

Algorithm 4 Back-propagation for two-layer neural networks in vectorized

notations.

.

1: Compute the values of z ∈ Rm, a ∈ Rm, and o

2: Compute δ[2] = (o − y) ∈ R

3: Compute δ[1] = (o − y) · W [2]⊤ ⊙ ReLU′(z) ∈ Rm×1

4: Compute

∂J

∂W [2] = δ[2]a⊤ ∈ R1×m

∂J

∂b[2] = δ[2] ∈ R

∂J

∂W [1] = δ[1]x⊤ ∈ Rm×d

∂J

∂b[1] = δ[1] ∈ Rm

5We will avoid using the notation ∂A

∂B for A that is not a real-valued variable.

6If you are familiar with the notion of total derivatives, we note that the dimensionality

here is diﬀerent from that for total derivatives.


98

Derivation using the chain rule for matrix multiplication.

To

have a succinct derivation of the backpropagation algorithm in Algorithm 4

without working with the complex indices, we state the extensions of the

chain rule in vectorized notations.

It requires more knowledge of matrix

calculus to state the most general result, and therefore we will introduce

a few special cases that are most relevant for deep learning.

Suppose J

is a real-valued output variable, z ∈ Rm is the intermediate variable and

W ∈ Rm×d, u ∈ Rd are the input variables. Suppose they satisfy:

z = Wu + b, where W ∈ Rm×d

J = J(z)

(7.37)

Then we can compute ∂J

∂u and

∂J

∂W by:

∂J

∂u = W ⊤∂J

∂z

(7.38)

∂J

∂W = ∂J

∂z · u⊤

(7.39)

∂J

∂b = ∂J

∂z

(7.40)

We can verify the dimensionality is indeed compatible because

∂J

∂z ∈ Rm,

W ⊤ ∈ Rd×m, ∂J

∂u ∈ Rd,

∂J

∂W ∈ Rm×d, u⊤ ∈ R1×d.

Here the chain rule in equation (7.38) only works for the special cases

where z = Wu. Another useful case is the following:

a = σ(z), where σ is an element-wise activation, z, a ∈ Rd

J = J(a)

Then, we have that

∂J

∂z = ∂J

∂a ⊙ σ′(z)

(7.41)

where σ′(·) is the element-wise derivative of the activation function σ, and

⊙ is element-wise product of two vectors of the same dimensionality.

Using equation (7.38), (7.39),and (7.41), we can verify the correctness of

Algorithm 4. Indeed, using the notations in the two-layer neural network

∂J

∂z = ∂J

∂a ⊙ ReLU′(z)

(by invoking equation (7.41) with setting

J ← J, a ← a, z ← a, σ ← ReLU.

)

= (o − y)W [2]⊤ ⊙ ReLU′(z)

( by invoking equation (7.38) with setting

J ← J, z ← o, W ← W [2], u ← a, b ← b[2])


99

Therefore, δ[1] = ∂J

∂z , and we verify the correctness of Line 3 in Algorithm 4.

Similarly, let’s verify the third equation in Line 4,

∂J

∂W [1] = ∂J

∂z · x⊤

( by invoking equation (7.39) with setting

J ← J, z ← z, W ← W [1], u ← x, b ← b[1])

= δ[1]x⊤

(because we have proved δ[1] = ∂J

∂z )

7.3.5

Multi-layer neural networks

In this section, we will derive the backpropagation algorithms for the model

deﬁned in (7.17). Recall that we have

a[1] = ReLU(W [1]x + b[1])

a[2] = ReLU(W [2]a[1] + b[2])

· · ·

a[r−1] = ReLU(W [r−1]a[r−2] + b[r−1])

a[r] = z[r] = W [r]a[r−1] + b[r]

J = 1

2(a[r] − y)2

Here we deﬁne both a[r] and z[r] as hθ(x) for notational simplicity.

Deﬁne

δ[k] = ∂J

∂z[k]

(7.42)

The backpropagation algorithm computes δ[k]’s from k = r to 1, and

computes

∂J

∂W [k] from δ[k] as described in Algorithm 5.

7.4

Vectorization over training examples

As we discussed in Section 7.1, in the implementation of neural networks,

we will leverage the parallelism across the multiple examples. This means

that we will need to write the forward pass (the evaluation of the outputs)

of the neural network and the backward pass (backpropagation) for multiple

training examples in matrix notation.

The basic idea.

The basic idea is simple. Suppose you have a training

set with three examples x(1), x(2), x(3). The ﬁrst-layer activations for each


100

Algorithm 5 Back-propagation for multi-layer neural networks.

.

1: Compute and store the values of a[k]’s and z[k]’s for k = 1, . . . , r − 1, and

J.

▷ This is often called the “forward pass”

2: Compute δ[r] =

∂J

∂z[r] = (z[r] − o).

3: for k = r − 1 to 1 do

4:

Compute

δ[k] = ∂J

∂z[k] =

�

W [k+1]⊤δ[k+1]�

⊙ ReLU′(z[k])

5:

Compute

∂J

∂W [k+1] = δ[k+1]a[k]⊤

∂J

∂b[k+1] = δ[k+1]

example are as follows:

z[1](1) = W [1]x(1) + b[1]

z[1](2) = W [1]x(2) + b[1]

z[1](3) = W [1]x(3) + b[1]

Note the diﬀerence between square brackets [·], which refer to the layer num-

ber, and parenthesis (·), which refer to the training example number. In-

tuitively, one would implement this using a for loop. It turns out, we can

vectorize these operations as well. First, deﬁne:

X =





|

|

|

x(1)

x(2)

x(3)

|

|

|



 ∈ Rd×3

(7.43)

Note that we are stacking training examples in columns and not rows. We

can then combine this into a single uniﬁed formulation:

Z[1] =





|

|

|

z[1](1)

z[1](2)

z[1](3)

|

|

|



 = W [1]X + b[1]

(7.44)

You may notice that we are attempting to add b[1] ∈ R4×1 to W [1]X ∈

R4×3. Strictly following the rules of linear algebra, this is not allowed. In


101

practice however, this addition is performed using broadcasting. We create

an intermediate ˜b[1] ∈ R4×3:

˜b[1] =





|

|

|

b[1]

b[1]

b[1]

|

|

|





(7.45)

We can then perform the computation: Z[1] = W [1]X + ˜b[1]. Often times, it

is not necessary to explicitly construct ˜b[1]. By inspecting the dimensions in

(7.44), you can assume b[1] ∈ R4×1 is correctly broadcast to W [1]X ∈ R4×3.

The matricization approach as above can easily generalize to multiple

layers, with one subtlety though, as discussed below.

Complications/Subtlety in the Implementation.

All the deep learn-

ing packages or implementations put the data points in the rows of a data

matrix. (If the data point itself is a matrix or tensor, then the data are con-

centrated along the zero-th dimension.) However, most of the deep learning

papers use a similar notation to these notes where the data points are treated

as column vectors.7 There is a simple conversion to deal with the mismatch:

in the implementation, all the columns become row vectors, row vectors be-

come column vectors, all the matrices are transposed, and the orders of the

matrix multiplications are ﬂipped. In the example above, using the row ma-

jor convention, the data matrix is X ∈ R3×d, the ﬁrst layer weight matrix

has dimensionality d × m (instead of m × d as in the two layer neural net

section), and the bias vector b[1] ∈ R1×m. The computation for the hidden

activation becomes

Z[1] = XW [1] + b[1] ∈ R3×m

(7.46)

7The instructor suspects that this is mostly because in mathematics we naturally mul-

tiply a matrix to a vector on the left hand side.


Part III

Generalization and

regularization

102


Chapter 8

Generalization

This chapter discusses tools to analyze and understand the generaliza-

tion of machine learning models, i.e, their performances on unseen test

examples.

Recall that for supervised learning problems, given a train-

ing dataset {(x(i), y(i))}n

i=1, we typically learn a model hθ by minimizing a

loss/cost function J(θ), which encourages hθ to ﬁt the data.

E.g., when

the loss function is the least square loss (aka mean squared error), we have

J(θ) = 1

n

�n

i=1(y(i) − hθ(x(i)))2. This loss function for training purposes is

oftentimes referred to as the training loss/error/cost.

However, minimizing the training loss is not our ultimate goal—it is

merely our approach towards the goal of learning a predictive model. The

most important evaluation metric of a model is the loss on unseen test exam-

ples, which is oftentimes referred to as the test error. Formally, we sample a

test example (x, y) from the so-called test distribution D, and measure the

model’s error on it, by, e.g., the mean squared error, (hθ(x) − y)2. The ex-

pected loss/error over the randomness of the test example is called the test

loss/error,1

L(θ) = E(x,y)∼D[(y − hθ(x))2]

(8.1)

Note that the measurement of the error involves computing the expectation,

and in practice, it can be approximated by the average error on many sampled

test examples, which are referred to as the test dataset. Note that the key

diﬀerence here between training and test datasets is that the test examples

1In theoretical and statistical literature, we oftentimes call the uniform distribution

over the training set {(x(i), y(i))}n

i=1, denoted by �D, an empirical distribution, and call

D the population distribution. Partly because of this, the training loss is also referred

to as the empirical loss/risk/error, and the test loss is also referred to as the population

loss/risk/error.

103


104

are unseen, in the sense that the training procedure has not used the test

examples. In classical statistical learning settings, the training examples are

also drawn from the same distribution as the test distribution D, but still

the test examples are unseen by the learning procedure whereas the training

examples are seen.2

Because of this key diﬀerence between training and test datasets, even

if they are both drawn from the same distribution D, the test error is not

necessarily always close to the training error.3 As a result, successfully min-

imizing the training error may not always lead to a small test error. We

typically say the model overﬁts the data if the model predicts accurately on

the training dataset but doesn’t generalize well to other test examples, that

is, if the training error is small but the test error is large. We say the model

underﬁts the data if the training error is relatively large4 (and in this case,

typically the test error is also relatively large.)

This chapter studies how the test error is inﬂuenced by the learning pro-

cedure, especially the choice of model parameterizations. We will decompose

the test error into “bias” and “variance” terms and study how each of them is

aﬀected by the choice of model parameterizations and their tradeoﬀs. Using

the bias-variance tradeoﬀ, we will discuss when overﬁtting and underﬁtting

will occur and be avoided.

We will also discuss the double descent phe-

nomenon in Section 8.2 and some classical theoretical results in Section 8.3.

2These days, researchers have increasingly been more interested in the setting with

“domain shift”, that is, the training distribution and test distribution are diﬀerent.

3the diﬀerence between test error and training error is often referred to as the gener-

alization gap. The term generalization error in some literature means the test error, and

in some other literature means the generalization gap.

4e.g., larger than the intrinsic noise level of the data in regression problems.


105

8.1

Bias-variance tradeoﬀ

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

training dataset

training data

ground truth h *

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

test dataset

test data

ground truth h *

Figure 8.1: A running example of training and test dataset for this section.

As an illustrating example, we consider the following training dataset and

test dataset, which are also shown in Figure 8.1. The training inputs x(i)’s are

randomly chosen and the outputs y(i) are generated by y(i) = h⋆(x(i)) + ξ(i)

where the function h⋆(·) is a quadratic function and is shown in Figure 8.1

as the solid line, and ξ(i) is the a observation noise assumed to be generated

from ∼ N(0, σ2).

A test example (x, y) also has the same input-output

relationship y = h⋆(x) + ξ where ξ ∼ N(0, σ2). It’s impossible to predict the

noise ξ, and therefore essentially our goal is to recover the function h⋆(·).

We will consider the test error of learning various types of models. When

talking about linear regression, we discussed the problem of whether to ﬁt

a “simple” model such as the linear “y = θ0 + θ1x,” or a more “complex”

model such as the polynomial “y = θ0 + θ1x + · · · θ5x5.”

We start with ﬁtting a linear model, as shown in Figure 8.2. The best

ﬁtted linear model cannot predict y from x accurately even on the training

dataset, let alone on the test dataset. This is because the true relationship

between y and x is not linear—any linear model is far away from the true

function h⋆(·). As a result, the training error is large and this is a typical

situation of underﬁtting.


106

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

training data

best fit linear model

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

test data

best fit linear model

Figure 8.2: The best ﬁt linear model has large training and test errors.

The issue cannot be mitigated with more training examples—even with

a very large amount of, or even inﬁnite training examples, the best ﬁtted

linear model is still inaccurate and fails to capture the structure of the data

(Figure 8.3). Even if the noise is not present in the training data, the issue

still occurs (Figure 8.4). Therefore, the fundamental bottleneck here is the

linear model family’s inability to capture the structure in the data—linear

models cannot represent the true quadratic function h⋆—, but not the lack of

the data. Informally, we deﬁne the bias of a model to be the test error even

if we were to ﬁt it to a very (say, inﬁnitely) large training dataset. Thus, in

this case, the linear model suﬀers from large bias, and underﬁts (i.e., fails to

capture structure exhibited by) the data.

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

fitting linear models on a large dataset

training data

ground truth h *

best fit linear model

Figure 8.3:

The best ﬁt linear

model on a much larger dataset

still has a large training error.

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

fitting linear models on a noiseless dataset

training data

ground truth h *

best fit linear model

Figure 8.4:

The best ﬁt linear

model on a noiseless dataset also

has a large training/test error.

Next, we ﬁt a 5th-degree polynomial to the data. Figure 8.5 shows that

it fails to learn a good model either. However, the failure pattern is diﬀerent

from the linear model case. Speciﬁcally, even though the learnt 5th-degree


107

polynomial did a very good job predicting y(i)’s from x(i)’s for training ex-

amples, it does not work well on test examples (Figure 8.5). In other words,

the model learnt from the training set does not generalize well to other test

examples—the test error is high. Contrary to the behavior of linear models,

the bias of the 5-th degree polynomials is small—if we were to ﬁt a 5-th de-

gree polynomial to an extremely large dataset, the resulting model would be

close to a quadratic function and be accurate (Figure 8.6). This is because

the family of 5-th degree polynomials contains all the quadratic functions

(setting θ5 = θ4 = θ3 = 0 results in a quadratic function), and, therefore,

5-th degree polynomials are in principle capable of capturing the structure

of the data.

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

training data

best fit 5-th degree model

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

test data

ground truth h *

best fit 5-th degree model

Figure 8.5: Best ﬁt 5-th degree polynomial has zero training error, but still

has a large test error and does not recover the the ground truth. This is a

classic situation of overﬁtting.

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

training data

best fit 5-th degree model

ground truth h *

fitting 5-th degree model on large dataset

Figure 8.6: The best ﬁt 5-th degree polynomial on a huge dataset nearly

recovers the ground-truth—suggesting that the culprit in Figure 8.5 is the

variance (or lack of data) but not bias.

The failure of ﬁtting 5-th degree polynomials can be captured by another


108

component of the test error, called variance of a model ﬁtting procedure.

Speciﬁcally, when ﬁtting a 5-th degree polynomial as in Figure 8.7, there is a

large risk that we’re ﬁtting patterns in the data that happened to be present

in our small, ﬁnite training set, but that do not reﬂect the wider pattern of

the relationship between x and y. These “spurious” patterns in the training

set are (mostly) due to the observation noise ξ(i), and ﬁtting these spurious

patters results in a model with large test error. In this case, we say the model

has a large variance.

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

training data

best fit 5-th degree model

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

training data

best fit 5-th degree model

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

training data

best fit 5-th degree model

fitting 5-th degree model on different datasets

Figure 8.7: The best ﬁt 5-th degree models on three diﬀerent datasets gen-

erated from the same distribution behave quite diﬀerently, suggesting the

existence of a large variance.

The variance can be intuitively (and mathematically, as shown in Sec-

tion 8.1.1) characterized by the amount of variations across models learnt

on multiple diﬀerent training datasets (drawn from the same underlying dis-

tribution). The “spurious patterns” are speciﬁc to the randomness of the

noise (and inputs) in a particular dataset, and thus are diﬀerent across mul-

tiple training datasets. Therefore, overﬁtting to the “spurious patterns” of

multiple datasets should result in very diﬀerent models. Indeed, as shown

in Figure 8.7, the models learned on the three diﬀerent training datasets are

quite diﬀerent, overﬁtting to the “spurious patterns” of each datasets.

Often, there is a tradeoﬀ between bias and variance. If our model is too

“simple” and has very few parameters, then it may have large bias (but small

variance), and it typically may suﬀer from underﬁttng. If it is too “complex”

and has very many parameters, then it may suﬀer from large variance (but

have smaller bias), and thus overﬁtting. See Figure 8.8 for a typical tradeoﬀ

between bias and variance.


109

Model Complexity

Error

Bias2

Variance

Test Error (= Bias2 +Variance)

Optimal Tradeoff

Figure 8.8: An illustration of the typical bias-variance tradeoﬀ.

As we will see formally in Section 8.1.1, the test error can be decomposed

as a summation of bias and variance. This means that the test error will

have a convex curve as the model complexity increases, and in practice we

should tune the model complexity to achieve the best tradeoﬀ. For instance,

in the example above, ﬁtting a quadratic function does better than either of

the extremes of a ﬁrst or a 5-th degree polynomial, as shown in Figure 8.9.

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

training data

best fit quadratic model

0.0

0.2

0.4

0.6

0.8

1.0

x

0.0

0.5

1.0

1.5

y

test data

best fit quadratic model

ground truth h *

Figure 8.9: Best ﬁt quadratic model has small training and test error because

quadratic model achieves a better tradeoﬀ.

Interestingly, the bias-variance tradeoﬀ curves or the test error curves

do not universally follow the shape in Figure 8.8, at least not universally

when the model complexity is simply measured by the number of parameters.

(We will discuss the so-called double descent phenomenon in Section 8.2.)

Nevertheless, the principle of bias-variance tradeoﬀ is perhaps still the ﬁrst

resort when analyzing and predicting the behavior of test errors.


110

8.1.1

A mathematical decomposition (for regression)

To formally state the bias-variance tradeoﬀ for regression problems, we con-

sider the following setup (which is an extension of the beginning paragraph

of Section 8.1).

• Draw a training dataset S = {x(i), y(i)}n

i=1 such that y(i) = h⋆(x(i))+ξ(i)

where ξ(i) ∈ N(0, σ2).

• Train a model on the dataset S, denoted by ˆhS.

• Take a test example (x, y) such that y = h⋆(x) + ξ where ξ ∼ N(0, σ2),

and measure the expected test error (averaged over the random draw of

the training set S and the randomness of ξ)56

MSE(x) = ES,ξ[(y − hS(x))2]

(8.2)

We will decompose the MSE into a bias and variance term. We start by

stating a following simple mathematical tool that will be used twice below.

Claim 8.1.1: Suppose A and B are two independent real random variables

and E[A] = 0. Then, E[(A + B)2] = E[A2] + E[B2].

As a corollary, because a random variable A is independent with a con-

stant c, when E[A] = 0, we have E[(A + c)2] = E[A2] + c2.

The proof of the claim follows from expanding the square: E[(A + B)2] =

E[A2] + E[B2] + 2E[AB] = E[A2] + E[B2]. Here we used the independence to

show that E[AB] = E[A]E[B] = 0.

Using Claim 8.1.1 with A = ξ and B = h⋆(x) − ˆhS(x), we have

MSE(x) = E[(y − hS(x))2] = E[(ξ + (h⋆(x) − hS(x)))2]

(8.3)

= E[ξ2] + E[(h⋆(x) − hS(x))2] (by Claim 8.1.1)

= σ2 + E[(h⋆(x) − hS(x))2]

(8.4)

Then, let’s deﬁne havg(x) = ES[hS(x)] as the “average model”—the model

obtained by drawing an inﬁnite number of datasets, training on them, and

averaging their predictions on x. Note that havg is a hypothetical model for

analytical purposes that can not be obtained in reality (because we don’t

5For simplicity, the test input x is considered to be ﬁxed here, but the same conceptual

message holds when we average over the choice of x’s.

6The subscript under the expectation symbol is to emphasize the variables that are

considered as random by the expectation operation.


111

have inﬁnite number of datasets). It turns out that for many cases, havg

is (approximately) equal to the the model obtained by training on a single

dataset with inﬁnite samples. Thus, we can also intuitively interpret havg this

way, which is consistent with our intuitive deﬁnition of bias in the previous

subsection.

We can further decompose MSE(x) by letting c = h⋆(x)−havg(x) (which is

a constant that does not depend on the choice of S!) and A = havg(x)−hS(x)

in the corollary part of Claim 8.1.1:

MSE(x) = σ2 + E[(h⋆(x) − hS(x))2]

(8.5)

= σ2 + (h⋆(x) − havg(x))2 + E[(havg − hS(x))2]

(8.6)

=

σ2

����

unavoidable

+ (h⋆(x) − havg(x))2

�

��

�

≜ bias2

+ var(hS(x))

�

��

�

≜ variance

(8.7)

We call the second term the bias (square) and the third term the variance. As

discussed before, the bias captures the part of the error that are introduced

due to the lack of expressivity of the model. Recall that havg can be thought

of as the best possible model learned even with inﬁnite data. Thus, the bias is

not due to the lack of data, but is rather caused by that the family of models

fundamentally cannot approximate the h⋆. For example, in the illustrating

example in Figure 8.2, because any linear model cannot approximate the

true quadratic function h⋆, neither can havg, and thus the bias term has to

be large.

The variance term captures how the random nature of the ﬁnite dataset

introduces errors in the learned model. It measures the sensitivity of the

learned model to the randomness in the dataset. It often decreases as the

size of the dataset increases.

There is nothing we can do about the ﬁrst term σ2 as we can not predict

the noise ξ by deﬁnition.

Finally, we note that the bias-variance decomposition for classiﬁcation

is much less clear than for regression problems.

There have been several

proposals, but there is as yet no agreement on what is the “right” and/or

the most useful formalism.

8.2

The double descent phenomenon

Model-wise double descent. Recent works have demonstrated that the

test error can present a “double descent” phenomenon in a range of machine


112

learning models including linear models and deep neural networks.7

The

conventional wisdom, as discussed in Section 8.1, is that as we increase the

model complexity, the test error ﬁrst decreases and then increases, as illus-

trated in Figure 8.8. However, in many cases, we empirically observe that

the test error can have a second descent—it ﬁrst decreases, then increases

to a peak around when the model size is large enough to ﬁt all the training

data very well, and then decreases again in the so-called overparameterized

regime, where the number of parameters is larger than the number of data

points. See Figure 8.10 for an illustration of the typical curves of test errors

against model complexity (measured by the number of parameters). To some

extent, the overparameterized regime with the second descent is considered as

new to the machine learning community—partly because lightly-regularized,

overparameterized models are only extensively used in the deep learning era.

A practical implication of the phenomenon is that one should not hold back

from scaling into and experimenting with over-parametrized models because

the test error may well decrease again to a level even smaller than the previ-

ous lowest point. Actually, in many cases, larger overparameterized models

always lead to a better test performance (meaning there won’t be a second

ascent after the second descent).

# parameters

test error

typically when # parameters

is sufficient to fit the data

classical regime:

bias-variance tradeoff

modern regime:

over-parameterization

Figure 8.10: A typical model-wise double descent phenomenon. As the num-

ber of parameters increases, the test error ﬁrst decreases when the number of

parameters is smaller than the training data. Then in the overparameterized

regime, the test error decreases again.

7The discovery of the phenomenon perhaps dates back to Opper [1995, 2001], and has

been recently popularized by Belkin et al. [2020], Hastie et al. [2019], etc.


113

Sample-wise double descent.

A priori, we would expect that more

training examples always lead to smaller test errors—more samples give

strictly more information for the algorithm to learn from. However, recent

work [Nakkiran, 2019] observes that the test error is not monotonically de-

creasing as we increase the sample size. Instead, as shown in Figure 8.11, the

test error decreases, and then increases and peaks around when the number

of examples (denoted by n) is similar to the number of parameters (denoted

by d), and then decreases again. We refer to this as the sample-wise dou-

ble descent phenomenon. To some extent, sample-wise double descent and

model-wise double descent are essentially describing similar phenomena—the

test error is peaked when n ≈ d.

Explanation and mitigation strategy.

The sample-wise double descent,

or, in particular, the peak of test error at n ≈ d, suggests that the existing

training algorithms evaluated in these experiments are far from optimal when

n ≈ d. We will be better oﬀ by tossing away some examples and run the

algorithms with a smaller sample size to steer clear of the peak. In other

words, in principle, there are other algorithms that can achieve smaller test

error when n ≈ d, but the algorithms evaluated in these experiments fail to

do so. The sub-optimality of the learning procedure appears to be the culprit

of the peak in both sample-wise and model-wise double descent.

Indeed, with an optimally-tuned regularization (which will be discussed

more in Section 9), the test error in the n ≈ d regime can be dramatically

improved, and the model-wise and sample-wise double descent are both mit-

igated. See Figure 8.11.

The intuition above only explains the peak in the model-wise and sample-

wise double descent, but does not explain the second descent in the model-

wise double descent—why overparameterized models are able to generalize

so well. The theoretical understanding of overparameterized models is an ac-

tive research area with many recent advances. A typical explanation is that

the commonly-used optimizers such as gradient descent provide an implicit

regularization eﬀect (which will be discussed in more detail in Section 9.2).

In other words, even in the overparameterized regime and with an unregular-

ized loss function, the model is still implicitly regularized, and thus exhibits

a better test performance than an arbitrary solution that ﬁts the data. For

example, for linear models, when n ≪ d, the gradient descent optimizer with

zero initialization ﬁnds the minimum norm solution that ﬁts the data (in-

stead of an arbitrary solution that ﬁts the data), and the minimum norm reg-

ularizer turns out to be a suﬃciently good for the overparameterized regime

(but it’s not a good regularizer when n ≈ d, resulting in the peak of test


114

error).

0

200

400

600

800

1000

Num Samples

0.00

0.25

0.50

0.75

1.00

1.25

1.50

1.75

2.00

T

est Error

T

est Error vs. # Samples

T

est Error



Figure 8.11:

Left: The sample-wise double descent phenomenon for linear

models. Right: The sample-wise double descent with diﬀerent regularization

strength for linear models. Using the optimal regularization parameter λ

(optimally tuned for each n, shown in green solid curve) mitigates double

descent. Setup: The data distribution of (x, y) is x ∼ N(0, Id) and y ∼

x⊤β + N(0, σ2) where d = 500, σ = 0.5 and ∥β∥2 = 1.8

Finally, we also remark that the double descent phenomenon has been

mostly observed when the model complexity is measured by the number of

parameters. It is unclear if and when the number of parameters is the best

complexity measure of a model. For example, in many situations, the norm

of the models is used as a complexity measure. As shown in Figure 8.12

right, for a particular linear case, if we plot the test error against the norm

of the learnt model, the double descent phenomenon no longer occurs. This

is partly because the norm of the learned model is also peaked around n ≈ d

(See Figure 8.12 (middle) or Belkin et al. [2019], Mei and Montanari [2022],

and discussions in Section 10.8 of James et al. [2021]).

For deep neural

networks, the correct complexity measure is even more elusive. The study of

double descent phenomenon is an active research topic.

8The ﬁgure is reproduced from Figure 1 of Nakkiran et al. [2020]. Similar phenomenon

are also observed in Hastie et al. [2022], Mei and Montanari [2022]


115

0

250

500

750

1000

# parameters

0.0

0.2

0.4

0.6

0.8

1.0

test error

test error vs. # params

0

200

400

600

800

1000

# parameters

0

10

20

30

40

norm

norm vs. # params

0

10

20

30

40

norm

0.0

0.2

0.4

0.6

0.8

1.0

test error

d=n

# parameters

test error vs. norm



0

200

400

600

800

1000

Figure 8.12: Left: The double descent phenomenon, where the number of pa-

rameters is used as the model complexity. Middle: The norm of the learned

model is peaked around n ≈ d. Right: The test error against the norm of

the learnt model. The color bar indicate the number of parameters and the

arrows indicates the direction of increasing model size. Their relationship

are closer to the convention wisdom than to a double descent. Setup: We

consider a linear regression with a ﬁxed dataset of size n = 500. The input

x is a random ReLU feature on Fashion-MNIST, and output y ∈ R10 is the

one-hot label. This is the same setting as in Section 5.2 of Nakkiran et al.

[2020].


116

8.3

Sample

complexity

bounds

(optional

readings)

8.3.1

Preliminaries

In this set of notes, we begin our foray into learning theory. Apart from

being interesting and enlightening in its own right, this discussion will also

help us hone our intuitions and derive rules of thumb about how to best

apply learning algorithms in diﬀerent settings. We will also seek to answer

a few questions: First, can we make formal the bias/variance tradeoﬀ that

was just discussed? This will also eventually lead us to talk about model

selection methods, which can, for instance, automatically decide what order

polynomial to ﬁt to a training set. Second, in machine learning it’s really

generalization error that we care about, but most learning algorithms ﬁt their

models to the training set. Why should doing well on the training set tell us

anything about generalization error? Speciﬁcally, can we relate error on the

training set to generalization error? Third and ﬁnally, are there conditions

under which we can actually prove that learning algorithms will work well?

We start with two simple but very useful lemmas.

Lemma. (The union bound). Let A1, A2, . . . , Ak be k diﬀerent events (that

may not be independent). Then

P(A1 ∪ · · · ∪ Ak) ≤ P(A1) + . . . + P(Ak).

In probability theory, the union bound is usually stated as an axiom

(and thus we won’t try to prove it), but it also makes intuitive sense: The

probability of any one of k events happening is at most the sum of the

probabilities of the k diﬀerent events.

Lemma. (Hoeﬀding inequality) Let Z1, . . . , Zn be n independent and iden-

tically distributed (iid) random variables drawn from a Bernoulli(φ) distri-

bution. I.e., P(Zi = 1) = φ, and P(Zi = 0) = 1 − φ. Let ˆφ = (1/n) �n

i=1 Zi

be the mean of these random variables, and let any γ &gt; 0 be ﬁxed. Then

P(|φ − ˆφ| &gt; γ) ≤ 2 exp(−2γ2n)

This lemma (which in learning theory is also called the Chernoﬀ bound)

says that if we take ˆφ—the average of n Bernoulli(φ) random variables—to

be our estimate of φ, then the probability of our being far from the true value

is small, so long as n is large. Another way of saying this is that if you have

a biased coin whose chance of landing on heads is φ, then if you toss it n


117

times and calculate the fraction of times that it came up heads, that will be

a good estimate of φ with high probability (if n is large).

Using just these two lemmas, we will be able to prove some of the deepest

and most important results in learning theory.

To simplify our exposition, let’s restrict our attention to binary classiﬁca-

tion in which the labels are y ∈ {0, 1}. Everything we’ll say here generalizes

to other problems, including regression and multi-class classiﬁcation.

We assume we are given a training set S = {(x(i), y(i)); i = 1, . . . , n} of size

n, where the training examples (x(i), y(i)) are drawn iid from some probability

distribution D. For a hypothesis h, we deﬁne the training error (also called

the empirical risk or empirical error in learning theory) to be

ˆε(h) = 1

n

n

�

i=1

1{h(x(i)) ̸= y(i)}.

This is just the fraction of training examples that h misclassiﬁes. When we

want to make explicit the dependence of ˆε(h) on the training set S, we may

also write this a ˆεS(h). We also deﬁne the generalization error to be

ε(h) = P(x,y)∼D(h(x) ̸= y).

I.e. this is the probability that, if we now draw a new example (x, y) from

the distribution D, h will misclassify it.

Note that we have assumed that the training data was drawn from the

same distribution D with which we’re going to evaluate our hypotheses (in

the deﬁnition of generalization error). This is sometimes also referred to as

one of the PAC assumptions.9

Consider the setting of linear classiﬁcation, and let hθ(x) = 1{θTx ≥ 0}.

What’s a reasonable way of ﬁtting the parameters θ? One approach is to try

to minimize the training error, and pick

ˆθ = arg min

θ

ˆε(hθ).

We call this process empirical risk minimization (ERM), and the resulting

hypothesis output by the learning algorithm is ˆh = hˆθ. We think of ERM

as the most “basic” learning algorithm, and it will be this algorithm that we

9PAC stands for “probably approximately correct,” which is a framework and set of

assumptions under which numerous results on learning theory were proved. Of these, the

assumption of training and testing on the same distribution, and the assumption of the

independently drawn training examples, were the most important.


118

focus on in these notes. (Algorithms such as logistic regression can also be

viewed as approximations to empirical risk minimization.)

In our study of learning theory, it will be useful to abstract away from

the speciﬁc parameterization of hypotheses and from issues such as whether

we’re using a linear classiﬁer. We deﬁne the hypothesis class H used by a

learning algorithm to be the set of all classiﬁers considered by it. For linear

classiﬁcation, H = {hθ : hθ(x) = 1{θTx ≥ 0}, θ ∈ Rd+1} is thus the set of

all classiﬁers over X (the domain of the inputs) where the decision boundary

is linear. More broadly, if we were studying, say, neural networks, then we

could let H be the set of all classiﬁers representable by some neural network

architecture.

Empirical risk minimization can now be thought of as a minimization over

the class of functions H, in which the learning algorithm picks the hypothesis:

ˆh = arg min

h∈H ˆε(h)

8.3.2

The case of ﬁnite H

Let’s start by considering a learning problem in which we have a ﬁnite hy-

pothesis class H = {h1, . . . , hk} consisting of k hypotheses. Thus, H is just a

set of k functions mapping from X to {0, 1}, and empirical risk minimization

selects ˆh to be whichever of these k functions has the smallest training error.

We would like to give guarantees on the generalization error of ˆh. Our

strategy for doing so will be in two parts: First, we will show that ˆε(h) is a

reliable estimate of ε(h) for all h. Second, we will show that this implies an

upper-bound on the generalization error of ˆh.

Take any one, ﬁxed, hi ∈ H. Consider a Bernoulli random variable Z

whose distribution is deﬁned as follows. We’re going to sample (x, y) ∼ D.

Then, we set Z = 1{hi(x) ̸= y}. I.e., we’re going to draw one example,

and let Z indicate whether hi misclassiﬁes it. Similarly, we also deﬁne Zj =

1{hi(x(j)) ̸= y(j)}. Since our training set was drawn iid from D, Z and the

Zj’s have the same distribution.

We see that the misclassiﬁcation probability on a randomly drawn

example—that is, ε(h)—is exactly the expected value of Z (and Zj). More-

over, the training error can be written

ˆε(hi) = 1

n

n

�

j=1

Zj.

Thus, ˆε(hi) is exactly the mean of the n random variables Zj that are drawn

iid from a Bernoulli distribution with mean ε(hi). Hence, we can apply the


119

Hoeﬀding inequality, and obtain

P(|ε(hi) − ˆε(hi)| &gt; γ) ≤ 2 exp(−2γ2n).

This shows that, for our particular hi, training error will be close to

generalization error with high probability, assuming n is large. But we don’t

just want to guarantee that ε(hi) will be close to ˆε(hi) (with high probability)

for just only one particular hi.

We want to prove that this will be true

simultaneously for all h ∈ H. To do so, let Ai denote the event that |ε(hi) −

ˆε(hi)| &gt; γ. We’ve already shown that, for any particular Ai, it holds true

that P(Ai) ≤ 2 exp(−2γ2n). Thus, using the union bound, we have that

P(∃ h ∈ H.|ε(hi) − ˆε(hi)| &gt; γ)

=

P(A1 ∪ · · · ∪ Ak)

≤

k

�

i=1

P(Ai)

≤

k

�

i=1

2 exp(−2γ2n)

=

2k exp(−2γ2n)

If we subtract both sides from 1, we ﬁnd that

P(¬∃ h ∈ H.|ε(hi) − ˆε(hi)| &gt; γ)

=

P(∀h ∈ H.|ε(hi) − ˆε(hi)| ≤ γ)

≥

1 − 2k exp(−2γ2n)

(The “¬” symbol means “not.”)

So, with probability at least 1 −

2k exp(−2γ2n), we have that ε(h) will be within γ of ˆε(h) for all h ∈ H.

This is called a uniform convergence result, because this is a bound that

holds simultaneously for all (as opposed to just one) h ∈ H.

In the discussion above, what we did was, for particular values of n and

γ, give a bound on the probability that for some h ∈ H, |ε(h) − ˆε(h)| &gt; γ.

There are three quantities of interest here: n, γ, and the probability of error;

we can bound either one in terms of the other two.

For instance, we can ask the following question: Given γ and some δ &gt; 0,

how large must n be before we can guarantee that with probability at least

1 − δ, training error will be within γ of generalization error? By setting

δ = 2k exp(−2γ2n) and solving for n, [you should convince yourself this is

the right thing to do!], we ﬁnd that if

n ≥

1

2γ2 log 2k

δ ,


120

then with probability at least 1 − δ, we have that |ε(h) − ˆε(h)| ≤ γ for all

h ∈ H. (Equivalently, this shows that the probability that |ε(h) − ˆε(h)| &gt; γ

for some h ∈ H is at most δ.)

This bound tells us how many training

examples we need in order make a guarantee. The training set size n that

a certain method or algorithm requires in order to achieve a certain level of

performance is also called the algorithm’s sample complexity.

The key property of the bound above is that the number of training

examples needed to make this guarantee is only logarithmic in k, the number

of hypotheses in H. This will be important later.

Similarly, we can also hold n and δ ﬁxed and solve for γ in the previous

equation, and show [again, convince yourself that this is right!] that with

probability 1 − δ, we have that for all h ∈ H,

|ˆε(h) − ε(h)| ≤

�

1

2n log 2k

δ .

Now, let’s assume that uniform convergence holds, i.e., that |ε(h)−ˆε(h)| ≤

γ for all h ∈ H. What can we prove about the generalization of our learning

algorithm that picked ˆh = arg minh∈H ˆε(h)?

Deﬁne h∗ = arg minh∈H ε(h) to be the best possible hypothesis in H. Note

that h∗ is the best that we could possibly do given that we are using H, so

it makes sense to compare our performance to that of h∗. We have:

ε(ˆh)

≤

ˆε(ˆh) + γ

≤

ˆε(h∗) + γ

≤

ε(h∗) + 2γ

The ﬁrst line used the fact that |ε(ˆh)−ˆε(ˆh)| ≤ γ (by our uniform convergence

assumption). The second used the fact that ˆh was chosen to minimize ˆε(h),

and hence ˆε(ˆh) ≤ ˆε(h) for all h, and in particular ˆε(ˆh) ≤ ˆε(h∗). The third

line used the uniform convergence assumption again, to show that ˆε(h∗) ≤

ε(h∗) + γ. So, what we’ve shown is the following: If uniform convergence

occurs, then the generalization error of ˆh is at most 2γ worse than the best

possible hypothesis in H!

Let’s put all this together into a theorem.

Theorem. Let |H| = k, and let any n, δ be ﬁxed. Then with probability at

least 1 − δ, we have that

ε(ˆh) ≤

�

min

h∈H ε(h)

�

+ 2

�

1

2n log 2k

δ .


121

This is proved by letting γ equal the √· term, using our previous argu-

ment that uniform convergence occurs with probability at least 1 − δ, and

then noting that uniform convergence implies ε(h) is at most 2γ higher than

ε(h∗) = minh∈H ε(h) (as we showed previously).

This also quantiﬁes what we were saying previously saying about the

bias/variance tradeoﬀ in model selection. Speciﬁcally, suppose we have some

hypothesis class H, and are considering switching to some much larger hy-

pothesis class H′ ⊇ H. If we switch to H′, then the ﬁrst term minh ε(h)

can only decrease (since we’d then be taking a min over a larger set of func-

tions). Hence, by learning using a larger hypothesis class, our “bias” can

only decrease. However, if k increases, then the second 2√· term would also

increase. This increase corresponds to our “variance” increasing when we use

a larger hypothesis class.

By holding γ and δ ﬁxed and solving for n like we did before, we can also

obtain the following sample complexity bound:

Corollary.

Let |H| = k, and let any δ, γ be ﬁxed.

Then for ε(ˆh) ≤

minh∈H ε(h) + 2γ to hold with probability at least 1 − δ, it suﬃces that

n

≥

1

2γ2 log 2k

δ

=

O

� 1

γ2 log k

δ

�

,

8.3.3

The case of inﬁnite H

We have proved some useful theorems for the case of ﬁnite hypothesis classes.

But many hypothesis classes, including any parameterized by real numbers

(as in linear classiﬁcation) actually contain an inﬁnite number of functions.

Can we prove similar results for this setting?

Let’s start by going through something that is not the “right” argument.

Better and more general arguments exist, but this will be useful for honing

our intuitions about the domain.

Suppose we have an H that is parameterized by d real numbers. Since we

are using a computer to represent real numbers, and IEEE double-precision

ﬂoating point (double’s in C) uses 64 bits to represent a ﬂoating point num-

ber, this means that our learning algorithm, assuming we’re using double-

precision ﬂoating point, is parameterized by 64d bits. Thus, our hypothesis

class really consists of at most k = 264d diﬀerent hypotheses. From the Corol-

lary at the end of the previous section, we therefore ﬁnd that, to guarantee


122

ε(ˆh) ≤ ε(h∗)+2γ, with to hold with probability at least 1−δ, it suﬃces that

n ≥ O

�

1

γ2 log 264d

δ

�

= O

�

d

γ2 log 1

δ

�

= Oγ,δ(d). (The γ, δ subscripts indicate

that the last big-O is hiding constants that may depend on γ and δ.) Thus,

the number of training examples needed is at most linear in the parameters

of the model.

The fact that we relied on 64-bit ﬂoating point makes this argument not

entirely satisfying, but the conclusion is nonetheless roughly correct: If what

we try to do is minimize training error, then in order to learn “well” using a

hypothesis class that has d parameters, generally we’re going to need on the

order of a linear number of training examples in d.

(At this point, it’s worth noting that these results were proved for an al-

gorithm that uses empirical risk minimization. Thus, while the linear depen-

dence of sample complexity on d does generally hold for most discriminative

learning algorithms that try to minimize training error or some approxima-

tion to training error, these conclusions do not always apply as readily to

discriminative learning algorithms. Giving good theoretical guarantees on

many non-ERM learning algorithms is still an area of active research.)

The other part of our previous argument that’s slightly unsatisfying is

that it relies on the parameterization of H. Intuitively, this doesn’t seem like

it should matter: We had written the class of linear classiﬁers as hθ(x) =

1{θ0 + θ1x1 + · · · θdxd ≥ 0}, with n + 1 parameters θ0, . . . , θd. But it could

also be written hu,v(x) = 1{(u2

0 − v2

0) + (u2

1 − v2

1)x1 + · · · (u2

d − v2

d)xd ≥ 0}

with 2d + 2 parameters ui, vi. Yet, both of these are just deﬁning the same

H: The set of linear classiﬁers in d dimensions.

To derive a more satisfying argument, let’s deﬁne a few more things.

Given a set S = {x(i), . . . , x(D)} (no relation to the training set) of points

x(i) ∈ X, we say that H shatters S if H can realize any labeling on S.

I.e., if for any set of labels {y(1), . . . , y(D)}, there exists some h ∈ H so that

h(x(i)) = y(i) for all i = 1, . . . D.

Given a hypothesis class H, we then deﬁne its Vapnik-Chervonenkis

dimension, written VC(H), to be the size of the largest set that is shattered

by H. (If H can shatter arbitrarily large sets, then VC(H) = ∞.)

For instance, consider the following set of three points:


123

�



�



�



x

x1

2

Can the set H of linear classiﬁers in two dimensions (h(x) = 1{θ0+θ1x1+

θ2x2 ≥ 0}) can shatter the set above? The answer is yes. Speciﬁcally, we

see that, for any of the eight possible labelings of these points, we can ﬁnd a

linear classiﬁer that obtains “zero training error” on them:

x

x1

2

x

x1

2

x

x1

2

x

x1

2

x

x1

2

x

x1

2

x

x1

2

x

x1

2

Moreover, it is possible to show that there is no set of 4 points that this

hypothesis class can shatter. Thus, the largest set that H can shatter is of

size 3, and hence VC(H) = 3.

Note that the VC dimension of H here is 3 even though there may be

sets of size 3 that it cannot shatter. For instance, if we had a set of three

points lying in a straight line (left ﬁgure), then there is no way to ﬁnd a linear

separator for the labeling of the three points shown below (right ﬁgure):


124

x

x1

2

�



�



�



x

x1

2

In order words, under the deﬁnition of the VC dimension, in order to

prove that VC(H) is at least D, we need to show only that there’s at least

one set of size D that H can shatter.

The following theorem, due to Vapnik, can then be shown. (This is, many

would argue, the most important theorem in all of learning theory.)

Theorem. Let H be given, and let D = VC(H). Then with probability at

least 1 − δ, we have that for all h ∈ H,

|ε(h) − ˆε(h)| ≤ O

��

D

n log n

D + 1

n log 1

δ

�

.

Thus, with probability at least 1 − δ, we also have that:

ε(ˆh) ≤ ε(h∗) + O

��

D

n log n

D + 1

n log 1

δ

�

.

In other words, if a hypothesis class has ﬁnite VC dimension, then uniform

convergence occurs as n becomes large. As before, this allows us to give a

bound on ε(h) in terms of ε(h∗). We also have the following corollary:

Corollary. For |ε(h) − ˆε(h)| ≤ γ to hold for all h ∈ H (and hence ε(ˆh) ≤

ε(h∗) + 2γ) with probability at least 1 − δ, it suﬃces that n = Oγ,δ(D).

In other words, the number of training examples needed to learn “well”

using H is linear in the VC dimension of H. It turns out that, for “most”

hypothesis classes, the VC dimension (assuming a “reasonable” parameter-

ization) is also roughly linear in the number of parameters. Putting these

together, we conclude that for a given hypothesis class H (and for an algo-

rithm that tries to minimize training error), the number of training examples

needed to achieve generalization error close to that of the optimal classiﬁer

is usually roughly linear in the number of parameters of H.


Chapter 9

Regularization and model

selection

9.1

Regularization

Recall that as discussed in Section 8.1, overftting is typically a result of using

too complex models, and we need to choose a proper model complexity to

achieve the optimal bias-variance tradeoﬀ. When the model complexity is

measured by the number of parameters, we can vary the size of the model

(e.g., the width of a neural net). However, the correct, informative complex-

ity measure of the models can be a function of the parameters (e.g., ℓ2 norm

of the parameters), which may not necessarily depend on the number of pa-

rameters. In such cases, we will use regularization, an important technique

in machine learning, control the model complexity and prevent overﬁtting.

Regularization typically involves adding an additional term, called a reg-

ularizer and denoted by R(θ) here, to the training loss/cost function:

Jλ(θ) = J(θ) + λR(θ)

(9.1)

Here Jλ is often called the regularized loss, and λ ≥ 0 is called the regular-

ization parameter. The regularizer R(θ) is a nonnegative function (in almost

all cases). In classical methods, R(θ) is purely a function of the parameter θ,

but some modern approach allows R(θ) to depend on the training dataset.1

The regularizer R(θ) is typically chosen to be some measure of the com-

plexity of the model θ. Thus, when using the regularized loss, we aim to

ﬁnd a model that both ﬁt the data (a small loss J(θ)) and have a small

1Here our notations generally omit the dependency on the training dataset for

simplicity—we write J(θ) even though it obviously needs to depend on the training dataset.

125


126

model complexity (a small R(θ)). The balance between the two objectives is

controlled by the regularization parameter λ. When λ = 0, the regularized

loss is equivalent to the original loss. When λ is a suﬃciently small positive

number, minimizing the regularized loss is eﬀectively minimizing the original

loss with the regularizer as the tie-breaker. When the regularizer is extremely

large, then the original loss is not eﬀective (and likely the model will have a

large bias.)

The most commonly used regularization is perhaps ℓ2 regularization,

where R(θ) =

1

2∥θ∥2

2.

It encourages the optimizer to ﬁnd a model with

small ℓ2 norm. In deep learning, it’s oftentimes referred to as weight de-

cay, because gradient descent with learning rate η on the regularized loss

Rλ(θ) is equivalent to shrinking/decaying θ by a scalar factor of 1 − ηλ and

then applying the standard gradient

θ ← θ − η∇Jλ(θ) = θ − ηλθ − η∇J(θ)

=

(1 − λη)θ

�

��

�

decaying weights

−η∇J(θ)

(9.2)

Besides encouraging simpler models, regularization can also impose in-

ductive biases or structures on the model parameters. For example, suppose

we had a prior belief that the number of non-zeros in the ground-truth model

parameters is small,2—which is oftentimes called sparsity of the model—, we

can impose a regularization on the number of non-zeros in θ, denoted by

∥θ∥0, to leverage such a prior belief. Imposing additional structure of the

parameters narrows our search space and makes the complexity of the model

family smaller,—e.g., the family of sparse models can be thought of as having

lower complexity than the family of all models—, and thus tends to lead to a

better generalization. On the other hand, imposing additional structure may

risk increasing the bias. For example, if we regularize the sparsity strongly

but no sparse models can predict the label accurately, we will suﬀer from

large bias (analogously to the situation when we use linear models to learn

data than can only be represented by quadratic functions in Section 8.1.)

The sparsity of the parameters is not a continuous function of the param-

eters, and thus we cannot optimize it with (stochastic) gradient descent. A

common relaxation is to use R(θ) = ∥θ∥1 as a continuous surrogate.3

2For linear models, this means the model just uses a few coordinates of the inputs to

make an accurate prediction.

3There has been a rich line of theoretical work that explains why ∥θ∥1 is a good sur-

rogate for encouraging sparsity, but it’s beyond the scope of this course. An intuition is:

assuming the parameter is on the unit sphere, the parameter with smallest ℓ1 norm also


127

The R(θ) = ∥θ∥1 (also called LASSO) and R(θ) =

1

2∥θ∥2

2 are perhaps

among the most commonly used regularizers for linear models. Other norm

and powers of norms are sometimes also used. The ℓ2 norm regularization is

much more commonly used with kernel methods because ℓ1 regularization is

typically not compatible with the kernel trick (the optimal solution cannot

be written as functions of inner products of features.)

In deep learning, the most commonly used regularizer is ℓ2 regularization

or weight decay. Other common ones include dropout, data augmentation,

regularizing the spectral norm of the weight matrices, and regularizing the

Lipschitzness of the model, etc. Regularization in deep learning is an ac-

tive research area, and it’s known that there is another implicit source of

regularization, as discussed in the next section.

9.2

Implicit regularization eﬀect

The implicit regularization eﬀect of optimizers, or implicit bias or algorithmic

regularization, is a new concept/phenomenon observed in the deep learning

era. It largely refers to that the optimizers can implicitly impose structures

on parameters beyond what has been imposed by the regularized loss.

In most classical settings, the loss or regularized loss has a unique global

minimum, and thus any reasonable optimizer should converge to that global

minimum and cannot impose any additional preferences. However, in deep

learning, oftentimes the loss or regularized loss has more than one (approx-

imate) global minima, and diﬀerence optimizers may converge to diﬀerent

global minima. Though these global minima have the same or similar train-

ing losses, they may be of diﬀerent nature and have dramatically diﬀerent

generalization performance. See Figures 9.1 and 9.2 and its caption for an

illustration and some experiment results. For example, it’s possible that one

global minimum gives a much more Lipschitz or sparse model than others

and thus has a better test error. It turns out that many commonly-used op-

timizers (or their components) prefer or bias towards ﬁnding global minima

of certain properties, leading to a better test performance.

happen to be the sparsest parameter with only 1 non-zero coordinate. Thus, sparsity and

ℓ1 norm gives the same extremal points to some extent.


128



θ

loss

Figure 9.1: An Illustration that diﬀerent global minima of the training loss

can have diﬀerent test performance.





Figure 9.2: Left: Performance of neural networks trained by two diﬀerent

learning rates schedules on the CIFAR-10 dataset. Although both exper-

iments used exactly the same regularized losses and the optimizers ﬁt the

training data perfectly, the models’ generalization performance diﬀer much.

Right: On a diﬀerent synthetic dataset, optimizers with diﬀerent initializa-

tions have the same training error but diﬀerent generalization performance.4

In summary, the takehome message here is that the choice of optimizer

does not only aﬀect minimizing the training loss, but also imposes implicit

regularization and aﬀects the generalization of the model. Even if your cur-

rent optimizer already converges to a small training error perfectly, you may

still need to tune your optimizer for a better generalization, .

4The setting is the same as in Woodworth et al. [2020], HaoChen et al. [2020]


129

One may wonder which components of the optimizers bias towards what

type of global minima and what type of global minima may generalize bet-

ter.

These are open questions that researchers are actively investigating.

Empirical and theoretical research have oﬀered some clues and heuristics.

In many (but deﬁnitely far from all) situations, among those setting where

optimization can succeed in minimizing the training loss, the use of larger

initial learning rate, smaller initialization, smaller batch size, and momen-

tum appears to help with biasing towards more generalizable solutions. A

conjecture (that can be proven in certain simpliﬁed case) is that stochas-

ticity in the optimization process help the optimizer to ﬁnd ﬂatter global

minima (global minima where the curvature of the loss is small), and ﬂat

global minima tend to give more Lipschitz models and better generalization.

Characterizing the implicit regularization eﬀect formally is still a challenging

open research question.

9.3

Model selection via cross validation

Suppose we are trying select among several diﬀerent models for a learning

problem. For instance, we might be using a polynomial regression model

hθ(x) = g(θ0 + θ1x + θ2x2 + · · · + θkxk), and wish to decide if k should be

0, 1, . . . , or 10. How can we automatically select a model that represents

a good tradeoﬀ between the twin evils of bias and variance5? Alternatively,

suppose we want to automatically choose the bandwidth parameter τ for

locally weighted regression, or the parameter C for our ℓ1-regularized SVM.

How can we do that?

For the sake of concreteness, in these notes we assume we have some

ﬁnite set of models M = {M1, . . . , Md} that we’re trying to select among.

For instance, in our ﬁrst example above, the model Mi would be an i-th

degree polynomial regression model. (The generalization to inﬁnite M is

not hard.6) Alternatively, if we are trying to decide between using an SVM,

a neural network or logistic regression, then M may contain these models.

5Given that we said in the previous set of notes that bias and variance are two very

diﬀerent beasts, some readers may be wondering if we should be calling them “twin” evils

here. Perhaps it’d be better to think of them as non-identical twins. The phrase “the

fraternal twin evils of bias and variance” doesn’t have the same ring to it, though.

6If we are trying to choose from an inﬁnite set of models, say corresponding to the

possible values of the bandwidth τ ∈ R+, we may discretize τ and consider only a ﬁnite

number of possible values for it. More generally, most of the algorithms described here

can all be viewed as performing optimization search in the space of models, and we can

perform this search over inﬁnite model classes as well.


130

Cross validation. Lets suppose we are, as usual, given a training set S.

Given what we know about empirical risk minimization, here’s what might

initially seem like a algorithm, resulting from using empirical risk minimiza-

tion for model selection:

1. Train each model Mi on S, to get some hypothesis hi.

2. Pick the hypotheses with the smallest training error.

This algorithm does not work. Consider choosing the degree of a poly-

nomial. The higher the degree of the polynomial, the better it will ﬁt the

training set S, and thus the lower the training error. Hence, this method will

always select a high-variance, high-degree polynomial model, which we saw

previously is often poor choice.

Here’s an algorithm that works better. In hold-out cross validation

(also called simple cross validation), we do the following:

1. Randomly split S into Strain (say, 70% of the data) and Scv (the remain-

ing 30%). Here, Scv is called the hold-out cross validation set.

2. Train each model Mi on Strain only, to get some hypothesis hi.

3. Select and output the hypothesis hi that had the smallest error ˆεScv(hi)

on the hold out cross validation set. (Here ˆεScv(h) denotes the average

error of h on the set of examples in Scv.) The error on the hold out

validation set is also referred to as the validation error.

By testing/validating on a set of examples Scv that the models were not

trained on, we obtain a better estimate of each hypothesis hi’s true general-

ization/test error. Thus, this approach is essentially picking the model with

the smallest estimated generalization/test error. The size of the validation

set depends on the total number of available examples. Usually, somewhere

between 1/4−1/3 of the data is used in the hold out cross validation set, and

30% is a typical choice. However, when the total dataset is huge, validation

set can be a smaller fraction of the total examples as long as the absolute

number of validation examples is decent. For example, for the ImageNet

dataset that has about 1M training images, the validation set is sometimes

set to be 50K images, which is only about 5% of the total examples.

Optionally, step 3 in the algorithm may also be replaced with selecting

the model Mi according to arg mini ˆεScv(hi), and then retraining Mi on the

entire training set S. (This is often a good idea, with one exception being

learning algorithms that are be very sensitive to perturbations of the initial


131

conditions and/or data. For these methods, Mi doing well on Strain does not

necessarily mean it will also do well on Scv, and it might be better to forgo

this retraining step.)

The disadvantage of using hold out cross validation is that it “wastes”

about 30% of the data. Even if we were to take the optional step of retraining

the model on the entire training set, it’s still as if we’re trying to ﬁnd a good

model for a learning problem in which we had 0.7n training examples, rather

than n training examples, since we’re testing models that were trained on

only 0.7n examples each time. While this is ﬁne if data is abundant and/or

cheap, in learning problems in which data is scarce (consider a problem with

n = 20, say), we’d like to do something better.

Here is a method, called k-fold cross validation, that holds out less

data each time:

1. Randomly split S into k disjoint subsets of m/k training examples each.

Lets call these subsets S1, . . . , Sk.

2. For each model Mi, we evaluate it as follows:

For j = 1, . . . , k

Train the model Mi on S1 ∪ · · · ∪ Sj−1 ∪ Sj+1 ∪ · · · Sk (i.e., train

on all the data except Sj) to get some hypothesis hij.

Test the hypothesis hij on Sj, to get ˆεSj(hij).

The estimated generalization error of model Mi is then calculated

as the average of the ˆεSj(hij)’s (averaged over j).

3. Pick the model Mi with the lowest estimated generalization error, and

retrain that model on the entire training set S. The resulting hypothesis

is then output as our ﬁnal answer.

A typical choice for the number of folds to use here would be k = 10.

While the fraction of data held out each time is now 1/k—much smaller

than before—this procedure may also be more computationally expensive

than hold-out cross validation, since we now need train to each model k

times.

While k = 10 is a commonly used choice, in problems in which data is

really scarce, sometimes we will use the extreme choice of k = m in order

to leave out as little data as possible each time. In this setting, we would

repeatedly train on all but one of the training examples in S, and test on that

held-out example. The resulting m = k errors are then averaged together to

obtain our estimate of the generalization error of a model. This method has


132

its own name; since we’re holding out one training example at a time, this

method is called leave-one-out cross validation.

Finally, even though we have described the diﬀerent versions of cross vali-

dation as methods for selecting a model, they can also be used more simply to

evaluate a single model or algorithm. For example, if you have implemented

some learning algorithm and want to estimate how well it performs for your

application (or if you have invented a novel learning algorithm and want to

report in a technical paper how well it performs on various test sets), cross

validation would give a reasonable way of doing so.

9.4

Bayesian statistics and regularization

In this section, we will talk about one more tool in our arsenal for our battle

against overﬁtting.

At the beginning of the quarter, we talked about parameter ﬁtting using

maximum likelihood estimation (MLE), and chose our parameters according

to

θMLE = arg max

θ

n

�

i=1

p(y(i)|x(i); θ).

Throughout our subsequent discussions, we viewed θ as an unknown param-

eter of the world. This view of the θ as being constant-valued but unknown

is taken in frequentist statistics. In the frequentist this view of the world, θ

is not random—it just happens to be unknown—and it’s our job to come up

with statistical procedures (such as maximum likelihood) to try to estimate

this parameter.

An alternative way to approach our parameter estimation problems is to

take the Bayesian view of the world, and think of θ as being a random

variable whose value is unknown.

In this approach, we would specify a

prior distribution p(θ) on θ that expresses our “prior beliefs” about the

parameters. Given a training set S = {(x(i), y(i))}n

i=1, when we are asked to

make a prediction on a new value of x, we can then compute the posterior

distribution on the parameters

p(θ|S)

=

p(S|θ)p(θ)

p(S)

=

��n

i=1 p(y(i)|x(i), θ)

�

p(θ)

�

θ (�n

i=1 p(y(i)|x(i), θ)p(θ)) dθ

(9.3)

In the equation above, p(y(i)|x(i), θ) comes from whatever model you’re using


133

for your learning problem. For example, if you are using Bayesian logistic re-

gression, then you might choose p(y(i)|x(i), θ) = hθ(x(i))y(i)(1−hθ(x(i)))(1−y(i)),

where hθ(x(i)) = 1/(1 + exp(−θTx(i))).7

When we are given a new test example x and asked to make it prediction

on it, we can compute our posterior distribution on the class label using the

posterior distribution on θ:

p(y|x, S) =

�

θ

p(y|x, θ)p(θ|S)dθ

(9.4)

In the equation above, p(θ|S) comes from Equation (9.3). Thus, for example,

if the goal is to the predict the expected value of y given x, then we would

output8

E[y|x, S] =

�

y

yp(y|x, S)dy

The procedure that we’ve outlined here can be thought of as doing “fully

Bayesian” prediction, where our prediction is computed by taking an average

with respect to the posterior p(θ|S) over θ. Unfortunately, in general it is

computationally very diﬃcult to compute this posterior distribution. This is

because it requires taking integrals over the (usually high-dimensional) θ as

in Equation (9.3), and this typically cannot be done in closed-form.

Thus, in practice we will instead approximate the posterior distribution

for θ. One common approximation is to replace our posterior distribution for

θ (as in Equation 9.4) with a single point estimate. The MAP (maximum

a posteriori) estimate for θ is given by

θMAP = arg max

θ

n

�

i=1

p(y(i)|x(i), θ)p(θ).

(9.5)

Note that this is the same formulas as for the MLE (maximum likelihood)

estimate for θ, except for the prior p(θ) term at the end.

In practical applications, a common choice for the prior p(θ) is to assume

that θ ∼ N(0, τ 2I). Using this choice of prior, the ﬁtted parameters θMAP will

have smaller norm than that selected by maximum likelihood. In practice,

this causes the Bayesian MAP estimate to be less susceptible to overﬁtting

than the ML estimate of the parameters.

For example, Bayesian logistic

regression turns out to be an eﬀective algorithm for text classiﬁcation, even

though in text classiﬁcation we usually have d ≫ n.

7Since we are now viewing θ as a random variable, it is okay to condition on it value,

and write “p(y|x, θ)” instead of “p(y|x; θ).”

8The integral below would be replaced by a summation if y is discrete-valued.


Part IV

Unsupervised learning

134


Chapter 10

Clustering and the k-means

algorithm

In the clustering problem, we are given a training set {x(1), . . . , x(n)}, and

want to group the data into a few cohesive “clusters.”

Here, x(i) ∈ Rd

as usual; but no labels y(i) are given. So, this is an unsupervised learning

problem.

The k-means clustering algorithm is as follows:

1. Initialize cluster centroids µ1, µ2, . . . , µk ∈ Rd randomly.

2. Repeat until convergence: {

For every i, set

c(i) := arg min

j

||x(i) − µj||2.

For each j, set

µj :=

�n

i=1 1{c(i) = j}x(i)

�n

i=1 1{c(i) = j} .

}

In the algorithm above, k (a parameter of the algorithm) is the number

of clusters we want to ﬁnd; and the cluster centroids µj represent our current

guesses for the positions of the centers of the clusters. To initialize the cluster

centroids (in step 1 of the algorithm above), we could choose k training

examples randomly, and set the cluster centroids to be equal to the values of

these k examples. (Other initialization methods are also possible.)

The inner-loop of the algorithm repeatedly carries out two steps: (i)

“Assigning” each training example x(i) to the closest cluster centroid µj, and

135


136













Figure 10.1: K-means algorithm. Training examples are shown as dots, and

cluster centroids are shown as crosses. (a) Original dataset. (b) Random ini-

tial cluster centroids (in this instance, not chosen to be equal to two training

examples). (c-f) Illustration of running two iterations of k-means. In each

iteration, we assign each training example to the closest cluster centroid

(shown by “painting” the training examples the same color as the cluster

centroid to which is assigned); then we move each cluster centroid to the

mean of the points assigned to it. (Best viewed in color.) Images courtesy

Michael Jordan.

(ii) Moving each cluster centroid µj to the mean of the points assigned to it.

Figure 10.1 shows an illustration of running k-means.

Is the k-means algorithm guaranteed to converge? Yes it is, in a certain

sense. In particular, let us deﬁne the distortion function to be:

J(c, µ) =

n

�

i=1

||x(i) − µc(i)||2

Thus, J measures the sum of squared distances between each training exam-

ple x(i) and the cluster centroid µc(i) to which it has been assigned. It can

be shown that k-means is exactly coordinate descent on J. Speciﬁcally, the

inner-loop of k-means repeatedly minimizes J with respect to c while holding

µ ﬁxed, and then minimizes J with respect to µ while holding c ﬁxed. Thus,


137

J must monotonically decrease, and the value of J must converge. (Usu-

ally, this implies that c and µ will converge too. In theory, it is possible for

k-means to oscillate between a few diﬀerent clusterings—i.e., a few diﬀerent

values for c and/or µ—that have exactly the same value of J, but this almost

never happens in practice.)

The distortion function J is a non-convex function, and so coordinate

descent on J is not guaranteed to converge to the global minimum. In other

words, k-means can be susceptible to local optima. Very often k-means will

work ﬁne and come up with very good clusterings despite this. But if you

are worried about getting stuck in bad local minima, one common thing to

do is run k-means many times (using diﬀerent random initial values for the

cluster centroids µj). Then, out of all the diﬀerent clusterings found, pick

the one that gives the lowest distortion J(c, µ).


Chapter 11

EM algorithms

In this set of notes, we discuss the EM (Expectation-Maximization) algorithm

for density estimation.

11.1

EM for mixture of Gaussians

Suppose that we are given a training set {x(1), . . . , x(n)} as usual. Since we

are in the unsupervised learning setting, these points do not come with any

labels.

We wish to model the data by specifying a joint distribution p(x(i), z(i)) =

p(x(i)|z(i))p(z(i)). Here, z(i) ∼ Multinomial(φ) (where φj ≥ 0, �k

j=1 φj = 1,

and the parameter φj gives p(z(i) = j)), and x(i)|z(i) = j ∼ N(µj, Σj). We

let k denote the number of values that the z(i)’s can take on. Thus, our

model posits that each x(i) was generated by randomly choosing z(i) from

{1, . . . , k}, and then x(i) was drawn from one of k Gaussians depending on

z(i). This is called the mixture of Gaussians model. Also, note that the

z(i)’s are latent random variables, meaning that they’re hidden/unobserved.

This is what will make our estimation problem diﬃcult.

The parameters of our model are thus φ, µ and Σ. To estimate them, we

can write down the likelihood of our data:

ℓ(φ, µ, Σ)

=

n

�

i=1

log p(x(i); φ, µ, Σ)

=

n

�

i=1

log

k

�

z(i)=1

p(x(i)|z(i); µ, Σ)p(z(i); φ).

However, if we set to zero the derivatives of this formula with respect to

138


139

the parameters and try to solve, we’ll ﬁnd that it is not possible to ﬁnd the

maximum likelihood estimates of the parameters in closed form. (Try this

yourself at home.)

The random variables z(i) indicate which of the k Gaussians each x(i)

had come from. Note that if we knew what the z(i)’s were, the maximum

likelihood problem would have been easy. Speciﬁcally, we could then write

down the likelihood as

ℓ(φ, µ, Σ) =

n

�

i=1

log p(x(i)|z(i); µ, Σ) + log p(z(i); φ).

Maximizing this with respect to φ, µ and Σ gives the parameters:

φj

=

1

n

n

�

i=1

1{z(i) = j},

µj

=

�n

i=1 1{z(i) = j}x(i)

�n

i=1 1{z(i) = j} ,

Σj

=

�n

i=1 1{z(i) = j}(x(i) − µj)(x(i) − µj)T

�n

i=1 1{z(i) = j}

.

Indeed, we see that if the z(i)’s were known, then maximum likelihood

estimation becomes nearly identical to what we had when estimating the

parameters of the Gaussian discriminant analysis model, except that here

the z(i)’s playing the role of the class labels.1

However, in our density estimation problem, the z(i)’s are not known.

What can we do?

The EM algorithm is an iterative algorithm that has two main steps.

Applied to our problem, in the E-step, it tries to “guess” the values of the

z(i)’s. In the M-step, it updates the parameters of our model based on our

guesses. Since in the M-step we are pretending that the guesses in the ﬁrst

part were correct, the maximization becomes easy. Here’s the algorithm:

Repeat until convergence: {

(E-step) For each i, j, set

w(i)

j

:= p(z(i) = j|x(i); φ, µ, Σ)

1There are other minor diﬀerences in the formulas here from what we’d obtained in

PS1 with Gaussian discriminant analysis, ﬁrst because we’ve generalized the z(i)’s to be

multinomial rather than Bernoulli, and second because here we are using a diﬀerent Σj

for each Gaussian.


140

(M-step) Update the parameters:

φj

:=

1

n

n

�

i=1

w(i)

j ,

µj

:=

�n

i=1 w(i)

j x(i)

�n

i=1 w(i)

j

,

Σj

:=

�n

i=1 w(i)

j (x(i) − µj)(x(i) − µj)T

�n

i=1 w(i)

j

}

In the E-step, we calculate the posterior probability of our parameters

the z(i)’s, given the x(i) and using the current setting of our parameters. I.e.,

using Bayes rule, we obtain:

p(z(i) = j|x(i); φ, µ, Σ) =

p(x(i)|z(i) = j; µ, Σ)p(z(i) = j; φ)

�k

l=1 p(x(i)|z(i) = l; µ, Σ)p(z(i) = l; φ)

Here, p(x(i)|z(i) = j; µ, Σ) is given by evaluating the density of a Gaussian

with mean µj and covariance Σj at x(i); p(z(i) = j; φ) is given by φj, and so

on. The values w(i)

j

calculated in the E-step represent our “soft” guesses2 for

the values of z(i).

Also, you should contrast the updates in the M-step with the formulas we

had when the z(i)’s were known exactly. They are identical, except that in-

stead of the indicator functions “1{z(i) = j}” indicating from which Gaussian

each datapoint had come, we now instead have the w(i)

j ’s.

The EM-algorithm is also reminiscent of the K-means clustering algo-

rithm, except that instead of the “hard” cluster assignments c(i), we instead

have the “soft” assignments w(i)

j . Similar to K-means, it is also susceptible

to local optima, so reinitializing at several diﬀerent initial parameters may

be a good idea.

It’s clear that the EM algorithm has a very natural interpretation of

repeatedly trying to guess the unknown z(i)’s; but how did it come about,

and can we make any guarantees about it, such as regarding its convergence?

In the next set of notes, we will describe a more general view of EM, one

2The term “soft” refers to our guesses being probabilities and taking values in [0, 1]; in

contrast, a “hard” guess is one that represents a single best guess (such as taking values

in {0, 1} or {1, . . . , k}).


141

that will allow us to easily apply it to other estimation problems in which

there are also latent variables, and which will allow us to give a convergence

guarantee.

11.2

Jensen’s inequality

We begin our discussion with a very useful result called Jensen’s inequality

Let f be a function whose domain is the set of real numbers. Recall that

f is a convex function if f ′′(x) ≥ 0 (for all x ∈ R). In the case of f taking

vector-valued inputs, this is generalized to the condition that its hessian H

is positive semi-deﬁnite (H ≥ 0). If f ′′(x) &gt; 0 for all x, then we say f is

strictly convex (in the vector-valued case, the corresponding statement is

that H must be positive deﬁnite, written H &gt; 0). Jensen’s inequality can

then be stated as follows:

Theorem. Let f be a convex function, and let X be a random variable.

Then:

E[f(X)] ≥ f(EX).

Moreover, if f is strictly convex, then E[f(X)] = f(EX) holds true if and

only if X = E[X] with probability 1 (i.e., if X is a constant).

Recall our convention of occasionally dropping the parentheses when writ-

ing expectations, so in the theorem above, f(EX) = f(E[X]).

For an interpretation of the theorem, consider the ﬁgure below.

a

E[X]

b

f(a)

f(b)

f(EX)

E[f(X)]

f

Here, f is a convex function shown by the solid line. Also, X is a random

variable that has a 0.5 chance of taking the value a, and a 0.5 chance of


142

taking the value b (indicated on the x-axis). Thus, the expected value of X

is given by the midpoint between a and b.

We also see the values f(a), f(b) and f(E[X]) indicated on the y-axis.

Moreover, the value E[f(X)] is now the midpoint on the y-axis between f(a)

and f(b). From our example, we see that because f is convex, it must be the

case that E[f(X)] ≥ f(EX).

Incidentally, quite a lot of people have trouble remembering which way

the inequality goes, and remembering a picture like this is a good way to

quickly ﬁgure out the answer.

Remark. Recall that f is [strictly] concave if and only if −f is [strictly]

convex (i.e., f ′′(x) ≤ 0 or H ≤ 0). Jensen’s inequality also holds for concave

functions f, but with the direction of all the inequalities reversed (E[f(X)] ≤

f(EX), etc.).

11.3

General EM algorithms

Suppose we have an estimation problem in which we have a training set

{x(1), . . . , x(n)} consisting of n independent examples. We have a latent vari-

able model p(x, z; θ) with z being the latent variable (which for simplicity is

assumed to take ﬁnite number of values). The density for x can be obtained

by marginalized over the latent variable z:

p(x; θ) =

�

z

p(x, z; θ)

(11.1)

We wish to ﬁt the parameters θ by maximizing the log-likelihood of the

data, deﬁned by

ℓ(θ)

=

n

�

i=1

log p(x(i); θ)

(11.2)

We can rewrite the objective in terms of the joint density p(x, z; θ) by

ℓ(θ)

=

n

�

i=1

log p(x(i); θ)

(11.3)

=

n

�

i=1

log

�

z(i)

p(x(i), z(i); θ).

(11.4)

But, explicitly ﬁnding the maximum likelihood estimates of the parameters

θ may be hard since it will result in diﬃcult non-convex optimization prob-


143

lems.3 Here, the z(i)’s are the latent random variables; and it is often the case

that if the z(i)’s were observed, then maximum likelihood estimation would

be easy.

In such a setting, the EM algorithm gives an eﬃcient method for max-

imum likelihood estimation. Maximizing ℓ(θ) explicitly might be diﬃcult,

and our strategy will be to instead repeatedly construct a lower-bound on ℓ

(E-step), and then optimize that lower-bound (M-step).4

It turns out that the summation �n

i=1 is not essential here, and towards a

simpler exposition of the EM algorithm, we will ﬁrst consider optimizing the

the likelihood log p(x) for a single example x. After we derive the algorithm

for optimizing log p(x), we will convert it to an algorithm that works for n

examples by adding back the sum to each of the relevant equations. Thus,

now we aim to optimize log p(x; θ) which can be rewritten as

log p(x; θ) = log

�

z

p(x, z; θ)

(11.5)

Let Q be a distribution over the possible values of z. That is, �

z Q(z) = 1,

Q(z) ≥ 0).

Consider the following:5

log p(x; θ)

=

log

�

z

p(x, z; θ)

=

log

�

z

Q(z)p(x, z; θ)

Q(z)

(11.6)

≥

�

z

Q(z) log p(x, z; θ)

Q(z)

(11.7)

The last step of this derivation used Jensen’s inequality.

Speciﬁcally,

f(x) = log x is a concave function, since f ′′(x) = −1/x2 &lt; 0 over its domain

3It’s mostly an empirical observation that the optimization problem is diﬃcult to op-

timize.

4Empirically, the E-step and M-step can often be computed more eﬃciently than op-

timizing the function ℓ(·) directly. However, it doesn’t necessarily mean that alternating

the two steps can always converge to the global optimum of ℓ(·). Even for mixture of

Gaussians, the EM algorithm can either converge to a global optimum or get stuck, de-

pending on the properties of the training data. Empirically, for real-world data, often EM

can converge to a solution with relatively high likelihood (if not the optimum), and the

theory behind it is still largely not understood.

5If z were continuous, then Q would be a density, and the summations over z in our

discussion are replaced with integrals over z.


144

x ∈ R+. Also, the term

�

z

Q(z)

�p(x, z; θ)

Q(z)

�

in the summation is just an expectation of the quantity [p(x, z; θ)/Q(z)] with

respect to z drawn according to the distribution given by Q.6 By Jensen’s

inequality, we have

f

�

Ez∼Q

�p(x, z; θ)

Q(z)

��

≥ Ez∼Q

�

f

�p(x, z; θ)

Q(z)

��

,

where the “z ∼ Q” subscripts above indicate that the expectations are with

respect to z drawn from Q. This allowed us to go from Equation (11.6) to

Equation (11.7).

Now, for any distribution Q, the formula (11.7) gives a lower-bound on

log p(x; θ). There are many possible choices for the Q’s. Which should we

choose? Well, if we have some current guess θ of the parameters, it seems

natural to try to make the lower-bound tight at that value of θ. I.e., we will

make the inequality above hold with equality at our particular value of θ.

To make the bound tight for a particular value of θ, we need for the step

involving Jensen’s inequality in our derivation above to hold with equality.

For this to be true, we know it is suﬃcient that the expectation be taken

over a “constant”-valued random variable. I.e., we require that

p(x, z; θ)

Q(z)

= c

for some constant c that does not depend on z. This is easily accomplished

by choosing

Q(z) ∝ p(x, z; θ).

Actually, since we know �

z Q(z) = 1 (because it is a distribution), this

further tells us that

Q(z)

=

p(x, z; θ)

�

z p(x, z; θ)

=

p(x, z; θ)

p(x; θ)

=

p(z|x; θ)

(11.8)

6We note that the notion p(x,z;θ)

Q(z)

only makes sense if Q(z) ̸= 0 whenever p(x, z; θ) ̸= 0.

Here we implicitly assume that we only consider those Q with such a property.


145

Thus, we simply set the Q’s to be the posterior distribution of the z’s given

x and the setting of the parameters θ.

Indeed, we can directly verify that when Q(z) = p(z|x; θ), then equa-

tion (11.7) is an equality because

�

z

Q(z) log p(x, z; θ)

Q(z)

=

�

z

p(z|x; θ) log p(x, z; θ)

p(z|x; θ)

=

�

z

p(z|x; θ) log p(z|x; θ)p(x; θ)

p(z|x; θ)

=

�

z

p(z|x; θ) log p(x; θ)

= log p(x; θ)

�

z

p(z|x; θ)

= log p(x; θ)

(because �

z p(z|x; θ) = 1)

For convenience, we call the expression in Equation (11.7) the evidence

lower bound (ELBO) and we denote it by

ELBO(x; Q, θ) =

�

z

Q(z) log p(x, z; θ)

Q(z)

(11.9)

With this equation, we can re-write equation (11.7) as

∀Q, θ, x,

log p(x; θ) ≥ ELBO(x; Q, θ)

(11.10)

Intuitively, the EM algorithm alternatively updates Q and θ by a) set-

ting Q(z) = p(z|x; θ) following Equation (11.8) so that ELBO(x; Q, θ) =

log p(x; θ) for x and the current θ, and b) maximizing ELBO(x; Q, θ) w.r.t θ

while ﬁxing the choice of Q.

Recall that all the discussion above was under the assumption that we

aim to optimize the log-likelihood log p(x; θ) for a single example x. It turns

out that with multiple training examples, the basic idea is the same and we

only needs to take a sum over examples at relevant places. Next, we will

build the evidence lower bound for multiple training examples and make the

EM algorithm formal.

Recall we have a training set {x(1), . . . , x(n)}. Note that the optimal choice

of Q is p(z|x; θ), and it depends on the particular example x. Therefore here

we will introduce n distributions Q1, . . . , Qn, one for each example x(i). For

each example x(i), we can build the evidence lower bound

log p(x(i); θ) ≥ ELBO(x(i); Qi, θ) =

�

z(i)

Qi(z(i)) log p(x(i), z(i); θ)

Qi(z(i))


146

Taking sum over all the examples, we obtain a lower bound for the log-

likelihood

ℓ(θ) ≥

�

i

ELBO(x(i); Qi, θ)

(11.11)

=

�

i

�

z(i)

Qi(z(i)) log p(x(i), z(i); θ)

Qi(z(i))

For any set of distributions Q1, . . . , Qn, the formula (11.11) gives a lower-

bound on ℓ(θ), and analogous to the argument around equation (11.8), the

Qi that attains equality satisﬁes

Qi(z(i))

=

p(z(i)|x(i); θ)

Thus, we simply set the Qi’s to be the posterior distribution of the z(i)’s

given x(i) with the current setting of the parameters θ.

Now, for this choice of the Qi’s, Equation (11.11) gives a lower-bound on

the loglikelihood ℓ that we’re trying to maximize. This is the E-step. In the

M-step of the algorithm, we then maximize our formula in Equation (11.11)

with respect to the parameters to obtain a new setting of the θ’s. Repeatedly

carrying out these two steps gives us the EM algorithm, which is as follows:

Repeat until convergence {

(E-step) For each i, set

Qi(z(i)) := p(z(i)|x(i); θ).

(M-step) Set

θ := arg max

θ

n

�

i=1

ELBO(x(i); Qi, θ)

= arg max

θ

�

i

�

z(i)

Qi(z(i)) log p(x(i), z(i); θ)

Qi(z(i))

.

(11.12)

}

How do we know if this algorithm will converge? Well, suppose θ(t) and

θ(t+1) are the parameters from two successive iterations of EM. We will now

prove that ℓ(θ(t)) ≤ ℓ(θ(t+1)), which shows EM always monotonically im-

proves the log-likelihood. The key to showing this result lies in our choice of


147

the Qi’s. Speciﬁcally, on the iteration of EM in which the parameters had

started out as θ(t), we would have chosen Q(t)

i (z(i)) := p(z(i)|x(i); θ(t)). We

saw earlier that this choice ensures that Jensen’s inequality, as applied to get

Equation (11.11), holds with equality, and hence

ℓ(θ(t)) =

n

�

i=1

ELBO(x(i); Q(t)

i , θ(t))

(11.13)

The parameters θ(t+1) are then obtained by maximizing the right hand side

of the equation above. Thus,

ℓ(θ(t+1)) ≥

n

�

i=1

ELBO(x(i); Q(t)

i , θ(t+1))

(because ineqaulity (11.11) holds for all Q and θ)

≥

n

�

i=1

ELBO(x(i); Q(t)

i , θ(t))

(see reason below)

= ℓ(θ(t))

(by equation (11.13))

where the last inequality follows from that θ(t+1) is chosen explicitly to be

arg max

θ

n

�

i=1

ELBO(x(i); Q(t)

i , θ)

Hence, EM causes the likelihood to converge monotonically. In our de-

scription of the EM algorithm, we said we’d run it until convergence. Given

the result that we just showed, one reasonable convergence test would be

to check if the increase in ℓ(θ) between successive iterations is smaller than

some tolerance parameter, and to declare convergence if EM is improving

ℓ(θ) too slowly.

Remark. If we deﬁne (by overloading ELBO(·))

ELBO(Q, θ) =

n

�

i=1

ELBO(x(i); Qi, θ) =

�

i

�

z(i)

Qi(z(i)) log p(x(i), z(i); θ)

Qi(z(i))

(11.14)

then we know ℓ(θ) ≥ ELBO(Q, θ) from our previous derivation. The EM

can also be viewed an alternating maximization algorithm on ELBO(Q, θ),

in which the E-step maximizes it with respect to Q (check this yourself), and

the M-step maximizes it with respect to θ.


148

11.3.1

Other interpretation of ELBO

Let ELBO(x; Q, θ) = �

z Q(z) log p(x,z;θ)

Q(z)

be deﬁned as in equation (11.9).

There are several other forms of ELBO. First, we can rewrite

ELBO(x; Q, θ) = Ez∼Q[log p(x, z; θ)] − Ez∼Q[log Q(z)]

= Ez∼Q[log p(x|z; θ)] − DKL(Q∥pz)

(11.15)

where we use pz to denote the marginal distribution of z (under the distri-

bution p(x, z; θ)), and DKL() denotes the KL divergence

DKL(Q∥pz) =

�

z

Q(z) log Q(z)

p(z)

(11.16)

In many cases, the marginal distribution of z does not depend on the param-

eter θ. In this case, we can see that maximizing ELBO over θ is equivalent

to maximizing the ﬁrst term in (11.15). This corresponds to maximizing the

conditional likelihood of x conditioned on z, which is often a simpler question

than the original question.

Another form of ELBO(·) is (please verify yourself)

ELBO(x; Q, θ) = log p(x) − DKL(Q∥pz|x)

(11.17)

where pz|x is the conditional distribution of z given x under the parameter

θ. This forms shows that the maximizer of ELBO(Q, θ) over Q is obtained

when Q = pz|x, which was shown in equation (11.8) before.

11.4

Mixture of Gaussians revisited

Armed with our general deﬁnition of the EM algorithm, let’s go back to our

old example of ﬁtting the parameters φ, µ and Σ in a mixture of Gaussians.

For the sake of brevity, we carry out the derivations for the M-step updates

only for φ and µj, and leave the updates for Σj as an exercise for the reader.

The E-step is easy. Following our algorithm derivation above, we simply

calculate

w(i)

j

= Qi(z(i) = j) = P(z(i) = j|x(i); φ, µ, Σ).

Here, “Qi(z(i) = j)” denotes the probability of z(i) taking the value j under

the distribution Qi.


149

Next, in the M-step, we need to maximize, with respect to our parameters

φ, µ, Σ, the quantity

n

�

i=1

�

z(i)

Qi(z(i)) log p(x(i), z(i); φ, µ, Σ)

Qi(z(i))

=

n

�

i=1

k

�

j=1

Qi(z(i) = j) log p(x(i)|z(i) = j; µ, Σ)p(z(i) = j; φ)

Qi(z(i) = j)

=

n

�

i=1

k

�

j=1

w(i)

j log

1

(2π)d/2|Σj|1/2 exp

�

− 1

2(x(i) − µj)TΣ−1

j (x(i) − µj)

�

· φj

w(i)

j

Let’s maximize this with respect to µl. If we take the derivative with respect

to µl, we ﬁnd

∇µl

n

�

i=1

k

�

j=1

w(i)

j log

1

(2π)d/2|Σj|1/2 exp

�

− 1

2(x(i) − µj)TΣ−1

j (x(i) − µj)

�

· φj

w(i)

j

=

−∇µl

n

�

i=1

k

�

j=1

w(i)

j

1

2(x(i) − µj)TΣ−1

j (x(i) − µj)

=

1

2

n

�

i=1

w(i)

l ∇µl2µT

l Σ−1

l x(i) − µT

l Σ−1

l µl

=

n

�

i=1

w(i)

l

�

Σ−1

l x(i) − Σ−1

l µl

�

Setting this to zero and solving for µl therefore yields the update rule

µl :=

�n

i=1 w(i)

l x(i)

�n

i=1 w(i)

l

,

which was what we had in the previous set of notes.

Let’s do one more example, and derive the M-step update for the param-

eters φj. Grouping together only the terms that depend on φj, we ﬁnd that

we need to maximize

n

�

i=1

k

�

j=1

w(i)

j log φj.

However, there is an additional constraint that the φj’s sum to 1, since they

represent the probabilities φj = p(z(i) = j; φ). To deal with the constraint


150

that �k

j=1 φj = 1, we construct the Lagrangian

L(φ) =

n

�

i=1

k

�

j=1

w(i)

j log φj + β(

k

�

j=1

φj − 1),

where β is the Lagrange multiplier.7 Taking derivatives, we ﬁnd

∂

∂φj

L(φ) =

n

�

i=1

w(i)

j

φj

+ β

Setting this to zero and solving, we get

φj =

�n

i=1 w(i)

j

−β

I.e., φj ∝ �n

i=1 w(i)

j . Using the constraint that �

j φj = 1, we easily ﬁnd

that −β = �n

i=1

�k

j=1 w(i)

j

= �n

i=1 1 = n. (This used the fact that w(i)

j

=

Qi(z(i) = j), and since probabilities sum to 1, �

j w(i)

j

= 1.) We therefore

have our M-step updates for the parameters φj:

φj := 1

n

n

�

i=1

w(i)

j .

The derivation for the M-step updates to Σj are also entirely straightfor-

ward.

11.5

Variational

inference

and

variational

auto-encoder (optional reading)

Loosely speaking, variational auto-encoder Kingma and Welling [2013] gen-

erally refers to a family of algorithms that extend the EM algorithms to more

complex models parameterized by neural networks. It extends the technique

of variational inference with the additional “re-parametrization trick” which

will be introduced below. Variational auto-encoder may not give the best

performance for many datasets, but it contains several central ideas about

how to extend EM algorithms to high-dimensional continuous latent variables

7We don’t need to worry about the constraint that φj ≥ 0, because as we’ll shortly see,

the solution we’ll ﬁnd from this derivation will automatically satisfy that anyway.


151

with non-linear models. Understanding it will likely give you the language

and backgrounds to understand various recent papers related to it.

As a running example, we will consider the following parameterization of

p(x, z; θ) by a neural network. Let θ be the collection of the weights of a

neural network g(z; θ) that maps z ∈ Rk to Rd. Let

z ∼ N(0, Ik×k)

(11.18)

x|z ∼ N(g(z; θ), σ2Id×d)

(11.19)

Here Ik×k denotes identity matrix of dimension k by k, and σ is a scalar that

we assume to be known for simplicity.

For the Gaussian mixture models in Section 11.4, the optimal choice of

Q(z) = p(z|x; θ) for each ﬁxed θ, that is the posterior distribution of z,

can be analytically computed. In many more complex models such as the

model (11.19), it’s intractable to compute the exact the posterior distribution

p(z|x; θ).

Recall that from equation (11.10), ELBO is always a lower bound for any

choice of Q, and therefore, we can also aim for ﬁnding an approximation of

the true posterior distribution. Often, one has to use some particular form

to approximate the true posterior distribution. Let Q be a family of Q’s that

we are considering, and we will aim to ﬁnd a Q within the family of Q that is

closest to the true posterior distribution. To formalize, recall the deﬁnition of

the ELBO lower bound as a function of Q and θ deﬁned in equation (11.14)

ELBO(Q, θ) =

n

�

i=1

ELBO(x(i); Qi, θ) =

�

i

�

z(i)

Qi(z(i)) log p(x(i), z(i); θ)

Qi(z(i))

Recall

that

EM

can

be

viewed

as

alternating

maximization

of

ELBO(Q, θ). Here instead, we optimize the EBLO over Q ∈ Q

max

Q∈Q max

θ

ELBO(Q, θ)

(11.20)

Now the next question is what form of Q (or what structural assumptions

to make about Q) allows us to eﬃciently maximize the objective above. When

the latent variable z are high-dimensional discrete variables, one popular as-

sumption is the mean ﬁeld assumption, which assumes that Qi(z) gives a

distribution with independent coordinates, or in other words, Qi can be de-

composed into Qi(z) = Q1

i (z1) · · · Qk

i (zk). There are tremendous applications

of mean ﬁeld assumptions to learning generative models with discrete latent

variables, and we refer to Blei et al. [2017] for a survey of these models and


152

their impact to a wide range of applications including computational biology,

computational neuroscience, social sciences. We will not get into the details

about the discrete latent variable cases, and our main focus is to deal with

continuous latent variables, which requires not only mean ﬁeld assumptions,

but additional techniques.

When z ∈ Rk is a continuous latent variable, there are several decisions to

make towards successfully optimizing (11.20). First we need to give a succinct

representation of the distribution Qi because it is over an inﬁnite number of

points. A natural choice is to assume Qi is a Gaussian distribution with some

mean and variance. We would also like to have more succinct representation

of the means of Qi of all the examples. Note that Qi(z(i)) is supposed to

approximate p(z(i)|x(i); θ). It would make sense let all the means of the Qi’s

be some function of x(i). Concretely, let q(·; φ), v(·; φ) be two functions that

map from dimension d to k, which are parameterized by φ and ψ, we assume

that

Qi = N(q(x(i); φ), diag(v(x(i); ψ))2)

(11.21)

Here diag(w) means the k × k matrix with the entries of w ∈ Rk on the

diagonal. In other words, the distribution Qi is assumed to be a Gaussian

distribution with independent coordinates, and the mean and standard de-

viations are governed by q and v. Often in variational auto-encoder, q and v

are chosen to be neural networks.8 In recent deep learning literature, often

q, v are called encoder (in the sense of encoding the data into latent code),

whereas g(z; θ) if often referred to as the decoder.

We remark that Qi of such form in many cases are very far from a good ap-

proximation of the true posterior distribution. However, some approximation

is necessary for feasible optimization. In fact, the form of Qi needs to satisfy

other requirements (which happened to be satisﬁed by the form (11.21))

Before optimizing the ELBO, let’s ﬁrst verify whether we can eﬃciently

evaluate the value of the ELBO for ﬁxed Q of the form (11.21) and θ. We

rewrite the ELBO as a function of φ, ψ, θ by

ELBO(φ, ψ, θ) =

n

�

i=1

Ez(i)∼Qi

�

log p(x(i), z(i); θ)

Qi(z(i))

�

,

(11.22)

where Qi = N(q(x(i); φ), diag(v(x(i); ψ))2)

Note that to evaluate Qi(z(i)) inside the expectation, we should be able to

compute the density of Qi.

To estimate the expectation Ez(i)∼Qi, we

8q and v can also share parameters. We sweep this level of details under the rug in this

note.


153

should be able to sample from distribution Qi so that we can build an

empirical estimator with samples. It happens that for Gaussian distribution

Qi = N(q(x(i); φ), diag(v(x(i); ψ))2), we are able to be both eﬃciently.

Now let’s optimize the ELBO. It turns out that we can run gradient ascent

over φ, ψ, θ instead of alternating maximization. There is no strong need to

compute the maximum over each variable at a much greater cost. (For Gaus-

sian mixture model in Section 11.4, computing the maximum is analytically

feasible and relatively cheap, and therefore we did alternating maximization.)

Mathematically, let η be the learning rate, the gradient ascent step is

θ := θ + η∇θELBO(φ, ψ, θ)

φ := φ + η∇φELBO(φ, ψ, θ)

ψ := ψ + η∇ψELBO(φ, ψ, θ)

Computing the gradient over θ is simple because

∇θELBO(φ, ψ, θ) = ∇θ

n

�

i=1

Ez(i)∼Qi

�

log p(x(i), z(i); θ)

Qi(z(i))

�

= ∇θ

n

�

i=1

Ez(i)∼Qi

�

log p(x(i), z(i); θ)

�

=

n

�

i=1

Ez(i)∼Qi

�

∇θ log p(x(i), z(i); θ)

�

,

(11.23)

But computing the gradient over φ and ψ is tricky because the sam-

pling distribution Qi depends on φ and ψ.

(Abstractly speaking, the is-

sue we face can be simpliﬁed as the problem of computing the gradi-

ent Ez∼Qφ[f(φ)] with respect to variable φ.

We know that in general,

∇Ez∼Qφ[f(φ)] ̸= Ez∼Qφ[∇f(φ)] because the dependency of Qφ on φ has to be

taken into account as well. )

The idea that comes to rescue is the so-called re-parameterization

trick: we rewrite z(i) ∼ Qi = N(q(x(i); φ), diag(v(x(i); ψ))2) in an equivalent

way:

z(i) = q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i) where ξ(i) ∼ N(0, Ik×k)

(11.24)

Here x ⊙ y denotes the entry-wise product of two vectors of the same

dimension. Here we used the fact that x ∼ N(µ, σ2) is equivalent to that

x = µ+ξσ with ξ ∼ N(0, 1). We mostly just used this fact in every dimension

simultaneously for the random variable z(i) ∼ Qi.


154

With this re-parameterization, we have that

Ez(i)∼Qi

�

log p(x(i), z(i); θ)

Qi(z(i))

�

(11.25)

= Eξ(i)∼N(0,1)

�

log p(x(i), q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i); θ)

Qi(q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i))

�

It follows that

∇φEz(i)∼Qi

�

log p(x(i), z(i); θ)

Qi(z(i))

�

= ∇φEξ(i)∼N(0,1)

�

log p(x(i), q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i); θ)

Qi(q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i))

�

= Eξ(i)∼N(0,1)

�

∇φ log p(x(i), q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i); θ)

Qi(q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i))

�

We can now sample multiple copies of ξ(i)’s to estimate the the expecta-

tion in the RHS of the equation above.9 We can estimate the gradient with

respect to ψ similarly, and with these, we can implement the gradient ascent

algorithm to optimize the ELBO over φ, ψ, θ.

There are not many high-dimensional distributions with analytically com-

putable density function are known to be re-parameterizable. We refer to

Kingma and Welling [2013] for a few other choices that can replace Gaussian

distribution.

9Empirically people sometimes just use one sample to estimate it for maximum com-

putational eﬃciency.


Chapter 12

Principal components analysis

In this set of notes, we will develop a method, Principal Components Analysis

(PCA), that tries to identify the subspace in which the data approximately

lies. PCA is computationally eﬃcient: it will require only an eigenvector

calculation (easily done with the eig function in Matlab).

Suppose we are given a dataset {x(i); i = 1, . . . , n} of attributes of n dif-

ferent types of automobiles, such as their maximum speed, turn radius, and

so on. Let x(i) ∈ Rd for each i (d ≪ n). But unknown to us, two diﬀerent

attributes—some xi and xj—respectively give a car’s maximum speed mea-

sured in miles per hour, and the maximum speed measured in kilometers per

hour. These two attributes are therefore almost linearly dependent, up to

only small diﬀerences introduced by rounding oﬀ to the nearest mph or kph.

Thus, the data really lies approximately on an n − 1 dimensional subspace.

How can we automatically detect, and perhaps remove, this redundancy?

For a less contrived example, consider a dataset resulting from a survey of

pilots for radio-controlled helicopters, where x(i)

1 is a measure of the piloting

skill of pilot i, and x(i)

2

captures how much he/she enjoys ﬂying. Because

RC helicopters are very diﬃcult to ﬂy, only the most committed students,

ones that truly enjoy ﬂying, become good pilots.

So, the two attributes

x1 and x2 are strongly correlated.

Indeed, we might posit that that the

data actually likes along some diagonal axis (the u1 direction) capturing the

intrinsic piloting “karma” of a person, with only a small amount of noise

lying oﬀ this axis. (See ﬁgure.) How can we automatically compute this u1

direction?

155


156

x1

x2 (enjoyment)

(skill)

1

u

u

2

We will shortly develop the PCA algorithm. But prior to running PCA

per se, typically we ﬁrst preprocess the data by normalizing each feature

to have mean 0 and variance 1. We do this by subtracting the mean and

dividing by the empirical standard deviation:

x(i)

j

← x(i)

j − µj

σj

where µj = 1

n

�n

i=1 x(i)

j

and σ2

j = 1

n

�n

i=1(x(i)

j − µj)2 are the mean variance of

feature j, respectively.

Subtracting µj zeros out the mean and may be omitted for data known

to have zero mean (for instance, time series corresponding to speech or other

acoustic signals). Dividing by the standard deviation σj rescales each coor-

dinate to have unit variance, which ensures that diﬀerent attributes are all

treated on the same “scale.” For instance, if x1 was cars’ maximum speed in

mph (taking values in the high tens or low hundreds) and x2 were the num-

ber of seats (taking values around 2-4), then this renormalization rescales

the diﬀerent attributes to make them more comparable. This rescaling may

be omitted if we had a priori knowledge that the diﬀerent attributes are all

on the same scale. One example of this is if each data point represented a

grayscale image, and each x(i)

j

took a value in {0, 1, . . . , 255} corresponding

to the intensity value of pixel j in image i.

Now, having normalized our data, how do we compute the “major axis

of variation” u—that is, the direction on which the data approximately lies?

One way is to pose this problem as ﬁnding the unit vector u so that when


157

the data is projected onto the direction corresponding to u, the variance of

the projected data is maximized. Intuitively, the data starts oﬀ with some

amount of variance/information in it. We would like to choose a direction u

so that if we were to approximate the data as lying in the direction/subspace

corresponding to u, as much as possible of this variance is still retained.

Consider the following dataset, on which we have already carried out the

normalization steps:

Now, suppose we pick u to correspond the the direction shown in the

ﬁgure below. The circles denote the projections of the original data onto this

line.


158

�

�





��

��





�



��

��





�



��

��





��

��

��

��

��

��

��















��

��

��

��









���

���

���

���

���

���

���















��

��

��

��









We see that the projected data still has a fairly large variance, and the

points tend to be far from zero. In contrast, suppose had instead picked the

following direction:

��



��

��





�



��

��





�

�





�����������

�����������

�����������

�����������

�����������

�����������

�����������

�����������

�����������

�����������

�����������

�����������

�����������

�����������

�����������

�����������

�����������



































������

������

������

������

������

������

������

������

������

������

������























��������

��������

��������

��������

��������

��������

��������

��������

��������

��������

��������

��������

��������

��������

��������































��������

��������

��������

��������

��������

��������

��������

��������

��������

��������

��������

��������

��������

��������

��������































�����

�����

�����

�����

�����

�����

�����

�����

















Here, the projections have a signiﬁcantly smaller variance, and are much

closer to the origin.

We would like to automatically select the direction u corresponding to

the ﬁrst of the two ﬁgures shown above. To formalize this, note that given a


159

unit vector u and a point x, the length of the projection of x onto u is given

by xTu. I.e., if x(i) is a point in our dataset (one of the crosses in the plot),

then its projection onto u (the corresponding circle in the ﬁgure) is distance

xTu from the origin. Hence, to maximize the variance of the projections, we

would like to choose a unit-length u so as to maximize:

1

n

n

�

i=1

(x(i)Tu)2 = 1

n

n

�

i=1

uTx(i)x(i)Tu

= uT

�

1

n

n

�

i=1

x(i)x(i)T

�

u.

We easily recognize that the maximizing this subject to ∥u∥2 = 1 gives the

principal eigenvector of Σ =

1

n

�n

i=1 x(i)x(i)T, which is just the empirical

covariance matrix of the data (assuming it has zero mean).1

To summarize, we have found that if we wish to ﬁnd a 1-dimensional

subspace with with to approximate the data, we should choose u to be the

principal eigenvector of Σ. More generally, if we wish to project our data

into a k-dimensional subspace (k &lt; d), we should choose u1, . . . , uk to be the

top k eigenvectors of Σ. The ui’s now form a new, orthogonal basis for the

data.2

Then, to represent x(i) in this basis, we need only compute the corre-

sponding vector

y(i) =





uT

1 x(i)

uT

2 x(i)

...

uT

k x(i)



 ∈ Rk.

Thus, whereas x(i) ∈ Rd, the vector y(i) now gives a lower, k-dimensional,

approximation/representation for x(i). PCA is therefore also referred to as

a dimensionality reduction algorithm. The vectors u1, . . . , uk are called

the ﬁrst k principal components of the data.

Remark. Although we have shown it formally only for the case of k = 1,

using well-known properties of eigenvectors it is straightforward to show that

1If you haven’t seen this before, try using the method of Lagrange multipliers to max-

imize uT Σu subject to that uT u = 1. You should be able to show that Σu = λu, for some

λ, which implies u is an eigenvector of Σ, with eigenvalue λ.

2Because Σ is symmetric, the ui’s will (or always can be chosen to be) orthogonal to

each other.


160

of all possible orthogonal bases u1, . . . , uk, the one that we have chosen max-

imizes �

i ∥y(i)∥2

2. Thus, our choice of a basis preserves as much variability

as possible in the original data.

PCA can also be derived by picking the basis that minimizes the ap-

proximation error arising from projecting the data onto the k-dimensional

subspace spanned by them. (See more in homework.)

PCA has many applications; we will close our discussion with a few exam-

ples. First, compression—representing x(i)’s with lower dimension y(i)’s—is

an obvious application. If we reduce high dimensional data to k = 2 or 3 di-

mensions, then we can also plot the y(i)’s to visualize the data. For instance,

if we were to reduce our automobiles data to 2 dimensions, then we can plot

it (one point in our plot would correspond to one car type, say) to see what

cars are similar to each other and what groups of cars may cluster together.

Another standard application is to preprocess a dataset to reduce its

dimension before running a supervised learning learning algorithm with the

x(i)’s as inputs.

Apart from computational beneﬁts, reducing the data’s

dimension can also reduce the complexity of the hypothesis class considered

and help avoid overﬁtting (e.g., linear classiﬁers over lower dimensional input

spaces will have smaller VC dimension).

Lastly, as in our RC pilot example, we can also view PCA as a noise

reduction algorithm.

In our example it, estimates the intrinsic “piloting

karma” from the noisy measures of piloting skill and enjoyment. In class, we

also saw the application of this idea to face images, resulting in eigenfaces

method. Here, each point x(i) ∈ R100×100 was a 10000 dimensional vector,

with each coordinate corresponding to a pixel intensity value in a 100x100

image of a face. Using PCA, we represent each image x(i) with a much lower-

dimensional y(i). In doing so, we hope that the principal components we

found retain the interesting, systematic variations between faces that capture

what a person really looks like, but not the “noise” in the images introduced

by minor lighting variations, slightly diﬀerent imaging conditions, and so on.

We then measure distances between faces i and j by working in the reduced

dimension, and computing ∥y(i) −y(j)∥2. This resulted in a surprisingly good

face-matching and retrieval algorithm.


Chapter 13

Independent components

analysis

Our next topic is Independent Components Analysis (ICA). Similar to PCA,

this will ﬁnd a new basis in which to represent our data. However, the goal

is very diﬀerent.

As a motivating example, consider the “cocktail party problem.” Here, d

speakers are speaking simultaneously at a party, and any microphone placed

in the room records only an overlapping combination of the d speakers’ voices.

But lets say we have d diﬀerent microphones placed in the room, and because

each microphone is a diﬀerent distance from each of the speakers, it records a

diﬀerent combination of the speakers’ voices. Using these microphone record-

ings, can we separate out the original d speakers’ speech signals?

To formalize this problem, we imagine that there is some data s ∈ Rd

that is generated via d independent sources. What we observe is

x = As,

where A is an unknown square matrix called the mixing matrix. Repeated

observations gives us a dataset {x(i); i = 1, . . . , n}, and our goal is to recover

the sources s(i) that had generated our data (x(i) = As(i)).

In our cocktail party problem, s(i) is an d-dimensional vector, and s(i)

j

is

the sound that speaker j was uttering at time i. Also, x(i) in an d-dimensional

vector, and x(i)

j

is the acoustic reading recorded by microphone j at time i.

Let W = A−1 be the unmixing matrix. Our goal is to ﬁnd W, so

that given our microphone recordings x(i), we can recover the sources by

computing s(i) = Wx(i). For notational convenience, we also let wT

i denote

161


162

the i-th row of W, so that

W =





— wT

1 —

...

— wT

d —



 .

Thus, wi ∈ Rd, and the j-th source can be recovered as s(i)

j

= wT

j x(i).

13.1

ICA ambiguities

To what degree can W = A−1 be recovered? If we have no prior knowledge

about the sources and the mixing matrix, it is easy to see that there are some

inherent ambiguities in A that are impossible to recover, given only the x(i)’s.

Speciﬁcally, let P be any d-by-d permutation matrix. This means that

each row and each column of P has exactly one “1.” Here are some examples

of permutation matrices:

P =





0

1

0

1

0

0

0

0

1



 ;

P =

� 0

1

1

0

�

;

P =

� 1

0

0

1

�

.

If z is a vector, then Pz is another vector that contains a permuted version

of z’s coordinates. Given only the x(i)’s, there will be no way to distinguish

between W and PW. Speciﬁcally, the permutation of the original sources is

ambiguous, which should be no surprise. Fortunately, this does not matter

for most applications.

Further, there is no way to recover the correct scaling of the wi’s. For in-

stance, if A were replaced with 2A, and every s(i) were replaced with (0.5)s(i),

then our observed x(i) = 2A · (0.5)s(i) would still be the same. More broadly,

if a single column of A were scaled by a factor of α, and the corresponding

source were scaled by a factor of 1/α, then there is again no way to determine

that this had happened given only the x(i)’s. Thus, we cannot recover the

“correct” scaling of the sources. However, for the applications that we are

concerned with—including the cocktail party problem—this ambiguity also

does not matter. Speciﬁcally, scaling a speaker’s speech signal s(i)

j

by some

positive factor α aﬀects only the volume of that speaker’s speech. Also, sign

changes do not matter, and s(i)

j

and −s(i)

j

sound identical when played on a

speaker. Thus, if the wi found by an algorithm is scaled by any non-zero real

number, the corresponding recovered source si = wT

i x will be scaled by the


163

same factor; but this usually does not matter. (These comments also apply

to ICA for the brain/MEG data that we talked about in class.)

Are these the only sources of ambiguity in ICA? It turns out that they

are, so long as the sources si are non-Gaussian. To see what the diﬃculty is

with Gaussian data, consider an example in which n = 2, and s ∼ N(0, I).

Here, I is the 2x2 identity matrix. Note that the contours of the density of

the standard normal distribution N(0, I) are circles centered on the origin,

and the density is rotationally symmetric.

Now, suppose we observe some x = As, where A is our mixing matrix.

Then, the distribution of x will be Gaussian, x ∼ N(0, AAT), since

Es∼N(0,I)[x] = E[As] = AE[s] = 0

Cov[x] = Es∼N(0,I)[xxT] = E[AssTAT] = AE[ssT]AT = A · Cov[s] · AT = AAT

Now, let R be an arbitrary orthogonal (less formally, a rotation/reﬂection)

matrix, so that RRT = RTR = I, and let A′ = AR. Then if the data had

been mixed according to A′ instead of A, we would have instead observed

x′ = A′s. The distribution of x′ is also Gaussian, x′ ∼ N(0, AAT), since

Es∼N(0,I)[x′(x′)T] = E[A′ssT(A′)T] = E[ARssT(AR)T] = ARRTAT = AAT.

Hence, whether the mixing matrix is A or A′, we would observe data from

a N(0, AAT) distribution. Thus, there is no way to tell if the sources were

mixed using A and A′. There is an arbitrary rotational component in the

mixing matrix that cannot be determined from the data, and we cannot

recover the original sources.

Our argument above was based on the fact that the multivariate standard

normal distribution is rotationally symmetric. Despite the bleak picture that

this paints for ICA on Gaussian data, it turns out that, so long as the data is

not Gaussian, it is possible, given enough data, to recover the d independent

sources.

13.2

Densities and linear transformations

Before moving on to derive the ICA algorithm proper, we ﬁrst digress brieﬂy

to talk about the eﬀect of linear transformations on densities.

Suppose a random variable s is drawn according to some density ps(s).

For simplicity, assume for now that s ∈ R is a real number. Now, let the

random variable x be deﬁned according to x = As (here, x ∈ R, A ∈ R). Let

px be the density of x. What is px?

Let W = A−1. To calculate the “probability” of a particular value of x,

it is tempting to compute s = Wx, then then evaluate ps at that point, and


164

conclude that “px(x) = ps(Wx).” However, this is incorrect. For example,

let s ∼ Uniform[0, 1], so ps(s) = 1{0 ≤ s ≤ 1}. Now, let A = 2, so x = 2s.

Clearly, x is distributed uniformly in the interval [0, 2]. Thus, its density is

given by px(x) = (0.5)1{0 ≤ x ≤ 2}. This does not equal ps(Wx), where

W = 0.5 = A−1. Instead, the correct formula is px(x) = ps(Wx)|W|.

More generally, if s is a vector-valued distribution with density ps, and

x = As for a square, invertible matrix A, then the density of x is given by

px(x) = ps(Wx) · |W|,

where W = A−1.

Remark. If you’re seen the result that A maps [0, 1]d to a set of volume |A|,

then here’s another way to remember the formula for px given above, that also

generalizes our previous 1-dimensional example. Speciﬁcally, let A ∈ Rd×d be

given, and let W = A−1 as usual. Also let C1 = [0, 1]d be the d-dimensional

hypercube, and deﬁne C2 = {As : s ∈ C1} ⊆ Rd to be the image of C1

under the mapping given by A. Then it is a standard result in linear algebra

(and, indeed, one of the ways of deﬁning determinants) that the volume of

C2 is given by |A|. Now, suppose s is uniformly distributed in [0, 1]d, so its

density is ps(s) = 1{s ∈ C1}. Then clearly x will be uniformly distributed

in C2. Its density is therefore found to be px(x) = 1{x ∈ C2}/vol(C2) (since

it must integrate over C2 to 1). But using the fact that the determinant

of the inverse of a matrix is just the inverse of the determinant, we have

1/vol(C2) = 1/|A| = |A−1| = |W|. Thus, px(x) = 1{x ∈ C2}|W| = 1{Wx ∈

C1}|W| = ps(Wx)|W|.

13.3

ICA algorithm

We are now ready to derive an ICA algorithm. We describe an algorithm

by Bell and Sejnowski, and we give an interpretation of their algorithm as a

method for maximum likelihood estimation. (This is diﬀerent from their orig-

inal interpretation involving a complicated idea called the infomax principal

which is no longer necessary given the modern understanding of ICA.)

We suppose that the distribution of each source sj is given by a density

ps, and that the joint distribution of the sources s is given by

p(s) =

d

�

j=1

ps(sj).


165

Note that by modeling the joint distribution as a product of marginals, we

capture the assumption that the sources are independent. Using our formulas

from the previous section, this implies the following density on x = As =

W −1s:

p(x) =

d

�

j=1

ps(wT

j x) · |W|.

All that remains is to specify a density for the individual sources ps.

Recall that, given a real-valued random variable z, its cumulative distri-

bution function (cdf) F is deﬁned by F(z0) = P(z ≤ z0) =

� z0

−∞ pz(z)dz and

the density is the derivative of the cdf: pz(z) = F ′(z).

Thus, to specify a density for the si’s, all we need to do is to specify some

cdf for it. A cdf has to be a monotonic function that increases from zero

to one. Following our previous discussion, we cannot choose the Gaussian

cdf, as ICA doesn’t work on Gaussian data. What we’ll choose instead as

a reasonable “default” cdf that slowly increases from 0 to 1, is the sigmoid

function g(s) = 1/(1 + e−s). Hence, ps(s) = g′(s).1

The square matrix W is the parameter in our model. Given a training

set {x(i); i = 1, . . . , n}, the log likelihood is given by

ℓ(W) =

n

�

i=1

�

d

�

j=1

log g′(wT

j x(i)) + log |W|

�

.

We would like to maximize this in terms W. By taking derivatives and using

the fact (from the ﬁrst set of notes) that ∇W|W| = |W|(W −1)T, we easily

derive a stochastic gradient ascent learning rule. For a training example x(i),

the update rule is:

W := W + α















1 − 2g(wT

1 x(i))

1 − 2g(wT

2 x(i))

...

1 − 2g(wT

d x(i))



 x(i)T + (W T)−1









 ,

1If you have prior knowledge that the sources’ densities take a certain form, then it

is a good idea to substitute that in here.

But in the absence of such knowledge, the

sigmoid function can be thought of as a reasonable default that seems to work well for

many problems. Also, the presentation here assumes that either the data x(i) has been

preprocessed to have zero mean, or that it can naturally be expected to have zero mean

(such as acoustic signals). This is necessary because our assumption that ps(s) = g′(s)

implies E[s] = 0 (the derivative of the logistic function is a symmetric function, and

hence gives a density corresponding to a random variable with zero mean), which implies

E[x] = E[As] = 0.


166

where α is the learning rate.

After the algorithm converges, we then compute s(i) = Wx(i) to recover

the original sources.

Remark. When writing down the likelihood of the data, we implicitly as-

sumed that the x(i)’s were independent of each other (for diﬀerent values

of i; note this issue is diﬀerent from whether the diﬀerent coordinates of

x(i) are independent), so that the likelihood of the training set was given

by �

i p(x(i); W). This assumption is clearly incorrect for speech data and

other time series where the x(i)’s are dependent, but it can be shown that

having correlated training examples will not hurt the performance of the al-

gorithm if we have suﬃcient data. However, for problems where successive

training examples are correlated, when implementing stochastic gradient as-

cent, it sometimes helps accelerate convergence if we visit training examples

in a randomly permuted order. (I.e., run stochastic gradient ascent on a

randomly shuﬄed copy of the training set.)


Chapter 14

Self-supervised learning and

foundation models

Despite its huge success, supervised learning with neural networks typically

relies on the availability of a labeled dataset of decent size, which is some-

times costly to collect. Recently, AI and machine learning are undergoing a

paradigm shift with the rise of models (e.g., BERT [Devlin et al., 2019] and

GPT-3 [Brown et al., 2020]) that are pre-trained on broad data at scale and

are adaptable to a wide range of downstream tasks. These models, called

foundation models by Bommasani et al. [2021], oftentimes leverage massive

unlabeled data so that much fewer labeled data in the downstream tasks are

needed. Moreover, though foundation models are based on standard deep

learning and transfer learning, their scale results in new emergent capabil-

ities. These models are typically (pre-)trained by self-supervised learning

methods where the supervisions/labels come from parts of the inputs.

This chapter will introduce the paradigm of foundation models and basic

related concepts.

14.1

Pretraining and adaptation

The foundation models paradigm consists of two phases: pretraining (or sim-

ply training) and adaptation. We ﬁrst pretrain a large model on a massive

unlabeled dataset (e.g., billions of unlabeled images).1 Then, we adapt the

pretrained model to a downstream task (e.g., detecting cancer from scan im-

ages). These downstream tasks are often prediction tasks with limited or

1Sometimes, pretraining can involve large-scale labeled datasets as well (e.g., the Ima-

geNet dataset).

167


168

even no labeled data. The intuition is that the pretrained models learn good

representations that capture intrinsic semantic structure/ information about

the data, and the adaptation phase customizes the model to a particular

downstream task by, e.g., retrieving the information speciﬁc to it. For ex-

ample, a model pretrained on massive unlabeled image data may learn good

general visual representations/features, and we adapt the representations to

solve biomedical imagining tasks.

We formalize the two phases below.

Pretraining.

Suppose

we

have

an

unlabeled

pretraining

dataset

{x(1), x(2) · · · , x(n)} that consists of n examples in Rd. Let φθ be a model that

is parameterized by θ and maps the input x to some m-dimensional represen-

tation φθ(x). (People also call φθ(x) ∈ Rm the embedding or features of the

example x.) We pretrain the model θ with a pretraining loss, which is often

an average of loss functions on all the examples: Lpre(θ) = 1

n

�n

i=1 ℓpre(θ, x(i)).

Here ℓpre is a so-called self-supervised loss on a single datapoint x(i), because

as shown later, e.g., in Section 14.3, the “supervision” comes from the data

point x(i) itself. It is also possible that the pretraining loss is not a sum

of losses on individual examples. We will discuss two pretraining losses in

Section 14.2 and Section 14.3.

We use some optimizers (mostly likely SGD or ADAM [Kingma and Ba,

2014]) to minimize Lpre(θ). We denote the obtained pretrained model by ˆθ.

Adaptation. For a downstream task, we usually have a labeled dataset

{(x(1)

task, y(1)

task), · · · , (x(ntask)

task

, y(ntask)

task

)} with ntask examples. The setting when

ntask = 0 is called zero-shot learning—the downstream task doesn’t have any

labeled examples. When ntask is relatively small (say, between 1 and 50), the

setting is called few-shot learning. It’s also pretty common to have a larger

ntask on the order of ranging from hundreds to tens of thousands.

An adaptation algorithm generally takes in a downstream dataset and the

pretrained model ˆθ, and outputs a variant of ˆθ that solves the downstream

task. We will discuss below two popular and general adaptation methods,

linear probe and ﬁnetuning. In addition, two other methods speciﬁc to lan-

guage problems are introduced in 14.3.1.

The linear probe approach uses a linear head on top of the representation

to predict the downstream labels. Mathematically, the adapted model out-

puts w⊤φˆθ(x), where w ∈ Rm is a parameter to be learned, and ˆθ is exactly

the pretrained model (ﬁxed). We can use SGD (or other optimizers) to train


169

w on the downstream task loss to predict the task label

min

w∈Rm

1

ntask

ntask

�

i=1

ℓtask(y(i)

task, w⊤φˆθ(x(i)

task))

(14.1)

E.g.,

if the downstream task is a regression problem,

we will have

ℓtask(ytask, w⊤φˆθ(xtask)) = (ytask − w⊤φˆθ(xtask))2.

The ﬁnetuning algorithm uses a similar structure for the downstream

prediction model, but also further ﬁnetunes the pretrained model (instead

of keeping it ﬁxed). Concretely, the prediction model is w⊤φθ(x) with pa-

rameters w and θ. We optimize both w and θ to ﬁt the downstream data,

but initialize θ with the pretrained model ˆθ. The linear head w is usually

initialized randomly.

minimize

w,θ

1

ntask

ntask

�

i=1

ℓtask(y(i)

task, w⊤φθ(x(i)

task))

(14.2)

with initialization

w ← random vector

(14.3)

θ ← ˆθ

(14.4)

Various other adaptation methods exists and are sometimes specialized

to the particular pretraining methods. We will discuss one of them in Sec-

tion 14.3.1.

14.2

Pretraining methods in computer vision

This section introduces two concrete pretraining methods for computer vi-

sion: supervised pretraining and contrastive learning.

Supervised pretraining.

Here, the pretraining dataset is a large-scale

labeled dataset (e.g., ImageNet), and the pretrained models are simply a

neural network trained with vanilla supervised learning (with the last layer

being removed). Concretely, suppose we write the learned neural network as

Uφˆθ(x), where U is the last (fully-connected) layer parameters, ˆθ corresponds

to the parameters of all the other layers, and φˆθ(x) are the penultimate

activations layer (which serves as the representation). We simply discard U

and use φˆθ(x) as the pretrained model.

Contrastive learning. Contrastive learning is a self-supervised pretraining

method that uses only unlabeled data. The main intuition is that a good

representation function φθ(·) should map semantically similar images to sim-

ilar representations, and that random pair of images should generally have


170

distinct representations. E.g., we may want to map images of two huskies to

similar representations, but a husky and an elephant should have diﬀerent

representations. One deﬁnition of similarity is that images from the same

class are similar. Using this deﬁnition will result in the so-called supervised

contrastive algorithms that work well when labeled pretraining datasets are

available.

Without labeled data, we can use data augmentation to generate a pair

of “similar” augmented images given an original image x. Data augmenta-

tion typically means that we apply random cropping, ﬂipping, and/or color

transformation on the original image x to generate a variant. We can take

two random augmentations, denoted by ˆx and ˜x, of the same original image

x, and call them a positive pair. We observe that positive pairs of images

are often semantically related because they are augmentations of the same

image. We will design a loss function for θ such that the representations of

a positive pair, φθ(ˆx), φθ(˜x), as close to each other as possible.

On the other hand, we can also take another random image z from the

pretraining dataset and generate an augmentation ˆz from z. Note that (ˆx, ˆz)

are from diﬀerent images; therefore, with a good chance, they are not seman-

tically related. We call (ˆx, ˆz) a negative or random pair.2 We will design a

loss to push the representation of random pairs, φθ(ˆx), φθ(ˆz), far away from

each other.

There are many recent algorithms based on the contrastive learning prin-

ciple, and here we introduce SIMCLR [Chen et al., 2020] as an concrete

example. The loss function is deﬁned on a batch of examples (x1, · · · , x(B))

with batch size B. The algorithm computes two random augmentations for

each example x(i) in the batch, denoted by ˆx(i) and ˜x(i). As a result, we

have the augmented batch of 2B examples: ˆx1, · · · , ˆx(B), ˜x1, · · · , ˜x(B). The

SIMCLR loss is deﬁned as3

Lpre(θ) = −

B

�

i=1

log

exp

�

φθ(ˆx(i))⊤φθ(˜x(i))

�

exp (φθ(ˆx(i))⊤φθ(˜x(i))) + �

j̸=i exp (φθ(ˆx(i))⊤φθ(˜x(j))).

The intuition is as follows. The loss is increasing in φθ(ˆx(i))⊤φθ(˜x(j)), and

thus minimizing the loss encourages φθ(ˆx(i))⊤φθ(˜x(j)) to be small, making

φθ(ˆx(i)) far away from φθ(˜x(j)). On the other hand, the loss is decreasing in

2Random pair may be a more accurate term because it’s still possible (though not

likely) that x and z are semantically related, so are ˆx and ˆz. But in the literature, the

term negative pair seems to be also common.

3This is a variant and simpliﬁcation of the original loss that does not change the essence

(but may change the eﬃciency slightly).


171

φθ(ˆx(i))⊤φθ(˜x(i)), and thus minimizing the loss encourages φθ(ˆx(i))⊤φθ(˜x(i))

to be large, resulting in φθ(ˆx(i)) and φθ(˜x(i)) to be close.4

14.3

Pretrained large language models

Natural language processing is another area where pretraining models are

particularly successful.

In language problems, an examples typically cor-

responds to a document or generally a sequence/trunk of words,5 denoted

by x = (x1, · · · , xT) where T is the length of the document/sequence,

xi ∈ {1, · · · , V } are words in the document, and V is the vocabulary size.6

A language model is a probabilistic model representing the probability of

a document, denoted by p(x1, · · · , xT). This probability distribution is very

complex because its support size is V T — exponential in the length of the

document. Instead of modeling the distribution of a document itself, we can

apply the chain rule of conditional probability to decompose it as follows:

p(x1, · · · , xT) = p(x1)p(x2 | x1) · · · p(xT | x1, · · · , xT−1).

(14.5)

Now the support of each of the conditional probability p(xt | x1, · · · , xt−1) is

V .

We will model the conditional probability p(xt | x1, · · · , xt−1) with some

parameterized form. To this end, we ﬁrst turn the discrete words into word

embeddings.

Let ei ∈ Rd be the embedding of the word i ∈ {1, 2, · · · , V }. We call

[e1, · · · , eV ] ∈ Rd×V the embedding matrix. The most commonly used model

is transformer [Vaswani et al., 2017]. We will introduce the basic concepts

regarding the inputs and outputs of a transformer, but treat the interme-

diate computation in transformer as a blackbox. We refer the students to

more advanced courses or the original paper for more details. The high-level

pipeline is visualized in Figure 14.1. Given a document (x1, · · · , xT), we ﬁrst

compute the corresponding word embeddings (ex1, · · · , exT ). Then, the word

embeddings is passed to a transformer model, which takes in a sequence of

4To see this, you can verify that the function − log

p

p+q is decreasing in p, and increasing

in q when p, q &gt; 0.

5In the practical implementations, typically all the data are concatenated into a single

sequence in some order, and each example typically corresponds a sub-sequence of consec-

utive words which may corresponds to a subset of a document or may span across multiple

documents.

6Technically, words may be decomposed into tokens which could be words or sub-words

(combinations of letters), but this note omits this technicality. In fact most commons words

are a single token themselves.


172

vectors and outputs a sequence of vectors (c2, · · · , cT+1). In particular, here

we use the autoregressive version of the transformers which makes sure that

ct only depends on x1, · · · , xt−1.7 The ct’s are often called the representations

or the contextualized embeddings, and we write ct = φθ(x1, . . . , xt−1) where

φθ denotes the mapping from the input to the representations.



Figure 14.1: The input-output interface of a transformer.

To learn the parameters θ in the transformer, we use ct to predict

the conditional probability p(xt | x1, · · · , xt−1).8

We parameterize p(xt |

x1, · · · , xt−1) by





p(xt = 1 | x1 · · · , xt−1)

p(xt = 2 | x1 · · · , xt−1)

...

p(xt = V | x1 · · · , xt−1)



 = softmax(Wtct) ∈ RV

(14.6)

= softmax(Wtφθ(x1, · · · , xt−1)),

(14.7)

where W ∈ RV ×d is a weight matrix that maps the contextualized embedding

ct to the logits.

In other words, Wt is an additional linear layer for the

prediction of the conditional probability. Recall that softmax(·) : RV �→ RV

maps the logits to the probabilities:

softmax(u) =





exp(u1)

�V

i=1 exp(ui)

...

exp(uV )

�V

i=1 exp(ui)





(14.8)

7This property no longer holds in masked language models [Devlin et al., 2019] where

the losses are also diﬀerent.

8Here t ≥ 2 and we omit the loss for predicting p(x1) for simplicity (which also doesn’t

aﬀect the performance much). To formally model p(x1), an option is to prepend a special

token x0 = ⊥ to the sequence, and then ask the language model to predict p(x1 | x0 = ⊥).


173

We train all the parameters θ in the transformers as well as the

parameters W

= (W1, . . . , WT) by the cross entropy loss.

Let pt =

softmax(Wtφθ(x1, · · · , xt−1)) ∈ RV be the predicted conditional probability

at position t. Let W be the concatenation of W1, · · · , WT. The loss function

is often called language modeling loss and deﬁned as

L(W, θ) =

T

�

t=2

(cross entropy loss at position t)

=

T

�

t=2

− log pt,xt,

(14.9)

where pt,j denotes the j-th entry of the probability vector pt.

14.3.1

Zero-shot learning and in-context learning

For language models, there are many ways to adapt a pretrained model to

downstream tasks. In this notes, we discuss three of them: ﬁnetuning, zero-

shot learning, and in-context learning.

Finetuning is not very common for the autoregressive language models that

we introduced in Section 14.3 but much more common for other variants

such as masked language models which has similar input-output interfaces

but are pretrained diﬀerently [Devlin et al., 2019]. The ﬁnetuning method is

the same as introduced generally in Section 14.1—the only question is how

we deﬁne the prediction task with an additional linear head. One option

is to treat cT+1 = φθ(x1, · · · , xT) as the representation and use w⊤cT+1 =

w⊤φθ(x1, · · · , xT) to predict task label.

As described in Section 14.1, we

initialize θ to the pretrained model ˆθ and then optimize both w and θ.

Zero-shot adaptation or zero-shot learning is the setting where there is no

input-output pairs from the downstream tasks. For language problems tasks,

typically the task is formatted as a question or a cloze test form via natural

language. For example, we can format an example as a question:

xtask = (xtask,1, · · · , xtask,T) = “Is the speed of light a universal constant?”

Then, we compute the most likely next word predicted by the lan-

guage model given this question, that is, computing argmaxxT +1p(xT+1 |

xtask,1, · · · , xtask,T). In this case, if the most likely next word xT+1 is “No”,

then we solve the task. (The speed of light is only a constant in vacuum).

We note that there are many ways to decode the answer from the language


174

models, e.g., instead of computing the argmax, we may use the language

model to generate a few words word. It is an active research question to ﬁnd

the best way to utilize the language models.

In-context learning is mostly used for few-shot settings where we have a

few labeled examples (x(1)

task, y(1)

task), · · · , (x(ntask)

task

, y(ntask)

task

). Given a test example

xtest, we construct a document (x1, · · · , xT), which is more commonly called

a “prompt” in this context, by concatenating the labeled examples and the

text example in some format. For example, we may construct the prompt as

follows

x1, · · · , xT

=

“Q: 2 ∼ 3 = ?

x(1)

task

A: 5

y(1)

task

Q: 6 ∼ 7 = ?

x(2)

task

A: 13

y(2)

task

· · ·

Q: 15 ∼ 2 = ?”

xtest

Then, we let the pretrained model generate the most likely xT+1, xT+2, · · · .

In this case, if the model can “learn” that the symbol ∼ means addition from

the few examples, we will obtain the following which suggests the answer is

17.

xT+1, xT+2, · · · = “A: 17”.

The area of foundation models is very new and quickly growing. The notes

here only attempt to introduce these models on a conceptual level with a

signiﬁcant amount of simpliﬁcation. We refer the readers to other materials,

e.g., Bommasani et al. [2021], for more details.


Part V

Reinforcement Learning and

Control

175


Chapter 15

Reinforcement learning

We now begin our study of reinforcement learning and adaptive control.

In supervised learning, we saw algorithms that tried to make their outputs

mimic the labels y given in the training set. In that setting, the labels gave

an unambiguous “right answer” for each of the inputs x. In contrast, for

many sequential decision making and control problems, it is very diﬃcult to

provide this type of explicit supervision to a learning algorithm. For example,

if we have just built a four-legged robot and are trying to program it to walk,

then initially we have no idea what the “correct” actions to take are to make

it walk, and so do not know how to provide explicit supervision for a learning

algorithm to try to mimic.

In the reinforcement learning framework, we will instead provide our al-

gorithms only a reward function, which indicates to the learning agent when

it is doing well, and when it is doing poorly. In the four-legged walking ex-

ample, the reward function might give the robot positive rewards for moving

forwards, and negative rewards for either moving backwards or falling over.

It will then be the learning algorithm’s job to ﬁgure out how to choose actions

over time so as to obtain large rewards.

Reinforcement learning has been successful in applications as diverse as

autonomous helicopter ﬂight, robot legged locomotion, cell-phone network

routing, marketing strategy selection, factory control, and eﬃcient web-page

indexing. Our study of reinforcement learning will begin with a deﬁnition of

the Markov decision processes (MDP), which provides the formalism in

which RL problems are usually posed.

176


177

15.1

Markov decision processes

A Markov decision process is a tuple (S, A, {Psa}, γ, R), where:

• S is a set of states. (For example, in autonomous helicopter ﬂight, S

might be the set of all possible positions and orientations of the heli-

copter.)

• A is a set of actions. (For example, the set of all possible directions in

which you can push the helicopter’s control sticks.)

• Psa are the state transition probabilities.

For each state s ∈ S and

action a ∈ A, Psa is a distribution over the state space. We’ll say more

about this later, but brieﬂy, Psa gives the distribution over what states

we will transition to if we take action a in state s.

• γ ∈ [0, 1) is called the discount factor.

• R : S × A �→ R is the reward function. (Rewards are sometimes also

written as a function of a state S only, in which case we would have

R : S �→ R).

The dynamics of an MDP proceeds as follows: We start in some state s0,

and get to choose some action a0 ∈ A to take in the MDP. As a result of our

choice, the state of the MDP randomly transitions to some successor state

s1, drawn according to s1 ∼ Ps0a0. Then, we get to pick another action a1.

As a result of this action, the state transitions again, now to some s2 ∼ Ps1a1.

We then pick a2, and so on. . . . Pictorially, we can represent this process as

follows:

s0

a0

−→ s1

a1

−→ s2

a2

−→ s3

a3

−→ . . .

Upon visiting the sequence of states s0, s1, . . . with actions a0, a1, . . ., our

total payoﬀ is given by

R(s0, a0) + γR(s1, a1) + γ2R(s2, a2) + · · · .

Or, when we are writing rewards as a function of the states only, this becomes

R(s0) + γR(s1) + γ2R(s2) + · · · .

For most of our development, we will use the simpler state-rewards R(s),

though the generalization to state-action rewards R(s, a) oﬀers no special

diﬃculties.


178

Our goal in reinforcement learning is to choose actions over time so as to

maximize the expected value of the total payoﬀ:

E

�

R(s0) + γR(s1) + γ2R(s2) + · · ·

�

Note that the reward at timestep t is discounted by a factor of γt. Thus, to

make this expectation large, we would like to accrue positive rewards as soon

as possible (and postpone negative rewards as long as possible). In economic

applications where R(·) is the amount of money made, γ also has a natural

interpretation in terms of the interest rate (where a dollar today is worth

more than a dollar tomorrow).

A policy is any function π : S �→ A mapping from the states to the

actions. We say that we are executing some policy π if, whenever we are

in state s, we take action a = π(s). We also deﬁne the value function for

a policy π according to

V π(s) = E

�

R(s0) + γR(s1) + γ2R(s2) + · · ·

�� s0 = s, π].

V π(s) is simply the expected sum of discounted rewards upon starting in

state s, and taking actions according to π.1

Given a ﬁxed policy π, its value function V π satisﬁes the Bellman equa-

tions:

V π(s) = R(s) + γ

�

s′∈S

Psπ(s)(s′)V π(s′).

This says that the expected sum of discounted rewards V π(s) for starting

in s consists of two terms: First, the immediate reward R(s) that we get

right away simply for starting in state s, and second, the expected sum of

future discounted rewards. Examining the second term in more detail, we

see that the summation term above can be rewritten Es′∼Psπ(s)[V π(s′)]. This

is the expected sum of discounted rewards for starting in state s′, where s′

is distributed according Psπ(s), which is the distribution over where we will

end up after taking the ﬁrst action π(s) in the MDP from state s. Thus, the

second term above gives the expected sum of discounted rewards obtained

after the ﬁrst step in the MDP.

Bellman’s equations can be used to eﬃciently solve for V π. Speciﬁcally,

in a ﬁnite-state MDP (|S| &lt; ∞), we can write down one such equation for

V π(s) for every state s. This gives us a set of |S| linear equations in |S|

variables (the unknown V π(s)’s, one for each state), which can be eﬃciently

solved for the V π(s)’s.

1This notation in which we condition on π isn’t technically correct because π isn’t a

random variable, but this is quite standard in the literature.


179

We also deﬁne the optimal value function according to

V ∗(s) = max

π

V π(s).

(15.1)

In other words, this is the best possible expected sum of discounted rewards

that can be attained using any policy. There is also a version of Bellman’s

equations for the optimal value function:

V ∗(s) = R(s) + max

a∈A γ

�

s′∈S

Psa(s′)V ∗(s′).

(15.2)

The ﬁrst term above is the immediate reward as before. The second term

is the maximum over all actions a of the expected future sum of discounted

rewards we’ll get upon after action a. You should make sure you understand

this equation and see why it makes sense.

We also deﬁne a policy π∗ : S �→ A as follows:

π∗(s) = arg max

a∈A

�

s′∈S

Psa(s′)V ∗(s′).

(15.3)

Note that π∗(s) gives the action a that attains the maximum in the “max”

in Equation (15.2).

It is a fact that for every state s and every policy π, we have

V ∗(s) = V π∗(s) ≥ V π(s).

The ﬁrst equality says that the V π∗, the value function for π∗, is equal to the

optimal value function V ∗ for every state s. Further, the inequality above

says that π∗’s value is at least a large as the value of any other other policy.

In other words, π∗ as deﬁned in Equation (15.3) is the optimal policy.

Note that π∗ has the interesting property that it is the optimal policy

for all states s. Speciﬁcally, it is not the case that if we were starting in

some state s then there’d be some optimal policy for that state, and if we

were starting in some other state s′ then there’d be some other policy that’s

optimal policy for s′. The same policy π∗ attains the maximum in Equa-

tion (15.1) for all states s. This means that we can use the same policy π∗

no matter what the initial state of our MDP is.

15.2

Value iteration and policy iteration

We now describe two eﬃcient algorithms for solving ﬁnite-state MDPs. For

now, we will consider only MDPs with ﬁnite state and action spaces (|S| &lt;


180

∞, |A| &lt; ∞). In this section, we will also assume that we know the state

transition probabilities {Psa} and the reward function R.

The ﬁrst algorithm, value iteration, is as follows:

Algorithm 6 Value Iteration

1: For each state s, initialize V (s) := 0.

2: for until convergence do

3:

For every state, update

V (s) := R(s) + max

a∈A γ

�

s′

Psa(s′)V (s′).

(15.4)

This algorithm can be thought of as repeatedly trying to update the

estimated value function using Bellman Equations (15.2).

There are two possible ways of performing the updates in the inner loop of

the algorithm. In the ﬁrst, we can ﬁrst compute the new values for V (s) for

every state s, and then overwrite all the old values with the new values. This

is called a synchronous update. In this case, the algorithm can be viewed as

implementing a “Bellman backup operator” that takes a current estimate of

the value function, and maps it to a new estimate. (See homework problem

for details.)

Alternatively, we can also perform asynchronous updates.

Here, we would loop over the states (in some order), updating the values one

at a time.

Under either synchronous or asynchronous updates, it can be shown that

value iteration will cause V to converge to V ∗. Having found V ∗, we can

then use Equation (15.3) to ﬁnd the optimal policy.

Apart from value iteration, there is a second standard algorithm for ﬁnd-

ing an optimal policy for an MDP. The policy iteration algorithm proceeds

as follows:

Thus, the inner-loop repeatedly computes the value function for the cur-

rent policy, and then updates the policy using the current value function.

(The policy π found in step (b) is also called the policy that is greedy with

respect to V .) Note that step (a) can be done via solving Bellman’s equa-

tions as described earlier, which in the case of a ﬁxed policy, is just a set of

|S| linear equations in |S| variables.

After at most a ﬁnite number of iterations of this algorithm, V will con-

verge to V ∗, and π will converge to π∗.2

2Note that value iteration cannot reach the exact V ∗ in a ﬁnite number of iterations,


181

Algorithm 7 Policy Iteration

1: Initialize π randomly.

2: for until convergence do

3:

Let V := V π.

▷ typically by linear system solver

4:

For each state s, let

π(s) := arg max

a∈A

�

s′

Psa(s′)V (s′).

Both value iteration and policy iteration are standard algorithms for solv-

ing MDPs, and there isn’t currently universal agreement over which algo-

rithm is better.

For small MDPs, policy iteration is often very fats and

converges with very few iterations.

However, for MDPs with large state

spaces, solving for V π explicitly would involve solving a large system of lin-

ear equations, and could be diﬃcult (and note that one has to solve the

linear system multiple times in policy iteration). In these problems, value

iteration may be preferred. For this reason, in practice value iteration seems

to be used more often than policy iteration. For some more discussions on

the comparison and connection of value iteration and policy iteration, please

see Section 15.5.

15.3

Learning a model for an MDP

So far, we have discussed MDPs and algorithms for MDPs assuming that the

state transition probabilities and rewards are known. In many realistic prob-

lems, we are not given state transition probabilities and rewards explicitly,

but must instead estimate them from data. (Usually, S, A and γ are known.)

For example, suppose that, for the inverted pendulum problem (see prob-

whereas policy iteration with an exact linear system solver, can. This is because when

the actions space and policy space are discrete and ﬁnite, and once the policy reaches the

optimal policy in policy iteration, then it will not change at all. On the other hand, even

though value iteration will converge to the V ∗, but there is always some non-zero error in

the learned value function.


182

lem set 4), we had a number of trials in the MDP, that proceeded as follows:

s(1)

0

a(1)

0

−→ s(1)

1

a(1)

1

−→ s(1)

2

a(1)

2

−→ s(1)

3

a(1)

3

−→ . . .

s(2)

0

a(2)

0

−→ s(2)

1

a(2)

1

−→ s(2)

2

a(2)

2

−→ s(2)

3

a(2)

3

−→ . . .

. . .

Here, s(j)

i

is the state we were at time i of trial j, and a(j)

i

is the cor-

responding action that was taken from that state. In practice, each of the

trials above might be run until the MDP terminates (such as if the pole falls

over in the inverted pendulum problem), or it might be run for some large

but ﬁnite number of timesteps.

Given this “experience” in the MDP consisting of a number of trials,

we can then easily derive the maximum likelihood estimates for the state

transition probabilities:

Psa(s′) = #times took we action a in state s and got to s′

#times we took action a in state s

(15.5)

Or, if the ratio above is “0/0”—corresponding to the case of never having

taken action a in state s before—the we might simply estimate Psa(s′) to be

1/|S|. (I.e., estimate Psa to be the uniform distribution over all states.)

Note that, if we gain more experience (observe more trials) in the MDP,

there is an eﬃcient way to update our estimated state transition probabilities

using the new experience. Speciﬁcally, if we keep around the counts for both

the numerator and denominator terms of (15.5), then as we observe more

trials, we can simply keep accumulating those counts. Computing the ratio

of these counts then given our estimate of Psa.

Using a similar procedure, if R is unknown, we can also pick our estimate

of the expected immediate reward R(s) in state s to be the average reward

observed in state s.

Having learned a model for the MDP, we can then use either value it-

eration or policy iteration to solve the MDP using the estimated transition

probabilities and rewards. For example, putting together model learning and

value iteration, here is one possible algorithm for learning in an MDP with

unknown state transition probabilities:

1. Initialize π randomly.

2. Repeat {

(a) Execute π in the MDP for some number of trials.


183

(b) Using the accumulated experience in the MDP, update our esti-

mates for Psa (and R, if applicable).

(c) Apply value iteration with the estimated state transition probabil-

ities and rewards to get a new estimated value function V .

(d) Update π to be the greedy policy with respect to V .

}

We note that, for this particular algorithm, there is one simple optimiza-

tion that can make it run much more quickly. Speciﬁcally, in the inner loop

of the algorithm where we apply value iteration, if instead of initializing value

iteration with V = 0, we initialize it with the solution found during the pre-

vious iteration of our algorithm, then that will provide value iteration with

a much better initial starting point and make it converge more quickly.

15.4

Continuous state MDPs

So far, we’ve focused our attention on MDPs with a ﬁnite number of states.

We now discuss algorithms for MDPs that may have an inﬁnite number of

states. For example, for a car, we might represent the state as (x, y, θ, ˙x, ˙y, ˙θ),

comprising its position (x, y); orientation θ; velocity in the x and y directions

˙x and ˙y; and angular velocity ˙θ. Hence, S = R6 is an inﬁnite set of states,

because there is an inﬁnite number of possible positions and orientations

for the car.3 Similarly, the inverted pendulum you saw in PS4 has states

(x, θ, ˙x, ˙θ), where θ is the angle of the pole. And, a helicopter ﬂying in 3d

space has states of the form (x, y, z, φ, θ, ψ, ˙x, ˙y, ˙z, ˙φ, ˙θ, ˙ψ), where here the roll

φ, pitch θ, and yaw ψ angles specify the 3d orientation of the helicopter.

In this section, we will consider settings where the state space is S = Rd,

and describe ways for solving such MDPs.

15.4.1

Discretization

Perhaps the simplest way to solve a continuous-state MDP is to discretize

the state space, and then to use an algorithm like value iteration or policy

iteration, as described previously.

For example, if we have 2d states (s1, s2), we can use a grid to discretize

the state space:

3Technically, θ is an orientation and so the range of θ is better written θ ∈ [−π, π) than

θ ∈ R; but for our purposes, this distinction is not important.


184

[t]



Here, each grid cell represents a separate discrete state ¯s.

We can

then approximate the continuous-state MDP via a discrete-state one

( ¯S, A, {P¯sa}, γ, R), where ¯S is the set of discrete states, {P¯sa} are our state

transition probabilities over the discrete states, and so on. We can then use

value iteration or policy iteration to solve for the V ∗(¯s) and π∗(¯s) in the

discrete state MDP ( ¯S, A, {P¯sa}, γ, R). When our actual system is in some

continuous-valued state s ∈ S and we need to pick an action to execute, we

compute the corresponding discretized state ¯s, and execute action π∗(¯s).

This discretization approach can work well for many problems. However,

there are two downsides. First, it uses a fairly naive representation for V ∗

(and π∗). Speciﬁcally, it assumes that the value function is takes a constant

value over each of the discretization intervals (i.e., that the value function is

piecewise constant in each of the gridcells).

To better understand the limitations of such a representation, consider a

supervised learning problem of ﬁtting a function to this dataset:

[t]

1

2

3

4

5

6

7

8

1.5

2

2.5

3

3.5

4

4.5

5

5.5

x

y


185

Clearly, linear regression would do ﬁne on this problem. However, if we

instead discretize the x-axis, and then use a representation that is piecewise

constant in each of the discretization intervals, then our ﬁt to the data would

look like this:

[t]

1

2

3

4

5

6

7

8

1.5

2

2.5

3

3.5

4

4.5

5

5.5

x

y

This piecewise constant representation just isn’t a good representation for

many smooth functions. It results in little smoothing over the inputs, and no

generalization over the diﬀerent grid cells. Using this sort of representation,

we would also need a very ﬁne discretization (very small grid cells) to get a

good approximation.

A second downside of this representation is called the curse of dimen-

sionality. Suppose S = Rd, and we discretize each of the d dimensions of the

state into k values. Then the total number of discrete states we have is kd.

This grows exponentially quickly in the dimension of the state space d, and

thus does not scale well to large problems. For example, with a 10d state, if

we discretize each state variable into 100 values, we would have 10010 = 1020

discrete states, which is far too many to represent even on a modern desktop

computer.

As a rule of thumb, discretization usually works extremely well for 1d

and 2d problems (and has the advantage of being simple and quick to im-

plement). Perhaps with a little bit of cleverness and some care in choosing

the discretization method, it often works well for problems with up to 4d

states. If you’re extremely clever, and somewhat lucky, you may even get it

to work for some 6d problems. But it very rarely works for problems any

higher dimensional than that.


186

15.4.2

Value function approximation

We now describe an alternative method for ﬁnding policies in continuous-

state MDPs, in which we approximate V ∗ directly, without resorting to dis-

cretization. This approach, called value function approximation, has been

successfully applied to many RL problems.

Using a model or simulator

To develop a value function approximation algorithm, we will assume that

we have a model, or simulator, for the MDP. Informally, a simulator is

a black-box that takes as input any (continuous-valued) state st and action

at, and outputs a next-state st+1 sampled according to the state transition

probabilities Pstat:

[t]



There are several ways that one can get such a model. One is to use

physics simulation. For example, the simulator for the inverted pendulum

in PS4 was obtained by using the laws of physics to calculate what position

and orientation the cart/pole will be in at time t + 1, given the current state

at time t and the action a taken, assuming that we know all the parameters

of the system such as the length of the pole, the mass of the pole, and so

on. Alternatively, one can also use an oﬀ-the-shelf physics simulation software

package which takes as input a complete physical description of a mechanical

system, the current state st and action at, and computes the state st+1 of the

system a small fraction of a second into the future.4

An alternative way to get a model is to learn one from data collected in

the MDP. For example, suppose we execute n trials in which we repeatedly

take actions in an MDP, each trial for T timesteps. This can be done picking

actions at random, executing some speciﬁc policy, or via some other way of

4Open Dynamics Engine (http://www.ode.com) is one example of a free/open-source

physics simulator that can be used to simulate systems like the inverted pendulum, and

that has been a reasonably popular choice among RL researchers.


187

choosing actions. We would then observe n state sequences like the following:

s(1)

0

a(1)

0

−→ s(1)

1

a(1)

1

−→ s(1)

2

a(1)

2

−→ · · ·

a(1)

T −1

−→ s(1)

T

s(2)

0

a(2)

0

−→ s(2)

1

a(2)

1

−→ s(2)

2

a(2)

2

−→ · · ·

a(2)

T −1

−→ s(2)

T

· · ·

s(n)

0

a(n)

0

−→ s(n)

1

a(n)

1

−→ s(n)

2

a(n)

2

−→ · · ·

a(n)

T −1

−→ s(n)

T

We can then apply a learning algorithm to predict st+1 as a function of st

and at.

For example, one may choose to learn a linear model of the form

st+1 = Ast + Bat,

(15.6)

using an algorithm similar to linear regression. Here, the parameters of the

model are the matrices A and B, and we can estimate them using the data

collected from our n trials, by picking

arg min

A,B

n

�

i=1

T−1

�

t=0

���s(i)

t+1 −

�

As(i)

t

+ Ba(i)

t

����

2

2 .

We could also potentially use other loss functions for learning the model.

For example, it has been found in recent work Luo et al. [2018] that using

∥ · ∥2 norm (without the square) may be helpful in certain cases.

Having learned A and B, one option is to build a deterministic model,

in which given an input st and at, the output st+1 is exactly determined.

Speciﬁcally, we always compute st+1 according to Equation (15.6). Alter-

natively, we may also build a stochastic model, in which st+1 is a random

function of the inputs, by modeling it as

st+1 = Ast + Bat + ϵt,

where here ϵt is a noise term, usually modeled as ϵt ∼ N(0, Σ). (The covari-

ance matrix Σ can also be estimated from data in a straightforward way.)

Here, we’ve written the next-state st+1 as a linear function of the current

state and action; but of course, non-linear functions are also possible. Specif-

ically, one can learn a model st+1 = Aφs(st) + Bφa(at), where φs and φa are

some non-linear feature mappings of the states and actions. Alternatively,

one can also use non-linear learning algorithms, such as locally weighted lin-

ear regression, to learn to estimate st+1 as a function of st and at. These

approaches can also be used to build either deterministic or stochastic sim-

ulators of an MDP.


188

Fitted value iteration

We now describe the ﬁtted value iteration algorithm for approximating

the value function of a continuous state MDP. In the sequel, we will assume

that the problem has a continuous state space S = Rd, but that the action

space A is small and discrete.5

Recall that in value iteration, we would like to perform the update

V (s)

:=

R(s) + γ max

a

�

s′ Psa(s′)V (s′)ds′

(15.7)

=

R(s) + γ max

a

Es′∼Psa[V (s′)]

(15.8)

(In Section 15.2, we had written the value iteration update with a summation

V (s) := R(s) + γ maxa

�

s′ Psa(s′)V (s′) rather than an integral over states;

the new notation reﬂects that we are now working in continuous states rather

than discrete states.)

The main idea of ﬁtted value iteration is that we are going to approxi-

mately carry out this step, over a ﬁnite sample of states s(1), . . . , s(n). Specif-

ically, we will use a supervised learning algorithm—linear regression in our

description below—to approximate the value function as a linear or non-linear

function of the states:

V (s) = θTφ(s).

Here, φ is some appropriate feature mapping of the states.

For each state s in our ﬁnite sample of n states, ﬁtted value iteration

will ﬁrst compute a quantity y(i), which will be our approximation to R(s) +

γ maxa Es′∼Psa[V (s′)] (the right hand side of Equation 15.8). Then, it will

apply a supervised learning algorithm to try to get V (s) close to R(s) +

γ maxa Es′∼Psa[V (s′)] (or, in other words, to try to get V (s) close to y(i)).

In detail, the algorithm is as follows:

1. Randomly sample n states s(1), s(2), . . . s(n) ∈ S.

2. Initialize θ := 0.

3. Repeat {

For i = 1, . . . , n {

5In practice, most MDPs have much smaller action spaces than state spaces. E.g., a car

has a 6d state space, and a 2d action space (steering and velocity controls); the inverted

pendulum has a 4d state space, and a 1d action space; a helicopter has a 12d state space,

and a 4d action space. So, discretizing this set of actions is usually less of a problem than

discretizing the state space would have been.


189

For each action a ∈ A {

Sample s′

1, . . . , s′

k ∼ Ps(i)a (using a model of the MDP).

Set q(a) = 1

k

�k

j=1 R(s(i)) + γV (s′

j)

// Hence, q(a) is an estimate of R(s(i)) +

γEs′∼Ps(i)a[V (s′)].

}

Set y(i) = maxa q(a).

// Hence,

y(i)

is an estimate of R(s(i)) +

γ maxa Es′∼Ps(i)a[V (s′)].

}

// In the original value iteration algorithm (over discrete states)

// we updated the value function according to V (s(i)) := y(i).

// In this algorithm, we want V (s(i)) ≈ y(i), which we’ll achieve

// using supervised learning (linear regression).

Set θ := arg minθ

1

2

�n

i=1

�

θTφ(s(i)) − y(i)�2

}

Above, we had written out ﬁtted value iteration using linear regression

as the algorithm to try to make V (s(i)) close to y(i). That step of the algo-

rithm is completely analogous to a standard supervised learning (regression)

problem in which we have a training set (x(1), y(1)), (x(2), y(2)), . . . , (x(n), y(n)),

and want to learn a function mapping from x to y; the only diﬀerence is that

here s plays the role of x. Even though our description above used linear re-

gression, clearly other regression algorithms (such as locally weighted linear

regression) can also be used.

Unlike value iteration over a discrete set of states, ﬁtted value iteration

cannot be proved to always to converge. However, in practice, it often does

converge (or approximately converge), and works well for many problems.

Note also that if we are using a deterministic simulator/model of the MDP,

then ﬁtted value iteration can be simpliﬁed by setting k = 1 in the algorithm.

This is because the expectation in Equation (15.8) becomes an expectation

over a deterministic distribution, and so a single example is suﬃcient to

exactly compute that expectation. Otherwise, in the algorithm above, we

had to draw k samples, and average to try to approximate that expectation

(see the deﬁnition of q(a), in the algorithm pseudo-code).


190

Finally, ﬁtted value iteration outputs V , which is an approximation to

V ∗. This implicitly deﬁnes our policy. Speciﬁcally, when our system is in

some state s, and we need to choose an action, we would like to choose the

action

arg max

a

Es′∼Psa[V (s′)]

(15.9)

The process for computing/approximating this is similar to the inner-loop of

ﬁtted value iteration, where for each action, we sample s′

1, . . . , s′

k ∼ Psa to

approximate the expectation. (And again, if the simulator is deterministic,

we can set k = 1.)

In practice, there are often other ways to approximate this step as well.

For example, one very common case is if the simulator is of the form st+1 =

f(st, at) + ϵt, where f is some deterministic function of the states (such as

f(st, at) = Ast + Bat), and ϵ is zero-mean Gaussian noise. In this case, we

can pick the action given by

arg max

a

V (f(s, a)).

In other words, here we are just setting ϵt = 0 (i.e., ignoring the noise in

the simulator), and setting k = 1.

Equivalent, this can be derived from

Equation (15.9) using the approximation

Es′[V (s′)]

≈

V (Es′[s′])

(15.10)

=

V (f(s, a)),

(15.11)

where here the expectation is over the random s′ ∼ Psa. So long as the noise

terms ϵt are small, this will usually be a reasonable approximation.

However, for problems that don’t lend themselves to such approximations,

having to sample k|A| states using the model, in order to approximate the

expectation above, can be computationally expensive.

15.5

Connections between Policy and Value

Iteration (Optional)

In the policy iteration, line 3 of Algorithm 7, we typically use linear system

solver to compute V π.

Alternatively, one can also the iterative Bellman

updates, similarly to the value iteration, to evaluate V π, as in the Procedure

VE(·) in Line 1 of Algorithm 8 below. Here if we take option 1 in Line 2 of

the Procedure VE, then the diﬀerence between the Procedure VE from the


191

Algorithm 8 Variant of Policy Iteration

1: procedure VE(π, k)

▷ To evaluate V π

2:

Option 1: initialize V (s) := 0; Option 2: Initialize from the current

V in the main algorithm.

3:

for i = 0 to k − 1 do

4:

For every state s, update

V (s) := R(s) + γ

�

s′

Psπ(s)(s′)V (s′).

(15.12)

return V

5:

Require: hyperparameter k.

6: Initialize π randomly.

7: for until convergence do

8:

Let V = VE(π, k).

9:

For each state s, let

π(s) := arg max

a∈A

�

s′

Psa(s′)V (s′).

(15.13)


192

value iteration (Algorithm 6) is that on line 4, the procedure is using the

action from π instead of the greedy action.

Using the Procedure VE, we can build Algorithm 8, which is a variant

of policy iteration that serves an intermediate algorithm that connects pol-

icy iteration and value iteration. Here we are going to use option 2 in VE

to maximize the re-use of knowledge learned before. One can verify indeed

that if we take k = 1 and use option 2 in Line 2 in Algorithm 8, then Algo-

rithm 8 is semantically equivalent to value iteration (Algorithm 6). In other

words, both Algorithm 8 and value iteration interleave the updates in (15.13)

and (15.12). Algorithm 8 alternate between k steps of update (15.12) and

one step of (15.13), whereas value iteration alternates between 1 steps of up-

date (15.12) and one step of (15.13). Therefore generally Algorithm 8 should

not be faster than value iteration, because assuming that update (15.12)

and (15.13) are equally useful and time-consuming, then the optimal balance

of the update frequencies could be just k = 1 or k ≈ 1.

On the other hand, if k steps of update (15.12) can be done much faster

than k times a single step of (15.12), then taking additional steps of equa-

tion (15.12) in group might be useful. This is what policy iteration is lever-

aging — the linear system solver can give us the result of Procedure VE with

k = ∞ much faster than using the Procedure VE for a large k. On the ﬂip

side, when such a speeding-up eﬀect no longer exists, e.g.,, when the state

space is large and linear system solver is also not fast, then value iteration is

more preferable.


Chapter 16

LQR, DDP and LQG

16.1

Finite-horizon MDPs

In Chapter 15, we deﬁned Markov Decision Processes (MDPs) and covered

Value Iteration / Policy Iteration in a simpliﬁed setting. More speciﬁcally we

introduced the optimal Bellman equation that deﬁnes the optimal value

function V π∗ of the optimal policy π∗.

V π∗(s) = R(s) + max

a∈A γ

�

s′∈S

Psa(s′)V π∗(s′)

Recall that from the optimal value function, we were able to recover the

optimal policy π∗ with

π∗(s) = argmaxa∈A

�

s′∈S

Psa(s′)V ∗(s′)

In this chapter, we’ll place ourselves in a more general setting:

1. We want to write equations that make sense for both the discrete and

the continuous case. We’ll therefore write

Es′∼Psa

�

V π∗(s′)

�

instead of

�

s′∈S

Psa(s′)V π∗(s′)

meaning that we take the expectation of the value function at the next

state. In the ﬁnite case, we can rewrite the expectation as a sum over

193


194

states. In the continuous case, we can rewrite the expectation as an

integral. The notation s′ ∼ Psa means that the state s′ is sampled from

the distribution Psa.

2. We’ll assume that the rewards depend on both states and actions. In

other words, R : S ×A → R. This implies that the previous mechanism

for computing the optimal action is changed into

π∗(s) = argmaxa∈A R(s, a) + γEs′∼Psa

�

V π∗(s′)

�

3. Instead of considering an inﬁnite horizon MDP, we’ll assume that we

have a ﬁnite horizon MDP that will be deﬁned as a tuple

(S, A, Psa, T, R)

with T &gt; 0 the time horizon (for instance T = 100). In this setting,

our deﬁnition of payoﬀ is going to be (slightly) diﬀerent:

R(s0, a0) + R(s1, a1) + · · · + R(sT, aT)

instead of (inﬁnite horizon case)

R(s0, a0) + γR(s1, a1) + γ2R(s2, a2) + . . .

∞

�

t=0

R(st, at)γt

What happened to the discount factor γ?

Remember that the intro-

duction of γ was (partly) justiﬁed by the necessity of making sure that

the inﬁnite sum would be ﬁnite and well-deﬁned. If the rewards are

bounded by a constant ¯R, the payoﬀ is indeed bounded by

|

∞

�

t=0

R(st)γt| ≤ ¯R

∞

�

t=0

γt

and we recognize a geometric sum! Here, as the payoﬀ is a ﬁnite sum,

the discount factor γ is not necessary anymore.


195

In this new setting, things behave quite diﬀerently. First, the optimal

policy π∗ might be non-stationary, meaning that it changes over time.

In other words, now we have

π(t) : S → A

where the superscript (t) denotes the policy at time step t. The dynam-

ics of the ﬁnite horizon MDP following policy π(t) proceeds as follows:

we start in some state s0, take some action a0 := π(0)(s0) according to

our policy at time step 0. The MDP transitions to a successor s1, drawn

according to Ps0a0. Then, we get to pick another action a1 := π(1)(s1)

following our new policy at time step 1 and so on...

Why does the optimal policy happen to be non-stationary in the ﬁnite-

horizon setting? Intuitively, as we have a ﬁnite numbers of actions to

take, we might want to adopt diﬀerent strategies depending on where

we are in the environment and how much time we have left. Imagine

a grid with 2 goals with rewards +1 and +10. At the beginning, we

might want to take actions to aim for the +10 goal. But if after some

steps, dynamics somehow pushed us closer to the +1 goal and we don’t

have enough steps left to be able to reach the +10 goal, then a better

strategy would be to aim for the +1 goal...

4. This observation allows us to use time dependent dynamics

st+1 ∼ P (t)

st,at

meaning that the transition’s distribution P (t)

st,at changes over time. The

same thing can be said about R(t). Note that this setting is a better

model for real life.

In a car, the gas tank empties, traﬃc changes,

etc. Combining the previous remarks, we’ll use the following general

formulation for our ﬁnite horizon MDP

�

S, A, P (t)

sa , T, R(t)�

Remark: notice that the above formulation would be equivalent to

adding the time into the state.


196

The value function at time t for a policy π is then deﬁned in the same

way as before, as an expectation over trajectories generated following

policy π starting in state s.

Vt(s) = E

�

R(t)(st, at) + · · · + R(T)(sT, aT)|st = s, π

�

Now, the question is

In this ﬁnite-horizon setting, how do we ﬁnd the optimal value function

V ∗

t (s) = max

π

V π

t (s)

It turns out that Bellman’s equation for Value Iteration is made for Dy-

namic Programming. This may come as no surprise as Bellman is one of

the fathers of dynamic programming and the Bellman equation is strongly

related to the ﬁeld.

To understand how we can simplify the problem by

adopting an iteration-based approach, we make the following observations:

1. Notice that at the end of the game (for time step T), the optimal value

is obvious

∀s ∈ S :

V ∗

T (s) := max

a∈A R(T)(s, a)

(16.1)

2. For another time step 0 ≤ t &lt; T, if we suppose that we know the

optimal value function for the next time step V ∗

t+1, then we have

∀t &lt; T, s ∈ S :

V ∗

t (s) := max

a∈A

�

R(t)(s, a) + Es′∼P (t)

sa

�

V ∗

t+1(s′)

��

(16.2)

With these observations in mind, we can come up with a clever algorithm

to solve for the optimal value function:

1. compute V ∗

T using equation (16.1).

2. for t = T − 1, . . . , 0:

compute V ∗

t using V ∗

t+1 using equation (16.2)


197

Side note We can interpret standard value iteration as a special case

of this general case, but without keeping track of time. It turns out that

in the standard setting, if we run value iteration for T steps, we get a γT

approximation of the optimal value iteration (geometric convergence). See

problem set 4 for a proof of the following result:

Theorem Let B denote the Bellman update and ||f(x)||∞ := supx |f(x)|.

If Vt denotes the value function at the t-th step, then

||Vt+1 − V ∗||∞ = ||B(Vt) − V ∗||∞

≤ γ||Vt − V ∗||∞

≤ γt||V1 − V ∗||∞

In other words, the Bellman operator B is a γ-contracting operator.

16.2

Linear Quadratic Regulation (LQR)

In this section, we’ll cover a special case of the ﬁnite-horizon setting described

in Section 16.1, for which the exact solution is (easily) tractable. This

model is widely used in robotics, and a common technique in many problems

is to reduce the formulation to this framework.

First, let’s describe the model’s assumptions. We place ourselves in the

continuous setting, with

S = Rd,

A = Rd

and we’ll assume linear transitions (with noise)

st+1 = Atst + Btat + wt

where At ∈ Rd×d, Bt ∈ Rd×d are matrices and wt ∼ N(0, Σt) is some

gaussian noise (with zero mean). As we’ll show in the following paragraphs,

it turns out that the noise, as long as it has zero mean, does not impact the

optimal policy!

We’ll also assume quadratic rewards

R(t)(st, at) = −s⊤

t Utst − a⊤

t Wtat


198

where Ut ∈ Rd×n, Wt ∈ Rd×d are positive deﬁnite matrices (meaning that

the reward is always negative).

Remark Note that the quadratic formulation of the reward is equivalent

to saying that we want our state to be close to the origin (where the reward

is higher). For example, if Ut = Id (the identity matrix) and Wt = Id, then

Rt = −||st||2 − ||at||2, meaning that we want to take smooth actions (small

norm of at) to go back to the origin (small norm of st). This could model a

car trying to stay in the middle of lane without making impulsive moves...

Now that we have deﬁned the assumptions of our LQR model, let’s cover

the 2 steps of the LQR algorithm

step 1 suppose

that

we

don’t

know

the

matrices

A, B, Σ.

To

esti-

mate them, we can follow the ideas outlined in the Value Ap-

proximation section of the RL notes.

First,

collect transitions

from an arbitrary policy.

Then,

use linear regression to ﬁnd

argminA,B

�n

i=1

�T−1

t=0

���s(i)

t+1 −

�

As(i)

t

+ Ba(i)

t

����

2

.

Finally, use a tech-

nique seen in Gaussian Discriminant Analysis to learn Σ.

step 2 assuming that the parameters of our model are known (given or esti-

mated with step 1), we can derive the optimal policy using dynamic

programming.

In other words, given

�

st+1

= Atst + Btat + wt

At, Bt, Ut, Wt, Σt known

R(t)(st, at)

= −s⊤

t Utst − a⊤

t Wtat

we want to compute V ∗

t . If we go back to section 16.1, we can apply

dynamic programming, which yields

1. Initialization step

For the last time step T,

V ∗

T (sT) = max

aT ∈A RT(sT, aT)

= max

aT ∈A −s⊤

T UTsT − a⊤

T WtaT

= −s⊤

T UtsT

(maximized for aT = 0)


199

2. Recurrence step

Let t &lt; T. Suppose we know V ∗

t+1.

Fact 1: It can be shown that if V ∗

t+1 is a quadratic function in st, then V ∗

t

is also a quadratic function. In other words, there exists some matrix Φ

and some scalar Ψ such that

if V ∗

t+1(st+1) = s⊤

t+1Φt+1st+1 + Ψt+1

then V ∗

t (st) = s⊤

t Φtst + Ψt

For time step t = T, we had Φt = −UT and ΨT = 0.

Fact 2: We can show that the optimal policy is just a linear function of

the state.

Knowing V ∗

t+1 is equivalent to knowing Φt+1 and Ψt+1, so we just need

to explain how we compute Φt and Ψt from Φt+1 and Ψt+1 and the other

parameters of the problem.

V ∗

t (st) = s⊤

t Φtst + Ψt

= max

at

�

R(t)(st, at) + Est+1∼P (t)

st,at[V ∗

t+1(st+1)]

�

= max

at

�

−s⊤

t Utst − a⊤

t Vtat + Est+1∼N(Atst+Btat,Σt)[s⊤

t+1Φt+1st+1 + Ψt+1]

�

where the second line is just the deﬁnition of the optimal value function

and the third line is obtained by plugging in the dynamics of our model

along with the quadratic assumption. Notice that the last expression is

a quadratic function in at and can thus be (easily) optimized1. We get

the optimal action a∗

t

a∗

t =

�

(B⊤

t Φt+1Bt − Vt)−1BtΦt+1At

�

· st

= Lt · st

where

Lt :=

�

(B⊤

t Φt+1Bt − Wt)−1BtΦt+1At

�

1Use the identity E

�

w⊤

t Φt+1wt

�

= Tr(ΣtΦt+1) with wt ∼ N (0, Σt)


200

which is an impressive result: our optimal policy is linear in st. Given

a∗

t we can solve for Φt and Ψt. We ﬁnally get the Discrete Ricatti

equations

Φt = A⊤

t

�

Φt+1 − Φt+1Bt

�

B⊤

t Φt+1Bt − Wt

�−1 BtΦt+1

�

At − Ut

Ψt = − tr (ΣtΦt+1) + Ψt+1

Fact 3: we notice that Φt depends on neither Ψ nor the noise Σt! As Lt

is a function of At, Bt and Φt+1, it implies that the optimal policy also

does not depend on the noise! (But Ψt does depend on Σt, which

implies that V ∗

t depends on Σt.)

Then, to summarize, the LQR algorithm works as follows

1. (if necessary) estimate parameters At, Bt, Σt

2. initialize ΦT := −UT and ΨT := 0.

3. iterate from t = T − 1 . . . 0 to update Φt and Ψt using Φt+1 and Ψt+1

using the discrete Ricatti equations. If there exists a policy that drives

the state towards zero, then convergence is guaranteed!

Using Fact 3, we can be even more clever and make our algorithm run

(slightly) faster!

As the optimal policy does not depend on Ψt, and the

update of Φt only depends on Φt, it is suﬃcient to update only Φt!

16.3

From non-linear dynamics to LQR

It turns out that a lot of problems can be reduced to LQR, even if dynamics

are non-linear. While LQR is a nice formulation because we are able to come

up with a nice exact solution, it is far from being general. Let’s take for

instance the case of the inverted pendulum. The transitions between states

look like









xt+1

˙xt+1

θt+1

˙θt+1







 = F

















xt

˙xt

θt

˙θt







 , at









where the function F depends on the cos of the angle etc.

Now, the

question we may ask is

Can we linearize this system?


201

16.3.1

Linearization of dynamics

Let’s suppose that at time t, the system spends most of its time in some state

¯st and the actions we perform are around ¯at. For the inverted pendulum, if

we reached some kind of optimal, this is true: our actions are small and we

don’t deviate much from the vertical.

We are going to use Taylor expansion to linearize the dynamics. In the

simple case where the state is one-dimensional and the transition function F

does not depend on the action, we would write something like

st+1 = F(st) ≈ F(¯st) + F ′(¯st) · (st − ¯st)

In the more general setting, the formula looks the same, with gradients

instead of simple derivatives

st+1 ≈ F(¯st, ¯at) + ∇sF(¯st, ¯at) · (st − ¯st) + ∇aF(¯st, ¯at) · (at − ¯at)

(16.3)

and now, st+1 is linear in st and at, because we can rewrite equation (16.3)

as

st+1 ≈ Ast + Bst + κ

where κ is some constant and A, B are matrices. Now, this writing looks

awfully similar to the assumptions made for LQR. We just have to get rid

of the constant term κ! It turns out that the constant term can be absorbed

into st by artiﬁcially increasing the dimension by one. This is the same trick

that we used at the beginning of the class for linear regression...

16.3.2

Diﬀerential Dynamic Programming (DDP)

The previous method works well for cases where the goal is to stay around

some state s∗ (think about the inverted pendulum, or a car having to stay

in the middle of a lane).

However, in some cases, the goal can be more

complicated.

We’ll cover a method that applies when our system has to follow some

trajectory (think about a rocket). This method is going to discretize the

trajectory into discrete time steps, and create intermediary goals around

which we will be able to use the previous technique! This method is called

Diﬀerential Dynamic Programming. The main steps are


202

step 1 come up with a nominal trajectory using a naive controller, that approx-

imate the trajectory we want to follow. In other words, our controller

is able to approximate the gold trajectory with

s∗

0, a∗

0 → s∗

1, a∗

1 → . . .

step 2 linearize the dynamics around each trajectory point s∗

t, in other words

st+1 ≈ F(s∗

t, a∗

t) + ∇sF(s∗

t, a∗

t)(st − s∗

t) + ∇aF(s∗

t, a∗

t)(at − a∗

t)

where st, at would be our current state and action. Now that we have

a linear approximation around each of these points, we can use the

previous section and rewrite

st+1 = At · st + Bt · at

(notice that in that case, we use the non-stationary dynamics setting

that we mentioned at the beginning of these lecture notes)

Note We can apply a similar derivation for the reward R(t), with a

second-order Taylor expansion.

R(st, at) ≈ R(s∗

t, a∗

t) + ∇sR(s∗

t, a∗

t)(st − s∗

t) + ∇aR(s∗

t, a∗

t)(at − a∗

t)

+ 1

2(st − s∗

t)⊤Hss(st − s∗

t) + (st − s∗

t)⊤Hsa(at − a∗

t)

+ 1

2(at − a∗

t)⊤Haa(at − a∗

t)

where Hxy refers to the entry of the Hessian of R with respect to x and

y evaluated in (s∗

t, a∗

t) (omitted for readability). This expression can be

re-written as

Rt(st, at) = −s⊤

t Utst − a⊤

t Wtat

for some matrices Ut, Wt, with the same trick of adding an extra dimen-

sion of ones. To convince yourself, notice that

�

1

x

�

·

�a

b

b

c

�

·

�1

x

�

= a + 2bx + cx2


203

step 3 Now, you can convince yourself that our problem is strictly re-written

in the LQR framework. Let’s just use LQR to ﬁnd the optimal policy

πt. As a result, our new controller will (hopefully) be better!

Note: Some problems might arise if the LQR trajectory deviates too

much from the linearized approximation of the trajectory, but that can

be ﬁxed with reward-shaping...

step 4 Now that we get a new controller (our new policy πt), we use it to

produce a new trajectory

s∗

0, π0(s∗

0) → s∗

1, π1(s∗

1) → . . . → s∗

T

note that when we generate this new trajectory, we use the real F and

not its linear approximation to compute transitions, meaning that

s∗

t+1 = F(s∗

t, a∗

t)

then, go back to step 2 and repeat until some stopping criterion.

16.4

Linear Quadratic Gaussian (LQG)

Often, in the real word, we don’t get to observe the full state st. For example,

an autonomous car could receive an image from a camera, which is merely

an observation, and not the full state of the world. So far, we assumed

that the state was available. As this might not hold true for most of the

real-world problems, we need a new tool to model this situation: Partially

Observable MDPs.

A POMDP is an MDP with an extra observation layer. In other words,

we introduce a new variable ot, that follows some conditional distribution

given the current state st

ot|st ∼ O(o|s)

Formally, a ﬁnite-horizon POMDP is given by a tuple

(S, O, A, Psa, T, R)

Within this framework, the general strategy is to maintain a belief state

(distribution over states) based on the observation o1, . . . , ot. Then, a policy

in a POMDP maps this belief states to actions.


204

In this section, we’ll present a extension of LQR to this new setting.

Assume that we observe yt ∈ Rn with m &lt; n such that

�

yt

= C · st + vt

st+1

= A · st + B · at + wt

where C ∈ Rn×d is a compression matrix and vt is the sensor noise (also

gaussian, like wt). Note that the reward function R(t) is left unchanged, as a

function of the state (not the observation) and action. Also, as distributions

are gaussian, the belief state is also going to be gaussian. In this new frame-

work, let’s give an overview of the strategy we are going to adopt to ﬁnd the

optimal policy:

step 1 ﬁrst, compute the distribution on the possible states (the belief state),

based on the observations we have. In other words, we want to compute

the mean st|t and the covariance Σt|t of

st|y1, . . . , yt ∼ N

�

st|t, Σt|t

�

to perform the computation eﬃciently over time, we’ll use the Kalman

Filter algorithm (used on-board Apollo Lunar Module!).

step 2 now that we have the distribution, we’ll use the mean st|t as the best

approximation for st

step 3 then set the action at := Ltst|t where Lt comes from the regular LQR

algorithm.

Intuitively, to understand why this works, notice that st|t is a noisy ap-

proximation of st (equivalent to adding more noise to LQR) but we proved

that LQR is independent of the noise!

Step 1 needs to be explicated. We’ll cover a simple case where there is

no action dependence in our dynamics (but the general case follows the same

idea). Suppose that

�

st+1

= A · st + wt,

wt ∼ N(0, Σs)

yt

= C · st + vt,

vt ∼ N(0, Σy)

As noises are Gaussians, we can easily prove that the joint distribution is

also Gaussian


205



















s1

...

st

y1

...

yt



















∼ N (µ, Σ)

for some µ, Σ

then, using the marginal formulas of gaussians (see Factor Analysis notes),

we would get

st|y1, . . . , yt ∼ N

�

st|t, Σt|t

�

However, computing the marginal distribution parameters using these

formulas would be computationally expensive! It would require manipulating

matrices of shape t × t. Recall that inverting a matrix can be done in O(t3),

and it would then have to be repeated over the time steps, yielding a cost in

O(t4)!

The Kalman ﬁlter algorithm provides a much better way of computing

the mean and variance, by updating them over time in constant time in

t! The kalman ﬁlter is based on two basics steps. Assume that we know the

distribution of st|y1, . . . , yt:

predict step compute st+1|y1, . . . , yt

update step compute st+1|y1, . . . , yt+1

and iterate over time steps! The combination of the predict and update

steps updates our belief states. In other words, the process looks like

(st|y1, . . . , yt)

predict

−−−−→ (st+1|y1, . . . , yt)

update

−−−−→ (st+1|y1, . . . , yt+1)

predict

−−−−→ . . .

predict step Suppose that we know the distribution of

st|y1, . . . , yt ∼ N

�

st|t, Σt|t

�

then, the distribution over the next state is also a gaussian distribution

st+1|y1, . . . , yt ∼ N

�

st+1|t, Σt+1|t

�

where


206

�

st+1|t

= A · st|t

Σt+1|t

= A · Σt|t · A⊤ + Σs

update step given st+1|t and Σt+1|t such that

st+1|y1, . . . , yt ∼ N

�

st+1|t, Σt+1|t

�

we can prove that

st+1|y1, . . . , yt+1 ∼ N

�

st+1|t+1, Σt+1|t+1

�

where

�

st+1|t+1

= st+1|t + Kt(yt+1 − Cst+1|t)

Σt+1|t+1

= Σt+1|t − Kt · C · Σt+1|t

with

Kt := Σt+1|tC⊤(CΣt+1|tC⊤ + Σy)−1

The matrix Kt is called the Kalman gain.

Now, if we have a closer look at the formulas, we notice that we don’t

need the observations prior to time step t! The update steps only depends

on the previous distribution. Putting it all together, the algorithm ﬁrst runs

a forward pass to compute the Kt, Σt|t and st|t (sometimes referred to as

ˆs in the literature). Then, it runs a backward pass (the LQR updates) to

compute the quantities Ψt, Ψt and Lt. Finally, we recover the optimal policy

with a∗

t = Ltst|t.


Chapter 17

Policy Gradient

(REINFORCE)

We will present a model-free algorithm called REINFORCE that does not

require the notion of value functions and Q functions. It turns out to be more

convenient to introduce REINFORCE in the ﬁnite horizon case, which will

be assumed throughout this note: we use τ = (s0, a0, . . . , sT−1, aT−1, sT) to

denote a trajectory, where T &lt; ∞ is the length of the trajectory. Moreover,

REINFORCE only applies to learning a randomized policy. We use πθ(a|s)

to denote the probability of the policy πθ outputting the action a at state s.

The other notations will be the same as in previous lecture notes.

The advantage of applying REINFORCE is that we only need to assume

that we can sample from the transition probabilities {Psa} and can query the

reward function R(s, a) at state s and action a,1 but we do not need to know

the analytical form of the transition probabilities or the reward function.

We do not explicitly learn the transition probabilities or the reward function

either.

Let s0 be sampled from some distribution µ. We consider optimizing the

expected total payoﬀ of the policy πθ over the parameter θ deﬁned as.

η(θ) ≜ E

�T−1

�

t=0

γtR(st, at)

�

(17.1)

Recall that st ∼ Pst−1at−1 and at ∼ πθ(·|st).

Also note that η(θ) =

Es0∼P [V πθ(s0)] if we ignore the diﬀerence between ﬁnite and inﬁnite hori-

zon.

1In this notes we will work with the general setting where the reward depends on both

the state and the action.

207


208

We aim to use gradient ascent to maximize η(θ). The main challenge

we face here is to compute (or estimate) the gradient of η(θ) without the

knowledge of the form of the reward function and the transition probabilities.

Let Pθ(τ) denote the distribution of τ (generated by the policy πθ), and

let f(τ) = �T−1

t=0 γtR(st, at). We can rewrite η(θ) as

η(θ) = Eτ∼Pθ [f(τ)]

(17.2)

We face a similar situations in the variational auto-encoder (VAE) setting

covered in the previous lectures, where the we need to take the gradient w.r.t

to a variable that shows up under the expectation — the distribution Pθ

depends on θ. Recall that in VAE, we used the re-parametrization techniques

to address this problem.

However it does not apply here because we do

know not how to compute the gradient of the function f. (We only have

an eﬃcient way to evaluate the function f by taking a weighted sum of the

observed rewards, but we do not necessarily know the reward function itself

to compute the gradient.)

The REINFORCE algorithm uses an another approach to estimate the

gradient of η(θ). We start with the following derivation:

∇θEτ∼Pθ [f(τ)] = ∇θ

�

Pθ(τ)f(τ)dτ

=

�

∇θ(Pθ(τ)f(τ))dτ

(swap integration with gradient)

=

�

(∇θPθ(τ))f(τ)dτ

(becaue f does not depend on θ)

=

�

Pθ(τ)(∇θ log Pθ(τ))f(τ)dτ

(because ∇ log Pθ(τ) = ∇Pθ(τ)

Pθ(τ) )

= Eτ∼Pθ [(∇θ log Pθ(τ))f(τ)]

(17.3)

Now we have a sample-based estimator for ∇θEτ∼Pθ [f(τ)]. Let τ (1), . . . , τ (n)

be n empirical samples from Pθ (which are obtained by running the policy

πθ for n times, with T steps for each run). We can estimate the gradient of

η(θ) by

∇θEτ∼Pθ [f(τ)] = Eτ∼Pθ [(∇θ log Pθ(τ))f(τ)]

(17.4)

≈ 1

n

n

�

i=1

(∇θ log Pθ(τ (i)))f(τ (i))

(17.5)


209

The next question is how to compute log Pθ(τ).

We derive an analyt-

ical formula for log Pθ(τ) and compute its gradient w.r.t θ (using auto-

diﬀerentiation). Using the deﬁnition of τ, we have

Pθ(τ) = µ(s0)πθ(a0|s0)Ps0a0(s1)πθ(a1|s1)Ps1a1(s2) · · · PsT −1aT −1(sT)

(17.6)

Here recall that µ to used to denote the density of the distribution of s0. It

follows that

log Pθ(τ) = log µ(s0) + log πθ(a0|s0) + log Ps0a0(s1) + log πθ(a1|s1)

+ log Ps1a1(s2) + · · · + log PsT −1aT −1(sT)

(17.7)

Taking gradient w.r.t to θ, we obtain

∇θ log Pθ(τ) = ∇θ log πθ(a0|s0) + ∇θ log πθ(a1|s1) + · · · + ∇θ log πθ(aT−1|sT−1)

Note that many of the terms disappear because they don’t depend on θ and

thus have zero gradients. (This is somewhat important — we don’t know how

to evaluate those terms such as log Ps0a0(s1) because we don’t have access to

the transition probabilities, but luckily those terms have zero gradients!)

Plugging the equation above into equation (17.4), we conclude that

∇θη(θ) = ∇θEτ∼Pθ [f(τ)] = Eτ∼Pθ

��T−1

�

t=0

∇θ log πθ(at|st)

�

· f(τ)

�

= Eτ∼Pθ

��T−1

�

t=0

∇θ log πθ(at|st)

�

·

�T−1

�

t=0

γtR(st, at)

��

(17.8)

We estimate the RHS of the equation above by empirical sample trajectories,

and the estimate is unbiased. The vanilla REINFORCE algorithm iteratively

updates the parameter by gradient ascent using the estimated gradients.

Interpretation of the policy gradient formula (17.8).

The quantity

∇θPθ(τ) = �T−1

t=0 ∇θ log πθ(at|st) is intuitively the direction of the change

of θ that will make the trajectory τ more likely to occur (or increase the

probability of choosing action a0, . . . , at−1), and f(τ) is the total payoﬀ of

this trajectory. Thus, by taking a gradient step, intuitively we are trying to

improve the likelihood of all the trajectories, but with a diﬀerent emphasis

or weight for each τ (or for each set of actions a0, a1, . . . , at−1). If τ is very

rewarding (that is, f(τ) is large), we try very hard to move in the direction


210

that can increase the probability of the trajectory τ (or the direction that

increases the probability of choosing a0, . . . , at−1), and if τ has low payoﬀ,

we try less hard with a smaller weight.

An interesting fact that follows from formula (17.3) is that

Eτ∼Pθ

�T−1

�

t=0

∇θ log πθ(at|st)

�

= 0

(17.9)

To see this, we take f(τ) = 1 (that is, the reward is always a constant),

then the LHS of (17.8) is zero because the payoﬀ is always a ﬁxed constant

�T

t=0 γt. Thus the RHS of (17.8) is also zero, which implies (17.9).

In fact, one can verify that Eat∼πθ(·|st)∇θ log πθ(at|st) = 0 for any ﬁxed t

and st.2 This fact has two consequences. First, we can simplify formula (17.8)

to

∇θη(θ) =

T−1

�

t=0

Eτ∼Pθ

�

∇θ log πθ(at|st) ·

�T−1

�

j=0

γjR(sj, aj)

��

=

T−1

�

t=0

Eτ∼Pθ

�

∇θ log πθ(at|st) ·

�T−1

�

j≥t

γjR(sj, aj)

��

(17.10)

where the second equality follows from

Eτ∼Pθ

�

∇θ log πθ(at|st) ·

� �

0≤j&lt;t

γjR(sj, aj)

��

= E

�

E [∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] ·

� �

0≤j&lt;t

γjR(sj, aj)

��

= 0

(because E [∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] = 0)

Note that here we used the law of total expectation. The outer expecta-

tion in the second line above is over the randomness of s0, a0, . . . , at−1, st,

whereas the inner expectation is over the randomness of at (conditioned on

s0, a0, . . . , at−1, st.) We see that we’ve made the estimator slightly simpler.

The second consequence of Eat∼πθ(·|st)∇θ log πθ(at|st) = 0 is the following: for

any value B(st) that only depends on st, it holds that

Eτ∼Pθ [∇θ log πθ(at|st) · B(st)]

= E [E [∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] B(st)]

= 0

(because E [∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] = 0)

2In general, it’s true that Ex∼pθ[∇ log pθ(x)] = 0.


211

Again here we used the law of total expectation.

The outer expecta-

tion in the second line above is over the randomness of s0, a0, . . . , at−1, st,

whereas the inner expectation is over the randomness of at (conditioned on

s0, a0, . . . , at−1, st.) It follows from equation (17.10) and the equation above

that

∇θη(θ) =

T−1

�

t=0

Eτ∼Pθ

�

∇θ log πθ(at|st) ·

�T−1

�

j≥t

γjR(sj, aj) − γtB(st)

��

=

T−1

�

t=0

Eτ∼Pθ

�

∇θ log πθ(at|st) · γt

�T−1

�

j≥t

γj−tR(sj, aj) − B(st)

��

(17.11)

Therefore, we will get a diﬀerent estimator for estimating the ∇η(θ) with a

diﬀerence choice of B(·). The beneﬁt of introducing a proper B(·) — which

is often referred to as a baseline — is that it helps reduce the variance of the

estimator.3 It turns out that a near optimal estimator would be the expected

future payoﬀ E

��T−1

j≥t γj−tR(sj, aj)|st

�

, which is pretty much the same as the

value function V πθ(st) (if we ignore the diﬀerence between ﬁnite and inﬁnite

horizon.) Here one could estimate the value function V πθ(·) in a crude way,

because its precise value doesn’t inﬂuence the mean of the estimator but only

the variance. This leads to a policy gradient algorithm with baselines stated

in Algorithm 9.4

3As a heuristic but illustrating example, suppose for a ﬁxed t, the future reward

�T −1

j≥t γj−tR(sj, aj) randomly takes two values 1000 + 1 and 1000 − 2 with equal proba-

bility, and the corresponding values for ∇θ log πθ(at|st) are vector z and −z. (Note that

because E [∇θ log πθ(at|st)] = 0, if ∇θ log πθ(at|st) can only take two values uniformly,

then the two values have to two vectors in an opposite direction.) In this case, without

subtracting the baseline, the estimators take two values (1000 + 1)z and −(1000 − 2)z,

whereas after subtracting a baseline of 1000, the estimator has two values z and 2z. The

latter estimator has much lower variance compared to the original estimator.

4We note that the estimator of the gradient in the algorithm does not exactly match

the equation 17.11. If we multiply γt in the summand of equation (17.13), then they will

exactly match. Removing such discount factors empirically works well because it gives a

large update.


212

Algorithm 9 Vanilla policy gradient with baseline

for i = 1, · · · do

Collect a set of trajectories by executing the current policy. Use R≥t

as a shorthand for �T−1

j≥t γj−tR(sj, aj)

Fit the baseline by ﬁnding a function B that minimizes

�

τ

�

t

(R≥t − B(st))2

(17.12)

Update the policy parameter θ with the gradient estimator

�

τ

�

t

∇θ log πθ(at|st) · (R≥t − B(st))

(17.13)


Bibliography

Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling

modern machine-learning practice and the classical bias–variance trade-

oﬀ. Proceedings of the National Academy of Sciences, 116(32):15849–15854,

2019.

Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for

weak features. SIAM Journal on Mathematics of Data Science, 2(4):1167–

1180, 2020.

David M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. Variational inference:

A review for statisticians. Journal of the American Statistical Association,

112(518):859–877, 2017.

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran

Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine

Bosselut, Emma Brunskill, et al. On the opportunities and risks of foun-

dation models. arXiv preprint arXiv:2108.07258, 2021.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-

plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sas-

try, Amanda Askell, et al. Language models are few-shot learners. Advances

in neural information processing systems, 33:1877–1901, 2020.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton.

A simple framework for contrastive learning of visual representations. In

International Conference on Machine Learning, pages 1597–1607. PMLR,

2020.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:

Pre-training of deep bidirectional transformers for language understand-

ing. In Proceedings of the 2019 Conference of the North American Chapter

of the Association for Computational Linguistics: Human Language Tech-

nologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.

213


214

Jeﬀ Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters:

Understanding the implicit bias of the noise covariance. arXiv preprint

arXiv:2006.08680, 2020.

Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.

Surprises in high-dimensional ridgeless least squares interpolation. 2019.

Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.

Surprises in high-dimensional ridgeless least squares interpolation.

The

Annals of Statistics, 50(2):949–986, 2022.

Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An

introduction to statistical learning, second edition, volume 112. Springer,

2021.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv

preprint arXiv:1312.6114, 2013.

Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and

Tengyu Ma. Algorithmic framework for model-based deep reinforcement

learning with theoretical guarantees. In International Conference on Learn-

ing Representations, 2018.

Song Mei and Andrea Montanari. The generalization error of random features

regression: Precise asymptotics and the double descent curve. Communi-

cations on Pure and Applied Mathematics, 75(4):667–766, 2022.

Preetum Nakkiran. More data can hurt for linear regression: Sample-wise

double descent. 2019.

Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal

regularization can mitigate double descent. 2020.

Manfred Opper. Statistical mechanics of learning: Generalization. The hand-

book of brain theory and neural networks, pages 922–925, 1995.

Manfred Opper. Learning to generalize. Frontiers of Life, 3(part 2):763–775,

2001.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,

Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you

need. arXiv preprint arXiv:1706.03762, 2017.


215

Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pe-

dro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and

rich regimes in overparametrized models. arXiv preprint arXiv:2002.09277,

2020.

