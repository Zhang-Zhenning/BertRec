


Computer Science &gt; Information Retrieval

arXiv:2010.06467 (cs)

[Submitted on 13 Oct 2020 (v1), last revised 19 Aug 2021 (this version, v3)]

Pretrained Transformers for Text Ranking: BERT and Beyond

Jimmy Lin, Rodrigo Nogueira, Andrew Yates

The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common

formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This survey

provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The

combination of transformers and self-supervised pretraining has been responsible for a paradigm shift in natural language processing (NLP),

information retrieval (IR), and beyond. In this survey, we provide a synthesis of existing work as a single point of entry for practitioners who

wish to gain a better understanding of how to apply transformers to text ranking problems and researchers who wish to pursue work in this

area. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in

multi-stage architectures and dense retrieval techniques that perform ranking directly. There are two themes that pervade our survey:

techniques for handling long documents, beyond typical sentence-by-sentence processing in NLP, and techniques for addressing the tradeoff

between effectiveness (i.e., result quality) and efficiency (e.g., query latency, model and index size). Although transformer architectures and

pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well understood and

represent mature techniques. However, there remain many open research questions, and thus in addition to laying out the foundations of

pretrained transformers for text ranking, this survey also attempts to prognosticate where the field is heading.

Comments: Final preproduction version of volume in Synthesis Lectures on Human Language Technologies by Morgan &amp; Claypool

Subjects:

Information Retrieval (cs.IR); Computation and Language (cs.CL)

Cite as:

arXiv:2010.06467 [cs.IR]

 

(or arXiv:2010.06467v3 [cs.IR] for this version)

 

https://doi.org/10.48550/arXiv.2010.06467 

Submission history

From: Jimmy Lin [view email] 

[v1] Tue, 13 Oct 2020 15:20:32 UTC (2,762 KB)

[v2] Wed, 28 Jul 2021 13:35:29 UTC (1,519 KB)

[v3] Thu, 19 Aug 2021 16:37:02 UTC (1,520 KB)





NASA ADS



Google Scholar



Semantic Scholar

Download:



PDF 



Other formats

(license)

Current browse context: cs.IR



&lt; prev 



next &gt;

 



new

 



recent

 



2010

Change to browse by: cs , cs.CL

References &amp; Citations

DBLP - CS Bibliography

listing | bibtex

Jimmy Lin

Rodrigo Nogueira

Andrew Yates



Export BibTeX Citation

Bookmark





 





 





 





Bibliographic Tools

Bibliographic and Citation Tools


Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)

Code, Data, Media

Demos

Related Papers

About arXivLabs

About

Help

 Contact

 Subscribe

Copyright

Privacy Policy

Web Accessibility Assistance

arXiv Operational Status 

Get status notifications via 

email or 

slack

Bibliographic and Citation Tools

Bibliographic Explorer (What is the Explorer?)

Litmaps (What is Litmaps?)

scite Smart Citations (What are Smart Citations?)

