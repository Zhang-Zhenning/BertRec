
Probabilistic Ranking Principle

Hongning Wang

CS@UVa


Notion of relevance

CS 4780: Information Retrieval

Relevance

D(Rep(q), Rep(d))    

Similarity

P(r=1|q,d)   r Î{0,1}

Probability of Relevance

P(d ®q) or P(q ®d)

Probabilistic inference

Different 

rep &amp; similarity

Vector space

model

(Salton et al., 75)

Prob. distr.

model

(Wong &amp; Yao, 89)

…

Generative Model

Regression

Model

(Fox 83)

Classical

prob. Model

(Robertson &amp; 

Sparck Jones, 76)

Doc

generation

Query

generation

LM

approach

(Ponte &amp; Croft, 98)

(Lafferty &amp; Zhai, 01a)

Prob. concept

space model

(Wong &amp; Yao, 95)

Different inference system

Inference 

network

model

(Turtle &amp; Croft, 91)

Today’s lecture

CS@UVa

2


Basic concepts in probability 

• Random experiment 

– An experiment with uncertain outcome (e.g., tossing a coin, picking a 

word from text)

• Sample space (S)

– All possible outcomes of an experiment, e.g., tossing 2 fair 

coins, S={HH, HT, TH, TT}

• Event (E)

– EÍS, E happens iff outcome is in S, e.g., E={HH} (all heads), 

E={HH,TT} (same face)

– Impossible event ({}), certain event (S)

• Probability of event

– 0 ≤ P(E) ≤ 1

CS 4780: Information Retrieval

CS@UVa

3


Essential probability concepts

• Probability of events

– Mutually exclusive events

• 𝑃 𝐴 ∪ 𝐵 = 𝑃 𝐴 + 𝑃(𝐵)

– General events

• 𝑃 𝐴 ∪ 𝐵 = 𝑃 𝐴 + 𝑃 𝐵 − 𝑃(𝐴 ∩ 𝐵)

– Independent events

• 𝑃 𝐴 ∩ 𝐵 = 𝑃 𝐴 𝑃 𝐵

CS 4780: Information Retrieval

Joint probability, or 

simply as 𝑃(𝐴, 𝐵)

CS@UVa

4


Essential probability concepts

CS 4780: Information Retrieval

• Conditional probability

– 𝑃 𝐵 𝐴 = 𝑃(𝐴, 𝐵)/𝑃 𝐴

– Bayes’ Rule: 𝑃 𝐵 𝐴 = 𝑃 𝐴 𝐵 𝑃(𝐵)/𝑃 𝐴

– For independent events, 𝑃 𝐵 𝐴 = 𝑃(𝐵)

• Total probability

– If 𝐴1, … , 𝐴) form a non-overlapping partition of S

• 𝑃(𝐵Ç𝑆) = 𝑃(𝐵Ç𝐴1) + ⋯ + 𝑃(𝐵Ç𝐴!)

• 𝑃 𝐴𝑖 𝐵 =

! 𝐵 𝐴𝑖 ! "!

! 𝐵 𝐴1 ! "" #⋯#! 𝐵 𝐴𝑛 ! "# ∝ 𝑃 𝐵 𝐴% 𝑃(𝐴%)

• This allows us to compute 𝑃(𝐴𝑖|𝐵) based on 𝑃(𝐵|𝐴%)

CS@UVa

5


Interpretation of Bayes’ rule

)

(

)

|

(

)

|

(

i

i

i

H

P

H

E

P

E

H

P

µ

CS 4780: Information Retrieval

Hypothesis space: 𝐻 = {𝐻1, … , 𝐻𝑛},     Evidence: 𝐸

If we want to pick the most likely hypothesis H*,  we can drop 𝑃(𝐸)

Posterior probability of 𝑯𝒊

Prior probability of 𝑯𝒊

Likelihood of data/evidence given 𝑯𝒊

𝑃 𝐻" 𝐸 = 𝑃 𝐸 𝐻" 𝑃(𝐻")

𝑃(𝐸)

CS@UVa

6


Theoretical justification of ranking

• As stated by William Cooper

– Rank by probability of relevance leads to the 

optimal retrieval effectiveness

CS 4780: Information Retrieval

“If a reference retrieval system’s response to each request is a ranking of the 

documents in the collections in order of decreasing probability of 

usefulness to the user who submitted the request, where the probabilities are 

estimated as accurately as possible on the basis of whatever data made 

available to the system for this purpose, then the overall effectiveness of the 

system to its users will be the best that is obtainable on the basis of that 

data.”

CS@UVa

7


Justification

• From decision theory

– Two types of loss

• Loss(retrieved|non-relevant)=𝑎#

• Loss(not retrieved|relevant)=𝑎$

– 𝜙(𝑑,, 𝑞): probability of 𝑑, being relevant to 𝑞

– Expected loss regarding to the decision of 

including 𝑑, in the final results

• Retrieve: 1 − 𝜙 𝑑", 𝑞

𝑎#

• Not retrieve: 𝜙 𝑑", 𝑞 𝑎$

CS 4780: Information Retrieval

Your decision criterion?

CS@UVa

8


Justification

• From decision theory

– We make decision by

• If 1 − 𝜙 𝑑", 𝑞

𝑎#&lt;𝜙 𝑑", 𝑞 𝑎$, retrieve 𝑑"

• Otherwise, not retrieve 𝑑"

– Check if 𝜙 𝑑,, 𝑞 &gt;

-!

-!.-"

– Rank documents by descending order of 𝜙 𝑑,, 𝑞

would minimize the loss

CS 4780: Information Retrieval

CS@UVa

9

Pop-up Quiz: Can you prove it?  


According to PRP, what we need is

• A relevance measure function F(q,d) 

– For all q, d1, d2,  

F(q,d1) &gt; F(q,d2) iff. p(Rel|q,d1) &gt;p(Rel|q,d2)

– Assumptions

• Independent relevance 

• Independent loss

• Sequential browsing

CS 4780: Information Retrieval

Most existing research on IR models so far has fallen into 

this line of thinking… (Limitations?) 

CS@UVa

10


Probability of relevance

• Three random variables

– Query Q

– Document D

– Relevance R Î {0,1}

• Goal: rank D based on P(R=1|Q,D)

– Compute P(R=1|Q,D)

– Actually, one only needs to compare P(R=1|Q,D1) 

with P(R=1|Q,D2), i.e., rank documents

• Several different ways to define P(R=1|Q,D) 

CS 4780: Information Retrieval

CS@UVa

11


Conditional models for P(R=1|Q,D) 

• Basic idea: relevance depends on how well a 

query matches a document

– P(R=1|Q,D)=g(Rep(Q,D),q)

• Rep(Q,D): feature representation of query-doc pair

– E.g., #matched terms, highest IDF of a matched term, docLen

– Using training data (with known relevance 

judgments) to estimate parameter q

– Apply the model to rank new documents

• Special case: logistic regression

CS 4780: Information Retrieval

a functional form

CS@UVa

12


Regression for ranking?

• Linear regression

– 𝑦 ← 𝑤/𝑋

– Relationship between a scalar dependent variable 

𝑦 and one or more explanatory variables

CS 4780: Information Retrieval



In a ranking problem:

𝑋 features about query-document pair

𝑦 relevance label of document for the given query

CS@UVa

13


Features/Attributes for ranking

• Typical features considered in ranking 

problems

M

X

n

n

N

M

X

DL

X

DAF

M

X

QL

X

QAF

M

X

M

t

t

M

t

M

t

j

j

j

j

log

log

1

log

1

log

1

6

1

5

4

1

3

2

1

1

=

-

=

=

=

=

=

å

å

å

Average Absolute Query Frequency

Query Length

Average Absolute Document Frequency

Document Length

Average Inverse Document Frequency

Number of Terms in common between 

query and document 

CS 4780: Information Retrieval

CS@UVa

14


Regression for ranking

• Linear regression

– 𝑦 ← 𝑤/𝑋

– Relationship between a scalar dependent variable 

𝑦 and one or more explanatory variables

CS 4780: Information Retrieval

Optimal 

regression model

x

y

Y is discrete in a ranking 

problem!

What if we have 

an outlier? 

1.00

0.50

0.25

0.00

0.75

CS@UVa

15


Regression for ranking

• Logistic regression

– P(R=1|Q,D) = 𝜎 𝑤/𝑋 =

0

0.123(45#6)

– Directly modeling posterior of document 

relevance

CS 4780: Information Retrieval

x

P(y|x)

1.00

0.50

0.25

0.00

0.75

What if we have 

an outlier? 

Sigmoid function

CS@UVa

16


Conditional models for P(R=1|Q,D)

Pros &amp; Cons

• Advantages

– Absolute probability of relevance available

– May re-use all the past relevance judgments

• Problems

– Performance heavily depends on the selection of 

features

– Little guidance on feature selection

• Will be covered with more details in later 

learning-to-rank discussions

CS 4780: Information Retrieval

CS@UVa

17


Recap: TF-IDF weighting

• Combining TF and IDF 

– Common in doc à high tf à high weight

– Rare in collectionà high idfà high weight

– 𝑤 𝑡, 𝑑 = 𝑇𝐹 𝑡, 𝑑 ×𝐼𝐷𝐹(𝑡)

• Most well-known document representation 

schema in IR! (G Salton et al. 1983)



“Salton was perhaps the 

leading computer scientist 

working in the field of 

information retrieval during his 

time.” - wikipedia

Gerard Salton Award

– highest achievement award in IR

CS@UVa

CS 4780: Information Retrieval

18


Recap: probabilistic ranking principle

• From decision theory

– We make decision by

• If 1 − 𝜙 𝑑", 𝑞

𝑎#&lt;𝜙 𝑑", 𝑞 𝑎$, retrieve 𝑑"

• Otherwise, not retrieve 𝑑"

– Check if 𝜙 𝑑,, 𝑞 &gt;

-!

-!.-"

– Rank documents by descending order of 𝜙 𝑑,, 𝑞

would minimize the loss

CS 4780: Information Retrieval

CS@UVa

19


Recap: conditional models for 

P(R=1|Q,D) 

• Basic idea: relevance depends on how well a 

query matches a document

– P(R=1|Q,D)=g(Rep(Q,D),q)

• Rep(Q,D): feature representation of query-doc pair

– E.g., #matched terms, highest IDF of a matched term, docLen

– Using training data (with known relevance 

judgments) to estimate parameter q

– Apply the model to rank new documents

• Special case: logistic regression

CS 4780: Information Retrieval

a functional form

CS@UVa

20


Generative models for P(R=1|Q,D)

• Basic idea

– Compute Odd(R=1|Q,D) using Bayes’ rule

• Assumption

– Relevance is a binary variable

• Variants

– Document “generation”

• P(Q,D|R)=P(D|Q,R)P(Q|R)

– Query “generation”

• P(Q,D|R)=P(Q|D,R)P(D|R)

CS 4780: Information Retrieval

)

0

(

)1

(

)

0

|

,

(

)1

|

,

(

)

,

|

0

(

)

,

|1

(

)

,

|1

(

=

=

=

=

=

=

=

=

=

R

P

R

P

R

D

Q

P

R

D

Q

P

D

Q

R

P

D

Q

R

P

D

Q

R

Odd

Ignored for ranking

CS@UVa

21


Document generation model

CS 4780: Information Retrieval

)

0

,

|

(

)1

,

|

(

)

0

|

(

)

0

,

|

(

)1

|

(

)1

,

|

(

)

0

|

,

(

)1

|

,

(

)

,

|1

(

=

=

µ

=

=

=

=

=

=

=

µ

=

R

Q

D

P

R

Q

D

P

R

Q

P

R

Q

D

P

R

Q

P

R

Q

D

P

R

D

Q

P

R

D

Q

P

D

Q

R

Odd

Model of relevant docs for Q

Model of non-relevant docs for Q

Assume independent attributes of A1…Ak ….(why?)

Let D=d1…dk, where dk Î{0,1} is the value of attribute Ak (Similarly Q=q1…qk )

Õ

Õ

Õ

=

=

=

=

=

=

=

=

=

=

=

=

=

=

=

=

=

=

µ

=

k

d

i

i

i

k

d

i

i

i

k

i

i

i

i

i

i

i

R

Q

A

P

R

Q

A

P

R

Q

A

P

R

Q

A

P

R

Q

d

A

P

R

Q

d

A

P

D

Q

R

Odd

0

,1

1

,1

1

)

0

,

|

0

(

)1

,

|

0

(

)

0

,

|1

(

)1

,

|1

(

)

0

,

|

(

)1

,

|

(

)

,

|1

(

Terms occur in doc

Terms do not occur in doc

document

relevant(R=1)

nonrelevant(R=0)

term present Ai=1

pi

ui

term absent  Ai=0

1-pi

1-ui

Ignored for ranking

information

retrieval

retrieved

is

helpful

for 

you

everyone

Doc1

1

1

0

1

1

1

0

1

Doc2

1

0

1

1

1

1

1

0

CS@UVa

22


Document generation model

CS 4780: Information Retrieval

Õ

Õ

Õ

Õ

Õ

Õ

Õ

=

=

=

=

=

=

=

=

=

=

=

=

=

=

=

=

-

-

-

-

=

-

-

»

=

=

=

=

=

=

=

=

=

=

=

=

=

µ

=

k

q

i

i

i

k

q

d

i

i

i

i

i

k

q

d

i

i

i

k

q

d

i

i

i

k

d

i

i

i

k

d

i

i

i

k

i

i

i

i

i

i

i

i

i

i

i

i

i

i

u

p

p

u

u

p

u

p

u

p

R

Q

A

P

R

Q

A

P

R

Q

A

P

R

Q

A

P

R

Q

d

A

P

R

Q

d

A

P

D

Q

R

Odd

1

,1

1

,1

1

,0

,1

1

,1

0

,1

1

,1

1

1

1

)

1(

)

1(

1

1

)

0

,

|

0

(

)1

,

|

0

(

)

0

,

|1

(

)1

,

|1

(

)

0

,

|

(

)1

,

|

(

)

,

|1

(

Terms occur in doc

Terms do not occur in doc

document

relevant(R=1)

nonrelevant(R=0)

term present Ai=1

pi

ui

term absent  Ai=0

1-pi

1-ui

Assumption: terms not occurring in 

the query are equally likely to 

occur in relevant and nonrelevant

documents, i.e., pt=ut

Important tricks

CS@UVa

23


Robertson-Sparck Jones Model

(Robertson &amp; Sparck Jones 76)

CS 4780: Information Retrieval

Two parameters for each term Ai: 

pi = P(Ai=1|Q,R=1): prob. that term Ai occurs in a relevant doc   

ui = P(Ai=1|Q,R=0): prob. that term Ai occurs in a non-relevant doc  

å

å

»

=

=

=

=

=

=

-

+

-

=

-

-

=

k

q

d

i

i

i

i

i

k

q

d

i

i

i

i

i

Rank

i

i

i

i

u

u

p

p

p

u

u

p

D

Q

R

O

1

,1

1

,1

1

log

1

log

)

1(

)

1(

log

)

,

|1

(

log

(RSJ model) 

How to estimate these parameters?

1

)

.

(

#

5.0

)

.

(

#

ˆ

1

)

.

(

#

5.0

)

.

(

#

ˆ

+

+

=

+

+

=

doc

nonrel

A

with

doc

nonrel

u

doc

rel

A

with

doc

rel

p

i

i

i

i

•

“+0.5” and “+1” can be justified by Bayesian estimation as priors 

CS@UVa

24

Suppose we have relevance judgments,


Parameter estimation

• General setting:

– Given a (hypothesized &amp; probabilistic) model that 

governs the random experiment

– The model gives probability of any data 𝑝(𝐷|𝜃) that 

depends on the parameter 𝜃

– Now, given actual sample data X={x1,…,xn},  what can 

we say about the value of 𝜃?

• Intuitively, take our best guess of 𝜃 -- “best” 

means “best explaining/fitting the data”

• Generally an optimization problem

CS 4780: Information Retrieval

CS@UVa

25


Maximum likelihood vs. Bayesian

• Maximum likelihood estimation

– “Best” means “data likelihood reaches maximum”

– Issue: small sample size

• Bayesian estimation 

– “Best” means being consistent with our “prior” 

knowledge and explaining data well

– A.k.a, Maximum a Posterior estimation

– Issue: how to define prior?

CS 4780: Information Retrieval

7𝜽 = 𝐚𝐫𝐠𝐦𝐚𝐱𝜽𝐏(𝐗|𝜽)

7𝜽 = 𝐚𝐫𝐠𝐦𝐚𝐱𝜽𝑷 𝜽 𝑿 = 𝐚𝐫𝐠𝐦𝐚𝐱𝜽𝐏 𝐗 𝜽 𝐏(𝜽)

ML: Frequentist’s point of view

MAP: Bayesian’s point of view

CS@UVa

26


Illustration of Bayesian estimation

CS 4780: Information Retrieval

Prior: p(q)

Likelihood:

p(X|q) X=(x1,…,xN)

Posterior:

p(q|X)µ p(X|q)p(q)

q

qa: prior mode 

qml: ML estimate

q: posterior mode 

CS@UVa

27


Maximum likelihood estimation

• Data: a document d with counts c(w1), …, c(wN)

• Model: multinomial distribution p(𝑊|𝜃) with parameters 

𝜃" = 𝑝(𝑤")

• Maximum likelihood estimator: H𝜃 = 𝑎𝑟𝑔𝑚𝑎𝑥(𝑝(𝑊|𝜃)

CS 4780: Information Retrieval

Using Lagrange multiplier approach, 

we’ll tune 𝜽𝒊 to maximize 𝑳(𝑾, 𝜽)

Set partial derivatives to zero

ML estimate

𝑝 𝑊 𝜃 =

𝑁

𝑐 𝑤% , … , 𝑐(𝑤&amp;) 6

'(%

&amp;

𝜃'

)(+") ∝ 6

'(%

&amp;

𝜃'

)(+")

log 𝑝 𝑊 𝜃 = ;

'(%

&amp;

𝑐 𝑤' log 𝜃'

𝐿 𝑊, 𝜃 = ;

'(%

&amp;

𝑐 𝑤' log 𝜃' + 𝜆 ;

'(%

&amp;

𝜃' − 1

𝜕𝐿

𝜕𝜃'

= 𝑐 𝑤'

𝜃'

+ 𝜆

→ 𝜃' = − 𝑐 𝑤'

𝜆

∑'(%

&amp;

𝜃'=1

𝜆 = − ;

'(%

&amp;

𝑐 𝑤'

Since

we have 

𝜃' =

𝑐 𝑤'

∑'(%

&amp;

𝑐 𝑤'

Requirement from probability



information

20%

retrieval

10%

computer

30%

science

31%

relevant

1%

literature

8%

information

retrieval

computer

science

relevant

literature



CS@UVa

28


Robertson-Sparck Jones Model

(Robertson &amp; Sparck Jones 76)

CS 4780: Information Retrieval

Two parameters for each term Ai: 

pi = P(Ai=1|Q,R=1): prob. that term Ai occurs in a relevant doc   

ui = P(Ai=1|Q,R=0): prob. that term Ai occurs in a non-relevant doc  

å

å

»

=

=

=

=

=

=

-

+

-

=

-

-

=

k

q

d

i

i

i

i

i

k

q

d

i

i

i

i

i

Rank

i

i

i

i

u

u

p

p

p

u

u

p

D

Q

R

O

1

,1

1

,1

1

log

1

log

)

1(

)

1(

log

)

,

|1

(

log

(RSJ model) 

How to estimate these parameters?

Suppose we have relevance judgments,

1

)

.

(

#

5.0

)

.

(

#

ˆ

1

)

.

(

#

5.0

)

.

(

#

ˆ

+

+

=

+

+

=

doc

nonrel

A

with

doc

nonrel

u

doc

rel

A

with

doc

rel

p

i

i

i

i

•

“+0.5” and “+1” can be justified by Bayesian estimation as priors 

Per-query estimation!

CS@UVa

29


RSJ Model without relevance info

(Croft &amp; Harper 79)

CS 4780: Information Retrieval

Suppose we do not have relevance judgments,

- We will assume pi to be a constant 

- Estimate ui by assuming all documents to be non-relevant

å

=

=

=

+

+

-

+

»

=

k

q

d

i

i

i

Rank

i

i

n

n

N

c

D

Q

R

O

1

,1

5.0

5.0

log

)

,

|1

(

log

N: # documents in collection

ni: # documents in which term Ai occurs

(RSJ model) 

å

å

»

=

=

=

=

=

=

-

+

-

=

-

-

=

k

q

d

i

i

i

i

i

k

q

d

i

i

i

i

i

Rank

i

i

i

i

u

u

p

p

p

u

u

p

D

Q

R

O

1

,1

1

,1

1

log

1

log

)

1(

)

1(

log

)

,

|1

(

log

IDF weighted Boolean model?

in

N

IDF

log

1+

=

Reminder:

information

retrieval

retrieved

is

helpful

for 

you

everyone

Doc1

1

1

0

1

1

1

0

1

Doc2

1

0

1

1

1

1

1

0

CS@UVa

30


RSJ Model: summary

• The most important classical probabilistic IR 

model

• Use only term presence/absence, thus also 

referred to as Binary Independence Model

– Essentially Naïve Bayes for doc ranking

– Designed for short catalog records

• When without relevance judgments, the model 

parameters must be estimated in an ad-hoc way

• Performance isn’t as good as tuned VS models

CS 4780: Information Retrieval

CS@UVa

31


Improving RSJ: adding TF 

CS 4780: Information Retrieval

Let D=d1…dk, where dk is the frequency count of term  Ak

Õ

Õ

Õ

Õ

³

=

=

=

³

=

=

=

=

=

=

=

=

=

=

µ

=

=

=

=

=

=

=

=

=

=

=

=

=

µ

=

=

k

d

i

i

i

i

i

i

i

k

d

i

i

i

k

d

i

i

i

i

i

k

i

i

i

i

i

i

i

i

R

Q

A

P

R

Q

d

A

P

R

Q

A

P

R

Q

d

A

P

R

Q

A

P

R

Q

A

P

R

Q

d

A

P

R

Q

d

A

P

R

Q

d

A

P

R

Q

d

A

P

D

Q

R

P

D

Q

R

P

1

,1

0

,1

1

,1

1

)1

,

|

0

(

)

0

,

|

(

)

0

,

|

0

(

)1

,

|

(

)

0

,

|

0

(

)1

,

|

0

(

)

0

,

|

(

)1

,

|

(

)

0

,

|

(

)1

,

|

(

)

,

|

0

(

)

,

|1

(

E

E

e

f

R

Q

E

P

e

f

R

Q

E

p

E

f

A

p

R

Q

E

P

E

f

A

p

R

Q

E

p

R

Q

f

A

p

f

E

i

f

E

i

i

i

i

i

i

µ

µ

µ

µ

-

-

+

=

=

+

=

=

=

!

)

,

|

(

!

)

,

|

(

)

|

(

)

,

|

(

)

|

(

)

,

|

(

)

,

|

(

2-Poisson mixture model for TF

Many more parameters to estimate! 

Compound with document length!

Eliteness: if the term is about 

the concept asked in the query

CS@UVa

32


BM25/Okapi approximation

(Robertson et al. 94)

• Idea: model p(D|Q,R) with a simpler function 

that approximates 2-Possion mixture model

• Observations:

– log O(R=1|Q,D) is a sum of term weights occurring 

in both query and document

– Term weight Wi= 0, if TFi=0

– Wi increases monotonically with TFi

– Wi has an asymptotic limit

• The simple function is 

CS 4780: Information Retrieval

)

1(

)

1(

log

)1

( 1

i

i

i

i

i

i

i

p

u

u

p

TF

K

k

TF

W

-

-

+

+

=

Õ

³

=

=

=

=

=

=

=

=

=

µ

=

=

k

d

i

i

i

i

i

i

i

i

R

Q

A

P

R

Q

d

A

P

R

Q

A

P

R

Q

d

A

P

D

Q

R

P

D

Q

R

P

1

,1

)1

,

|

0

(

)

0

,

|

(

)

0

,

|

0

(

)1

,

|

(

)

,

|

0

(

)

,

|1

(

CS@UVa

33


Adding doc. length

• Incorporating doc length

– Motivation: the 2-Poisson model assumes equal 

document length

– Implementation: penalize long doc

•

CS 4780: Information Retrieval

)

1(

)

1(

log

)1

( 1

i

i

i

i

i

i

i

p

u

u

p

TF

K

k

TF

W

-

-

+

+

=

)|

|

|

|

)

1

((

1

d

avg

d

b

b

k

K

´

+

-

=

where

Pivoted document length 

normalization

CS@UVa

34


Adding query TF

• Incorporating query TF

– Motivation

• Natural symmetry between document and query

– Implementation: a similar TF transformation as in 

document TF

• The final formula is called BM25, achieving top 

TREC performance

CS 4780: Information Retrieval

)

1(

)

1(

log

)1

(

)1

(

1

i

i

i

i

i

i

i

s

s

i

i

p

u

u

p

TF

K

k

TF

QTF

k

k

QTF

W

-

-

+

+

+

+

=

BM: best match

CS@UVa

35


The BM25 formula 

CS 4780: Information Retrieval



“Okapi TF/BM25 TF”

becomes IDF when no 

relevance info is available

CS@UVa

36


The BM25 formula 

• A closer look

– 𝑏 is usually set to [0.75, 1.2]

– 𝑘0is usually set to [1.2, 2.0]

– 𝑘7 is usually set to (0, 1000]

CS 4780: Information Retrieval

i

i

i

i

n

i

i

qtf

k

k

qtf

D

avg

D

b

b

k

tf

k

tf

q

IDF

D

q

rel

+

+

+

-

+

+

=å

=

2

2

1

1

1

)1

(

)|

|

|

|

1(

)1

(

)

(

)

,

(

TF-IDF component for document

TF component for query

Vector space model 

with TF-IDF schema!

CS@UVa

37


Extensions of “Doc Generation” models

• Capture term dependence [Rijsbergen &amp; Harper 78]

• Alternative ways to incorporate TF [Croft 83, Kalt96]

• Feature/term selection for feedback [Okapi’s TREC 

reports]

• Estimate of the relevance model based on 

pseudo feedback [Lavrenko &amp; Croft 01]

CS 4780: Information Retrieval

to be covered later  

CS@UVa

38


Query generation models

CS 4780: Information Retrieval

))

0

|

(

)

0

,

|

(

(

)

0

|

(

)

1

|

(

)

1

,

|

(

)

0

|

(

)

0

,

|

(

)

1

|

(

)

1

,

|

(

)

0

|

,

(

)

1

|

,

(

)

,

|

1

(

=

»

=

=

=

=

µ

=

=

=

=

=

=

=

µ

=

R

Q

P

R

D

Q

P

Assume

R

D

P

R

D

P

R

D

Q

P

R

D

P

R

D

Q

P

R

D

P

R

D

Q

P

R

D

Q

P

R

D

Q

P

D

Q

R

O

Assuming uniform document prior, we have

)

1

,

|

(

)

,

|

1

(

=

µ

=

R

D

Q

P

D

Q

R

O

Now, the question is how to compute                            ?

)

1

,

|

(

=

R

D

Q

P

Generally involves two steps:

(1) estimate a language model based on D

(2) compute the query likelihood according to the estimated model

Language models, we will cover it in the next lecture!

Query likelihood p(q| qd)

Document prior

CS@UVa

39


What you should know

• Essential concepts in probability

• Justification of ranking by relevance

• Derivation of RSJ model

• Maximum likelihood estimation

• BM25 formula

CS 4780: Information Retrieval

CS@UVa

40


Today’s reading

• Chapter 11. Probabilistic information retrieval

– 11.2 The Probability Ranking Principle

– 11.3 The Binary Independence Model

– 11.4.3 Okapi BM25: a non-binary model

CS 4780: Information Retrieval

CS@UVa

41

