
Published in

Analytics Vidhya



May 28, 2020

·

7 min read

N-Gram Language Models

Language Modeling is nothing but a process of predicting what word

comes next.

A language model learns the probability of word occurrence based on examples of text or

the training data






You can also think of a Language Model or LM is a task of assigning a

probability to a sentence or sequence

→

So how would you learn a language model?

he assumption of an n-gram LM is the next word depend only on the previous n-1

words

T


→

→

→

→

→


Text generation with the help of the Brown Corpus from NLTK using python

Import the word corpus

Generate the next 30 words or tokens for this sentence

The 4-gram LM

select the top three words.


you can

find the code associated with this text generation problem at GitHub

The generated text looks like

Estimating N-gram probability of a sentence

Check the words that occurred only once in the corpus


Convert the words that occurred once as the instances of special word OOV

Tokenize the sentence and check whether the tokens are in the corpus or not.

Convert the tokens and words into lowercase

Bigram Probability

Tokens of the sentence


All the bigrams of the sentence

Respective Bigram Probabilities

The function that calculates the probability of a single bigram

Probability of a sentence = Product of Probability of all bigrams

The bigram probability of the sentence

7.928268578305691e-15

Trigram Probability

The trigram probability of the sentence

Trigram Probabilities


Unigram Probability

The unigram probability of the sentence

Unigram Probabilities

You can find the code related to this probability estimation of the sentence using N-grams

at GitHub

1

Analytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem

https://www.analyticsvidhya.com



Read more from Analytics Vidhya

Machine Learning

NLP

Deep Learning

Language Model

Ngrams






