
Lecture 7: Conjugate Priors

Marina Meil˘a

mmp@stat.washington.edu

Department of Statistics

University of Washington

February 1, 2018


Exponential families have conjugate priors

Examples:Bernoulli model with Beta prior

Examples:Multivariate normal with Normal-Inverse Wishart prior

Example: Poisson distribution

Reading B&amp;S: 5.2, Hoﬀ: 3.3,7.1–3


The posterior pθ|x1:n in an exponential family

▶ Exponential family model in canonical form

pX (x; θ) = c(x)eθT x−ψ(θ)

(1)

Likelihood of sample x1:n with mean ¯x = (�

i xi)/n

px|θ = c(x)exT θ−ψ(θ)

(2)

Prior for parameter θ

pθ(θ; parameters)

the parameters will be speciﬁed shortly

(3)

By Bayes’ rule

pθ|x1:n ∝ C(x1:n)en(¯xT θ−ψ(θ))+ln pθ(θ;parameters)

(4)

with C(x1:n) = �

i c(xi).

▶ Let’s look at the exponent

n( ¯xT θ

����

bilinear

− ψ(θ)

����

ln Z(θ)

) + ln pθ(θ; parameters)

(5)

▶ First two terms look like an exponential family in θ. What would it take to make

the posterior be an exponential family?

Answer: ln pθ(θ; parameters) = ν0(µ0T θ − ψ(θ))+constant(ν0, µ0).


The conjugate prior

▶ A prior

pθ(θ; ν0, µ0) ∝

1

Z(ν0, µ0) eν0(µ0T θ−ψ(θ))

(6)

is called conjugate prior for the exponential family deﬁned by (1)

▶ The normalization constant is

Z(ν0, µ0) = eφ(ν0,µ0) =

�

Θ

eν0(µ0T θ−ψ(θ))dθ

(7)

▶ The posterior is now

pθ|x1:n ∝ en(¯xT θ−ψ(θ))+ν0(µ0T θ−ψ(θ)) = e

(n+ν0)

�

n¯xT +ν0µ0T

n+ν0

θ−ψ(θ)

�

−φ(ν0,µ0))

(8)

with hyper-parameters ν = n + ν0, µ = n¯xT +ν0µ0T

n+ν0

Exercise Why did the factor

C(x1:n) disappear?

▶ Hence, the ν parameter behaves like an equivalent sample size and the µ

parameter like a mean value parameter and νµ like a equivalent suﬃcient

statistic

▶ When n ≫ ν0 the inﬂuence of the prior becomes neglijible, while for n ≪ ν0 , the

prior sets the model mean of near µ0


Bernoulli model with Beta prior

▶ See Lecture 6.


Multivariate Normal

▶ The multivariate normal distribution in p dimensions is

Normal(x, µ, Σ) =

1

(2π)p/2√

det Σ

e− 1

2 (x−µ)T Σ−1(x−µ)

(9)

with x, µ ∈ Rp and Σ ∈ Rp×p positive deﬁnite.

Remark When pX ∝ e− 1

2 X T AX+bT X , X ∼ Normal(A−1b, A−1)

▶ Useful to separate prior pµ,Σ = pµ|ΣpΣ


The conjugate prior on µ

pµ(µ; µ0, Λ0) = Normal(µ; µ0, Λ0)

(10)

▶ Data suﬃcient statistics n, ¯x, S, S = 1

n

�n

i=1(xi − µ)(xi − µ)T the sample

covariance matrix

▶ The posterior covariance Λ−1 = Λ0−1 + nΣ−1

▶ Cov−1 is called a precision matrix.

▶ The posterior mean µ = Λ−1(Λ0−1µ0 + n¯x)

▶ Data aﬀects posterior of µ only via ¯x, n


Deﬁning prior over Σ

▶ Idea “prior parameter is a suﬃcient statistic”. Hence, conjugate distribution

should be the distribution of statistics from Normal(0, S0)

▶ Assume z1:ν0 ∼ i.i.d.N(0, S0), zi ∈ Rp.

▶ Then Sν0 = �ν0

i′=1 zizT

i

is a covariance matrix (= ν0× sample covar of z1:ν0), and it is

non-singular w.p. 1 for ν0 ≥ p.

▶ The distribution of S0ν0 is the Wishart distribution

▶ We set the conjugate prior for Σ−1 to be this distribution

▶ . . . and we say Σ is distributed as the Inverse Wishart


Wishart and Inverse Wishart

▶ The Wishart distribution with ν0 degrees of freedom, over S+

p the group of

positive deﬁnite p × p matrices

pK (K; ν0, S0) =

1

2ν0p/2 det S0Γp

� ν0

2

� det K (ν0−p−1)/2e− 1

2 trace S0−1K

(11)

with Γp

� ν0

2

�

= πp(p−1)/4 �p

j=1 Γ

�

ν0

2 − j−1

2

�

▶ E[Σ−1] = ν0S0

▶ E[Σ] =

1

ν0−p−1 S0−1

▶ Posterior parameters ν0 + n, S0−1 + nS(µ)

▶ again, posterior parameters and suﬃcient statistics combine linearly

▶ Posterior expectation of Σ =

1

ν0+n−p−1 [S0 + nS(µ)]−1


Univariate Normal and its conjugate prior

pX (x; µ, σ2) =

1

√

2πσ2 e−

1

2σ2 (x−µ)2

(12)

▶ Prior pµ|σ2,µ0,λ0 = Normal(µ; µ0, λ0)

▶ Posterior pµ|x1:n,σ2,µ0,λ0 = Normal(µ; µ, λ)

▶ Posterior mean E[µ] = 1

λ

�

1

λ0 µ0 + n¯x

�

▶ 1/λ0 is equivalent sample size

▶ Posterior variance

1

λ0

=

1

λ0 + n 1

σ2

▶ Precision increases with observing data


The Poisson distribution and the Gamma prior

▶ Poisson distribution PX (x) =

1

x! e−λλx =

1

Γ(x−1) eθx−eθ with θ = ln λ.

▶ The conjugate prior is then

pλ|µ ∝ e(θµ−eθ)ν = eθ(νµ)e−νeθ

(13)

▶ Changing the variable back to λ we have. dθ = dλ/λ and

pλ|ν,µ = λνµe−νλ 1

λ ∝ gamma(λ; µν, µ)

(14)

▶ Recall that the mean of gamma(α, β) is α

β ; hence, E[λ] = µ.

