


May 10, 2020

·

4 min read

Text classification with feature selection using

mutual information

p(“fun” | tweet is positive)

Mutual Information

Eq. 1: Mutual Information




Eq. 2: Conditional entropy decomposition

The classification task

'count'

'word'

'unicode'

'english'

True

'best_mutual_info'

'clf'

CountVectorizer

SelectKBest

mutual_info_best

MultinomialNB

The Results


Python code for conditional entropy

calc_conditional_entroy_x

calc_conditional_entropy_over_all_x()

Python code for mutual information


calc_mutual_information_using_cond_entropy

calc_mutual_information_for_word 

calc_mutual_information_using_cond_entropy.

Text Classification

Python

NLP




Follow



Data scientist &amp; software engineer — with a background in machine learning, statistics &amp; natural language processing





Feature Engineering

Machine Lear

