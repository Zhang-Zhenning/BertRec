


Published in

Towards Data Science



Mar 16, 2019

·

5 min read

Save

Skip-Gram: NLP context words prediction algorithm

word embeddings








skip-gram example

sat

cat, mat

-1 and 3

sat

0

the

Architecture

The Skip-gram model architecture (Source: https://arxiv.org/pdf/1301.3781.pdf Mikolov el al.)

Variables we’ll be using

N

N = context window

Working steps

one hot encoding


Probability function

softmax probability

j

c

c

j

c

Loss function

Loss function

c

L

Advantages

Disadvantages

N

c


6



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





Machine Learning

NLP

Word2vec

Skip Gram

Unsupervised Learning

