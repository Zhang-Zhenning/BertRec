


Review

ML

Baum-Welch

Gaussians

Summary

Example











Lecture 15: Baum-Welch

Mark Hasegawa-Johnson

All content CC-SA 4.0 unless otherwise speciﬁed.

ECE 417: Multimedia Signal Processing, Fall 2021




Review

ML

Baum-Welch

Gaussians

Summary

Example



1

Review: Hidden Markov Models



2

Maximum-Likelihood Training of an HMM



3

Baum-Welch: the EM Algorithm for Markov Models



4

Gaussian Observation Probabilities



5

Summary



6

Written Example




Review

ML

Baum-Welch

Gaussians

Summary

Example



Outline





1

Review: Hidden Markov Models



2

Maximum-Likelihood Training of an HMM



3

Baum-Welch: the EM Algorithm for Markov Models



4

Gaussian Observation Probabilities



5

Summary



6

Written Example




Review

ML

Baum-Welch

Gaussians

Summary

Example



Hidden Markov Model



1

2

3

⃗x

⃗x

⃗x

a11

a12

a13

b1(⃗x)

a22

a21

a23

b2(⃗x)

a33

a32

a31

b3(⃗x)



1 Start in state qt = i with pmf πi.



2 Generate an observation, ⃗x, with pdf bi(⃗x).



3 Transition to a new state, qt+1 = j, according to pmf aij.



4 Repeat.




Review

ML

Baum-Welch

Gaussians

Summary

Example



The Three Problems for an HMM





1 Recognition: Given two diﬀerent HMMs, Λ1 and Λ2, and an

observation sequence X. Which HMM was more likely to have

produced X? In other words, p(X|Λ1) &gt; p(X|Λ2)?



2 Segmentation: What is p(qt = i|X, Λ)?



3 Training: Given an initial HMM Λ, and an observation

sequence X, can we ﬁnd Λ′ such that p(X|Λ′) &gt; p(X|Λ)?




Review

ML

Baum-Welch

Gaussians

Summary

Example



The Forward Algorithm



Deﬁnition: αt(i) ≡ p(⃗x1, . . . , ⃗xt, qt = i|Λ). Computation:



1 Initialize:

α1(i) = πibi(⃗x1),

1 ≤ i ≤ N



2 Iterate:

αt(j) =

N

�

i=1

αt−1(i)aijbj(⃗xt),

1 ≤ j ≤ N, 2 ≤ t ≤ T



3 Terminate:

p(X|Λ) =

N

�

i=1

αT(i)




Review

ML

Baum-Welch

Gaussians

Summary

Example



The Backward Algorithm



Deﬁnition: βt(i) ≡ p(⃗xt+1, . . . , ⃗xT|qt = i, Λ). Computation:



1 Initialize:

βT(i) = 1,

1 ≤ i ≤ N



2 Iterate:

βt(i) =

N

�

j=1

aijbj(⃗xt+1)βt+1(j),

1 ≤ i ≤ N, 1 ≤ t ≤ T − 1



3 Terminate:

p(X|Λ) =

N

�

i=1

πibi(⃗x1)β1(i)




Review

ML

Baum-Welch

Gaussians

Summary

Example



Segmentation





1 The State Posterior:

γt(i) = p(qt = i|X, Λ) =

αt(i)βt(i)

�N

k=1 αt(k)βt(k)



2 The Segment Posterior:

ξt(i, j) = p(qt = i, qt+1 = j|X, Λ)

=

αt(i)aijbj(⃗xt+1)βt+1(j)

�N

k=1

�N

ℓ=1 αt(k)akℓbℓ(⃗xt+1)βt+1(ℓ)




Review

ML

Baum-Welch

Gaussians

Summary

Example



The Three Problems for an HMM





1 Recognition: Given two diﬀerent HMMs, Λ1 and Λ2, and an

observation sequence X. Which HMM was more likely to have

produced X? In other words, p(X|Λ1) &gt; p(X|Λ2)?



2 Segmentation: What is p(qt = i|X, Λ)?



3 Training: Given an initial HMM Λ, and an observation

sequence X, can we ﬁnd Λ′ such that p(X|Λ′) &gt; p(X|Λ)?




Review

ML

Baum-Welch

Gaussians

Summary

Example



Outline





1

Review: Hidden Markov Models



2

Maximum-Likelihood Training of an HMM



3

Baum-Welch: the EM Algorithm for Markov Models



4

Gaussian Observation Probabilities



5

Summary



6

Written Example




Review

ML

Baum-Welch

Gaussians

Summary

Example



Maximum Likelihood Training



Suppose we’re given several observation sequences of the form

X = [⃗x1, . . . , ⃗xT]. Suppose, also, that we have some initial guess

about the values of the model parameters (our initial guess doesn’t

have to be very good). Maximum likelihood training means we

want to compute a new set of parameters, Λ′ =

�

π′

i, a′

ij, b′

j(⃗x)

�

that maximize p(X|Λ′).



1 Initial State Probabilities: Find values of π′

i, 1 ≤ i ≤ N,

that maximize p(X|Λ′).



2 Transition Probabilities: Find values of a′

ij, 1 ≤ i, j ≤ N,

that maximize p(X|Λ′).



3 Observation Probabilities: Learn b′

j(⃗x). What does that

mean, actually?




Review

ML

Baum-Welch

Gaussians

Summary

Example



Learning the Observation Probabilities



There are four typical ways of learning the observation

probabilities, bj(⃗x).



1 Vector quantize ⃗x, using some VQ method. Suppose ⃗x is the

kth codevector; then we just need to learn bj(k) such that

bj(j) ≥ 0,

K−1

�

k=0

bj(k) = 1



2 Model bj(k) as a Gaussian or mixture Gaussian, and learn its

parameters.



3 Model bj(k) as a neural net, and learn its parameters.




Review

ML

Baum-Welch

Gaussians

Summary

Example



Maximum Likelihood Training



For now, suppose that we have the following parameters that we

need to learn:



1 Initial State Probabilities: π′

i such that

π′

i ≥ 0,

N

�

i=1

π′

i = 1



2 Transition Probabilities: a′

ij such that

a′

ij ≥ 0,

N

�

j=1

a′

ij = 1



3 Observation Probabilities: b′

j(k) such that

b′

j(k) ≥ 0,

K

�

k=1

b′

j(k) = 1




Review

ML

Baum-Welch

Gaussians

Summary

Example



Maximum Likelihood Training with Known State Sequence



Impossible assumption: Suppose that we actually know the state

sequences, Q = [q1, . . . , qT], matching with each observation

sequence X = [⃗x1, . . . , ⃗xT]. Then what would be the

maximum-likelihood parameters?




Review

ML

Baum-Welch

Gaussians

Summary

Example



Maximum Likelihood Training with Known State Sequence



Our goal is to ﬁnd Λ = {πi, aij, bj(k)} in order to maximize

L(Λ) = ln p(Q, X|Λ)

= ln πq1 + ln bq1(x1) + ln aq1,q2 + bq2(x2) + . . .

= ln πq1 +

N

�

i=1





N

�

j=1

nij ln aij +

K

�

k=1

mik ln bi(k)





where



nij is the number of times we saw (qt = i, qt+1 = j),



mik is the number of times we saw (qt = i, kt = k)




Review

ML

Baum-Welch

Gaussians

Summary

Example



Maximum Likelihood Training with Known State Sequence



L(Λ) = ln πq1 +

N

�

i=1





N

�

j=1

nij ln aij +

K

�

k=1

mik ln bi(k)





When we diﬀerentiate that, we ﬁnd the following derivatives:

∂L

∂πi

=

�

1

πi

i = q1

0

otherwise

∂L

∂aij

= nij

aij

∂L

∂bj(k) = mjk

bj(k)

These derivatives are never equal to zero! What went wrong?




Review

ML

Baum-Welch

Gaussians

Summary

Example



Maximum Likelihood Training with Known State Sequence



Here’s the problem: we forgot to include the constraints

�

i πi = 1, �

j aij = 1, and �

k bj(k) = 1!

We can include the constraints using the method of Lagrange

multipliers. If we do that, we wind up with the solutions

π′

i =

�

1

λ

i = q1

0

otherwise

a′

ij = nij

µi

bj(k)′ = mjk

νj

where λ, µi, and νj are arbitrary constants (called Lagrange

multipliers) that we can set to any value we want, provided that

the constraints are satisﬁed.




Review

ML

Baum-Welch

Gaussians

Summary

Example



Maximum Likelihood Training with Known State Sequence



Using the Lagrange multiplier method, we can show that the

maximum likelihood parameters for the HMM are:



1 Initial State Probabilities:

π′

i = # state sequences that start with q1 = i

# state sequences in training data



2 Transition Probabilities:

a′

ij = # frames in which qt−1 = i, qt = j

# frames in which qt−1 = i



3 Observation Probabilities:

b′

j(k) = # frames in which qt = j, kt = k

# frames in which qt = j




Review

ML

Baum-Welch

Gaussians

Summary

Example



Outline





1

Review: Hidden Markov Models



2

Maximum-Likelihood Training of an HMM



3

Baum-Welch: the EM Algorithm for Markov Models



4

Gaussian Observation Probabilities



5

Summary



6

Written Example




Review

ML

Baum-Welch

Gaussians

Summary

Example



Expectation Maximization



When the true state sequence is unknown, then we can’t maximize

the likelihood p(X, Q|Λ′) directly. Instead, we maximize the

expected log likelihood. This is an instance of the EM algorithm,

where the visible training dataset is

Dv = {⃗x1, . . . , ⃗xT}

and the hidden dataset is

Dh = {q1, . . . , qT}




Review

ML

Baum-Welch

Gaussians

Summary

Example



Expectation Maximization: the M-Step



In the M-step of EM, we use the E-step probabilities to calculate

the expected maximum likelihood estimators:



1 Initial State Probabilities:

π′

i = E [# state sequences that start with q1 = i]

# state sequences in training data



2 Transition Probabilities:

π′

i = E [# frames in which qt−1 = i, qt = j]

E [# frames in which qt−1 = i]



3 Observation Probabilities:

b′

j(k) = E [# frames in which qt = j, kt = k]

E [# frames in which qt = j]




Review

ML

Baum-Welch

Gaussians

Summary

Example



Expectation Maximization: the E-Step



In order to ﬁnd quantities like “the expected number of times

q1 = i,” we need to do the E-Step of EM. The E-step calculates

probabilities like:

p(Dh|Dv, Λ)

For example, in order to re-estimate bj(k), we need to know the #

frames in which qt = i. For that, we need

p(qt = i|⃗x1, ⃗x2, . . . , ⃗xT, Λ)

. . . but this is something we already know! It is

p(qt = i|⃗x1, ⃗x2, . . . , ⃗xT, Λ) = γt(i)




Review

ML

Baum-Welch

Gaussians

Summary

Example



Expectation Maximization: the E-Step



Similarly, in order to re-estimate aij, we need to know the #

frames in which qt−1 = i and qt = j. For that, we need

p(qt−1 = i, qt = j|⃗x1, ⃗x2, . . . , ⃗xT, Λ):



In the tth frame, the event qt = i, qt+1 = j either happens, or

it doesn’t happen.



So the following expectation is actually just a probability:

E

�

# times during the tth frame, in which qt = i, qt+1 = j

�

= p(qt = i, qt+1 = j)

= ξt(i, j)




Review

ML

Baum-Welch

Gaussians

Summary

Example



The Baum-Welch Algorithm





1 Initial State Probabilities:

π′

i = E [# state sequences that start with q1 = i]

# state sequences in training data

=

�

sequences γ1(i)

# sequences




Review

ML

Baum-Welch

Gaussians

Summary

Example



The Baum-Welch Algorithm





1 Initial State Probabilities:

π′

i =

�

sequences γ1(i)

# sequences



2 Transition Probabilities:

a′

ij = E [# frames in which qt−1 = i, qt = j]

E [# frames in which qt−1 = i]

=

�T−1

t=1 ξt(i, j)

�N

j=1

�T−1

t=1 ξt(i, j)




Review

ML

Baum-Welch

Gaussians

Summary

Example



The Baum-Welch Algorithm





1 Initial State Probabilities:

π′

i =

�

sequences γ1(i)

# sequences



2 Transition Probabilities:

a′

ij =

�T−1

t=1 ξt(i, j)

�N

j=1

�T−1

t=1 ξt(i, j)



3 Observation Probabilities:

b′

j(k) = E [# frames in which qt = j, kt = k]

E [# frames in which qt = j]




Review

ML

Baum-Welch

Gaussians

Summary

Example



The Baum-Welch Algorithm





1 Initial State Probabilities:

π′

i =

�

sequences γ1(i)

# sequences



2 Transition Probabilities:

a′

ij =

�T−1

t=1 ξt(i, j)

�N

j=1

�T−1

t=1 ξt(i, j)



3 Observation Probabilities:

b′

j(k) =

�

t:⃗xt=k γt(j)

�

t γt(j)




Review

ML

Baum-Welch

Gaussians

Summary

Example



Outline





1

Review: Hidden Markov Models



2

Maximum-Likelihood Training of an HMM



3

Baum-Welch: the EM Algorithm for Markov Models



4

Gaussian Observation Probabilities



5

Summary



6

Written Example




Review

ML

Baum-Welch

Gaussians

Summary

Example



Baum-Welch with Gaussian Probabilities



The requirement that we vector-quantize the observations is a

problem. It means that we can’t model the observations very

precisely.

It would be better if we could model the observation likelihood,

bj(⃗x), as a probability density in the space ⃗x ∈ ℜD. One way is to

use a parameterized function that is guaranteed to be a properly

normalized pdf. For example, a Gaussian:

bi(⃗x) = N (⃗x; ⃗µi, Σi)




Review

ML

Baum-Welch

Gaussians

Summary

Example



Diagonal-Covariance Gaussian pdf



Let’s assume the feature vector has D dimensions,

⃗xt = [xt,1, . . . , xt,D]. The Gaussian pdf is

bi(⃗xt) =

1

(2π)D/2|Σi|1/2 e− 1

2 (⃗xt−⃗µi)Σ−1

i

(⃗xt−⃗µi)T

The logarithm of a Gaussian is

ln bi(⃗xt) = −1

2

�

(⃗xt − ⃗µi)TΣ−1

i

(⃗xt − ⃗µi) + ln |Σi| + C

�

where the constant is C = D ln(2π).




Review

ML

Baum-Welch

Gaussians

Summary

Example



Expectation maximization



Expectation maximization maximizes the expected log probability,

i.e.,

E [ln bi(⃗xt)] = −1

2

N

�

i=1

γt(i)

�

(⃗xt − ⃗µi)TΣ−1

i

(⃗xt − ⃗µi) + ln |Σi| + C

�

If we include all of the frames, then we get

E [ln p(X, Q|Λ)] = other terms

− 1

2

T

�

t=1

N

�

i=1

γt(i)

�

(⃗xt − ⃗µi)TΣ−1

i

(⃗xt − ⃗µi) + ln |Σi| + C

�

where the “other terms” are about aij and πi, and have nothing to

do with ⃗µi or Σi.




Review

ML

Baum-Welch

Gaussians

Summary

Example



M-Step: optimum ⃗µ



First, let’s optimize ⃗µ. We want

0 = ∇⃗µq

T

�

t=1

N

�

i=1

γt(i)(⃗xt − ⃗µi)TΣ−1

i

(⃗xt − ⃗µi)

Re-arranging terms, we get

⃗µ′

q =

�T

t=1 γt(q)⃗xt

�T

t=1 γt(q)




Review

ML

Baum-Welch

Gaussians

Summary

Example



M-Step: optimum Σ



Second, let’s optimize Σi. In order to do this, we need to talk

about the gradient of a scalar w.r.t. a matrix. Let’s suppose that

Σ =





σ2

1

· · ·

ρ1,D

...

...

...

ρD,1

· · ·

σ2

D





When we talk about ∇Σf (Σ), for some scalar function f (·), what

we mean is the matrix whose elements are

∇Σf (Σ) =





∂f

∂σ2

1

· · ·

∂f

∂ρ1,D

...

...

...

∂f

∂ρD,1

· · ·

∂f

∂σ2

D








Review

ML

Baum-Welch

Gaussians

Summary

Example



M-Step: optimum Σ



In particular, for a positive-deﬁnite, symmetric Σ, it’s possible to

show that

∇Σ ln |Σ| = Σ−1

and

∇Σ(⃗x − ⃗µ)TΣ−1(⃗x − ⃗µ) = −Σ−1(⃗x − ⃗µ)(⃗x − ⃗µ)TΣ−1




Review

ML

Baum-Welch

Gaussians

Summary

Example



Minimizing the cross-entropy: optimum σ



Taking advantage of those facts, let’s ﬁnd

0 = ∇Σq

T

�

t=1

N

�

i=1

γt(i)

�

ln |Σi| + (⃗xt − ⃗µi)TΣ−1

i

(⃗xt − ⃗µi)

�

Re-arranging terms, we get

Σ′

q =

�T

t=1 γt(q)(⃗xt − ⃗µq)(⃗xt − ⃗µq)T

�T

t=1 γt(q)




Review

ML

Baum-Welch

Gaussians

Summary

Example



Summary: Gaussian Observation PDFs



So we can use Gaussians for bj(⃗x):



E-Step:

γt(i) =

αt(i)βt(i)

�

i′ αt(i′)βt(i′)



M-Step:

⃗µ′

i =

�T

t=1 γt(i)⃗xt

�T

t=1 γt(i)

Σ′

i =

�T

t=1 γt(i)(⃗xt − ⃗µi)(⃗xt − ⃗µi)T

�T

t=1 γt(i)




Review

ML

Baum-Welch

Gaussians

Summary

Example



Outline





1

Review: Hidden Markov Models



2

Maximum-Likelihood Training of an HMM



3

Baum-Welch: the EM Algorithm for Markov Models



4

Gaussian Observation Probabilities



5

Summary



6

Written Example




Review

ML

Baum-Welch

Gaussians

Summary

Example



The Baum-Welch Algorithm: Initial and Transition

Probabilities





1 Initial State Probabilities:

π′

i =

�

sequences γ1(i)

# sequences



2 Transition Probabilities:

a′

ij =

�T−1

t=1 ξt(i, j)

�N

j=1

�T−1

t=1 ξt(i, j)




Review

ML

Baum-Welch

Gaussians

Summary

Example



The Baum-Welch Algorithm: Observation Probabilities





1 Discrete Observation Probabilities:

b′

j(k) =

�

t:⃗xt=k γt(j)

�

t γt(j)



2 Gaussian Observation PDFs:

⃗µ′

i =

�T

t=1 γt(i)⃗xt

�T

t=1 γt(i)

Σ′

i =

�T

t=1 γt(i)(⃗xt − ⃗µi)(⃗xt − ⃗µi)T

�T

t=1 γt(i)




Review

ML

Baum-Welch

Gaussians

Summary

Example



Outline





1

Review: Hidden Markov Models



2

Maximum-Likelihood Training of an HMM



3

Baum-Welch: the EM Algorithm for Markov Models



4

Gaussian Observation Probabilities



5

Summary



6

Written Example




Review

ML

Baum-Welch

Gaussians

Summary

Example



Written Example



In a second-order Markov process, qt depends on both qt−2 and

qt−1, thus the model parameters are:

πij = p(q1 = i, q2 = j)

(1)

aijk = p(qt = k|qt−2 = i, qt−1 = i)

(2)

bk(⃗x) = p(⃗x|qt = k)

(3)

Suppose you have a sequence of observations for which you have

already αt(i, j) and βt(i, j), deﬁned as

αt(i, j) = p(⃗x1, . . . , ⃗xt, qt−1 = i, qt = j|Λ)

(4)

βt(i, j) = p(⃗xt+1, . . . , ⃗xT|qt−1 = i, qt = j, Λ)

(5)

In terms of the quantities deﬁned in Eqs. (1) through (5), ﬁnd a

formula that re-estimates a′

ijk so that, unless aijk is already optimal,

p(X|πi, a′

ijk, bj(⃗x)) &gt; p(X|πi, aijk, bj(⃗x))

