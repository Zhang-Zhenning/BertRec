


Log in

Sign up



robertspierre

1,864

10

28



Javier TG

Maximum likelihood as minimizing the dissimilarity between the empirical distriution and the model distribution

Asked 3 years, 5 months ago

Modified 2 years, 7 months ago

Viewed 2k times

5

 

 

I am reading Ian Goodfellow "Deep Learning" book. At page 128 it says

Starting from the definition of maximum likelihood estimator written in the text:

Is there a formal proof for this? What is the intuition behind the formulation of maximum likelihood estimator as minimizing the KL divergence between the empirical distribution and the model distribution?

Share

Improve this question

asked Nov 6, 2019 at 0:20

1 Answer

Sorted by:

6

 

 

This is a late response but I hope it may help:

Proof (this proof is basically a summary of the explanations from the author):

To go for the proof, first we can follow the procedure given by the author which allows us to have a more convenient expression:

θML =arg

max

θ

pmodel(X;θ)

=arg

max

θ

m

∏

i=1pmodel(x(i);θ)

=arg

max

θ

m

∑

i=1log(pmodel(x(i);θ))

=arg

max

θ

1

m

m

∑

i=1log(pmodel(x(i);θ))

=arg

max

θ

m

∑

i=1

1

mlog(pmodel(x(i);θ))

Note this last expression is the expectation of this log(pmodel(x;θ)) function with respect to the empirical distribution defined by the training data (ˆpdata) which puts a probability of 1/m on each of the m points x(1),x(2),...,x(m).

So we can equivalently write this last expression as:

θML =arg

max

θ Ex∼ ˆpdatalog(pmodel(x;θ))

Hence, obtaining the value of θ which satisfies this expresion will maximize the likelihood of pmodel(x,θ) being the statistical model that best fits our set of data samples X.

But we can also get the parameter θ that maximizes the likelihood using the KL divergence of the probability distributions ˆpdata (empirical distribution of X) and pmodel (our statistical model that we are using to fit X):

DKL(ˆpdata ∥ pmodel) =Ex∼ ˆpdata[log(ˆpdata(x))−log(pmodel(x;θ))]

This is because of the explanation given by the author, which is that Ex∼ ˆpdatalog(ˆpdata(x)) does not depend on θ (it only depends on the data generating process), so it can be trated as a constant. Hence we can adress the same

problem of finding the value of θ that maximizes the likelihood by minimizing this KL divergence, because this is the same as minimizing:

Ex∼ ˆpdata[−log(pmodel(x;θ))]

Which is just the negative form of the simplified expression for θML that we have written earlier from the perspective of the likelihood.

So, to sum up, we can also calculate the parameter θML by:

θML =arg

min

θ DKL(ˆpdata ∥ pmodel) =arg

min

θ Ex∼ ˆpdata[−log(pmodel(x;θ))]

Intuition:

With this said, I believe we can think of using the KL divergence for maximizing the likelihood as a way of making the predicted distribution (pmodel(X,θ)) as close as possible to the empirical distribution.

Thereby with our predicted distribution and by sampling it, we would be able to obtain a set of samples similar to the initial ones (X). So this may mean that we have correctly calculate the true distribution of the data X.

Share

Improve this answer

edited Sep 9, 2020 at 22:53

answered Sep 9, 2020 at 20:07

Ask Question

One way to interpret maximum likelihood estimation is to view it as minimizing the dissimilarity between the empirical distribution ˆpdata, defined by the training set and the model distribution, with the degree of dissimilarity

between the two measured by the KL divergence. The KL divergence is given by

DKL(ˆpdata||pmodel) =Ex∼ ˆpdata[logˆpdata−logpmodel(x)]

θML =arg

max

θ pmodel(X;θ)

maximum-likelihood distance

kullback-leibler

empirical-cumulative-distr-fn

empirical-likelihood

Cite

Follow





Highest score (default)

Cite

Follow


CROSS VALIDATED

Tour

Help

Chat

Contact

Feedback

COMPANY

Stack Overflow

Teams

Advertising

Collectives

Talent

About

Press

Legal

Privacy Policy

Terms of Service

Cookie Settings

Cookie Policy

STACK EXCHANGE NETWORK

Technology

Culture &amp; recreation

Life &amp; arts

Science

Professional

Business

API

Data

Blog

Facebook

Twitter

LinkedIn

Instagram

Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2023.4.21.43403

1,178

1

6

17

3

There's a detailed discussion in the context of parameter survival models in this paper by Nils Lid Hjort: jstor.org/stable/1403683 The formal expression of idea goes back at least to Huber in 1967, but that paper is harder to find.

– Thomas Lumley

Sep 9, 2020 at 22:13

Your Answer

By clicking “Post Your Answer”, you agree to our terms of service, privacy policy and cookie policy

Not the answer you're looking for? Browse other questions tagged maximum-likelihood distance

kullback-leibler

empirical-cumulative-distr-fn

empirical-likelihood  or ask your own question.

Linked

4

Why use KL-Divergence as loss over MLE?

3

Maximum likelihood estimator and KL divergence

Related

7

What is the distribution of the (arbitrarily) weighted Maximum Likelihood Estimator?

1

What happens to the log likelihood when the maximum likelihood estimate does not exist?

4

MLE as an expectation over the empirical distribution

2

Maximum likelihood and Minimizing Kullback–Leibler divergence to the ecdf?! (i.e.: finite sample statement?)

Hot Network Questions



Generic Doubly-Linked-Lists C implementation



Is sawmill necessarily a building?



“Parabolic”, suborbital and ballistic trajectories all follow elliptic paths. Is there a generic term for these trajectories?



Has the cause of a rocket failure ever been mis-identified, such that another launch failed due to the same problem?



Are there two Pooh?

more hot questions

 Question feed



Post Your Answer

Featured on Meta



New blog post from our CEO Prashanth: Community is the future of AI



Improving the copy in the close modal and post notices - 2023 edition

Your privacy

By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.

 

Accept all cookies

Necessary cookies only

Customize settings

