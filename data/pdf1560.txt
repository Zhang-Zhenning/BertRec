
Lecture 11: The Good-Turing Estimate

Scribes: Ellis Weng, Andrew Owens

March 4, 2010

1

Introduction

In many language-related tasks, it would be extremely useful to know the

probability that a sentence or word sequence will occur in a document. How-

ever, there is not enough data to account for all word sequences. Thus, n-gram

models are used to approximate the probability of word sequences. Making an

independence assumption between the n-grams reduces some of the problems

with data sparsity, but even n-gram models can have sparsity problems. For

example, the Google corpus has 1 trillion words of running English text. There

are 13 million words that occur over 200 times, so there are at least 169 trillion

potential bigrams - much more than the 1 trillion words in the corpus. Smooth-

ing is a strategy used to account for this data sparsity. In this lecture, we will

explore Good-Turing smoothing, a particular kind of smoothing.

2

Setup

Suppose we have the set of all possible item types: X = {x1, ..., xm}. These

item types may be n-grams, but for simplicity, we will consider unigram item

types. For example, X = {the, bad, cat, dog} .

We also have a sequence W of N independent samples: W = w1, ..., wn,

where wk ∈ X. We want to estimate θ[j], the probability that a future sample

will be xj. We can assume θ[j] &gt; 0 because we want to account for the possi-

bility of a word occurring even if it does not appear in the corpus. This implies

that the relative frequency estimate #(xj)

N

, where #(xj) is the count of xj in W,

is not desirable or accurate for small counts. Here we run into a problem: how

can we estimate the probability of something we have never seen before?

In order to reduce the number of parameters, we introduce the idea of ty-

ing parameters based on observed events in W, a key idea in Good-Turing

smoothing. We can reduce the number of parameters by making the following

assumption: if #(xj) = #(xj′), then θ[j] = θ[j′]. In other words, if two words

appear the same number of times in the corpus, we assume that they have the

same probability of occurring in general. This assumption is not entirely real-

istic; it may be a coincidence that these two items appeared the same number

1


of times. However, this assumption signiﬁcantly reduces the number of the

parameters.

With this assumption, we introduce the notation θ(r) to mean the proba-

bility of a word occurring given that it appeared r times in W. We also let

Nr denote the number of item types that occur exactly r times in W. In other

words, Nr = |{xj : #(xj) = r}|. For example, if W = the, bad, cat, the, cat then:

N0 = 1 because dog does not appear in W,

N1 = 1 because “bad” appears once in W,

N2 = 2 because the words “cat” and “the” appear twice in W.

With these deﬁnitions, the following property holds:

N =

�

r

rNr.

(1)

We now introduce the Good-Turing estimate for θ(r).

ˆθ(r) = 1

N (r + 1)Nr+1

Nr

(2)

This estimate seems strange at this point, but we will present two deriva-

tions to justify it.

As a sanity check, we verify that the sum of the word-

occurrence probabilities is 1.

�

j

ˆθ[j]

=

�

r

ˆθ(r)Nr

(3)

=

1

N

�

r

[(r + 1)Nr+1

Nr

]Nr

(4)

=

1

N

�

r

(r + 1)Nr+1.

(5)

We show �

r(r + 1)Nr+1 = �

r rNr. All of the terms in the right hand side

are also present in the left hand side (except for the term where r = 0, which

contributes nothing). The only term that appears on the left hand side but not

the right is (rm+1)Nrm+1 where rm is the maximum number of times any word

appears in the corpus. Since Nrm+1 = 0, this term also contributes nothing to

the summation. Thus equality holds, and

�

j

ˆθ[j] = 1

N

�

r

(r + 1)Nr+1 = 1

N

�

r

rNr = N/N = 1.

(6)

3

First Derivation

For the ﬁrst derivation, we will make up a “generative” story for W. Start by

assuming that we have access to θ[j] (remember that we’re trying to derive ˆθ(r)

2


and the problem is that the θ[j]′s for different terms that occur exactly r times

might be different). Draw j (hence θ[j]) uniformly at random from {1, 2, ..., m}.

Then ﬂip a coin N times, with θ[j] being the probability of success (e.g. Yes, Yes,

No, ..., No, Yes). The number of successes is the number of times the word xj

is generated by our model. If xj appears exactly r times, then throw θ[j] into

the average for ˆθ(r). At the end of this process, ˆθ(r) will (approximately) be the

average of the θ[j] for which #(xj) = r. More precisely, we set

ˆθ(r) = E[θ[j] | #(xj) = r] =

�

j

θ[j] Pr(θ[j] | #(xj) = r).

(7)

We want a generative model, so we would like to condition on the “gener-

ator,” θ[j]. We do this by applying the Bayes ﬂip.

�

j

θ[j] Pr(#(xj) = r | θ[j]) Pr(θ[j])

�

j′ Pr(#(xj′) = r | θ[j′]) Pr(θ[j′])

(8)

We are assuming a uniform prior on θ[j] (i.e. P(θ[j]) = 1/m), so the Pr(θ[j])

and Pr(θ[j′]) terms cancel.

�

j

θ[j] Pr(#(xj) = r | θ[j])

�

j′ Pr(#(xj′) = r | θ[j′])

(9)

Now we rewrite the numerator and denominator in terms of the probability

mass function for the binomial distribution.

�

j

θ[j]

�N

r

�

θ[j]r(1 − θ[j])N−r

�

j′

�N

r

�

θ[j′]r(1 − θ[j′])N−r

(10)

Let Ein N[Nr] be the expected value of Nr given that we ﬂipped N coins at

each step of our experiment. Then we can rewrite the equation as

1

Ein N[Nr]

�

j

θ[j]

�N

r

�

θ[j]r(1 − θ[j])N−r.

(11)

We cannot immediately rewrite the numerator in terms of Ein N[Nr] be-

cause of the extra θ[j] term. However, it is possible to write it in terms of

Ein N+1[Nr+1]. Observe that

3


θ[j]

�N

r

�

θ[j]r(1 − θ[j])N−r

(12)

=

N!

(N − r)!r!θ[j]r+1(1 − θ[j])N−r

(13)

=

r + 1

N + 1

N + 1

r + 1

N!

(N − r)!r!θ[j]r+1(1 − θ[j])(N+1)−(r+1)

(14)

=

r + 1

N + 1

(N + 1)!

(N − r)!(r + 1)!θ[j]r+1(1 − θ[j])(N+1)−(r+1)

(15)

=

r + 1

N + 1

�N + 1

r + 1

�

θ[j]r+1(1 − θ[j])(N+1)−(r−1)

(16)

=

r + 1

N + 1Ein N+1[Nr+1].

(17)

Therefore

ˆθ(r) = r + 1

N + 1

Ein N+1[Nr+1]

Ein N[Nr]

(18)

Now we plug in our observed values for Ein N[Nr] and Ein N+1[Nr+1].

These are Nr and Nr+1 respectively. This yields

ˆθ(r) =

1

N + 1(r + 1)Nr+1

Nr

.

(19)

For large N,

1

N+1 ≈ 1

N , so ﬁnally we can rewrite the above equation as

ˆθ(r) = 1

N (r + 1)Nr+1

Nr

.

(20)

We will explore this approximation and an alternate explanation more in

exercise 4.

This estimate has the nice property that

N0ˆθ(0) = N0

1

N

N1

N0

= N1

N .

(21)

In other words, the total probability mass assigned to unseen events is the

same as the relative occurrence of words that appear just once! This makes

sense, because appearing zero times is not so different from appearing once in

a relatively small sample.

One potential problem with this estimate is that it does not assign enough

probability mass to events that occur a large number of times. For example, if

rM is the maximum number of times any word was observed, then

ˆθ(rM) = 1

N (rM + 1)NrM+1

NrM

= 0,

(22)

because NrM+1 = 0 (i.e. there is no word that appeared rM + 1 times).

4


4

Second Derivation

We will also examine another way to derive the Good-Turing estimation based

on the concept of “deleted etimation” proposed by [3] (also see [4]). The idea

behind this derivation is to divide W into two sets: the “train” set and the

“heldout” set. The train set will be used to determine which terms occur r

times, while the heldout set is used to estimate θ(r).

Let Heldcounts(r) be the number of times r-count items occur in the held-

out set.

For example, let X = {the, bad, dog, cat}, W = the, cat, the, cat, the, dog,

the, cat, the, dog, cat. The train set and the heldout set are partitioned in the

following manner:

Train: the, cat, the cat

Heldout: the, dog, the, cat, the, dog, cat

In this scenario the Heldcounts are as follows:

Heldcounts(0) = 2. The 0-count items are “dog” and “bad”; “dog” occurs

twice in the heldout set.

Heldcounts(1) = 0. There are no 1-count items in the train set.

Heldcounts(2) = 5. The 2-count items are “the” and “cat”; there are 5 of these

items in the heldout set.

In order to estimate θ(r) for a given heldout set H, we can take ˆθ(r) values

to be the max-likelihood estimates.

We introduce the non-normalized likelihood for a multinomial distribution

F(H) = C

�

j

θ[j]#ho(xj),

(23)

where C is the multinomial coefﬁcient. Note that C is a constant, so we can re-

move it to get an equation that is equivalent under ranking. We will maximize

this equation subject to the constraint

�

j

ˆθ[j] = 1

(24)

With our deﬁnition of Heldcounts, we can rewrite the likelihood as

F(H) =

�

r

θ(r)Heldcounts(r)

(25)

and the constraint as

�

r

ˆθ(r)Nr = 1.

(26)

We will continue this derivation in the next lecture.

5

Exercises

1. This exercise is to test your understanding of the basic notation and con-

cepts used in Good-Turing smoothing. Suppose we have the following

5


set of possible item types: X = {apple, banana, carrots, dates, eggs, frogs, grapes}.

And suppose we have a sequence of N independent samples: W =

apple apple apple banana banana dates dates eggs eggs eggs frogs grapes grapes

(a) Calculate the empirical (observed relative-frequency) probabilities,

θe(r).

(b) Calculate the Good-Turing probability estimates, ˆθ(r), based on W.

(c) Verify that �

r ˆθ(r)Nr = 1.

2.

(a) What would the Good-Turing estimates be for the following ob-

served values: N0 = 1, N1 = 0, N2 = 1, N3 = 0, N4 = 1?

(b) What problems do you run into when you try to calculate these es-

timates? How might you correct these problems?

3. Show that ˆθ(0) = ˆθ(1) = ... = ˆθ(m) ∝ 1/N if Nr = s e−λ

r! λr (i.e. Nr

has Poisson form), where s is a positive constant. Note that e−λ

r! λr ≤ 1

because it is the density function of the Poisson distribution, so the s term

acts as a scale factor that expands the range of Nr to the interval [0, s].

This exercise is based on a fact in [1].

4. In equation (20), we replaced the “normalization” term

1

N+1 with

1

N to

get

ˆθ(r) = 1

N (r + 1)Nr+1

Nr

.

(a) Argue that

1

N+1 is not the correct normalization for (r + 1) Nr+1

Nr

by

showing that if we use

1

N+1 as the normalization, then we get an

invalid probability distribution for the resulting word occurrence

probability distribution.

(b) What went wrong? How did our derivation produce the wrong nor-

malization for

ˆ

θ(r)?

6

Solutions

1. N0 = 1 (carrots)

N1 = 1 (frogs)

N2 = 3 (banana, dates, grapes)

N3 = 2 (apple, eggs)

(a) θe(0) = 0/13

θe(1) = 1/13

θe(2) = 3/13

θe(3) = 2/13

6


(b) ˆθ(0) =

1

13(1) 1

1 =

1

13

ˆθ(1) =

1

13(2) 3

1 =

6

13

ˆθ(2) =

1

13(3) 2

3 =

6

39

ˆθ(3) =

1

13(4) 0

2 = 0

(c) ˆθ(0) + ˆθ(1) + 3(ˆθ(2)) + 2(ˆθ(3)) =

1

13 + 6

13 + 6

13 + 0 = 13

13 = 1

2.

(a) ˆθ(0) = 1

6(1) 0

1 = 0

6

ˆθ(1) = 1

6(2) 1

0 = undeﬁned

ˆθ(2) = 1

6(3) 0

1 = 0

6

ˆθ(3) = 1

6(4) 1

0 = undeﬁned

ˆθ(4) = 1

6(5) 0

1 = 0

6

(b) There are at least two problems with these estimates. First of all,

there are undeﬁned values if r = 1 or r = 3. This might not be a

problem because one can argue that if there are no items that appear

once, or if there are no items that appear 3 times in W, then there

should be no probability associated with these values of r. However,

there is another potential problem: the probabilities do not sum to

1. In real data samples, we can expect that there are some Nr values

that are zero, so this could be a problem in practice.

This example suggests that there are problems that arise when using

the Good-Turing estimation with a dataset that has some Nr values

equal to 0. One way to ﬁx this problem is to smooth the Nr counts so

that they are all nonzero. For example, we can use linear regression

to interpolate values for unobserved θ(r), as in [2].

3. We have

ˆθ(r)

=

r + 1

N

se−λλr+1

(r + 1)!

r!

se−λλr

(27)

=

λ/N.

(28)

Note that λ cannot be a free parameter, since there is only one value of λ

that normalizes the probability distribution.

4.

(a) Let ˆθ

′(r) be the new value for ˆθ(r) that we get under this normal-

ization scheme and similarly let ˆθ

′[j] be the new value for ˆθ[j]. Note

that ˆθ

′(r) =

N

N+1 ˆθ(r) and thus ˆθ

′[j] =

N

N+1 ˆθ[j]. We showed previ-

ously that if we use the Good-Turing estimate for ˆθ(r), then �

j ˆθ[j] =

1. Therefore �

j ˆθ

′[j] =

N

N+1 and thus θ

′ is not a valid probability

distribution.

7


(b) We used Nr+1 as an estimate for Ein N+1[Nr+1]. However, Nr+1

was based on observing N items rather than N + 1, so really it is an

approximation for Ein N[Nr+1]. Therefore we need a correction.

7

References

1. I. J. Good. The population frequencies of species and the estimation of

population parameters. Biometrika 40: 237-264 (1953).

2. W. A. Gale. Good-Turing Smoothing Without Tears. Journal of Quantita-

tive Linguistics 2: 217-237 (1995).

3. Frederick Jelink and Robert Mercer. Probability distribution estimation

from sparse data. IBM Technical Disclosure Bulletin 28: 2591-2594 (1985).

4. Arthur N´adas. On Turing’s formula for word probabilities. IEEE Transac-

tions on Acoustics, Speech and Signal Processing ASSP-33(6):1414-1416, 1985.

8

