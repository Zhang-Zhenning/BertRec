
Expectation Maximization (EM) Algorithm

Motivating Example:

• Have two coins: Coin 1 and Coin 2

• Each has it’s own probability of seeing “H” on any one ﬂip. Let

p1

=

P( H on Coin 1 )

p2

=

P( H on Coin 2 )

• Select a coin at random and ﬂip that one coin m times.

• Repeat this process n times.

• Now have data

X11

X12

· · ·

X1m

X21

X22

· · ·

X2m

...

...

...

...

Xn1

Xn2

· · ·

Xnm

Y1

Y2

...

Yn

Here, the Xij are Bernoulli random variables taking values in {0, 1} where

Xij =











1

,

if the jth ﬂip for the ith coin chosen is H

0

,

if the jth ﬂip for the ith coin chosen is T

and the Yi live in {1, 2} and indicate which coin was used on the nth trial.

Note that all the X’s are independent and, in particular

Xi1, Xi2, . . . , Xim|Yi = j

iid

∼ Bernoulli(pj)

We can write out the joint pdf of all nm + n random variables and formally come up

with MLEs for p1 and p2. Call these MLEs �p1 and �p2. They will turn out as expected:

�p1

=

total # of times Coin 1 came up H

total # times Coin 1 was ﬂipped

�p2

=

total # of times Coin 2 came up H

total # times Coin 2 was ﬂipped

• Now suppose that the Yi are not observed but we still want MLEs for p1 and p2. The

data set now consists of only the X’s and is “incomplete”.

• The goal of the EM Algorithm is to ﬁnd MLEs for p1 and p2 in this case.


Notation for the EM Algorithm:

• Let X be observed data, generated by some distribution depending on some parameters.

Here, X represents something high-dimensional. (In the coin example it is an n × m

matrix.) These data may or may not be iid. (In the coin example it is a matrix with

iid observations in each row.) X will be called an “incomplete data set”.

• Let Y be some “hidden” or “unobserved data” depending on some parameters. Here,

Y can have some general dimension. (In the coin example, Y is a vector.)

• Let Z = (X, Y ) represent the “complete” data set. We say that it is a “completion”

of the data given by X.

• Assume that the distribution of Z (likely a big fat joint distribution) depends on some

(likely high-dimensional) parameter θ and that we can write the pdf for Z as

f(z; θ) = f(x, y; θ) = f(y|x; θ)f(x; θ).

It will be convenient to think of the parameter θ as “given” and to write this instead

as

f(z|θ) = f(x, y|θ) = f(y|x, θ)f(x|θ).

(Note: Here, the f’s are diﬀerent pdfs identiﬁed by their arguments. For example

f(x) = fX(x) and f(y) = fY (y). We will use subscripts only if it becomes necessary.)

• We usually use L(θ) to denote a likelihood function and it always depends on some

random variables which are not shown by this notation. Because there are many groups

of random variables here, we will be more explicit and write L(θ|Z) or L(θ|X) to denote

the complete likelihood and incomplete likelihood functions, respectively.

• The complete likelihood function is

L(θ|Z) = L(θ|X, Y ) = f(X, Y |θ).

• The incomplete likelihood function is

L(θ|X) = f(X|θ).


The Algorithm

The EM Algorithm is a numerical iterative for ﬁnding an MLE of θ. The rough idea is to

start with an initial guess for θ and to use this and the observed data X to “complete” the

data set by using X and the guessed θ to postulate a value for Y , at which point we can then

ﬁnd an MLE for θ in the usual way. The actual idea though is slightly more sophisticated.

We will use an initial guess for θ and postulate an entire distribution for Y , ultimately

averaging out the unknown Y . Speciﬁcally, we will look at the expected complete likelihood

(or log-likelihood when it is more convenient) E[L(θ|X, Y )] where the expectation is taken

over the conditional distribution for the random vector Y given X and our guess for θ.

We proceed as follows.

1 Let k = 0. Give an initial estimate for θ. Call it �θ(k).

2 Given observed data X and assuming that �θ(k) is correct for the parameter θ, ﬁnd the

conditional density f(y|X, �θ(k)) for the completion variables.

3 Calculate the conditional expected log-likelihood or “Q-function”:

Q(θ|�θ(k)) = E[ln f(X, Y |θ)|X, �θ(k)].

Here, the expectation is with respect to the conditional distribution of Y given X and

�θ(k) and thus can be written as

Q(θ|�θ(k)) =

�

ln(f(X, y|θ)) · f(y|X, �θ(k)) dy.

(The integral is high-dimensional and is taken over the space where Y lives.)

4 Find the θ that maximizes Q(θ|�θ(k)). Call this �θ(k+1).

Let k = k + 1 and return to Step 2 .

The EM Algorithm is iterated until the estimate for θ stops changing. Usually, a tolerance

ε is set and the algorithm is iterated until

||�θ(k+1) − �θ(k)|| &lt; ε.

We will show that this stopping rule makes sense in the sense that once that distance is less

than ε it will remain less than ε.


Figure 1: Visualization for Jensen’s Inequality for a Convex Function



Jensen’s Inequality

The EM algorithm is derived from Jensen’s inequality, so we review it here.

Let X be a random variable with mean µ = E[X], and let g be a convex function. Then

g(E[X]) ≤ E[g(X)].

To prove Jensen’s inequality, visualize the convex function g and a tangent line at the point

(µ, g(µ)), as despicted in Figure 1.

Note that, by convexity of g, the line is always below g:

ℓ(x) ≤ g(x)

∀x.

So,

m(x − µ) + g(µ) ≤ g(x)

∀x,

and therefore, we can plug in the random variable X to get

m(X − µ) + g(µ) ≤ g(X).

Taking the expected value of both sides leaves us with

g(µ) ≤ E[g(X)]

which is the desired result

g(E[X]) ≤ E[g(X)].


Note that, if g is concave, then the negative of g is convex. Applying Jensen’s inequality to

−g and then multiplying through by −1 gives

g(E[X]) ≥ E[g(X)].

So, we now know, for example, that

ln(E[X]) ≥ E[ln(X)].

Derivation of the EM Algorithm

• Imagine we have some data X with joint pdf f(X|θ).

• Let ℓ(θ) = ln f(X|θ) denote the log-likelihood.

• Suppose we are trying to guess at θ and improve our guesses through some sort of

iteration. Let �θ(n) be our nth iteration guess.

• We would like to ﬁnd a new value for θ that satisﬁes

ℓ(θ) ≥ ℓ(�θ(n)).

• We will introduce some hidden variables Y , either because we are actually working with

a model that has hidden (unobserved) variables or “artiﬁcially” because they make the

maximization more tractable.

• So, we may write

ℓ(θ) − ℓ(�θ(n))

=

ln f(X|θ) − ln f(X|�θ(n))

=

ln

��

f(X|y, θ)f(y|θ) dy

�

− ln f(X|�θ(n))

=

ln

�� f(X|y, θ)f(y|θ)

f(y|X, �θ(n))

f(y|X, �θ(n)) dy

�

− ln f(X|�θ(n))

• Note that the thing in parentheses is an expectation with respect to the distribution

of Y |X, �θ(n). So, applying Jensen’s inequality, we have

ℓ(θ) − ℓ(�θ(n))

≥

�

ln

�f(X|y, θ)f(y|θ)

f(y|X, �θ(n))

�

f(y|X, �θ(n)) dy − ln f(X|�θ(n))

=

�

ln

�

f(X|y, θ)f(y|θ)

f(y|X, �θ(n))f(X|�θ(n))

�

f(y|X, �θ(n)) dy

Call that entire integral d(θ|�θ(n)).

We then have

ℓ(θ) ≥ ℓ(�θ(n)) + d(θ|�θ(n))


Figure 2: A Horrible Temporary Image



• Note that

d( �

θ(n)|�θ(n))

=

�

ln

� f(X|y, �θ(n))f(y|�θ(n))

f(y|X, �θ(n))f(X|�θ(n))

�

f(y|X, �θ(n)) dy

=

�

ln

�f(X, y|�θ(n))

f(X, y|�θ(n))

�

f(y|X, �θ(n)) dy

=

�

ln (1) f(y|X, �θ(n)) dy = 0

• So, we have that ℓ(�θ(n)) + d(θ|θ(n)) is bounded above by ℓ(θ) and that it is equal to

this upper bound when θ = �θ(n). This is visualized in Figure 2.

If we maximize ℓ(�θ(n)) + d(θ|θ(n)) with respect to θ (equivalently, maximize d(θ|θ(n))

with respect to θ), we may improve towards maximizing ℓ(θ). We know, at least, that

we will not get worse.

• Maximizing

d(θ|�θ(n)) =

�

ln

�

f(X|y, θ)f(y|θ)

f(y|X, �θ(n))f(X|�θ(n))

�

f(y|X, �θ(n)) dy

with respect to θ is equivalent to maximizing

�

ln (f(X|y, θ)f(y|θ)) f(y|X, �θ(n)) dy

with respect to θ.

Note that this is

�

ln f(X, y|θ) f(y|X, �θ(n)) dy

which is

EY [ln f(X, Y |θ)|X �θ(n)].


• So, if we could compute this expectation, maximize it with respect to θ, call the result

�θ(n+1) and iterate, we can improve towards ﬁnding the θ that maximizes the likelihood

(or at least not get worse). In other words, we can improve towards ﬁnding the MLE

of θ.

These expectation and maximization steps are precisely the EM algorithm!

The EM Algorithm for Mixture Densities

Assume that we have a random sample X1, X2, . . . , Xn is a random sample from the mixture

density

f(x|θ) =

N

�

j=1

pifj(x|θj).

Here, x has the same dimension as one of the Xi and θ is the parameter vector

θ = (p1, p2, . . . , pN, θ1, θ2, . . . , θN).

Note that �N

i=j pj = 1 and each θi may be high-dimensional.

An Xi sampled from f(x|θ) may be interpreted as being sampled from one of the densities

f1(x|θ1), f2(x|θ2), . . . , fN(x|θN)

with respective probabilities

p1, p2, . . . , pN.

• The likelihood is

L(θ|X) =

n

�

i=1

f(Xi|θ) =

n

�

i=1

N

�

j=1

pjfj(Xi|θ).

• The log-likelihood is

ℓ(θ|X) = ln L(X|θ) =

N

�

i=1

ln





N

�

j=1

pjfj(Xi|θj)



 .

• The log of the sum makes this diﬃcult to work with. In order to make this prob-

lem more tractable, we will consider X as incomplete data and we will augment the

data with additional variables Y1, Y2, . . . , Yn which indicate the component that the

corresponing X’s come from.

Speciﬁcally, for k = 1, 2, . . . , N,if Yi = k then Xi ∼ fk(x|θk), and pk = P(Yi = k).


• Now L(X|θ) is thought of an an incomplete likelihood and the complete likelihood is

L(θ|X, Y )

=

f(X, Y |θ) = �n

i=1 f(Xi, Yi|θ)

=

�n

i=1 f(Xi|Yi, θ)f(Yi|θ).

• Note that

f(yi|θ) = P(Yi = yi|θ) = pyi

and that

f(xi|yi, θ) = fyi(xi|θyi).

• So,

ℓ(θ|x, y) = ln L(θ|x, y) =

n

�

i=1

ln[pyifyi(xi|θyi)].

• To apply the EM algorithm, we will start with some initial guesses (k = 0) for the

parameters:

�θ(k) = (�p(k)

1 , �p(k)

2 , . . . , �p(k)

N , �θ(k)

1 , �θ(k)

2 , . . . , �θ(k)

N ).

• Then, we compute

f(yi|xi, �θ(k)) = f(xi|yi, �θ(k)) · f(yi|�θ(k))

f(xi|�θ(k))

=

�p(k)

yi fyi(xi|�θy(k)

i )

�N

j=1 �p(k)

j fj(xi|�θ(k))

for yi = 1, 2, . . . , N.

• The EM algorithm “Q-function” is

Q(θ|�θ(k))

=

E[ln f(X, Y |θ)

�

��

�

L(θ|X,Y )

|X, �θ(k)]

=

�

y ln L(θ|X, y) · f(y|X, �θ(k)).

Here,

f(y|X, �θ(k)) =

n

�

i=1

f(yi|xi, �θ(k)),

but we will ultimately expand our expression for Q(θ|�θ(k)) into terms involving the

f(yi|xi, �θ(k)) and not need to write this joint density down explicitly.

[TO BE CONTINUED...]

