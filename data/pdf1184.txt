
KL Divergence Online Demo

KL Divergence Online Demo

To try out Shiny, I created an interactive visualization for Kullback-Leibler divergence (or KL

Divergence). Right now, it only supports two univariate Gaussians, which should be sufficient to build

some intuition.

If you like it, let me know! If it turns out to be popular, I might add more features, or create similar

visualizations for other concepts!

Please Wait



What is KL Divergence? What am I seeing?

What is KL Divergence? What am I seeing?

gnarlyware

gnarlyware


← PREVIOUS

 PREVIOUS

POST

POST

NEXT POST

NEXT POST

→

What is KL Divergence? What am I seeing?

What is KL Divergence? What am I seeing?

Consider an unknown probability distribution 

, which we’re trying to approximate with

probability distribution 

, then:

can informally be interpreted as the amount of information being lost by using  to approximate . As

you might imagine, this has several applications in Machine Learning. A recurring pattern is to fit

parameters to a model by minimizing an approximation of 

 (ie, making  “as similar” to  as

possible). This blog post elaborates in a fun and informative way. If you have never heard about KL

divergence before, Bishop provides a more formal (but still easy to understand) introduction in

Section 1.6 of Pattern Recognition and Machine Learning.

Suggested exercises with the interactive plot

Suggested exercises with the interactive plot

Using the visualization tool, you can reason about the following questions:

Is 

? Always? Never?

When is 

?

Let 

 and 

. Which is larger: 

 or 

? Why?

Is 

 ever negative? When, or why not?













p(x)

q(x)

KL(p∣∣q) = −

p(x)ln

dx

∫

p(x)

q(x)

q

p

KL(p∣∣q)

q

p

KL(p∣∣q) = KL(q∣∣p)

KL(p∣∣q) = 0

r(x) = N(0,1)

s(x) = N(0,2)

KL(r∣∣s)

KL(s∣∣r)

KL(p∣∣q)

 

 

Fredrik Nilsson  • © 2021  •  gnarlyware

Hugo v0.87.0 powered  •  Theme Beautiful Hugo adapted from Beautiful Jekyll



















