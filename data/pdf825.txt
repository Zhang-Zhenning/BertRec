
RESEARCH-ARTICLE

Relevance-based Word Embedding

Authors: 

 

Authors Info &amp; Claims

SIGIR '17: Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval

• August 2017 • Pages 505–514 • https://doi.org/10.1145/3077136.3080831

Published: 07 August 2017 Publication History



Hamed Zamani,



W. Bruce Croft



Next 

Pages 505–514

 Previous



SIGIR '17: Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval

Relevance-based Word Embedding





Learning a high-dimensional dense representation for vocabulary terms, also known as a word embedding, has recently attracted

much attention in natural language processing and information retrieval tasks. The embedding vectors are typically learned based on

term proximity in a large corpus. This means that the objective in well-known word embedding algorithms, e.g., word2vec, is to

accurately predict adjacent word(s) for a given word or context. However, this objective is not necessarily equivalent to the goal of

many information retrieval (IR) tasks. The primary objective in various IR tasks is to capture relevance instead of term proximity,

syntactic, or even semantic similarity. This is the motivation for developing unsupervised relevance-based word embedding models

that learn word representations based on query-document relevance information. In this paper, we propose two learning models with

different objective functions; one learns a relevance distribution over the vocabulary set for each query, and the other classifies each

term as belonging to the relevant or non-relevant class for each query. To train our models, we used over six million unique queries

and the top ranked documents retrieved in response to each query, which are assumed to be relevant to the query. We extrinsically

evaluate our learned word representation models using two IR tasks: query expansion and query classification. Both query expansion

experiments on four TREC collections and query classification experiments on the KDD Cup 2005 dataset suggest that the relevance-

based word embedding models significantly outperform state-of-the-art proximity-based embedding models, such as word2vec and

GloVe.

ABSTRACT

 Get Access

 







 79  1,792

 Sign in





IR

IR








References

1.

2.

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

Nasreen Abdul-jaleel, James Allan, W. Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Donald Metzler, Mark D. Smucker, Trevor

Strohman, Howard Turtle, and Courtney Wade. 2004. UMass at TREC 2004: Novelty and HARD. In TREC '04.

Qingyao Ai, Liu Yang, Jiafeng Guo, and W. Bruce Croft. 2016. Analysis of the Paragraph Vector Model for Information Retrieval ICTIR

'16. 133--142.

Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on Measuring the

Divergence from Randomness. ACM Trans. Inf. Syst. Vol. 20, 4 (2002), 357--389. 

P. D. Bruza and D. Song. 2002. Inferring Query Models by Computing Information Flow CIKM '02. 260--269.

Stephane Clinchant and Florent Perronnin. 2013. Aggregating Continuous Word Embeddings for Information Retrieval CVSC@ACL

'13. 100--109.

Gordon V. Cormack, Mark D. Smucker, and Charles L. Clarke. 2011. Efficient and Effective Spam Filtering and Re-ranking for Large

Web Datasets. Inf. Retr., Vol. 14, 5 (2011), 441--465. 

Bruce Croft, Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice (1st ed.). Addison-Wesley

Publishing Company.

Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by Latent

Semantic Analysis. Vol. 41, 6 (1990), 391--407.

Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Croft. 2017. Neural Ranking Models with Weak

Supervision. In SIGIR '17.

Fernando Diaz. 2015. Condensed List Relevance Models. In ICTIR '15. 313--316. 

Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. Query Expansion with Locally-Trained Word Embeddings ACL '16.

Michael U. Gutmann and Aapo Hyvärinen. 2012. Noise-contrastive Estimation of Unnormalized Statistical Models, with Applications to

Natural Image Statistics. J. Mach. Learn. Res., Vol. 13, 1 (2012), 307--361.

Kalervo Järvelin and Jaana Kekäläinen 2002. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Syst. Vol. 20, 4

(Oct. 2002), 422--446. 

Yufeng Jing and W. Bruce Croft. 1994. An Association Thesaurus for Information Retrieval RIAO '94. 146--160.

Tom Kenter and Maarten de Rijke. 2015. Short Text Similarity with Word Embeddings. In CIKM '15. 1411--1420. 

Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From Word Embeddings to Document Distances. In ICML

'15. 957--966.

Saar Kuzi, Anna Shtok, and Oren Kurland. 2016. Query Expansion Using Word Embeddings. In CIKM '16. 1929--1932. 

John Lafferty and Chengxiang Zhai. 2001. Document Language Models, Query Models, and Risk Minimization for Information Retrieval

SIGIR '01. 111--119.

Victor Lavrenko, Martin Choquette, and W. Bruce Croft. 2002. Cross-lingual Relevance Models. In SIGIR '02. 175--182. 

Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In SIGIR '01. 120--127. 

Omer Levy and Yoav Goldberg. 2014. Neural Word Embedding as Implicit Matrix Factorization. NIPS '14. 2177--2185.


22.

23.

24.

25.

26.

27.

28.

29.

30.

31.

32.

33.

34.

35.

36.

37.

38.

39.

40.

41.

42.

43.

Ying Li, Zijian Zheng, and Honghua (Kathy) Dai. 2005. KDD CUP-2005 Report: Facing a Great Challenge. SIGKDD Explor. Newsl., Vol.

7, 2 (2005), 91--99.

Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-yi Wang. 2015. Representation Learning Using Multi-Task

Deep Neural Networks for Semantic Classification and Information Retrieval. In NAACL '15. 912--921. 

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases

and their Compositionality NIPS '13. 3111--3119.

Andriy Mnih and Geoffrey E Hinton. 2009. A Scalable Hierarchical Distributed Language Model. NIPS '09. 1081--1088.

Frederic Morin and Yoshua Bengio. 2005. Hierarchical Probabilistic Neural Network Language Model AISTATS '05. 246--252.

Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search InfoScale '06.

Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. EMNLP '14.

1532--1543. 

Jay M. Ponte and W. Bruce Croft. 1998. A Language Modeling Approach to Information Retrieval SIGIR '98. 275--281.

Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Hamed Zamani. 2017. Word Embedding Causes Topic Shifting; Exploit Global

Context! SIGIR '17.

Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Guido Zuccon. 2016. Generalizing Translation Models in the Probabilistic Relevance

Framework CIKM '16. 711--720.

J. J. Rocchio. 1971. Relevance Feedback in Information Retrieval. The SMART Retrieval System: Experiments in Automatic Document

Processing. 313--323.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning representations by back-propagating errors. Nature Vol. 323 (Oct.

1986), 533--536. 

T. Saracevic. 2016. The Notion of Relevance in Information Science: Everybody knows what relevance is. But, what is it really?

Morgan &amp; Claypool Publishers.

Alessandro Sordoni, Yoshua Bengio, and Jian-Yun Nie. 2014. Learning Concept Embeddings for Query Expansion by Quantum

Entropy Minimization AAAI '14. 1586--1592.

Tao Tao and ChengXiang Zhai. 2006. Regularized Estimation of Mixture Models for Robust Pseudo-relevance Feedback SIGIR '06.

162--169.

Ivan Vuliç and Marie-Francine Moens. 2015. Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word

Embeddings. In SIGIR '15. 363--372.

Jinxi Xu and W. Bruce Croft. 1996. Query Expansion Using Local and Global Document Analysis SIGIR '96. 4--11.

Hamed Zamani, Michael Bendersky, Xuanhui Wang, and Mingyang Zhang. 2017. Situational Context for Ranking in Personal Search

WWW '17. 1531--1540.

Hamed Zamani and W. Bruce Croft. 2016. Embedding-based Query Language Models. In ICTIR '16. 147--156. 

Hamed Zamani and W. Bruce Croft. 2016. Estimating Embedding Vectors for Queries. In ICTIR '16. 123--132. 

Hamed Zamani, Javid Dadashkarimi, Azadeh Shakery, and W. Bruce Croft. 2016. Pseudo-Relevance Feedback Based on Matrix

Factorization CIKM '16. 1483--1492.

ChengXiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for


Read More

Read More

Read More

44.

45.

46.

47.

Subtopic Retrieval SIGIR '03. 10--17. 

Chengxiang Zhai and John Lafferty. 2001. Model-based Feedback in the Language Modeling Approach to Information Retrieval CIKM

'01. 403--410.

Chengxiang Zhai and John Lafferty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM

Trans. Inf. Syst. Vol. 22, 2 (2004), 179--214. 

Guoqing Zheng and Jamie Callan. 2015. Learning to Reweight Terms with Distributed Representations SIGIR '15. 575--584.

Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu. 2015. Learning Continuous Word Embedding with Metadata for Question

Retrieval in Community Question Answering. In ACL '15. 250--259. 

Index Terms



Relevance-based Word Embedding

Computing methodologies

Machine learning

Learning paradigms

Unsupervised learning

Machine learning approaches

Neural networks

Information systems

Information retrieval

Information retrieval query processing

Query reformulation

Query representation

Recommendations

Embedding-based Query Language Models

Word-embedding-based pseudo-relevance feedback for Arabic information retrieval

Query dependent pseudo-relevance feedback based on wikipedia


Check if you have access through your login credentials or your institution to get full access on this article.



Sign in

Get this Publication

Information

Contributors

Comments

Login options

Full Access

SIGIR '17: Proceedings of the 40th International ACM SIGIR Conference on Research and

Development in Information Retrieval

August 2017 1476 pages

ISBN:

9781450350228

DOI:

10.1145/3077136

Published in


Bibliometrics

Citations



79

DOI:

10.1145/3077136

General Chairs:

Noriko Kando,



Tetsuya Sakai,



Hideo Joho,Program Chairs:

Hang Li,



Arjen P. de Vries,

Ryen W. White

Copyright © 2017 ACM

Association for Computing Machinery

New York, NY, United States

Published: 7 August 2017

Request permissions about this article.

Request Permissions

embedding vector

word representation

query classification

query expansion

neural network

Research-Article

SIGIR '17 Paper Acceptance Rate 78 of 362 submissions, 22%Overall Acceptance Rate 792 of 3,983 submissions, 20%

More



Publisher

Publication History

Permissions

Check for updates

Author Tags

Qualifiers

Acceptance Rates


View or Download as a PDF file.

 PDF

View online with eReader.

 eReader

Figures

Other

https://dl.acm.org/doi/abs/10.1145/3077136.3080831?

casa_token=_wtB_f06d1MAAAAA%3A7Phagqbz4dnVqLyhUH8Jqtf147SJgZ61DQfw6Bw4Xy52kSA08eG_LzWO4xzx7Js9zWBZ03OwVKZs



 Copy Link

178

37

79

Total

Citations

View Citations

1,792

Total

Downloads

Downloads (Last 12 months)

Downloads (Last 6 weeks)



View Author Metrics

Article Metrics

Other Metrics

PDF Format

eReader

Share this Publication link

Share on Social Media




View Table Of Contents

















0





Categories

















About

















Join









Connect













The ACM Digital Library is published by the Association for Computing Machinery. Copyright © 2023 ACM, Inc.

Terms of Usage 

Privacy Policy 

Code of Ethics

 





