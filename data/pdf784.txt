


Member-only story

Understanding BERT — (Bidirectional Encoder

Representations from Transformers)

·

Published in

Towards Data Science

10 min read

·

Aug 6, 2020

Listen

Share







We


Difference between BERT and previous embedding techniques.

Taking a look under the hood:

How the pre-trained BERT was trained:

The input and output:

How is BERT different from other embedding generating

algorithms like Word2Vector or GloVe?

tie

tie

tie

tie

Taking a look under the hood


source: http://jalammar.github.io/illustrated-bert/

self-attention heads

feed-forward neural network.

source: http://jalammar.github.io/illustrated-bert/

source: http://jalammar.github.io/illustrated-bert/

source: https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-is-it-

a-game-changer-in-nlp-7cca943cf3ad

Training the model

Masked Language Model (MLM):


Next Sentence Prediction (NSP):

The Input and Output

Input:

[CLS]

[SEP]

[CLS]

[SEP]

[‘[CLS]’, ‘I’, ‘have’, ‘a’, ‘pen’, ‘[SEP]’, ‘the’, ‘pen’, ‘is’, ‘red’, ‘[SEP]’]

source: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-

f8b21a9b6270

Token Embeddings

Segment Embeddings


Segment Embeddings

[‘[CLS]’, ‘I’, ‘have’, ‘a’, ‘pen’, ‘[SEP]’, ‘the’, ‘pen’, ‘is’, ‘red’,

‘[SEP]’]

[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]

0

1

Mask tokens:

1

0

Position Embeddings: 

Output:

[CLS]

source: http://jalammar.github.io/illustrated-bert/


source: http://jalammar.github.io/illustrated-bert/

Usability

source: http://jalammar.github.io/illustrated-bert/


Final note



Follow



136 Followers

·

Writer for 

Towards Data Science

Learning something new epoch by epoch.

NLP

Deeplearing

Machine Learning

Artificial Intelligence

Education








AI-based Indian license plate detector.

·

·






Zero-ETL, ChatGPT, And The Future of Data Engineering

·






The Portfolio that Got Me a Data Scientist Job

·

·






How powerful can an ensemble of linear models be?

·

·

See all from Sarthak Vajpayee

·

See all from Towards Data Science






Fine-Tune Transformer Models For Question Answering On Custom Data

·

·






Understanding and Coding the Attention Mechanism — The Magic Behind

Transformers

·

·






You’re Using ChatGPT Wrong! Here’s How to Be Ahead of 99% of ChatGPT Users

·

·






Interpreting the Prediction of BERT Model for Text Classification

·

·






Language Models: GPT and GPT-2

·

·






Essential Guide to Foundation Models and Large Language Models

·

·

See more recommendations



