


Published in

Towards Data Science



May 7, 2021

·

36 min read

·

Save

Neural Language Models

From feedforward through stacked recurrent

Photo by Markus Spiske on Unsplash

Suggest auto-completes

Recognize handwriting

Recognize speech

Detect and correct spelling errors

Translate languages









Early Neural Language Model


Recurrent Neural Networks

Basic RNN

x

y

x

x

y

y

y

x

x

x

x

x

h

y

x

h

x

y

h

h

f h

x

y

g h

The State-change and Output Functions


f h,x

S 1 Whh h

Wxh x

g h

S 2 Why h

S1

S2

f h x

S1 g h

S2

RNNs in Language Modeling

x

h

y

S2

S2 u

u

x

x

y

Learning

Whh Wxh

Why 

Whh 

Wxh

Why 


x

y

y

x

x

x

f

g

f

f

f

f

x

y

x

y

f 

f


A Long-range Example

x

x

x


x

x

W h

W

Gated Recurrent Unit (GRU)


h

A h

B x

C

h

h

x

hnew

h

x

h

x

h

h

s

h

h

x

hnew

A x

B s

h

C

hnew

x

C

A

B 

A B

s


Filtering Gate Equation

s

D x

E h

F

D E

F

s

Example to illustrate the update gate mechanism

23


h

hnew

Update Gate Usage Equation

u

h

1 u

h

u

hnew

Anomaly Detection Example Refined


h

hnew 

h

hnew

hnew

h

h

hnew

u

u

Update Gate Equation

u

A x

B h

C

A B

C

Putting All the Equations Together

s

A x

B h

C

hnew

D x

E s

h

F

u

G x

H h

I

h

1 u

h

u

hnew

x

h

s

hnew

u

x

h

s

u

h

h

hnew

u

“Learning”


A B D E G

H 

C F

I

Two for one: Anomaly detection also solves forecasting

First, anomalies not labeled

h

h

hnew

h

h

hnew

hnew

h

hnew

h

hnew

hnew

h

hnew

hnew

h

hnew

h

h

h

h

1 u

h

u

hnew

h

hnew

h

h

h

u


u

A x

B h

C

hnew

hnew

h

u

0

u

1

4 

hnew

h

h

h

4

hnew

u

1

4 

h

Feature Engineering


f

Anomalies are labeled: Human or computer

u

u

0     Direct GRU to continue 'old normal'

u

Direct GRU to move towards 'most recent

                         values'

Direct GRU to move towards ‘most recent values’ 

 

h

hnew

u

h

hnew

u

What about the filter gate?

s

h(

), 

x


Forecasting and Filter Gate

s

But how do we learn s

?

s

s

v

h

 

h

f h

v

h


f 

h

h

v

h

h

f

s

h

h

v

f

h

v

s

h

s

  h

s

v

GRU and The Vanishing Gradient Problem

h

1 u

h

u

hnew

h

A h

B x

C


Stacked RNNs

h

f h

x

f h,x

S1 Whh h

Wxh x

h

h

x

f h( ),h(

),

S1 Whh( ) h( )

Wxh( ) h(

)

h

x

h

h

h

h

Inference


Greedy decoding

Example

x

x

Example

Beam search

References

Recurrent Neural Network




Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





Hidden Mark Models

Deep Dives

Neural Networks

Machine Learning Models

