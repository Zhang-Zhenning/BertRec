


10 Leading Language Models For NLP In 2022

10 Leading Language Models For NLP In 2022

June 17, 2022 by Mariya Yao



UPDATE: We have published the updated version of this article, considering the latest research

UPDATE: We have published the updated version of this article, considering the latest research

advances in large language models. Check out 

advances in large language models. Check out Top 6 NLP Language Models Transforming AI in 2023..

The introduction of transfer learning and pretrained language models in natural language processing (NLP) pushed

forward the limits of language understanding and generation. Transfer learning and applying transformers to

different downstream NLP tasks have become the main trend of the latest research advances.

At the same time, there is a controversy in the NLP community regarding the research value of the huge pretrained

language models occupying the leaderboards. While lots of AI experts agree with Anna Rogers’s statement that

getting state-of-the-art results just by using more data and computing power is not research news, other NLP

opinion leaders point out some positive moments in the current trend, like, for example, the possibility of seeing the

fundamental limitations of the current paradigm.

Anyway, the latest improvements in NLP language models seem to be driven not only by the massive boosts in

computing capacity but also by the discovery of ingenious ways to lighten models while maintaining high

performance.

To help you stay up to date with the latest breakthroughs in language modeling, we’ve summarized research

papers featuring the key language models introduced during the last few years.

Subscribe to our AI Research mailing list at the bottom of this article to be alerted when we release new

 at the bottom of this article to be alerted when we release new

summaries.

summaries.

MENU


If you’d like to skip around, here are the papers we featured:

1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

2. GPT2: Language Models Are Unsupervised Multitask Learners

3. XLNet: Generalized Autoregressive Pretraining for Language Understanding

4. RoBERTa: A Robustly Optimized BERT Pretraining Approach

5. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

6. T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

7. GPT3: Language Models Are Few-Shot Learners

8. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators

9. DeBERTa: Decoding-enhanced BERT with Disentangled Attention

10. PaLM: Scaling Language Modeling with Pathways

Important Pretrained Language Models

Important Pretrained Language Models

1. 1. BERT: Pre-training of Deep Bidirectional Transformers for Language

Understanding, by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

, by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

Kristina Toutanova

Kristina Toutanova

Original Abstract

Original Abstract

We introduce a new language representation model called BERT, which stands for Bidirectional Encoder

Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train

deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the

pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art

models for a wide range of tasks, such as question answering and language inference, without substantial task-

specific architecture modifications.

BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural

language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement),

MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2

(1.5% absolute improvement), outperforming human performance by 2.0%.

Our Summary

Our Summary

A Google AI team presents a new cutting-edge model for Natural Language Processing (NLP) – BERT

BERT, or

BBidirectional EEncoder RRepresentations from TTransformers. Its design allows the model to consider the context from

both the left and the right sides of each word. While being conceptually simple, BERT obtains new state-of-the-art

results on eleven NLP tasks, including question answering, named entity recognition and other tasks related to

general language understanding.




What’s the core idea of this paper?

What’s the core idea of this paper?

Training a deep bidirectional model by randomly masking a percentage of input tokens – thus, avoiding

cycles where words can indirectly “see themselves”.

Also pre-training a sentence relationship model by building a simple binary classification task to predict

whether sentence B immediately follows sentence A, thus allowing BERT to better understand relationships

between sentences.

Training a very big model (24 Transformer blocks, 1024-hidden, 340M parameters) with lots of data (3.3

billion word corpus).

What’s the key achievement?

What’s the key achievement?

Advancing the state-of-the-art for 11 NLP tasks, including:

getting a GLUE score of 80.4%, which is 7.6% of absolute improvement from the previous best result;

achieving 93.2% accuracy on SQuAD 1.1 and outperforming human performance by 2%.

Suggesting a pre-trained model, which doesn’t require any substantial architecture modifications to be

applied to specific NLP tasks.

What does the AI community think?

What does the AI community think?

BERT model marks a new era of NLP.

In a nutshell, two unsupervised tasks together (“fill in the blank” and “does sentence B comes after sentence

A?” ) provide great results for many NLP tasks.

Pre-training of language models becomes a new standard.

What are future research areas?

What are future research areas?

Testing the method on a wider range of tasks.

Investigating the linguistic phenomena that may or may not be captured by BERT.

What are possible business applications?

What are possible business applications?

BERT may assist businesses with a wide range of NLP problems, including:


chatbots for better customer experience;

analysis of customer reviews;

the search for relevant information, etc.

Where can you get implementation code?

Where can you get implementation code?

Google Research has released an official Github repository with Tensorflow code and pre-trained models for

BERT.

PyTorch implementation of BERT is also available on GitHub.

2. 2. Language Models Are Unsupervised Multitask Learners, by Alec

, by Alec

Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya

Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya

Sutskever

Sutskever

Original Abstract

Original Abstract

Natural language processing tasks, such as question answering, machine translation, reading comprehension, and

summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that

language models begin to learn these tasks without any explicit supervision when trained on a new dataset of

millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by

the language model reach 55 F1 on the CoQA dataset – matching or exceeding the performance of 3 out of 4

baseline systems without using the 127,000+ training examples. The capacity of the language model is essential

to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across

tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8

tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model

reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path

towards building language processing systems which learn to perform tasks from their naturally occurring

demonstrations.

Our Summary

Our Summary

In this paper, the OpenAI team demonstrates that pre-trained language models can be used to solve downstream

tasks without any parameter or architecture modifications. They have trained a very big model, a 1.5B-parameter

Transformer, on a large and diverse dataset that contains text scraped from 45 million webpages. The model

generates coherent paragraphs of text and achieves promising, competitive or state-of-the-art results on a wide

variety of tasks.

What’s the core idea of this paper?

What’s the core idea of this paper?

Training the language model on the large and diverse dataset:

selecting webpages that have been curated/filtered by humans;

cleaning and de-duplicating the texts, and removing all Wikipedia documents to minimize overlapping

of training and test sets;

using the resulting WebText dataset with slightly over 8 million documents for a total of 40 GB of text.

Using a byte-level version of Byte Pair Encoding (BPE) for input representation.

Building a very big Transformer-based model, GPT-2

GPT-2:

the largest model includes 1542M parameters and 48 layers;

the model mainly follows the OpenAI GPT model with few modifications (i.e., expanding vocabulary and

context size, modifying initialization etc.).

What’s the key achievement?

What’s the key achievement?


Getting state-of-the-art results on 7 out of 8 tested language modeling datasets.

Showing quite promising results in commonsense reasoning, question answering, reading comprehension,

and translation.

Generating coherent texts, for example, a news article about the discovery of talking unicorns.

What does the AI community think?

What does the AI community think?

“The researchers built an interesting dataset, applying now-standard tools and yielding an impressive model.”

– Zachary C. Lipton, an assistant professor at Carnegie Mellon University.

What are future research areas?

What are future research areas?

Investigating fine-tuning on benchmarks such as decaNLP and GLUE to see whether the huge dataset and

capacity of GPT-2 can overcome the inefficiencies of BERT’s unidirectional representations.

What are possible business applications?

What are possible business applications?

In terms of practical applications, the performance of the GPT-2 model without any fine-tuning is far from

usable but it shows a very promising research direction.

Where can you get implementation code?

Where can you get implementation code?

Initially, OpenAI decided to release only a smaller version of GPT-2 with 117M parameters. The decision not

to release larger models was taken “due to concerns about large language models being used to generate

deceptive, biased, or abusive language at scale”.

In November, OpenAI finally released its largest 1.5B-parameter model. The code is available here.

Hugging Face has introduced a PyTorch implementation of the initially released GPT-2 model.

3. 3. XLNet: Generalized Autoregressive Pretraining for Language

Understanding, by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime

, by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime

Carbonell, Ruslan Salakhutdinov, Quoc V. Le

Carbonell, Ruslan Salakhutdinov, Quoc V. Le

Original Abstract

Original Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT

achieves better performance than pretraining approaches based on autoregressive language modeling. However,

relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers

from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized

autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected

likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its

autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art

autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin,

and achieves state-of-the-art results on 18 tasks including question answering, natural language inference,

sentiment analysis, and document ranking.

Our Summary

Our Summary

The researchers from Carnegie Mellon University and Google have developed a new model, XLNet, for natural

language processing (NLP) tasks such as reading comprehension, text classification, sentiment analysis, and

others. XLNet is a generalized autoregressive pretraining method that leverages the best of both autoregressive

language modeling (e.g., Transformer-XL) and autoencoding (e.g., BERT) while avoiding their limitations. The

experiments demonstrate that the new model outperforms both BERT and Transformer-XL and achieves state-of-


the-art performance on 18 NLP tasks.



What’s the core idea of this paper?

What’s the core idea of this paper?

XLNet combines the bidirectional capability of BERT

bidirectional capability of BERT with the autoregressive technology of

autoregressive technology of

Transformer-XL

Transformer-XL:

Like BERT, XLNet uses a bidirectional context, which means it looks at the words before and after a

given token to predict what it should be. To this end, XLNet maximizes the expected log-likelihood of a

sequence with respect to all possible permutations

all possible permutations of the factorization order.

As an autoregressive language model, XLNet doesn’t rely on data corruption, and thus avoids BERT’s

limitations due to masking – i.e., pretrain-finetune discrepancy and the assumption that unmasked

tokens are independent of each other.

To further improve architectural designs for pretraining, XLNet integrates the segment recurrence mechanism

and relative encoding scheme of Transformer-XL.

What’s the key achievement?

What’s the key achievement?

XLnet outperforms BERT on 20 tasks, often by a large margin.

The new model achieves state-of-the-art performance on 18 NLP tasks including question answering, natural

language inference, sentiment analysis, and document ranking.

What does the AI community think?

What does the AI community think?

The paper was accepted for oral presentation at NeurIPS 2019, the leading conference in artificial intelligence.

“The king is dead. Long live the king. BERT’s reign might be coming to an end. XLNet, a new model by people

from CMU and Google outperforms BERT on 20 tasks.” – Sebastian Ruder, a research scientist at Deepmind.

“XLNet will probably be an important tool for any NLP practitioner for a while…[it is] the latest cutting-edge

technique in NLP.” – Keita Kurita, Carnegie Mellon University.


What are future research areas?

What are future research areas?

Extending XLNet to new areas, such as computer vision and reinforcement learning.

What are possible business applications?

What are possible business applications?

XLNet may assist businesses with a wide range of NLP problems, including:

chatbots for first-line customer support or answering product inquiries;

sentiment analysis for gauging brand awareness and perception based on customer reviews and social

media;

the search for relevant information in document bases or online, etc.

Where can you get implementation code?

Where can you get implementation code?

The authors have released the official Tensorflow implementation of XLNet.

PyTorch implementation of the model is also available on GitHub.

4. 4. RoBERTa: A Robustly Optimized BERT Pretraining Approach, by

, by

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi

Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov

Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov

Original Abstract

Original Abstract

Language model pretraining has led to significant performance gains but careful comparison between different

approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes,

and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication

study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters

and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance

of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD.

These results highlight the importance of previously overlooked design choices, and raise questions about the

source of recently reported improvements. We release our models and code.

Our Summary

Our Summary

Natural language processing models have made significant advances thanks to the introduction of pretraining

methods, but the computational expense of training has made replication and fine-tuning parameters difficult. In

this study, Facebook AI and the University of Washington researchers analyzed the training of Google’s

Bidirectional Encoder Representations from Transformers (BERT) model and identified several changes to the

training procedure that enhance its performance. Specifically, the researchers used a new, larger dataset for

training, trained the model over far more iterations, and removed the next sequence prediction training objective.

The resulting optimized model, RoBERTa (Robustly Optimized BERT Approach), matched the scores of the recently

introduced XLNet model on the GLUE benchmark.

What’s the core idea of this paper?

What’s the core idea of this paper?

The Facebook AI research team found that BERT was significantly undertrained and suggested an improved

recipe for its training, called RoBERTa:

More data: 160GB of text instead of the 16GB dataset originally used to train BERT.

Longer training: increasing the number of iterations from 100K to 300K and then further to 500K.

Larger batches: 8K instead of 256 in the original BERT base model.

Larger byte-level BPE vocabulary with 50K subword units instead of character-level BPE vocabulary of

size 30K.


Removing the next sequence prediction objective from the training procedure.

Dynamically changing the masking pattern applied to the training data.

What’s the key achievement?

What’s the key achievement?

RoBERTa outperforms BERT in all individual tasks on the General Language Understanding Evaluation

(GLUE) benchmark.

The new model matches the recently introduced XLNet model on the GLUE benchmark and sets a new state

of the art in four out of nine individual tasks.

What are future research areas?

What are future research areas?

Incorporating more sophisticated multi-task finetuning procedures.

What are possible business applications?

What are possible business applications?

Big pretrained language frameworks like RoBERTa can be leveraged in the business setting for a wide range

of downstream tasks, including dialogue systems, question answering, document classification, etc.

Where can you get implementation code?

Where can you get implementation code?

The models and code used in this study are available on GitHub.

5. 5. ALBERT: A Lite BERT for Self-supervised Learning of Language

Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman,

, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman,

Kevin Gimpel, Piyush Sharma, Radu Soricut

Kevin Gimpel, Piyush Sharma, Radu Soricut

Original Abstract

Original Abstract

Increasing model size when pretraining natural language representations often results in improved performance on

downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory

limitations, longer training times, and unexpected model degradation. To address these problems, we present two

parameter-reduction techniques to lower memory consumption and increase the training speed of BERT.

Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better

compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence

coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best

model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer

parameters compared to BERT-large.

Our Summary

Our Summary

The Google Research team addresses the problem of the continuously growing size of the pretrained language

models, which results in memory limitations, longer training time, and sometimes unexpectedly degraded

performance. Specifically, they introduce A Lite BERT (ALBERT)

A Lite BERT (ALBERT) architecture that incorporates two parameter-

reduction techniques: factorized embedding parameterization

factorized embedding parameterization and cross-layer parameter sharing

cross-layer parameter sharing. In

addition, the suggested approach includes a self-supervised loss for sentence-order prediction

sentence-order prediction to improve inter-

sentence coherence. The experiments demonstrate that the best version of ALBERT sets new state-of-the-art

results on GLUE, RACE, and SQuAD benchmarks while having fewer parameters than BERT-large.

What’s the core idea of this paper?

What’s the core idea of this paper?

It is not reasonable to further improve language models by making them larger because of memory

limitations of available hardware, longer training times, and unexpected degradation of model performance


with the increased number of parameters.

To address this problem, the researchers introduce the ALBERT

ALBERT architecture that incorporates two

parameter-reduction techniques:

factorized embedding parameterization

factorized embedding parameterization, where the size of the hidden layers is separated from the

size of vocabulary embeddings by decomposing the large vocabulary-embedding matrix into two small

matrices;

cross-layer parameter sharing

cross-layer parameter sharing to prevent the number of parameters from growing with the depth of

the network.

The performance of ALBERT is further improved by introducing the self-supervised loss for sentence-order

sentence-order

prediction

prediction to address BERT’s limitations with regard to inter-sentence coherence.

What’s the key achievement?

What’s the key achievement?

With the introduced parameter-reduction techniques, the ALBERT configuration with 18× fewer parameters

and 1.7× faster training compared to the original BERT-large model achieves only slightly worse performance.

The much larger ALBERT configuration, which still has fewer parameters than BERT-large, outperforms all of

the current state-of-the-art language modes by getting:

89.4% accuracy on the RACE benchmark;

89.4 score on the GLUE benchmark; and

An F1 score of 92.2 on the SQuAD 2.0 benchmark.

What does the AI community think?

What does the AI community think?

The paper has been submitted to ICLR 2020 and is available on the OpenReview forum, where you can see

the reviews and comments of NLP experts. The reviewers are mainly very appreciative of the presented paper.

What are future research areas?

What are future research areas?

Speeding up training and inference through methods like sparse attention and block attention.

Further improving the model performance through hard example mining, more efficient model training, and

other approaches.

What are possible business applications?

What are possible business applications?

The ALBERT language model can be leveraged in the business setting to improve performance on a wide

range of downstream tasks, including chatbot performance, sentiment analysis, document mining, and text

classification.

Where can you get implementation code?

Where can you get implementation code?

The original implementation of ALBERT is available on GitHub.

A TensorFlow implementation of ALBERT is also available here.

A PyTorch implementation of ALBERT can be found here and here.

6. 6. Exploring the Limits of Transfer Learning with a Unified Text-to-Text

Transformer, by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine

, by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine

Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu

Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu

Original Abstract

Original Abstract

Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream

task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer


learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the

landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language

problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled

datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the

insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art

results on many benchmarks covering summarization, question answering, text classification, and more. To

facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.

Our Summary

Our Summary

The Google research team suggests a unified approach to transfer learning in NLP with the goal to set a new state

of the art in the field. To this end, they propose treating each NLP problem as a “text-to-text” problem. Such a

framework allows using the same model, objective, training procedure, and decoding process for different tasks,

including summarization, sentiment analysis, question answering, and machine translation. The researchers call

their model a Text-to-Text Transfer Transformer (T5)

Text-to-Text Transfer Transformer (T5) and train it on the large corpus of web-scraped data to

get state-of-the-art results on a number of NLP tasks.



What’s the core idea of this paper?

What’s the core idea of this paper?

The paper has several important contributions:

Providing a comprehensive perspective on where the NLP field stands by exploring and comparing

existing techniques.

Introducing a new approach to transfer learning in NLP by suggesting treating every NLP problem as a

text-to-text

text-to-text task:

The model understands which tasks should be performed thanks to the task-specific prefix added

to the original input sentence (e.g., “translate English to German:”, “summarize:”).

Presenting and releasing a new dataset consisting of hundreds of gigabytes of clean web-scraped

English text, the Colossal Clean Crawled Corpus (C4)

Colossal Clean Crawled Corpus (C4).

Training a large (up to 11B parameters) model, called Text-to-Text Transfer Transformer (T5)

Text-to-Text Transfer Transformer (T5) on

the C4 dataset.

What’s the key achievement?

What’s the key achievement?

The T5 model with 11 billion parameters achieved state-of-the-art performance on 17 out of 24 tasks

considered, including:

a GLUE score of 89.7 with substantially improved performance on CoLA, RTE, and WNLI tasks;

an Exact Match score of 90.06 on the SQuAD dataset;

a SuperGLUE score of 88.9, which is a very significant improvement over the previous state-of-the-art

result (84.6) and very close to human performance (89.8);

a ROUGE-2-F score of 21.55 on the CNN/Daily Mail abstractive summarization task.


What are future research areas?

What are future research areas?

Researching the methods to achieve stronger performance with cheaper models.

Exploring more efficient knowledge extraction techniques.

Further investigating the language-agnostic models.

What are possible business applications?

What are possible business applications?

Even though the introduced model has billions of parameters and can be too heavy to be applied in the

business setting, the presented ideas can be used to improve the performance on different NLP tasks,

including summarization, question answering, and sentiment analysis.

Where can you get implementation code?

Where can you get implementation code?

The pretrained models together with the dataset and code are released on GitHub.

7. 7. Language Models are Few-Shot Learners, by Tom B. Brown, Benjamin

, by Tom B. Brown, Benjamin

Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,

Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,

Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,

Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,

Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,

Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,

Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens

Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens

Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott

Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott

Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,

Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,

Alec Radford, Ilya Sutskever, Dario Amodei

Alec Radford, Ilya Sutskever, Dario Amodei

Original Abstract

Original Abstract

Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large

corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method

still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast,

humans can generally perform a new language task from only a few examples or from simple instructions –

something which current NLP systems still largely struggle to do. Here we show that scaling up language models

greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-

of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion

parameters, 10× more than any previous non-sparse language model, and test its performance in the few-shot

setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot

demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many

NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-

the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or

performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning

still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web

corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty

distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3

in general.

Our Summary

Our Summary

The OpenAI research team draws attention to the fact that the need for a labeled dataset for every new language

task limits the applicability of language models. Considering that there is a wide range of possible tasks and it’s

often difficult to collect a large labeled training dataset, the researchers suggest an alternative solution, which is

scaling up language models to improve task-agnostic few-shot performance. They test their solution by training a


175B-parameter autoregressive language model, called GPT-3

GPT-3, and evaluating its performance on over two dozen

NLP tasks. The evaluation under few-shot learning, one-shot learning, and zero-shot learning demonstrates that

GPT-3 achieves promising results and even occasionally outperforms the state of the art achieved by fine-tuned

models.



What’s the core idea of this paper?

What’s the core idea of this paper?

The GPT-3

GPT-3 model uses the same model and architecture as GPT-2, including the modified initialization, pre-

normalization, and reversible tokenization.

However, in contrast to GPT-2, it uses alternating dense and locally banded sparse attention patterns in the

layers of the transformer, as in the Sparse Transformer.

The model is evaluated in three different settings:

Few-shot learning

Few-shot learning, when the model is given a few demonstrations of the task (typically, 10 to 100) at

inference time but with no weight updates allowed.

One-shot learning

One-shot learning, when only one demonstration is allowed, together with a natural language

description of the task.

Zero-shot learning

Zero-shot learning, when no demonstrations are allowed and the model has access only to a natural

language description of the task.

What’s the key achievement?

What’s the key achievement?

The GPT-3 model without fine-tuning achieves promising results on a number of NLP tasks, and even

occasionally surpasses state-of-the-art models that were fine-tuned for that specific task:

On the CoQA

CoQA benchmark, 81.5 F1 in the zero-shot setting, 84.0 F1 in the one-shot setting, and 85.0 F1

in the few-shot setting, compared to the 90.7 F1 score achieved by fine-tuned SOTA.

On the TriviaQA

TriviaQA benchmark, 64.3% accuracy in the zero-shot setting, 68.0% in the one-shot setting,

and 71.2% in the few-shot setting, surpassing the state of the art (68%) by 3.2%.

On the LAMBADA

LAMBADA dataset, 76.2 % accuracy in the zero-shot setting, 72.5% in the one-shot setting, and

86.4% in the few-shot setting, surpassing the state of the art (68%) by 18%.

The news articles generated by the 175B-parameter GPT-3 model are hard to distinguish from real ones,

according to human evaluations (with accuracy barely above the chance level at ~52%).

What are future research areas?

What are future research areas?


Improving pre-training sample efficiency.

Exploring how few-shot learning works.

Distillation of large models down to a manageable size for real-world applications.

What does the AI community think?

What does the AI community think?

“The GPT-3 hype is way too much. It’s impressive (thanks for the nice compliments!) but it still has serious

weaknesses and sometimes makes very silly mistakes. AI is going to change the world, but GPT-3 is just a

very early glimpse. We have a lot still to figure out.” – Sam Altman, CEO and co-founder of OpenAI.

“I’m shocked how hard it is to generate text about Muslims from GPT-3 that has nothing to do with violence…

or being killed…” – Abubakar Abid, CEO and founder of Gradio.

“No. GPT-3 fundamentally does not understand the world that it talks about. Increasing corpus further will

allow it to generate a more credible pastiche but not fix its fundamental lack of comprehension of the world. 

Demos of GPT-4 will still require human cherry picking.” – Gary Marcus, CEO and founder of Robust.ai.

“Extrapolating the spectacular performance of GPT3 into the future suggests that the answer to life, the

universe and everything is just 4.398 trillion parameters.” – Geoffrey Hinton, Turing Award winner.

What are possible business applications?

What are possible business applications?

The model with 175B parameters is hard to apply to real business problems due to its impractical resource

requirements, but if the researchers manage to distill this model down to a workable size, it could be applied

to a wide range of language tasks, including question answering and ad copy generation.

Where can you get implementation code?

Where can you get implementation code?

The code itself is not available, but some dataset statistics together with unconditional, unfiltered 2048-token

samples from GPT-3 are released on GitHub.

8. 8. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than

Generators, by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher

, by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher

D. Manning

D. Manning

Original Abstract

Original Abstract

Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens

with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when

transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an

alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of

masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a

small generator network. Then, instead of training a model that predicts the original identities of the corrupted

tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a

generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM

because the task is defined over all input tokens rather than just the small subset that was masked out. As a result,

the contextual representations learned by our approach substantially outperform the ones learned by BERT given

the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a

model on one GPU for 4 days that outperforms GPT (trained using 30× more compute) on the GLUE natural

language understanding benchmark. Our approach also works well at scale, where it performs comparably to

RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same

amount of compute.

Our Summary

Our Summary


The pre-training task for popular language models like BERT and XLNet involves masking a small subset of

unlabeled input and then training the network to recover this original input. Even though it works quite well, this

approach is not particularly data-efficient as it learns from only a small fraction of tokens (typically ~15%). As an

alternative, the researchers from Stanford University and Google Brain propose a new pre-training task called

replaced token detection

replaced token detection. Instead of masking, they suggest replacing some tokens with plausible alternatives

generated by a small language model. Then, the pre-trained discriminator is used to predict whether each token is

an original or a replacement. As a result, the model learns from all input tokens instead of the small masked

fraction, making it much more computationally efficient. The experiments confirm that the introduced approach

leads to significantly faster training and higher accuracy on downstream NLP tasks.



What’s the core idea of this paper?

What’s the core idea of this paper?

Pre-training methods that are based on masked language modeling are computationally inefficient as they

use only a small fraction of tokens for learning.

Researchers propose a new pre-training task called replaced token detection

replaced token detection, where:

some tokens are replaced by samples from a small generator network;

a model is pre-trained as a discriminator to distinguish between original and replaced tokens.

The introduced approach, called ELECTRA

ELECTRA (EEfficiently LLearning an EEncoder that CClassifies TToken

RReplacements AAccurately):

enables the model to learn from all input tokens instead of the small masked-out subset;

is not adversarial, despite the similarity to GAN, as the generator producing tokens for replacement is

trained with maximum likelihood.

What’s the key achievement?

What’s the key achievement?

Demonstrating that the discriminative task of distinguishing between real data and challenging negative

samples is more efficient than existing generative methods for language representation learning.

Introducing a model that substantially outperforms state-of-the-art approaches while requiring less pre-

training compute:

ELECTRA-Small gets a GLUE score of 79.9 and outperforms a comparably small BERT model with a

score of 75.1 and a much larger GPT model with a score of 78.8.

An ELECTRA model that performs comparably to XLNet and RoBERTa uses only 25% of their pre-

training compute.

ELECTRA-Large outscores the alternative state-of-the-art models on the GLUE and SQuAD benchmarks

while still requiring less pre-training compute.

What does the AI community think?

What does the AI community think?

The paper was selected for presentation at ICLR 2020, the leading conference in deep learning.

What are possible business applications?

What are possible business applications?

Because of its computational efficiency, the ELECTRA approach can make the application of pre-trained text

encoders more accessible to business practitioners.


Where can you get implementation code?

Where can you get implementation code?

The original TensorFlow implementation and pre-trained weights are released on GitHub.

9. 9. DeBERTa: Decoding-enhanced BERT with Disentangled Attention, by

, by

Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen

Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen

Original Abstract 

Original Abstract 

Recent progress in pre-trained neural language models has significantly improved the performance of many

natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-

enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel

techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors

that encode its content and position, respectively, and the attention weights among words are computed using

disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is

used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In

addition, a new virtual adversarial training method is used for fine-tuning to improve models’ generalization. We

show that these techniques significantly improve the efficiency of model pre-training and the performance of both

natural language understanding (NLU) and natural language generation (NLG) downstream tasks. Compared to

RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range

of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs.

90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that

consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single

DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first

time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the

SuperGLUE leaderboard as of January 6, 2021, outperforming the human baseline by a decent margin (90.3 versus

89.8).

Our Summary 

Our Summary 

The authors from Microsoft Research propose DeBERTa, with two main improvements over BERT, namely

disentangled attention

disentangled attention and an enhanced mask decoder

enhanced mask decoder. DeBERTa has two vectors representing a token/word

by encoding content and relative position respectively. The self-attention mechanism in DeBERTa processes self-

attention of content-to-content, content-to-position, and also position-to-content, while the self-attention in BERT is

equivalent to only having the first two components. The authors hypothesize that position-to-content self-attention

is also needed to comprehensively model relative positions in a sequence of tokens. Furthermore, DeBERTa is

equipped with an enhanced mask decoder, where the absolute position of the token/word is also given to the

decoder along with the relative information. A single scaled-up variant of DeBERTa surpasses the human baseline

on the SuperGLUE benchmark for the first time. The ensemble DeBERTa is the top-performing method on

SuperGLUE at the time of this publication.

What’s the core idea of this paper?

What’s the core idea of this paper?

Disentangled attention

Disentangled attention: In the original BERT, the content embedding and position embedding are added

before self-attention and the self-attention is applied only on the output of content and position vectors. The

authors hypothesize that this only accounts for content-to-content self-attention and content-to-position self-

attention and that we need position-to-content self-attention as well to model position information

completely. DeBERTa has two separate vectors representing content and position and self-attention is

calculated between all possible pairs, i.e., content-to-content, content-to-position, position-to-content, and

position-to-position. Position-to-position self-attention is trivially 1 all the time and has no information, so it

is not computed. 


Enhanced mask decoder

Enhanced mask decoder: The authors hypothesize that the model needs absolute position information to

understand syntactical nuances such as subject-object characterization. So, DeBERTa is provided with

absolute position information along with relative position information. The absolute position embedding is

provided to the last decoder layer just before the softmax layer, which gives the output.



Scale-invariant fine-tuning

Scale-invariant fine-tuning: A virtual adversarial training algorithm called scale-invariant fine-tuning is

used as a regularization method to increase generalization. The word embeddings are perturbed to a small

extent and trained to produce the same output as they would on non-perturbed word embeddings. The word

embedding vectors are normalized to stochastic vectors (where the sum of the elements in a vector is 1) to be

invariant to the number of parameters in the model. 

What’s the key achievement?

What’s the key achievement?

Compared to the current state-of-the-art method RoBERTa-Large, the DeBERTA model trained on half the

training data achieves:

an improvement of +0.9% in accuracy on MNLI (91.1% vs. 90.2%),

an improvement of +2.3% in accuracy on SQuAD v2.0 (90.7% vs. 88.4%),

an improvement of +3.6% in accuracy on RACE (86.8% vs. 83.2%)

A single scaled-up variant of DeBERTa surpasses the human baseline on the SuperGLUE benchmark for the

first time (89.9 vs. 89.8). The ensemble DeBERTa is the top-performing method on SuperGLUE at the time of

this publication, outperforming the human baseline by a decent margin (90.3 versus 89.8).

What does the AI community think?

What does the AI community think?

The paper has been accepted to ICLR 2021, one of the key conferences in deep learning.

What are future research areas?

What are future research areas?

Improving pretraining by introducing other useful information, in addition to positions, with the Enhanced

Mask Decoder (EMD) framework.

A more comprehensive study of scale-invariant fine-tuning (SiFT).


What are possible business applications?

What are possible business applications?

The contextual representations of pretrained language modeling could be used in search, question

answering, summarization, virtual assistants, and chatbots, among other tasks.

Where can you get implementation code?

Where can you get implementation code?

The implementation of DeBERTa is available on GitHub.

10. 

10. PaLM: Scaling Language Modeling with Pathways, by Aakanksha

, by Aakanksha

Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav

Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav

Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,

Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,

Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko,

Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko,

Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer,

Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer,

Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner

Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner

Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,

Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,

Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa

Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa

Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,

Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,

Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,

Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,

Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani

Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani

Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan

Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan

Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,

Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,

Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi

Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi

Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason

Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason

Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah

Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah

Fiedel

Fiedel

Original Abstract

Original Abstract

Large language models have been shown to achieve remarkable performance across a variety of natural language

tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to

adapt the model to a particular application. To further our understanding of the impact of scale on few-shot

learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call

Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system

which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by

achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation

benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the

finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance

on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous

improvements from model scale, meaning that performance steeply increased as we scaled to our largest model.

PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a

wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the

extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations

related to large language models and discuss potential mitigation strategies.

Our Summary

Our Summary

The Google Research team contributed a lot in the area of pre-trained language models with their BERT, ALBERT,

and T5 models. One of their latest contributions is the Pathways Language Model (PaLM), a 540-billion parameter,

dense decoder-only Transformer model trained with the Pathways system. The goal of the Pathways system is to


orchestrate distributed computation for accelerators. With its help, the team was able to efficiently train a single

model across multiple TPU v4 Pods. The experiments on hundreds of language understanding and generation

tasks demonstrated that PaLM achieves state-of-the-art few-shot performance across most tasks with

breakthrough capabilities demonstrated in language understanding, language generation, reasoning, and code-

related tasks.



What’s the core idea of this paper?

What’s the core idea of this paper?

The main idea of the paper is to scale training of a 540-billion parameter language model with the Pathways

system:

The team was using data parallelism at the Pod level across two Cloud TPU v4 Pods while using

standard data and model parallelism within each Pod.

They were able to scale training to 6144 TPU v4 chips, the largest TPU-based system configuration

used for training to date.

The model achieved a training efficiency of 57.8% hardware FLOPs utilization, which, as the authors

claim, is the highest yet achieved training efficiency for large language models at this scale. 

The training data for the PaLM model included a combination of English and multilingual datasets

containing high-quality web documents, books, Wikipedia, conversations, and GitHub code.

What’s the key achievement?

What’s the key achievement?

Numerous experiments demonstrate that model performance steeply increased as the team scaled to their

largest model.

PaLM 540B achieved breakthrough performance on multiple very difficult tasks:

Language understanding and generation

Language understanding and generation. The introduced model surpassed the few-shot

performance of prior large models on 28 out of 29 tasks that include question-answering tasks, cloze

and sentence-completion tasks, in-context reading comprehension tasks, common-sense reasoning

tasks, SuperGLUE tasks, and more. PaLM’s performance on BIG-bench tasks showed that it can

distinguish cause and effect, as well as understand conceptual combinations in appropriate contexts.

Reasoning

Reasoning. With 8-shot prompting, PaLM solves 58% of the problems in GSM8K, a benchmark of

thousands of challenging grade school level math questions, outperforming the prior top score of 55%

achieved by fine-tuning the GPT-3 175B model. PaLM also demonstrates the ability to generate explicit

explanations in situations that require a complex combination of multi-step logical inference, world

knowledge, and deep language understanding.

Code generation

Code generation. PaLM performs on par with the fine-tuned Codex 12B while using 50 times less

Python code for training, confirming that large language models transfer learning from both other

programming languages and natural language data more effectively.




What are future research areas?

What are future research areas?

Combining the scaling capabilities of the Pathways system with novel architectural choices and training

schemes.

What are possible business applications?

What are possible business applications?

Similarly to other recently introduced pre-trained language models, PaLM can be applied in a wide range of

downstream tasks, including conversational AI, question answering, machine translation, document

classification, ad copy generation, code bug fixing, and more.

Where can you get implementation code?

Where can you get implementation code?

So far, there was no official code implementation release for PaLM but it actually uses a standard

Transformer model architecture, with some customizations. 

Pytorch implementation of the specific Transformer architecture from PaLM can be accessed on GitHub.

If you like these research summaries, you might be also interested in the following articles:

If you like these research summaries, you might be also interested in the following articles:

2020’s Top AI &amp; Machine Learning Research Papers

GPT-3 &amp; Beyond: 10 NLP Research Papers You Should Read

The Latest Breakthroughs in Conversational AI Agents

What Every NLP Engineer Needs To Know About Pre-Trained Language Models

Enjoy this article? Sign up for more AI research updates.

Enjoy this article? Sign up for more AI research updates.

We’ll let you know when we release more summary articles like this one.

Email Address

Email Address **


Name

Name **

First

Last

Company

Company **

What areas of AI research are you interested in? Select all that apply

What areas of AI research are you interested in? Select all that apply **

What is your biggest challenge with AI research?

What is your biggest challenge with AI research? **

SUBMIT

SUBMIT

 Natural Language Processing (NLP)

 Chatbots &amp; Conversational AI

 Computer Vision

 Ethics &amp; Safety

 Robotics

 Machine Learning

 Deep Learning

 Reinforcement Learning

 Generative Models

 Other (Please Describe Below)

Related

Related






About Mariya Yao

Mariya is the co-author of Applied AI: A Handbook For Business Leaders and

former CTO at Metamaven. She "translates" arcane technical concepts into

actionable business advice for executives and designs lovable products

people actually want to use. Follow her on Twitter at @thinkmariya to raise

your AI IQ.

 

 

Comments

Comments

ahlam st says

July 23, 2020 at 3:54 pm

Thank you for this article post excellent.

Reply


















ferahtia_FS says

April 16, 2023 at 1:35 am

Merci pour ce partage

Reply

ahlam st says

September 1, 2020 at 4:25 am

Thank you so much

Reply

brahim d staps says

December 17, 2020 at 1:32 am

Very interesting post thank you for sharing

Reply

ferahtia_FS says

April 11, 2023 at 5:38 am

Fantastic post.

Thanks For Sharing such an informative article

Reply

Ellen says

May 29, 2021 at 1:19 pm

Clearly – the best way to present the topic.. Appreciated. Thank you!

Reply












soundos says

September 9, 2022 at 6:51 am

great post thank you for your valuable information

Reply

saber says

September 11, 2022 at 3:24 am

Often this website is a tour of all the websites you want about this and you don’t know who to ask. thank

you.

Reply

kamir bouchareb st says

September 22, 2022 at 2:37 am

ggggggggggggggggggggggggggggggggggg

Reply

saber says

September 28, 2022 at 2:39 am

thanks for share your valuable posting.

Reply

saber says

October 26, 2022 at 2:55 am

Thanks for all the information.

Reply












kamir bouchareb st says

November 7, 2022 at 3:08 am

ﻟﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺦ

Reply

amel fs says

November 7, 2022 at 3:49 am

Very informative post. Thanks for share it with us

Reply

amel snv says

November 7, 2022 at 3:52 am

Very informative post. Thanks.

Reply

amel snv says

November 7, 2022 at 3:57 am

Very informative post. Thanks ….

Reply

ferahtia_FS says

November 7, 2022 at 8:06 am

Merci pour ce partage et ce travail.

Merci pour cet article très complet.

Reply












ferahtia.FS says

November 7, 2022 at 8:07 am

Thank you very much.

Your article is very useful

Reply

amel fs says

November 9, 2022 at 3:34 am

Nice article I found it very helpful,thanks for the article I am very interested

Reply

amel snv says

November 9, 2022 at 3:34 am

Nice article I found it very helpful,thanks for the article.

Reply

kamir bouchareb st says

November 9, 2022 at 8:07 am

ﻟﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺦ

Reply

hadil says

November 10, 2022 at 5:50 am

very nice post well done thank you

Reply














hadil says

November 13, 2022 at 4:30 am

Un travail magnifique merci encore une fois

Reply

hadil says

November 13, 2022 at 4:48 am

great post thank you for your valuable information

Reply

amel fs says

November 13, 2022 at 4:52 am

Thank you so much for organizing this quality information

Reply

amel snv says

November 13, 2022 at 4:58 am

Thank you so much

Reply

kamir bouchareb st says

November 13, 2022 at 6:37 am

gooooooooooooooooooooooooood

Reply

amel fs says

November 14, 2022 at 3:49 am












Un travail magnifique merci encore une fois

Reply

djamila_st says

November 16, 2022 at 7:18 am

thank you for giving me wonderful information

Reply

amel fs says

November 17, 2022 at 3:30 am

Great blog.Amazing article.Very interesting post for me.

Thanks for sharing.

Reply

amel snv says

November 17, 2022 at 3:32 am

Great blog.Amazing article.Very interesting post for me.

Reply

djamila_st says

November 20, 2022 at 2:55 am

He’s in that general vicinity on my spreadsheet.

http://virtuelcampus.univ-msila.dz/factech

Reply

malika says

November 20, 2022 at 9:01 am












good

Reply

ferahtia.FS says

April 16, 2023 at 1:35 am

Very interesting post. This is my first time visit here. I found so many interesting stuff in your blog

especially its discussion. Thanks for the post!

Reply

malika says

November 20, 2022 at 9:04 am

thank you

Reply

malika says

November 20, 2022 at 9:07 am

great job

Reply

amel fs says

November 21, 2022 at 3:43 am

I love to reading this post. This is a very informative article. Thank you for sharing this!

Reply

amel snv says

November 21, 2022 at 3:45 am

I love to reading this post. This is a very informative article..












Reply

ferahtia.FS says

November 23, 2022 at 2:47 am

Thank You For Sharing the Post

Reply

amel fs says

November 24, 2022 at 4:28 am

Goood post. I absolutely love this website. Thanks!

Reply

djamila_st says

November 24, 2022 at 2:24 pm

This is wonderful For the article I read, thank you very much

Reply

djamila_st says

November 26, 2022 at 12:51 pm

Great post, adding workflows and checklists to our customer onboarding process and business

configurations are helping us reduce our setup time. Thanks, great article!

Reply

amel fs says

November 27, 2022 at 4:34 am

Really enjoyed this! Very well presented – thanks!

Reply












amel snv says

November 27, 2022 at 4:39 am

Really enjoyed this! Very well presented …

Reply

kamir bouchareb st says

November 27, 2022 at 8:15 am

ﻟﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺨﺦ

Reply

djamila_st says

November 27, 2022 at 1:12 pm

Great round up! Like you, Skyscanner is usually the first site I check and I also like Adioso.

Reply

malika says

November 28, 2022 at 2:29 am

i love this

Reply

malika says

November 28, 2022 at 2:29 am

goooooooood

Reply












djamila_st says

November 28, 2022 at 3:16 am

What a wonderful pen and this information contained in the article.

You write for us lines of creativity that impress everyone who looks at it..

Reply

amel fs says

November 28, 2022 at 3:21 am

very nice thank you

Reply

amel snv says

November 28, 2022 at 4:05 am

very nice thanks

Reply

djamila_st says

November 29, 2022 at 5:50 am

Hello Guys This Website Article Related Your Content

Reply

djamila_st says

November 30, 2022 at 1:59 pm

merci pour toutes ces informations

Reply














djamila_st says

December 1, 2022 at 2:33 am

Great post, adding workflows and checklists to our customer . Thanks, great article!

Reply

amel fs says

December 1, 2022 at 3:09 am

Goood post. I absolutely love this website. Thank you for sharing

Reply

amel snv says

December 1, 2022 at 3:19 am

Goood post. I absolutely love this website. ..

Reply

djamila_st says

December 1, 2022 at 12:48 pm

Thank you for your suggestion Preeti, we will try to explain in a more specific way on our future articles.

Reply

djamila_st says

December 3, 2022 at 1:09 pm

Clearly the best way to present the topic.. Appreciated. Thank you

Reply

djamila_st says

December 4, 2022 at 3:23 am












Your posts are very instructive, which I later learned a lot from. It was great I hope you get more new

posts!

Reply

djamila_st says

December 5, 2022 at 2:50 am

Very good web-site you’ve going here.thank you

Reply

ferahtia_FS says

December 5, 2022 at 4:02 am

thank you very nice website article

Reply

Hadil says

December 5, 2022 at 5:47 pm

Thanks

Reply

djamila_st says

December 6, 2022 at 4:28 am

these are good examples of game but i think u can explain more by adding examples on academic topic

http://virtuelcampus.univ-msila.dz/factech

Reply

djamila_st says

December 7, 2022 at 12:20 am












Your posts are very instructive, which I later learned a lot from. It was great I hope you get more new

posts! thank you

Reply

djamila_st says

December 8, 2022 at 2:41 am

It is very useful for writing and useful for reading.thank you

Reply

djamila_st says

December 10, 2022 at 1:14 pm

Thanks for sharing this information. I really like your blog post very much.

Reply

djamila_st says

December 11, 2022 at 3:18 am

thank you very nice website article

http://virtuelcampus.univ-msila.dz/factech/

Reply

djamila_st says

December 12, 2022 at 4:30 am

Your posts are very instructive, which I later learned a lot from.

Reply

ferahtia_FS says

December 12, 2022 at 8:43 am










Merci pour cet article très complet.

Reply

djamila_st says

December 13, 2022 at 3:50 am

Excellent blog post……….. I certainly appreciate this website.

Keep writing! thank you

Reply

djamila_st says

December 14, 2022 at 3:28 am

Thank you for your suggestion Preeti, we will try to explain in a more specific way on our future articles.

beautiful article

Reply

djamila_st says

December 15, 2022 at 2:31 am

Your posts are very instructive, which I later learned a lot from. It was great I hope you get more new

posts! I am one of your Permanent Visitors . thank you

Reply

djamila_st says

December 17, 2022 at 12:56 pm

Thank you for creating this blog. I am hoping this will be well-known to everyone so that it could help a

lot. good article

Reply














djamila_st says

December 18, 2022 at 2:37 am

thank you very nice website article. nice topic

Reply

hadil says

December 18, 2022 at 4:13 am

Thank you

Reply

hadil says

December 18, 2022 at 4:17 am

Thank you for sharing such an informative blog

Reply

djamila_st says

December 19, 2022 at 2:33 am

I appreciate encouraging posts like this. I think we all need a little encouragement, some days.thank you

Reply

djamila_st says

December 20, 2022 at 5:23 am

Thank you for this help full artical.continu

Reply

djamila_st says

December 21, 2022 at 6:05 am










Thank you very much! Games are so interesting! I try to use them at my lessons.

Reply

djamila_st says

December 22, 2022 at 3:40 am

Your posts are very instructive, which I later learned a lot from this article . thank you for this .

Reply

djamila_st says

December 24, 2022 at 1:31 pm

Your posts are very instructive, which I later learned a lot from. It was great I hope you get more new

posts! beautiful article

Reply

djamila_st says

December 25, 2022 at 4:04 am

Nous avions tout ce que nous voulions en tant que famille, sauf une bonne santé.

Reply

djamila_st says

December 26, 2022 at 4:41 am

these are good examples of game but i think u can explain more by adding examples on academic topic.

thank you

http://virtuelcampus.univ-msila.dz/factech/

Reply












djamila_st says

December 27, 2022 at 3:47 am

It is a very heart toughing poem as it is referring to those little children who don’t have anywhere to go

and are unable

Reply

djamila_st says

December 28, 2022 at 6:29 am

It is a very heart toughing poem as it is referring to those little children who don’t have anywhere to go

and are unable to eat because of their shabby circumstances.

Reply

djamila_st says

December 29, 2022 at 2:40 pm

Thanks for talking about this precious and fabulous facts right here. good

Reply

djamila_st says

December 31, 2022 at 2:50 pm

Thank you very much for writing such an interesting article on this topic. This has really made me think

and I hope to read more.good

Reply

djamila_st says

January 1, 2023 at 7:40 am

This article is an appealing wealth of informative data that is interesting and well-written. I commend

your hard work on this and thank you for this information.

Reply












djamila_st says

January 2, 2023 at 9:18 am

Good post but I was wondering if you could write a litte more on this subject? I’d be very thankful if you

could elaborate a little bit further. Appreciate it!

Reply

hadil says

January 3, 2023 at 4:18 am

very good well done thank you

Reply

djamila_st says

January 3, 2023 at 7:01 am

It is a very heart toughing poem as it is referring to those little children who don’t have anywhere to go

and are unable to eat because of their shabby circumstances. thank you

Reply

djamila_st says

January 4, 2023 at 2:47 pm

Thanks for your insight for your fantastic posting. I’m glad I have taken the time to see this.

Reply

djamila_st says

January 5, 2023 at 7:52 am

Excellent and very exciting site. Love to watch. Keep Rocking.

Reply












djamila_st says

January 7, 2023 at 7:50 am

Thanks for sharing the info, keep up the good work going…. I really enjoyed exploring your site. good

resource

Reply

djamila_st says

January 8, 2023 at 3:49 am

It is a very heart toughing poem as it is referring to those little children who don’t have anywhere to go

and are unable to eat because of their shabby circumstances. good thanks

Reply

djamila_st says

January 9, 2023 at 4:02 am

Thanks for taking the time to discuss this, I feel strongly that love and read more on this topic.

Reply

hadil says

January 9, 2023 at 5:24 am

Thank you Very nice

Reply

djamila_st says

January 10, 2023 at 12:56 pm

It is an excellent blog, I have ever seen. I found all the material on this blog utmost unique and well

written.

http://virtuelcampus.univ-msila.dz/factech

Reply












djamila_st says

January 11, 2023 at 4:13 am

Thanks for your valuable post, Really nice and very informative

http://virtuelcampus.univ-msila.dz/factech

Reply

djamila_st says

January 12, 2023 at 1:38 pm

Thanks for sharing the info, keep up the good work going…. I really enjoyed exploring your site. good

resource…

Reply

djamila_st says

January 14, 2023 at 8:23 am

We are really grateful for your blog post. You will find a lot of approaches after visiting your post. Great

work

Reply

djamila_st says

January 15, 2023 at 3:20 pm

Thanks for the blog loaded with so many information. Stopping by your blog helped me to get what I was

looking for.

Reply

djamila_st says

January 16, 2023 at 2:45 pm

This article is an appealing wealth of informative data that is interesting and well-written. I commend

your hard work on this and thank you for this information. You’ve got what it takes to get attention.

Reply












djamila_st says

January 17, 2023 at 4:51 am

Hello, I have browsed most of your posts. This post is probably where I got the most useful information

for my research. Thanks for posting, maybe we can see more on this. Are you aware of any other websites

on this subject

Reply

djamila_st says

January 18, 2023 at 2:45 pm

good post. I just stumbled upon your blog and wanted to say that I have really enjoyed reading your blog

posts. Any way I’ll be subscribing to your feed and I hope you post again soon. Big thanks for the useful

info.

Reply

djamila_st says

January 19, 2023 at 2:54 am

My comment is to give you a lots of thanks for this post, it helped me to find this topic and save my time.

Reply

djamila_st says

January 21, 2023 at 11:55 am

I am curiously much more interested in some other good informative topics.

Reply

ahmed staps says

January 22, 2023 at 8:26 am

This article is an appealing wealth of informative data that is interesting and well-written

Reply












ahmed staps says

January 22, 2023 at 8:27 am

Thanks for the blog loaded with so many information.

Reply

djamila_st says

January 23, 2023 at 2:33 pm

Continuing education for licensed professional engineers to complete the required Engineering PDH in

order to meet the state licensing board guidelines for license renewal.

Reply

djamila_st says

January 25, 2023 at 2:22 pm

I am worndering to find such an informative content.

Reply

djamila_st says

January 26, 2023 at 2:55 pm

This article is an appealing wealth of informative data that is interesting and well-written. I commend

your hard work on this and thank you for this information.good

Reply

Kashmir Packages says

January 27, 2023 at 3:45 am

Thank you ever so for you blog. Really looking forward to read more.

Reply












djamila_st says

January 29, 2023 at 2:24 pm

Im no expert, but I believe you just made an excellent point. You certainly fully understand what youre

speaking about, and I can truly get behind that.

Reply

hadil says

January 30, 2023 at 4:10 am

ﺷﻜﺮا

Reply

ahmed staps says

February 5, 2023 at 3:06 am

Thank

Reply

ahmed staps says

February 6, 2023 at 3:16 am

thank you for this information

Reply

ahmed staps says

February 7, 2023 at 4:16 am

Thank you for sharing excellent information.

Reply












ahmed staps says

February 8, 2023 at 4:35 am

Thankس

Reply

ahmed staps says

February 9, 2023 at 3:46 am

thank you thank you

Reply

Asdad says

February 9, 2023 at 6:30 am

Akademie ist die perfekte Plattform, um Ihre Masterarbeit lektorieren und lektorieren zu lassen

https://akademily.de/lektorat-korrekturlesen-masterarbeit/. Die Akademie bietet professionelle

Lektoratsdienste, die Rechtschreib-, Zeichensetzungs-, Grammatik-, Satzbau- und Tippfehler in Ihrer

Masterarbeit korrigieren. Darüber hinaus bietet Akademily auch Textformatierungen an, um

sicherzustellen, dass sie den Standards akademischer Institutionen entsprechen. Das Team erfahrener

Redakteure gewährleistet Genauigkeit und Konsistenz für ein ausgefeiltes Dokument, das Sie stolz als

Endprodukt einreichen können. Mit dem Lektorat und Korrektorat der Akademie können Sie sicher sein,

dass Ihre Masterarbeit vor der Abgabe perfektioniert ist.

Reply

ahmed staps says

February 12, 2023 at 3:37 am

ﺷﻜﺮا

Reply

djamila_st says

February 12, 2023 at 4:22 am

Thanks for your valuable post, Really nice and very informative

Reply












ahmed staps says

February 13, 2023 at 4:07 am

thanks for the last information

Reply

ahmed staps says

February 15, 2023 at 2:31 am

Useful information, thank you for posting

Reply

ahmed staps says

February 16, 2023 at 5:42 am

http://virtuelcampus.univ-msila.dz/inst-staps/

Reply

djamila_st says

February 15, 2023 at 2:59 pm

Positive site, where did u come up with the information on this posting?I have read a few of the articles on

your website now, and I really like your style. Thanks a million and please keep up the effective work.

Reply

ahmed staps says

February 16, 2023 at 5:41 am

Informations très utiles, merci pour la contribution

Reply












djamila_st says

February 16, 2023 at 5:43 am

This is a great inspiring article.I am pretty much pleased with your good work.You put really very helpful

information…

Reply

ahmed staps says

February 19, 2023 at 3:18 am

Very valuable information, thank you very much

Reply

djamila_st says

February 19, 2023 at 3:02 pm

I can’t imagine focusing long enough to research; much less write this kind of article. You’ve outdone

yourself with this material. This is great content.

Reply

ahmed staps says

February 20, 2023 at 8:56 am

Merci beaucoup pour les informations que vous nous fournissez

Reply

ferahtia_FS says

February 21, 2023 at 3:49 am

http://virtuelcampus.univ-msila.dz/facscience/

Reply














djamila_st says

February 21, 2023 at 2:38 pm

Good day.i am doing research right now and your blog really helped me

Reply

djamila_st says

February 22, 2023 at 3:08 am

Positive site, where did u come up with the information on this posting? I’m pleased I discovered it though,

ill be checking back soon to find out what additional posts you include.

Reply

ahmed staps says

February 22, 2023 at 6:02 am

Thank you very much for providing very useful information

Reply

ferahtia.FS says

April 10, 2023 at 5:49 pm

Merci beaucoup

Reply

ibrahim murat gündüz says

February 24, 2023 at 12:51 am

I would recommend my profile is important to me, I invite you to discuss this topic.

Reply

ferahtia.FS says

April 10, 2023 at 5:50 pm










Thanks so much

Reply

ahmed staps says

February 27, 2023 at 2:47 am

thanks

Reply

djamila_st says

February 27, 2023 at 4:49 am

Thanks for your valuable post, Really nice and very informative . good

Reply

djamila_st says

February 28, 2023 at 4:46 am

Wow! This could be one of the most useful blogs we have ever come across on thesubject. Actually

excellent info! I’m also an expert in this topic so I can understand your effort.good

http://virtuelcampus.univ-msila.dz/factech

Reply

mekki razika says

March 1, 2023 at 3:22 am

goooooooood

Reply












djamila_st says

March 1, 2023 at 4:01 am

This article is an appealing wealth of informative data that is interesting and well-written. I commend

your hard work on this and thank you for this information. good

http://virtuelcampus.univ-msila.dz/factech/

Reply

ahmed staps says

March 1, 2023 at 8:17 am

Thank you for the work you are doing. We wish you more brilliance. You may find something useful to

you. Visit our website

Reply

djamila_st says

March 2, 2023 at 3:02 am

It is a very heart toughing poem as it is referring to those little children who don’t have anywhere to go

and are unable to eat because of their shabby circumstances. thank’s

http://virtuelcampus.univ-msila.dz/factech

Reply

djamila_st says

March 4, 2023 at 3:02 pm

It is a very heart toughing poem as it is referring to those little children who don’t have anywhere to go

and are unable to eat because of their shabby circumstances. good

http://virtuelcampus.univ-msila.dz/factech

Reply

kamir bouchareb st says

March 5, 2023 at 2:53 am

GOOOOOOOOOOOOOOOOOD










Reply

djamila_st says

March 5, 2023 at 2:55 pm

Thanks for sharing the info, keep up the good work going…. I really enjoyed exploring your site. good

resource. continu..

http://virtuelcampus.univ-msila.dz/factech

Reply

djamila_st says

March 7, 2023 at 2:27 pm

It’s extremely pleasant and meanful. it’s extremely cool blog. Connecting is exceptionally valuable

thing.you have truly helped bunches of individuals who visit blog and give them usefull data.good

http://virtuelcampus.univ-msila.dz/factech

Reply

ahmed staps says

March 9, 2023 at 7:24 am

Good site, thank you for the information

Reply

NetSuite ERP Solution says

March 10, 2023 at 3:45 am

Thanks for sharing; this is a superb blog post. I’m actually looking forward to reading more. Amazing.

Reply












ahmed staps says

March 12, 2023 at 2:55 am

http:

Reply

djamila_st says

March 12, 2023 at 4:20 am

Good post but I was wondering if you could write a litte more on this subject? I’d be very thankful if you

could elaborate a little bit further.

Reply

Jacketoria says

March 21, 2023 at 5:41 am

Thanks for sharing this information to us. but i need more information like this please can you post more

of this.

Reply

ferahtia_FS says

March 27, 2023 at 3:54 am

facscience 

ferahtia-Very good combination of information, thank you

For Sharing Informative Post

Reply

Terrassenüberdachung says

March 31, 2023 at 8:07 am

Great website … will use this site again in the future � THANK YOU !!!! Greetings from Germany

Reply












ferahtia.FS says

April 2, 2023 at 5:21 am

facscience 

ferahtia-Very good combination of information, thank you

For Sharing Informative Post

Reply

ahmed staps says

April 4, 2023 at 3:48 am

Thank you very much for publishing this information. May you always be distinguished for what you are

working for

Reply

The Equalizer Daniel Siani Puffer Jacket says

April 7, 2023 at 2:17 am

Excellent article. Very interesting to read. I really love to read such a nice article. Thanks! keep rocking.

Reply

Küchenrückwand says

April 9, 2023 at 12:24 pm

Thanks for the great website

Reply

djamila_st says

April 10, 2023 at 5:55 am

I am worndering to find such an informative content. thank you

Reply












salima says

April 10, 2023 at 6:56 am

Informative and detailed! Your blogs are always so great!

Reply

ahmed staps says

April 11, 2023 at 5:47 am

Hello, thank you for the information

Reply

ferahtia_FS says

April 12, 2023 at 4:40 am

ferahtia 

ferahtia-Very good combination of information, thank you

For Sharing Informative Post

Reply

kamir bouchareb st says

April 13, 2023 at 3:55 am

goooood

Reply

Salima says

April 18, 2023 at 3:33 am

I really liked your blog.Really thank you! Much obliged

Reply














djamila_st says

April 23, 2023 at 4:16 am

I really enjoyed

Reply

kamir bouchareb st says

April 24, 2023 at 9:08 am

THQNK YOU

Reply

ahmed staps says

April 25, 2023 at 3:18 am

Thank you very much

Reply

djamila_st says

April 25, 2023 at 4:03 am

good

Reply

Salima says

April 25, 2023 at 5:56 am

Thanks for your valuable post, Really nice and very informative . good

Reply

amel fs says

April 26, 2023 at 3:43 am










Thanks for sharing your thoughts!

i love your idea

so usefull

Reply

amel snv says

April 26, 2023 at 3:49 am

Thanks for sharing your thoughts!

i love your idea

Reply

Trith Wal Di says

April 26, 2023 at 11:23 am

Thanks for the in-depth wonderful article you turned out here Enjoyed reading the article above and thank

you for sharing good knowledge and information it’s very helpful. I was searching for a travel blog and

found your blog site. I like your high-quality blog site design plus your posting abilities. Keep doing it

Reply

djamila_st says

April 27, 2023 at 4:24 am

Wonderful read. Very well-crafted blog post that really made my day. Please keep up good writing like

this.

Reply

djamila_st says

April 30, 2023 at 2:52 am

Really nice and informative post . thank you

Reply








amel fs says

April 30, 2023 at 3:32 am

Thanks for sharing such an amazing post with us.

very good

Great post!

Reply

amel snv says

April 30, 2023 at 3:37 am

Thanks for sharing such an amazing post with us.

very good

Reply

ahmed staps says

April 30, 2023 at 4:46 am

Thank you the information

Reply

Leave a Reply

Leave a Reply

Your email address will not be published. Required fields are marked *

Comment *





Name

Email

Website


POST COMMENT

Learn Applied AI

Learn Applied AI

We create and source the best content about applied artificial intelligence for business. Be the FIRST to

understand and apply technical breakthroughs to your enterprise.

Name

Name

First

Last

Company

Company

Email

Email

SUBMIT

SUBMIT

FOLLOW US

FOLLOW US

POPULAR ARTICLES

POPULAR ARTICLES

Top 6 NLP Language Models Transforming AI In 2023

Top 6 NLP Language Models Transforming AI In 2023

NeurIPS 2021 – 10 Papers You Shouldn’t Miss

NeurIPS 2021 – 10 Papers You Shouldn’t Miss

A Guide To Knowledge Graphs

A Guide To Knowledge Graphs

Search the site ...

�

�

�


Why Graph Theory Is Cooler Than You Thought

Why Graph Theory Is Cooler Than You Thought

BERT Inner Workings

BERT Inner Workings

Pretrain Transformers Models in PyTorch Using Hugging Face Transformers

Pretrain Transformers Models in PyTorch Using Hugging Face Transformers

More Articles

TOPICS

TOPICS

Bots

Brands

Business

China

Commerce

Computer Vision

Conversational AI

Customer Service

Cybersecurity

Data Science &amp; Engineering

Design

Education

Ethics &amp; Safety

Finance

Gaming

Healthcare

HR &amp; Recruiting

Infrastructure

Leadership &amp; Management

Manufacturing

Marketing

Natural Language Processing

Reinforcement Learning

Research

Retail &amp; CPG


Society

Technical Guide

Technology

ABOUT TOPBOTS

ABOUT TOPBOTS

Expert Contributors

Terms of Service &amp; Privacy Policy

Contact TOPBOTS

Copyright © 2023 TOPBOTS

Share This





