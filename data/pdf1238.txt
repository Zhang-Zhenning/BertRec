
Tweet

MIND &amp; BODY, RESEARCH, SCIENCE &amp; ENVIRONMENT

Will computers ever truly understand what we’re saying?

By Robert Sanders, Media relations | JANUARY 11, 2016

From Apple’s Siri to Honda’s robot Asimo, machines seem to be getting better and better at communicating with

humans.

But some neuroscientists caution that today’s computers will never truly understand what we’re saying because they do

not take into account the context of a conversation the way people do.

Asimo, built by Honda and the world’s most advanced robot, can run, climb stairs and converse, but it still confuses a hand

raised with a question and a hand raised to take a photo. Courtesy of Honda.

Specifically, say University of California, Berkeley, postdoctoral fellow Arjen Stolk and his Dutch colleagues, machines

don’t develop a shared understanding of the people, place and situation – often including a long social history – that is

key to human communication. Without such common ground, a computer cannot help but be confused.

“People tend to think of communication as an exchange of linguistic signs or gestures, forgetting that much of

communication is about the social context, about who you are communicating with,” Stolk said.

The word “bank,” for example, would be interpreted one way if you’re holding a credit card but a different way if you’re

holding a fishing pole. Without context, making a “V” with two fingers could mean victory, the number two, or “these

are the two fingers I broke.”

“All these subtleties are quite crucial to understanding one another,” Stolk said, perhaps more so than the words and

signals that computers and many neuroscientists focus on as the key to communication. “In fact, we can understand

one another without language, without words and signs that already have a shared meaning.”

Babies and parents, not to mention strangers lacking a common language, communicate effectively all the time, based

solely on gestures and a shared context they build up over even a short time.

Stolk argues that scientists and engineers should focus more on the contextual aspects of mutual understanding, basing



Share 0





Reddit



Email





Print











News


his argument on experimental evidence from brain scans

that humans achieve nonverbal mutual understanding

using unique computational and neural mechanisms.

Some of the studies Stolk has conducted suggest that a

breakdown in mutual understanding is behind social

disorders such as autism.

“This shift in understanding how people communicate

without any need for language provides a new

theoretical and empirical foundation for understanding

normal social communication, and provides a new

window into understanding and treating disorders of

social communication in neurological and

neurodevelopmental disorders,” said Dr. Robert Knight, a

UC Berkeley professor of psychology in the campus’s

Helen Wills Neuroscience Institute and a professor of

neurology and neurosurgery at UCSF.

Stolk and his colleagues discuss the importance of conceptual alignment for mutual understanding in an opinion piece

appearing Jan. 11 in the journal Trends in Cognitive Sciences.

Brain scans pinpoint site for ‘meeting of minds’

To explore how brains achieve mutual understanding, Stolk created a game that requires two players to communicate

the rules to each other solely by game movements, without talking or even seeing one another, eliminating the

influence of language or gesture. He then placed both players in an fMRI (functional magnetic resonance imager) and

scanned their brains as they nonverbally communicated with one another via computer.

He found that the same regions of the brain – located in

the poorly understood right temporal lobe, just above the

ear – became active in both players during attempts to

communicate the rules of the game. Critically, the superior

temporal gyrus of the right temporal lobe maintained a

steady, baseline activity throughout the game but became

more active when one player suddenly understood what

the other player was trying to communicate. The brain’s

right hemisphere is more involved in abstract thought and

social interactions than the left hemisphere.

“These regions in the right temporal lobe increase in

activity the moment you establish a shared meaning for

something, but not when you communicate a signal,”

Stolk said. “The better the players got at understanding



As two people conversing rely more and more on previously

shared concepts, the same area of their brains – the right

superior temporal gyrus – becomes more active (blue is

activity in communicator, orange is activity in interpreter). This

suggests that this brain region is key to mutual understanding

as people continually update their shared understanding of the

context of the conversation to improve mutual understanding.



A game in which players try to communicate the rules without

talking or even seeing one another helps neuroscientists

isolate the parts of the brain responsible for mutual


each other, the more active this region became.”

This means that both players are building a similar

conceptual framework in the same area of the brain, constantly testing one another to make sure their concepts align,

and updating only when new information changes that mutual understanding. The results were reported in 2014 in the

Proceedings of the National Academy of Sciences.

“It is surprising,” said Stolk, “that for both the communicator, who has static input while she is planning her move, and

the addressee, who is observing dynamic visual input during the game, the same region of the brain becomes more

active over the course of the experiment as they improve their mutual understanding.”

Robots’ statistical reasoning

Robots and computers, on the other hand, converse based on a statistical analysis of a word’s meaning, Stolk said. If

you usually use the word “bank” to mean a place to cash a check, then that will be the assumed meaning in a

conversation, even when the conversation is about fishing.

“Apple’s Siri focuses on statistical regularities, but

communication is not about statistical regularities,” he

said. “Statistical regularities may get you far, but it is not

how the brain does it. In order for computers to

communicate with us, they would need a cognitive

architecture that continuously captures and updates the

conceptual space shared with their communication

partner during a conversation.”

Hypothetically, such a dynamic conceptual framework

would allow computers to resolve the intrinsically

ambiguous communication signals produced by a real

person, including drawing upon information stored years

earlier.

Stolk’s studies have pinpointed other brain areas critical

to mutual understanding. In a 2014 study, he used brain stimulation to disrupt a rear portion of the temporal lobe and

found that it is important for integrating incoming signals with knowledge from previous interactions. A later study

found that in patients with damage to the frontal lobe (the ventromedial prefrontal cortex), decisions to communicate

are no longer fine-tuned to stored knowledge about an addressee. Both studies could explain why such patients appear

socially awkward in everyday social interactions.

Stolk plans future studies with Knight using fine-tuned brain mapping on the actual surfaces of the brains of volunteers,

so-called electrocorticography.

Stolk said he wrote the new paper in hopes of moving the study of communication to a new level with a focus on

conceptual alignment.

understanding.



A computer would have a hard time understanding this

conversation, but humans get it immediately. That’s because

human communicators share a conceptual space or common

ground that enables them to quickly interpret a situation.

Words and signs are merely a means to seek and provide

evidence for such mutual understanding. Artwork (Edward

Hopper’s Nighthawks) courtesy of the Art Institute of Chicago.


Tweet

“Most cognitive neuroscientists focus on the signals themselves, on the words, gestures and their statistical

relationships, ignoring the underlying conceptual ability that we use during communication and the flexibility of

everyday life,” he said. “Language is very helpful, but it is a tool for communication, it is not communication per se. By

focusing on language, you may be focusing on the tool, not on the underlying mechanism, the cognitive architecture we

have in our brain that helps us to communicate.”

Stolk’s co-authors are Ivan Toni of the Donders Institute for Brain, Cognition and Behavior at Radboud University in the

Netherlands, where the studies were conducted, and Lennart Verhagen of the University of Oxford.

RELATED INFORMATION

Conceptual Alignment: How Brains Achieve Mutual Understanding (TICS)

Arjen Stolk’s website

Robert Knight lab website

Helen Wills Neuroscience Institute

Topics: neuroscience, press releases, psychology, technology and engineering



Share 0





Reddit



Email





Print



TOP STORIES



A $25-an-hour minimum wage for medical workers...




Julie Chavez Rodriguez, ‘visionary’...



Community engagement improves wildlife...


All news

Topics A-Z

BROWSE

    

FOLLOW US

RELATED STORIES



A $25-an-hour minimum wage for medical workers...



Community engagement improves wildlife...

VIDEOS

PODCASTS



Video: Berkeley Law tackles border killing case



Berkeley Talks: ChatGPT developer John Schulman...







@BOBTHESCIGUY

Stop zapping cells or using viruses to deliver #CRISPR! New @igisci technique employs

peptides to more gently slip… https://t.co/bnsGpIPW97

about 3 days ago


About us

Contact the news team

Enroll in our online media training

Find an Expert

ABOUT

Sign up for email news alerts

Subscribe to The Berkeleyan, our weekly email newsletter

SUBSCRIBE



Copyright © 2023 UC Regents; all rights reserved

Accessibility

Nondiscrimination

Privacy

