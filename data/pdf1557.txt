
Language Models

LM Jelinek-Mercer Smoothing and LM Dirichlet Smoothing

Web Search

13

Slides based on the books:






Overview

14

Application

Multimedia 

documents

User

Information 

analysis

Indexes

Ranking

Query

Documents

Indexing

Query

Results

Query 

processing

Crawler




























































































What is a language model?

â€¢ We can view a finite state automaton as a deterministic 

language model.

â€¢ I wish I wish I wish I wish . . .  Cannot generate: â€œwish I wishâ€ 

or â€œI wish Iâ€. 

â€¢ Our basic model: each document was generated by a 

different automaton like this except that these automata are 

probabilistic.

15

15




A probabilistic language model

â€¢ This is a one-state probabilistic finite-state automaton (a 

unigram LM) and the state emission distribution for its one 

state q1.

â€¢ STOP is not a word, but a special symbol indicating that the 

automaton stops.  

String = â€œfrog said that toad likes frog STOPâ€

P(string) = 0.01 Â· 0.03 Â· 0.04 Â· 0.01 Â· 0.02 Â· 0.01 Â· 0.2= 0.0000000000048





16


A language model per document

String = â€œfrog said that toad likes frog STOPâ€

P(string|Md1 ) = 0.01 Â· 0.03 Â· 0.04 Â· 0.01 Â· 0.02 Â· 0.01 Â· 0.02 = 0.0000000000048 = 4.8 Â· 10-12

P(string|Md2 ) = 0.01 Â· 0.03 Â· 0.05 Â· 0.02 Â· 0.02 Â· 0.01 Â· 0.02 = 0.0000000000120 = 12 Â· 10-12

P(string|Md1 ) &lt;  P(string|Md2 )

â€¢ Thus, document d2 is â€œmore relevantâ€ to the string â€œfrog said that toad likes frog 

STOPâ€ than d1 is.

17




Types of language models

â€¢ Unigrams:

â€¢ Bigrams:

â€¢ Multinomial distributions over words:

18

ğ‘ğ‘¢ğ‘›ğ‘– ğ‘¡1ğ‘¡2ğ‘¡3ğ‘¡4 = ğ‘ ğ‘¡1 ğ‘ ğ‘¡2 ğ‘ ğ‘¡3 ğ‘ ğ‘¡4

ğ‘ğ‘ğ‘– ğ‘¡1ğ‘¡2ğ‘¡3ğ‘¡4 = ğ‘ ğ‘¡1 ğ‘ ğ‘¡2|ğ‘¡1 ğ‘ ğ‘¡3|ğ‘¡2 ğ‘ ğ‘¡4|ğ‘¡3

ğ‘ ğ‘‘ =

ğ‘™ğ‘‘!

ğ‘“ğ‘¡1,ğ‘‘!ğ‘“ğ‘¡2,ğ‘‘! â€¦ğ‘“ğ‘¡ğ‘›,ğ‘‘! ğ‘ ğ‘¡1

ğ‘“ğ‘¡1,ğ‘‘ğ‘ ğ‘¡2

ğ‘“ğ‘¡2,ğ‘‘â€¦ğ‘ ğ‘¡ğ‘›

ğ‘“ğ‘¡ğ‘›,ğ‘‘


Probability Ranking Principle (PRP)

â€¢ PRP in action: Rank all documents by ğ‘ ğ‘Ÿ = 1|ğ‘, ğ‘‘

â€¢ Theorem: Using the PRP is optimal, in that it minimizes the loss (Bayes risk) 

under 1/0 loss

â€¢ Provable if all probabilities correct, etc.  [e.g., Ripley 1996]

â€¢ Using odds, we reach a more convenient formulation of ranking :

19

ğ‘ ğ‘Ÿ|ğ‘, ğ‘‘ = ğ‘ ğ‘‘, ğ‘ ğ‘Ÿ ğ‘(ğ‘Ÿ)

ğ‘(ğ‘‘, ğ‘)

O ğ‘… ğ‘, ğ‘‘ = ğ‘ ğ‘Ÿ = 1|ğ‘, ğ‘‘

ğ‘ ğ‘Ÿ = 0|ğ‘, ğ‘‘


Language models

â€¢ In language models, we do a different formulation towards 

the query posterior given the document as a model.

O ğ‘… ğ‘, ğ‘‘ = ğ‘ ğ‘Ÿ = 1|ğ‘, ğ‘‘

ğ‘ ğ‘Ÿ = 0|ğ‘, ğ‘‘ = ğ‘ ğ‘‘, ğ‘ ğ‘Ÿ = 1 ğ‘ ğ‘Ÿ = 1

ğ‘ ğ‘‘, ğ‘ ğ‘Ÿ = 0 ğ‘ ğ‘Ÿ = 0

= ğ‘ ğ‘ ğ‘‘, ğ‘Ÿ ğ‘ ğ‘‘|ğ‘Ÿ ğ‘ ğ‘Ÿ

ğ‘ ğ‘ ğ‘‘, Ò§ğ‘Ÿ ğ‘ ğ‘‘| Ò§ğ‘Ÿ ğ‘

Ò§ğ‘Ÿ âˆ log ğ‘ ğ‘|ğ‘‘, ğ‘Ÿ ğ‘ ğ‘Ÿ|ğ‘‘

ğ‘ ğ‘|ğ‘‘, Ò§ğ‘Ÿ ğ‘

Ò§ğ‘Ÿ|ğ‘‘

= log ğ‘(ğ‘|ğ‘‘, ğ‘Ÿ) âˆ’ log ğ‘ ğ‘ ğ‘‘, Ò§ğ‘Ÿ + log ğ‘ ğ‘Ÿ|ğ‘‘

ğ‘

Ò§ğ‘Ÿ ğ‘‘

20


Language models

â€¢ The fist term computes the probability that the query has been 

generated by the document model

â€¢ The second term can measure the quality of the document with 

respect to other indicators not contained in the query (e.g. 

PageRank or number of links)

log ğ‘(ğ‘|ğ‘‘, ğ‘Ÿ) âˆ’ log ğ‘ ğ‘ ğ‘‘, Ò§ğ‘Ÿ + log ğ‘ ğ‘Ÿ|ğ‘‘

ğ‘

Ò§ğ‘Ÿ ğ‘‘

â‰ˆ log ğ‘(ğ‘|ğ‘‘, ğ‘Ÿ) + logit ğ‘ ğ‘Ÿ|ğ‘‘

21


How to compute ğ‘ ğ‘ ğ‘‘ ?

â€¢ We will make the same conditional independence assumption as for 

Naive Bayes (we dropped the r variable)

â€¢ |q| length of query; 

â€¢ ğ‘¡ğ‘– the token occurring at position i in the query

â€¢ This is equivalent to:

â€¢ ğ‘¡ğ‘“ğ‘¡,ğ‘ is the term frequency (# occurrences) of t in q

â€¢ Multinomial model (omitting constant factor) 

22

ğ‘ ğ‘ ğ‘€ğ‘‘ =

à·‘

ğ‘¡âˆˆ ğ‘âˆ©ğ‘‘

ğ‘ ğ‘¡ ğ‘€ğ‘‘

ğ‘¡ğ‘“ğ‘¡,ğ‘

ğ‘ ğ‘ ğ‘€ğ‘‘ = à·‘

ğ‘–=0

|ğ‘|

ğ‘ ğ‘¡ğ‘– ğ‘€ğ‘‘


Parameter estimation

â€¢ The parameters ğ‘ ğ‘¡ ğ‘€ğ‘‘ are obtained from the document data as the 

maximum likelihood estimate:

â€¢ A single t with ğ‘ ğ‘¡ ğ‘€ğ‘‘ = 0 will make ğ‘ ğ‘ ğ‘€ğ‘‘ = Ï‚ ğ‘ ğ‘¡ ğ‘€ğ‘‘ zero.

â€¢ This can be smoothed with the prior knowledge we have about the 

collection.

23

ğ‘ ğ‘¡ ğ‘€ğ‘‘

ğ‘šğ‘™ = ğ‘“ğ‘¡,ğ‘‘

|ğ‘‘|


Smoothing

â€¢ Key intuition: A non-occurring term is possible (even though it didnâ€™t 

occur), . . .

. . . but no more likely than would be expected by chance in the collection.

â€¢ The maximum likelihood language model ğ‘€ğ¶

ğ‘šğ‘™ based on the term 

frequencies in the collection as a whole:

â€¢ ğ‘™ğ‘¡ is the number of times the term shows up in the collection

â€¢ ğ‘™ğ¶ is the number of terms in the whole collection.

â€¢ We will use ğ‘ ğ‘¡ ğ‘€ğ¶

ğ‘šğ‘™ to â€œsmoothâ€ ğ‘ ğ‘¡ ğ‘‘ away from zero. 

24

ğ‘ ğ‘¡ ğ‘€ğ¶

ğ‘šğ‘™ = ğ‘™ğ‘¡

ğ‘™ğ¶


LM with Jelineck-Mercer smoothing

â€¢ The first approach we can do is to create a mixture model with both 

distributions:

â€¢ Mixes the probability from the document with the general collection 

frequency of the word.

â€¢ High value of Î»: â€œconjunctive-likeâ€ search â€“ tends to retrieve documents 

containing all query words.

â€¢ Low value of Î»: more disjunctive, suitable for long queries

â€¢ Correctly setting Î» is very important for good performance.

25

ğ‘ ğ‘ ğ‘‘, ğ¶ = ğœ† âˆ™ ğ‘ ğ‘ ğ‘€ğ‘‘ + 1 âˆ’ ğœ† âˆ™ ğ‘ ğ‘ ğ‘€ğ‘


Mixture model: Summary

â€¢ What we model: The user has some background knowledge about the 

collection and has a â€œdocument in mindâ€ and generates the query from 

this document.

â€¢ The equation represents the probability that the document that the 

user had in mind was in fact this one.

26

ğ‘ ğ‘ ğ‘‘, ğ¶ â‰ˆ

à·‘

ğ‘¡âˆˆ{ğ‘âˆ©ğ‘‘}

ğœ† âˆ™ ğ‘ ğ‘¡ ğ‘€ğ‘‘ + 1 âˆ’ ğœ† âˆ™ ğ‘ ğ‘¡ ğ‘€ğ‘


LM with Dirichlet smoothing

â€¢ We can use the prior knowledge about the mean of each term.

â€¢ The mean of the term in the collection should be our starting point 

when computing the term average on a document:

â€¢ Imagine that we can add a fractional number occurrences to each term 

frequency.

â€¢ Add ğœ‡ = 1000 occurrences of terms to a document according to the 

collection distribution.

â€¢ The frequency of each term ğ‘¡ğ‘– would increase ğœ‡ âˆ™ ğ‘€ğ‘(ğ‘¡ğ‘–)

â€¢ The length of each document increases by 1000.

â€¢ This will change the way we compute the mean of a term on a 

document.

27


Dirichlet smoothing

â€¢ We end up with the maximum a posteriori estimate of the term 

average:

â€¢ This is equivalent to using a Dirichlet prior with appropriate parameters.

â€¢ The ranking function becomes:

28

ğ‘ ğ‘¡ ğ‘€ğ‘‘

ğ‘€ğ´ğ‘ƒ = ğ‘“ğ‘¡,ğ‘‘ + ğœ‡ âˆ™ ğ‘€ğ‘(ğ‘¡)

ğ‘‘ + ğœ‡

ğ‘ ğ‘ ğ‘‘ = à·‘

ğ‘¡âˆˆğ‘

ğ‘“ğ‘¡,ğ‘‘ + ğœ‡ âˆ™ ğ‘€ğ‘ ğ‘¡

ğ‘‘ + ğœ‡

ğ‘ğ‘¡


Experimental comparison

TREC45

Gov2

1998

1999

2005

2006

Method

P@10

MAP

P@10

MAP

P@10

MAP

P@10

MAP

Binary

0.256

0.141

0.224

0.148

0.069

0.050

0.106

0.083

2-Poisson

0.402

0.177

0.406

0.207

0.418

0.171

0.538

0.207

BM25

0.424

0.178

0.440

0.205

0.471

0.243

0.534

0.277

LMJM

0.390

0.179

0.432

0.209

0.416

0.211

0.494

0.257

LMD

0.450

0.193

0.428

0.226

0.484

0.244

0.580

0.293

BM25F

0.482

0.242

0.544

0.277

BM25+PRF

0.452

0.239

0.454

0.249

0.567

0.277

0.588

0.314

RRF

0.462

0.215

0.464

0.252

0.543

0.297

0.570

0.352

LR

0.446

0.266

0.588

0.309

RankSVM

0.420

0.234

0.556

0.268

29


Experimental comparison

â€¢ For long queries, the Jelinek-Mercer smoothing performs better than 

the Dirichlet smoothing. 

â€¢ For short queries, the Dirichlet smoothing performs better than the 

Jelinek-Mercer smoothing.

30

Chengxiang Zhai and John Lafferty. 2004. A study of smoothing methods for language models 

applied to information retrieval. ACM Trans. Inf. Syst. 22, 2 (April 2004), 179-214.

Method

Query

AP

Prec@10

Prec@20

LMJM

Title

0.227

0.323

0.265

LMD

Title

0.256

0.352

0.289

LMJM

Long

0.280

0.388

0.315

LMD

Long

0.279

0.373

0.303


Summary

â€¢ Language Models 

â€¢ Jelinek-Mercer smoothing 

â€¢ Dirichlet smoothing

â€¢ Both models need to estimate one single parameter from the whole 

collection

â€¢ (although there are known values that work well).

â€¢ References:

Chapter 12

Sections 9.1, 9.2 and 9.3

31





