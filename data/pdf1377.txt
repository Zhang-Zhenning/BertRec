
Published in

MTI Technology



May 19, 2020

·

18 min read

N-gram language models

Part 1: Unigram model

Background

n-gram models

unigram model

model interpolation

higher n-gram models






expectation-maximization algorithm

Data

Unigram language model

What is a unigram?

Training the model

For simplicity, all words are lower-cased in the language model, and punctuations are ignored. The [END] token

marks the end of the sentence, and will be explained shortly.

unigram language model

Estimated probability of the unigram ‘dream’ from training text

Evaluating the model


The role of ending symbols

[END]

Evaluation metric: average log likelihood

For n-gram models, log of base 2 is often used due to its link to information theory (see here, page 21)

average log likelihood

Dealing with unknown unigrams

Laplace smoothing

Laplace smoothing

artificial unigram called [UNK]

pseudo-count of k


Effect of Laplace smoothing

Percentages of top 10 most and least common words (before and after add-one smoothing). Left and right bar

charts are not plotted at the same scale.

Coding the unigram model

Data pre-processing

tokenization

tokenize_raw_test


generate_tokenized_sentences


sent_tokenize

sent_tokenize

replace_characters


RegexpTokenizer

[-

\’\w]+

\w

replace_characters

[END]

tokenized_sentence.append(‘[END]’)


Training the model

UnigramCounter

counts

token_count


UnigramModel

UnigramCounter

counts

k

probs

train

k=1


Evaluating the model

UnigramCounter

evaluate

UnigramModel

UnigramCounter


probs

Result

Average log likelihood and similarity of unigram distributions


Average log likelihood and similarity of unigram distributions

Average log likelihood: the average of the trained log probabilities of each word in our evaluation text

evaluate

UnigramModel

probability of the unigram in the evaluation text

probability of

the unigram in the training text

For the average log likelihood to be maximized, the unigram

distributions between the training and the evaluation texts have to be as

similar as possible.

Comparison of unigram distributions

Poor Ned :’(


Statistical models are likely to be useless as predictors if the training

sets and the test sets are as different as Shakespeare and The Wall

Street Journal.

Interpolation of unigram model

model interpolation

Relationship with Laplace smoothing

uniform probability


model interpolation

Varying interpolation weight

Effects of interpolation

Reduce overfit of unigram model

less overfit

Reduce variance of estimate

bias-variance trade off


under-fitting model

over-fitting model

the more different the evaluation text is from the training text, the more we need to

interpolate our unigram model with the uniform

Final remarks


References

shrinkage

3

A place where MTI-ers can publish ideas about new technologies, agile concepts and their working experiences



Read more from MTI Technology





Naturallanguageprocessing

Data Science

Statistics

Ngrams

Machine Learning

