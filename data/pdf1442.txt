
An

Empirical

Study

of

Smo

othing

T

ec

hniques

for

Language

Mo

deling

Stanley

F.

Chen

and

Josh

ua

Go

o

dman

TR-0-	

August

		



Cen

ter

for

Researc

h

in

Computing

T

ec

hnology

Harv

ard

Univ

ersit

y

Cam

bridge,

Massac

h

usetts





An

Empirical

Study

of

Smo

othing

T

ec

hniques

for

Language

Mo

deling

Stanley

F.

Chen

Sc

ho

ol

of

Computer

Science

Carnegie

Mellon

Univ

ersit

y

000

F

orb

es

Av

e.

Pittsburgh,

P

A



sfc@cs.cmu.

edu

Josh

ua

Go

o

dman

Engineering

Sciences

Lab

oratory

Harv

ard

Univ

ersit

y

0

Oxford

St.

Cam

bridge,

MA

0

goodman@ee

cs.

ha

rva

rd.

edu

July

,

		

Abstract

W

e

presen

t

a

tutorial

in

tro

duction

to

n-gram

mo

dels

for

language

mo

deling

and

surv

ey

the

most

widely-used

smo

othing

algorithms

for

suc

h

mo

dels.

W

e

then

presen

t

an

extensiv

e

empirical

comparison

of

sev

eral

of

these

smo

othing

tec

hniques,

including

those

describ

ed

b

y

Jelinek

and

Mercer

(	0),

Katz

(	),

Bell,

Cleary

,

and

Witten

(		0),

Ney

,

Essen,

and

Kneser

(		),

and

Kneser

and

Ney

(		).

W

e

in

v

estigate

ho

w

factors

suc

h

as

training

data

size,

training

corpus

(e.g.,

Bro

wn

v

ersus

W

all

Street

Journal),

coun

t

cuto�s,

and

n-gram

order

(bigram

v

ersus

trigram)

a�ect

the

relativ

e

p

erformance

of

these

metho

ds,

whic

h

is

measured

through

the

cross-en

trop

y

of

test

data.

Our

results

sho

w

that

previous

comparisons

ha

v

e

not

b

een

complete

enough

to

fully

c

haracterize

smo

othing

algorithm

p

erformance.

W

e

in

tro

duce

metho

dologies

for

analyzing

smo

othing

algorithm

e�cacy

in

detail,

and

using

these

tec

hniques

w

e

motiv

ate

a

no

v

el

v

ariation

of

Kneser-Ney

smo

othing

that

consisten

tly

outp

erforms

all

other

algorithms

ev

aluated.

Finally

,

results

sho

wing

that

impro

v

ed

language

mo

del

smo

othing

leads

to

impro

v

ed

sp

eec

h

recognition

p

erformance

are

presen

ted.



In

tro

duction

L

anguage

mo

dels

are

a

staple

in

man

y

domains

including

sp

eec

h

recognition,

optical

c

haracter

recognition,

handwriting

recognition,

mac

hine

translation,

and

sp

elling

correction

(Ch

urc

h,

	;

Bro

wn

et

al.,

		0;

Hull,

		;

Kernighan,

Ch

urc

h,

and

Gale,

		0;

Srihari

and

Baltus,

		).

The

dominan

t

tec

hnology

in

language

mo

deling

is

n-gr

am

mo

dels,

whic

h

are

straigh

tforw

ard

to

construct

except

for

the

issue

of

smo

othing,

a

tec

hnique

used

to

b

etter

estimate

probabilities

when

there

is

insu�cien

t

data

to

estimate

probabilities

accurately

.

An

enormous

n

um

b

er

of

tec

hniques

ha

v

e

b

een

prop

osed

for

smo

othing

n-gram

mo

dels,

man

y

more

than

w

e

could

p

ossibly

describ

e

here;

ho

w

ev

er,

there

has

b

een

a

conspicuous

absence

of

studies

that

systematically

compare

the

relativ

e

p

erformance

of

more

than

just

a

few

of

these

algorithms

on

m

ultiple

data

sets.

As

a

result,

from

the

literature

it

is

imp

ossible

to

gauge

the

relativ

e

p

erformance

of

existing

algorithms

in

all

but

a

handful

of

situations.

In

this

w

ork,

w

e

attempt

to

disp

el

some

of

the

m

ystery

surrounding

smo

othing

b

y

determining

whic

h

algorithms

w

ork

w

ell

in

whic

h

situations,

and

wh

y

.

W

e

b

egin

b

y

giving

a

tutorial

in

tro

duction




to

n-gram

mo

dels

and

smo

othing,

and

surv

ey

the

most

widely-used

smo

othing

tec

hniques.

W

e

then

presen

t

an

extensiv

e

empirical

comparison

of

sev

eral

of

these

smo

othing

tec

hniques,

including

those

describ

ed

b

y

Jelinek

and

Mercer

(	0),

Katz

(	),

Bell,

Cleary

,

and

Witten

(		0),

Ney

,

Essen,

and

Kneser

(		),

and

Kneser

and

Ney

(		).

W

e

describ

e

exp

erimen

ts

that

systematically

v

ary

a

wide

range

of

v

ariables,

including

training

data

size,

corpus,

coun

t

cuto�s,

and

n-gram

order,

and

sho

w

that

most

of

these

v

ariables

signi�can

tly

a�ect

the

relativ

e

p

erformance

of

algorithms.

W

e

in

tro

duce

metho

dologies

for

analyzing

smo

othing

algorithm

p

erformance

in

detail,

and

using

these

tec

hniques

w

e

motiv

ate

a

no

v

el

v

ariation

of

Kneser-Ney

smo

othing

that

consisten

tly

outp

erforms

all

other

algorithms

ev

aluated.

Finally

,

w

e

presen

t

results

sho

wing

that

b

etter

smo

othing

algorithms

lead

to

b

etter

sp

eec

h

recognition

p

erformance,

yielding

up

to

a

%

absolute

di�erence

in

w

ord-error

rate.

This

w

ork

is

an

extension

of

our

previously

rep

orted

researc

h

(Chen

and

Go

o

dman,

		;

Chen,

		).

This

pap

er

is

structured

as

follo

ws:

In

the

remainder

of

this

section,

w

e

presen

t

an

in

tro

duction

to

language

mo

deling,

n-gram

mo

dels,

and

smo

othing.

In

Section

,

w

e

surv

ey

previous

w

ork

on

smo

othing

n-gram

mo

dels.

In

Section

,

w

e

describ

e

our

no

v

el

v

ariation

of

Kneser-Ney

smo

othing.

In

Section

,

w

e

discuss

v

arious

asp

ects

of

our

exp

erimen

tal

metho

dology

,

including

the

details

of

our

implemen

tati

ons

of

v

arious

smo

othing

algorithms,

parameter

optimization,

and

data

sets.

In

Section

,

w

e

presen

t

the

results

of

all

of

our

exp

erimen

ts.

Finally

,

in

Section



w

e

summarize

the

most

imp

ortan

t

conclusions

of

this

w

ork.

.

Language

Mo

deling

and

n-Gram

Mo

dels

A

language

mo

del

is

usually

form

ulated

as

a

probabilit

y

distribution

p(s)

o

v

er

strings

s

that

attempts

to

re�ect

ho

w

frequen

tly

a

string

s

o

ccurs

as

a

sen

tence.

F

or

example,

for

a

lan-

guage

mo

del

describing

sp

ok

en

language,

w

e

migh

t

ha

v

e

p(hello

)

�

0:0

since

p

erhaps

one

out

of

ev

ery

h

undred

sen

tences

a

p

erson

sp

eaks

is

hello.

On

the

other

hand,

w

e

w

ould

ha

v

e

p(chicken

funky

o

verlo

ad

ketchup

)

�

0

and

p(asbestos

gallops

gallantl

y

)

�

0

since

it

is

extremely

unlik

ely

an

y

one

w

ould

utter

either

string.

Notice

that

unlik

e

in

linguistics,

grammat-

icalit

y

is

irrelev

an

t

in

language

mo

deling;

ev

en

though

the

string

asbestos

gallops

gallantl

y

is

gramma

tical,

w

e

still

assign

it

a

near-zero

probabilit

y

.

The

most

widely-used

language

mo

dels,

b

y

far,

are

n-gram

language

mo

dels.

W

e

in

tro

duce

these

mo

dels

b

y

considering

the

case

n

=

;

these

mo

dels

are

called

bigr

am

mo

dels.

First,

w

e

notice

that

for

a

sen

tence

s

comp

osed

of

the

w

ords

w



�

�

�

w

l

,

without

loss

of

generalit

y

w

e

can

express

p(s)

as

p(s)

=

p(w



)p(w



jw



)p(w



jw



w



)

�

�

�

p(w

l

jw



�

�

�

w

l�

)

=

l

Y

i=

p(w

i

jw



�

�

�

w

i�

)

In

bigram

mo

dels,

w

e

mak

e

the

appro

ximation

that

the

probabilit

y

of

a

w

ord

dep

ends

only

on

the

iden

tit

y

of

the

immediately

preceding

w

ord,

giving

us

p(s)

=

l

Y

i=

p(w

i

jw



�

�

�

w

i�

)

�

l

Y

i=

p(w

i

jw

i�

)

()

T

o

mak

e

p(w

i

jw

i�

)

meaningful

for

i

=

,

w

e

can

pad

the

b

eginning

of

the

sen

tence

with

a

distinguished

tok

en

&lt;bos&gt;;

that

is,

w

e

pretend

w

0

is

&lt;bos&gt;.

In

addition,

to

mak

e

the

sum

of

the

probabilities

of

all

strings

P

s

p(s)

equal

,

it

is

necessary

to

place

a

distinguished

tok

en




&lt;eos&gt;

at

the

end

of

sen

tences

and

to

include

this

in

the

pro

duct

in

equation

().



F

or

example,

to

calculate

p(John

read

a

book

)

w

e

w

ould

tak

e

p(John

read

a

book

)

=

p(John

j&lt;bos&gt;)p(read

jJohn)p(a

jread)p(book

ja

)p(&lt;eos&gt;

jbook

)

T

o

estimate

p(w

i

jw

i�

),

the

frequency

with

whic

h

the

w

ord

w

i

o

ccurs

giv

en

that

the

last

w

ord

is

w

i�

,

w

e

can

simply

coun

t

ho

w

often

the

bigram

w

i�

w

i

o

ccurs

in

some

text

and

normalize.

Let

c(w

i�

w

i

)

denote

the

n

um

b

er

of

times

the

bigram

w

i�

w

i

o

ccurs

in

the

giv

en

text.

Then,

w

e

can

tak

e

p(w

i

jw

i�

)

=

c(w

i�

w

i

)



P

w

i

c(w

i�

w

i

)

()

The

text

a

v

ailable

for

building

a

mo

del

is

called

tr

aining

data.

F

or

n-gram

mo

dels,

the

amoun

t

of

training

data

used

is

t

ypically

man

y

millio

ns

of

w

ords.

The

estimate

for

p(w

i

jw

i�

)

giv

en

in

equation

()

is

called

the

maximum

likeliho

o

d

(ML)

estimate

of

p(w

i

jw

i�

),

b

ecause

this

assignmen

t

of

probabilities

yields

the

bigram

mo

del

that

assigns

the

highest

probabilit

y

to

the

training

data

of

all

p

ossible

bigram

mo

dels.

F

or

n-gram

mo

dels

where

n

&gt;

,

instead

of

conditioning

the

probabilit

y

of

a

w

ord

on

the

iden

tit

y

of

just

the

preceding

w

ord,

w

e

condition

this

probabilit

y

on

the

iden

tit

y

of

the

last

n

�



w

ords.

Generalizing

equation

()

to

n

&gt;

,

w

e

get

p(s)

=

l+

Y

i=

p(w

i

jw

i�

i�n+

)

()

where

w

j

i

denotes

the

w

ords

w

i

�

�

�

w

j

and

where

w

e

tak

e

w

�n+

through

w

0

to

b

e

&lt;bos&gt;

and

w

l+

to

b

e

&lt;eos&gt;

.

T

o

estimate

the

probabilities

p(w

i

jw

i�

i�n+

),

the

analogous

equation

to

equation

()

is

p(w

i

jw

i�

i�n+

)

=

c(w

i

i�n+

)



P

w

i

c(w

i

i�n+

)

()

In

practice,

the

largest

n

in

wide

use

is

n

=

;

this

mo

del

is

referred

to

as

a

trigr

am

mo

del.

The

w

ords

w

i�

i�n+

preceding

the

curren

t

w

ord

w

i

are

sometimes

called

the

history.

Notice

that

the

sum

P

w

i

c(w

i

i�n+

)

is

equal

to

the

coun

t

of

the

history

c(w

i�

i�n+

);

b

oth

forms

are

used

in

this

text.

W

e

sometimes

refer

to

the

v

alue

n

of

an

n-gram

mo

del

as

its

or

der.

This

terminology

comes

from

the

area

of

Mark

o

v

mo

dels

(Mark

o

v,

	),

of

whic

h

n-gram

mo

dels

are

an

instance.

In

particular,

an

n-gram

mo

del

can

b

e

in

terpreted

as

a

Mark

o

v

mo

del

of

order

n

�

.

Let

us

consider

a

small

example.

Let

our

training

data

S

b

e

comp

osed

of

the

three

sen

tences

(

\John

read

Moby

Dick";

\Mar

y

read

a

different

book"

;

\She

read

a

book

by

Cher

"

)

and

let

us

calculate

p(John

read

a

book

)

for

the

maxim

um

lik

eliho

o

d

bigram

mo

del.

W

e

ha

v

e

p(Johnj&lt;bos&gt;

)

=

c(&lt;bos&gt;

John

)



P

w

c(&lt;bos&gt;w

)

=











Without

this,

the

sum

of

the

probabilitie

s

of

all

strings

of

a

giv

en

length

is

,

and

the

sum

of

the

probabiliti

es

of

all

strings

is

then

in�nite.




p(readjJohn

)

=

c(John

read

)



P

w

c(John

w

)

=







p(a

jread)

=

c(read

a)



P

w

c(read

w

)

=







p(book

ja)

=

c(a

book

)



P

w

c(a

w

)

=







p(&lt;eos&gt;

jbook

)

=

c(book

&lt;eos&gt;

)



P

w

c(book

w

)

=







giving

us

p(John

read

a

book

)

=

p(John

j&lt;bos&gt;)p(readjJohn

)p(ajread)p(book

ja)p(&lt;eos&gt;

jbook)

=







�



�







�







�







�

0:0

.

Smo

othing

No

w,

consider

the

sen

tence

Cher

read

a

book.

W

e

ha

v

e

p(readjCher)

=

c(Cher

read

)



P

w

c(Cher

w

)

=

0





giving

us

p(Cher

read

a

book

)

=

0.

Ob

viously

,

this

is

an

underestimate

for

the

probabilit

y

of

Cher

read

a

book

as

there

is

some

probabilit

y

that

the

sen

tence

o

ccurs.

T

o

sho

w

wh

y

it

is

imp

ortan

t

that

this

probabilit

y

should

b

e

giv

en

a

nonzero

v

alue,

w

e

turn

to

the

primary

application

for

language

mo

dels,

sp

e

e

ch

r

e

c

o

gnition.

In

sp

eec

h

recognition,

one

attempts

to

�nd

the

sen

tence

s

that

maximi

zes

p(sjA)

=

p(Ajs)p(s)



p(A)

for

a

giv

en

acoustic

signal

A.

If

p(s)

is

zero,

then

p(sjA)

will

b

e

zero

and

the

string

s

will

nev

er

b

e

considered

as

a

transcription,

regardless

of

ho

w

unam

biguous

the

acoustic

signal

is.

Th

us,

whenev

er

a

string

s

suc

h

that

p(s)

=

0

o

ccurs

during

a

sp

eec

h

recognition

task,

an

error

will

b

e

made.

Assigning

all

strings

a

nonzero

probabilit

y

helps

prev

en

t

errors

in

sp

eec

h

recognition.

Smo

othing

is

used

to

address

this

problem.

The

term

smo

othing

describ

es

tec

hniques

for

adjusting

the

maxim

um

lik

eliho

o

d

estimate

of

probabilities

(as

in

equations

()

and

())

to

pro

duce

more

accurate

probabilities.

The

name

smo

othing

comes

from

the

fact

that

these

tec

hniques

tend

to

mak

e

distributions

more

uniform,

b

y

adjusting

lo

w

probabilities

suc

h

as

zero

probabilities

up

w

ard,

and

high

probabilities

do

wn

w

ard.

Not

only

do

smo

othing

metho

ds

generally

prev

en

t

zero

probabilities,

but

they

also

attempt

to

impro

v

e

the

accuracy

of

the

mo

del

as

a

whole.

Whenev

er

a

probabilit

y

is

estimated

from

few

coun

ts,

smo

othing

has

the

p

oten

tial

to

signi�can

tly

impro

v

e

estimation.

T

o

giv

e

an

example,

one

simple

smo

othing

tec

hnique

is

to

pretend

eac

h

bigram

o

ccurs

once

more

than

it

actually

do

es

(Lidstone,

	0;

Johnson,

	;

Je�reys,

	),

yielding

p(w

i

jw

i�

)

=



+

c(w

i�

w

i

)



P

w

i

[

+

c(w

i�

w

i

)]

=



+

c(w

i�

w

i

)



jV

j

+

P

w

i

c(w

i�

w

i

)

()










Jelinek-Mercer



N�

adas



Katz







bigram







	









trigram



	



	









T

able

:

P

erplexities

rep

orted

b

y

Katz

and

N�

adas

on

00-sen

tence

test

set

for

three

di�eren

t

smo

othing

algorithms

where

V

is

the

v

o

cabulary

,

the

set

of

all

w

ords

b

eing

considered.



Let

us

reconsider

the

previous

example

using

this

new

distribution,

and

let

us

tak

e

our

v

o

cabulary

V

to

b

e

the

set

of

all

w

ords

o

ccurring

in

the

training

data

S

,

so

that

w

e

ha

v

e

jV

j

=

.

F

or

the

sen

tence

John

read

a

book,

w

e

no

w

ha

v

e

p(John

read

a

book

)

=

p(John

j&lt;bos&gt;)p(readjJohn

)p(ajread)p(book

ja)p(&lt;eos&gt;

jbook)

=







�







�







�







�







�

0:000

In

other

w

ords,

w

e

estimate

that

the

sen

tence

John

read

a

book

o

ccurs

ab

out

once

ev

ery

ten

thousand

sen

tences.

This

is

m

uc

h

more

reasonable

than

the

maxim

um

lik

eliho

o

d

estimate

of

0.0,

or

ab

out

once

ev

ery

sev

en

teen

sen

tences.

F

or

the

sen

tence

Cher

read

a

book,

w

e

ha

v

e

p(Cher

read

a

book

)

=

p(Cher

j&lt;bos&gt;)p(readjCher)p(a

jread)p(book

ja)p(&lt;eos&gt;

jbook

)

=







�







�







�







�







�

0:0000

Again,

this

is

more

reasonable

than

the

zero

probabilit

y

assigned

b

y

the

maxim

um

lik

eliho

o

d

mo

del.

While

smo

othing

is

a

cen

tral

issue

in

language

mo

deling,

the

literature

lac

ks

a

de�nitiv

e

com-

parison

b

et

w

een

the

man

y

existing

tec

hniques.

Previous

studies

(Nadas,

	;

Katz,

	;

Ch

urc

h

and

Gale,

		;

MacKa

y

and

P

eto,

		;

Kneser

and

Ney

,

		)

only

compare

a

small

n

um

b

er

of

metho

ds

(t

ypically

t

w

o)

on

one

or

t

w

o

corp

ora

and

using

a

single

training

set

size.

As

a

result,

it

is

curren

tly

di�cult

for

a

researc

her

to

in

telligen

tly

c

ho

ose

among

smo

othing

sc

hemes.

In

this

w

ork,

w

e

carry

out

an

extensiv

e

empirical

comparison

of

the

most

widely-used

smo

othing

tec

hniques,

including

those

describ

ed

b

y

Jelinek

and

Mercer

(	0),

Katz

(	),

Bell,

Cleary

,

and

Witten

(		0),

Ney

,

Essen,

and

Kneser

(		),

and

Kneser

and

Ney

(		).

W

e

carry

out

exp

erimen

ts

o

v

er

man

y

training

set

sizes

on

v

aried

corp

ora

using

n-grams

of

v

arious

order,

and

sho

w

ho

w

these

factors

a�ect

the

relativ

e

p

erformance

of

smo

othing

tec

hniques.

F

or

the

metho

ds

with

parameters

that

can

b

e

tuned

to

impro

v

e

p

erformance,

w

e

p

erform

an

automated

searc

h

for

optimal

v

alues

and

sho

w

that

sub-optimal

parameter

selection

can

signi�can

tly

decrease

p

erformance.

T

o

our

kno

wledge,

this

is

the

�rst

smo

othing

w

ork

that

systematically

in

v

estigates

an

y

of

these

issues.

Our

results

mak

e

it

apparen

t

that

previous

ev

aluations

of

smo

othing

tec

hniques

ha

v

e

not

b

een

thorough

enough

to

pro

vide

an

adequate

c

haracterization

of

the

relativ

e

p

erformance

of

di�eren

t

algorithms.

F

or

instance,

Katz

(	)

compares

his

algorithm

with

an

unsp

eci�ed

v

ersion

of

Jelinek-Mercer

deleted

estimation

and

with

N�

adas

smo

othing

(Nadas,

	)

using

a

single

training





Notice

that

if

V

is

tak

en

to

b

e

in�nite,

the

denominato

r

is

in�nite

and

all

probabilities

are

set

to

zero.

In

practice,

v

o

cabulari

es

are

t

ypically

�xed

to

b

e

tens

of

thousands

of

w

ords

or

less.

All

w

ords

not

in

the

v

o

cabulary

are

mapp

ed

to

a

single

distinguished

w

ord,

usually

called

the

unknown

wor

d.




corpus

and

a

single

test

set

of

00

sen

tences.

The

p

erplexities

rep

orted

are

displa

y

ed

in

T

able

.

Katz

concludes

that

his

algorithm

p

erforms

at

least

as

w

ell

as

Jelinek-Mercer

smo

othing

and

N�

adas

smo

othing.

In

Section

..,

w

e

will

sho

w

that,

in

fact,

the

relativ

e

p

erformance

of

Katz

and

Jelinek-Mercer

smo

othing

dep

ends

on

training

set

size,

with

Jelinek-Mercer

smo

othing

p

erforming

b

etter

on

smaller

training

sets,

and

Katz

smo

othing

p

erforming

b

etter

on

larger

sets.

In

addition

to

ev

aluating

the

o

v

erall

p

erformance

of

v

arious

smo

othing

tec

hniques,

w

e

pro

vide

more

detailed

analyses

of

p

erformance.

W

e

examine

the

p

erformance

of

di�eren

t

algorithms

on

n-grams

with

particular

n

um

b

ers

of

coun

ts

in

the

training

data;

w

e

�nd

that

Katz

smo

othing

p

erforms

w

ell

on

n-grams

with

large

coun

ts,

while

Kneser-Ney

smo

othing

is

b

est

for

small

coun

ts.

W

e

calculate

the

relativ

e

impact

on

p

erformance

of

small

coun

ts

and

large

coun

ts

for

di�eren

t

training

set

sizes

and

n-gram

orders,

and

use

this

data

to

explain

the

v

ariation

in

p

erformance

of

di�eren

t

algorithms

in

di�eren

t

situations.

Finally

,

w

e

use

this

detailed

analysis

to

motiv

ate

a

mo

di�cation

to

Kneser-Ney

smo

othing;

the

resulting

algorithm

consisten

tly

outp

erforms

all

other

algorithms

ev

aluated.

While

smo

othing

is

one

tec

hnique

for

addressing

sparse

data

issues,

there

are

n

umerous

other

tec

hniques

that

can

b

e

applied,

suc

h

as

w

ord

classing

(Bro

wn

et

al.,

		b)

or

decision-tree

mo

dels

(Bahl

et

al.,

		).

Ho

w

ev

er,

these

other

tec

hniques

in

v

olv

e

the

use

of

mo

dels

other

than

n-gram

mo

dels.

W

e

constrain

our

discussion

of

smo

othing

to

tec

hniques

where

the

structure

of

a

mo

del

is

unc

hanged

but

where

the

metho

d

used

to

estimate

the

probabilities

of

the

mo

del

is

mo

di�ed.

Smo

othing

can

b

e

applied

to

these

alternativ

e

mo

dels

as

w

ell,

and

it

remains

to

b

e

seen

whether

impro

v

ed

smo

othing

for

n-gram

mo

dels

will

lead

to

impro

v

ed

p

erformance

for

these

other

mo

dels.

.

P

erformance

Ev

aluation

The

most

common

metric

for

ev

aluating

a

language

mo

del

is

the

probabilit

y

that

the

mo

del

assigns

to

test

data,

or

the

deriv

ativ

e

measures

of

cr

oss-entr

opy

and

p

erplexity.

F

or

a

smo

othed

n-gram

mo

del

that

has

probabilities

p(w

i

jw

i�

i�n+

),

w

e

can

calculate

the

probabilit

y

of

a

sen

tence

p(s)

using

equation

().

Then,

for

a

test

set

T

comp

osed

of

the

sen

tences

(t



;

:

:

:

;

t

l

T

)

w

e

can

calculate

the

probabilit

y

of

the

test

set

p(T

)

as

the

pro

duct

of

the

probabilities

of

all

sen

tences

in

the

set:

p(T

)

=

l

T

Y

i=

p(t

i

)

The

measure

of

cross-en

trop

y

can

b

e

motiv

ated

using

the

w

ell-kno

wn

relation

b

et

w

een

predic-

tion

and

compression

(Bell,

Cleary

,

and

Witten,

		0;

Co

v

er

and

Thomas,

		).

In

particular,

giv

en

a

language

mo

del

that

assigns

probabilit

y

p(T

)

to

a

text

T

,

w

e

can

deriv

e

a

compression

algorithm

that

enco

des

the

text

T

using

�

log



p(T

)

bits.

The

cross-en

trop

y

H

p

(T

)

of

a

mo

del

p(w

i

jw

i�

i�n+

)

on

data

T

is

de�ned

as

H

p

(T

)

=

�





W

T

log



p(T

)

()

where

W

T

is

the

length

of

the

text

T

measured

in

w

ords.



This

v

alue

can

b

e

in

terpreted

as

the

a

v

erage

n

um

b

er

of

bits

needed

to

enco

de

eac

h

of

the

W

T

w

ords

in

the

test

data

using

the

compression

algorithm

asso

ciated

with

mo

del

p(w

i

jw

i�

i�n+

).

W

e

sometimes

refer

to

cross-en

trop

y

as

just

entr

opy.





In

this

w

ork,

w

e

include

the

end-of-sen

t

enc

e

tok

en

&lt;eos&gt;

when

computing

W

T

,

but

not

the

b

eginning-o

f-

sen

tence

tok

ens.




The

p

erplexit

y

PP

p

(T

)

of

a

mo

del

p

is

the

recipro

cal

of

the

(geometric)

a

v

erage

probabilit

y

assigned

b

y

the

mo

del

to

eac

h

w

ord

in

the

test

set

T

,

and

is

related

to

cross-en

trop

y

b

y

the

equation

PP

p

(T

)

=



H

p

(T

)

Clearly

,

lo

w

er

cross-en

tropies

and

p

erplexities

are

b

etter.

T

ypical

p

erplexities

yielded

b

y

n-gram

mo

dels

on

English

text

range

from

ab

out

0

to

almost

000

(corresp

onding

to

cross-en

tropies

from

ab

out



to

0

bits/w

ord),

dep

ending

on

the

t

yp

e

of

text.

In

this

w

ork,

w

e

tak

e

the

p

erformance

of

an

algorithm

to

b

e

its

cross-en

trop

y

on

test

data.

As

the

cross-en

trop

y

of

a

mo

del

on

test

data

giv

es

the

n

um

b

er

of

bits

required

to

enco

de

that

data,

cross-en

trop

y

is

a

direct

measure

of

application

p

erformance

for

the

task

of

text

compression.

F

or

other

applications,

it

is

generally

assumed

that

lo

w

er

en

trop

y

correlates

with

b

etter

p

erformance.

F

or

sp

eec

h

recognition,

it

has

b

een

sho

wn

that

this

correlation

is

reasonably

strong

(Chen,

Beefer-

man,

and

Rosenfeld,

		).

In

Section

..,

w

e

presen

t

results

that

indicate

that

this

correlation

is

esp

ecially

strong

when

considering

only

n-gram

mo

dels

that

di�er

in

the

smo

othing

tec

hnique

used.



Previous

W

ork

In

this

section,

w

e

surv

ey

a

n

um

b

er

of

smo

othing

algorithms

for

n-gram

mo

dels.

This

list

is

b

y

no

means

exhaustiv

e,

but

includes

the

algorithms

used

in

the

ma

jorit

y

of

language

mo

deling

w

ork.

The

algorithms

(except

for

those

describ

ed

in

Section

.	)

are

presen

ted

in

c

hronological

order

of

in

tro

duction.

W

e

�rst

describ

e

additiv

e

smo

othing,

a

v

ery

simple

tec

hnique

that

p

erforms

rather

p

o

orly

.

Next,

w

e

describ

e

the

Go

o

d-T

uring

estimate;

this

tec

hnique

is

not

used

alone,

but

is

the

basis

for

later

tec

hniques

suc

h

as

Katz

smo

othing.

W

e

then

discuss

Jelinek-Mercer

and

Katz

smo

othing,

t

w

o

tec

hniques

that

generally

w

ork

w

ell.

After

that,

w

e

describ

e

Witten-Bell

smo

othing;

while

Witten-Bell

smo

othing

is

w

ell-kno

wn

in

the

compression

comm

unit

y

,

w

e

will

later

sho

w

that

it

has

medio

cre

p

erformance

compared

to

some

of

the

other

tec

hniques

w

e

describ

e.

W

e

go

on

to

discuss

absolute

discoun

ting,

a

simple

tec

hnique

with

mo

dest

p

erformance

that

forms

the

basis

for

the

last

tec

hnique

w

e

describ

e,

Kneser-Ney

smo

othing.

Kneser-Ney

smo

othing

w

orks

v

ery

w

ell,

and

v

ariations

w

e

describ

e

in

Section



outp

erform

all

other

tested

tec

hniques.

In

Section

.,

w

e

describ

e

a

simple

framew

ork

that

can

b

e

used

to

express

most

p

opular

smo

othing

metho

ds,

and

recap

the

surv

ey

ed

algorithms

in

terms

of

this

framew

ork.

This

section

summarizes

the

original

descriptions

of

previous

algorithms,

but

do

es

not

include

the

details

of

our

implemen

tations

of

these

algorithms;

this

information

is

presen

ted

instead

in

Section

..

As

man

y

of

the

original

texts

omit

imp

ortan

t

details,

our

implemen

tations

sometimes

di�er

signi�can

tly

from

the

original

algorithm

description.

.

Additiv

e

Smo

othing

One

of

the

simplest

t

yp

es

of

smo

othing

used

in

practice

is

additive

smo

othing

(Lidstone,

	0;

Johnson,

	;

Je�reys,

	),

whic

h

is

just

a

generalization

of

the

metho

d

giv

en

in

equation

().

Instead

of

pretending

eac

h

n-gram

o

ccurs

once

more

than

it

do

es,

w

e

pretend

it

o

ccurs

�

times

more

than

it

do

es,

where

t

ypically

0

&lt;

�

�

,

i.e.,

p

add

(w

i

jw

i�

i�n+

)

=

�

+

c(w

i

i�n+

)



�

jV

j

+

P

w

i

c(w

i

i�n+

)

()

 


Lidstone

and

Je�reys

adv

o

cate

taking

�

=

.

Gale

and

Ch

urc

h

(		0;

		)

ha

v

e

argued

that

this

metho

d

generally

p

erforms

p

o

orly

.

.

Go

o

d-T

uring

Estimate

The

Go

o

d-T

uring

estimate

(Go

o

d,

	)

is

cen

tral

to

man

y

smo

othing

tec

hniques.

The

Go

o

d-

T

uring

estimate

states

that

for

an

y

n-gram

that

o

ccurs

r

times,

w

e

should

pretend

that

it

o

ccurs

r

�

times

where

r

�

=

(r

+

)

n

r

+



n

r

()

and

where

n

r

is

the

n

um

b

er

of

n-grams

that

o

ccur

exactly

r

times

in

the

training

data.

T

o

con

v

ert

this

coun

t

to

a

probabilit

y

,

w

e

just

normalize:

for

an

n-gram

�

with

r

coun

ts,

w

e

tak

e

p

GT

(�)

=

r

�



N

(	)

where

N

=

P



r

=0

n

r

r

�

.

Notice

that

N

=



X

r

=0

n

r

r

�

=



X

r

=0

(r

+

)n

r

+

=



X

r

=

r

n

r

i.e.,

N

is

equal

to

the

original

n

um

b

er

of

coun

ts

in

the

distribution.

T

o

deriv

e

this

estimate,

assume

that

there

are

a

total

of

s

di�eren

t

n-grams

�



;

:

:

:

;

�

s

and

that

their

true

probabilities

or

frequencies

are

p



;

:

:

:

;

p

s

,

resp

ectiv

ely

.

No

w,

let

us

estimate

the

true

probabilit

y

of

an

n-gram

�

i

that

o

ccurs

r

times

in

some

data,

giv

en

that

w

e

don't

kno

w

the

iden

tit

y

of

the

n-gram

�

i

but

that

w

e

do

kno

w

the

candidate

probabilities

p



;

:

:

:

;

p

s

.

W

e

can

in

terpret

this

as

calculating

the

v

alue

E

(p

i

jc(�

i

)

=

r

),

where

E

denotes

exp

ected

v

alue

and

where

c(�

i

)

denotes

the

n

um

b

er

of

times

the

n-gram

�

i

o

ccurs

in

the

giv

en

data.

This

can

b

e

expanded

as

E

(p

i

jc(�

i

)

=

r

)

=

s

X

j

=

p(i

=

j

jc(�

i

)

=

r

)p

j

(0)

The

probabilit

y

p(i

=

j

jc(�

i

)

=

r

)

is

the

probabilit

y

that

an

unkno

wn

n-gram

�

i

with

r

coun

ts

is

actually

the

j

th

n-gram

�

j

(with

corresp

onding

frequency

p

j

).

W

e

can

rewrite

this

as

p(i

=

j

jc(�

i

)

=

r

)

=

p(c(�

j

)

=

r

)



P

s

j

=

p(c(�

j

)

=

r

)

=

�

N

r

�

p

r

j

(

�

p

j

)

N

�r



P

s

j

=

�

N

r

�

p

r

j

(

�

p

j

)

N

�r

=

p

r

j

(

�

p

j

)

N

�r



P

s

j

=

p

r

j

(

�

p

j

)

N

�r

where

N

=

P

s

j

=

c(�

j

),

the

total

n

um

b

er

of

coun

ts.

Substituting

this

in

to

equation

(0),

w

e

get

E

(p

i

jc(�

i

)

=

r

)

=

P

s

j

=

p

r

+

j

(

�

p

j

)

N

�r



P

s

j

=

p

r

j

(

�

p

j

)

N

�r

()

No

w,

consider

E

N

(n

r

),

the

exp

ected

n

um

b

er

of

n-grams

with

exactly

r

coun

ts

giv

en

that

there

are

a

total

of

N

coun

ts.

This

is

equal

to

the

sum

of

the

probabilit

y

that

eac

h

n-gram

has

exactly

r

coun

ts:

E

N

(n

r

)

=

s

X

j

=

p(c(�

j

)

=

r

)

=

s

X

j

=

�

N

r

�

p

r

j

(

�

p

j

)

N

�r

0


W

e

can

substitute

this

expression

in

to

equation

()

to

yield

E

(p

i

jc(�

i

)

=

r

)

=

r

+





N

+



E

N

+

(n

r

+

)



E

N

(n

r

)

This

is

an

estimate

for

the

exp

ected

probabilit

y

of

an

n-gram

�

i

with

r

coun

ts;

to

express

this

in

terms

of

a

corrected

coun

t

r

�

w

e

use

equation

(	)

to

get

r

�

=

N

p(�

i

)

=

N

r

+





N

+



E

N

+

(n

r

+

)



E

N

(n

r

)

�

(r

+

)

n

r

+



n

r

Notice

that

the

appro

ximations

E

N

(n

r

)

�

n

r

and

E

N

+

(n

r

+

)

�

n

r

+

are

used

in

the

ab

o

v

e

equation.

In

other

w

ords,

w

e

use

the

empirical

v

alues

of

n

r

to

estimate

what

their

exp

ected

v

alues

are.

The

Go

o

d-T

uring

estimate

cannot

b

e

used

when

n

r

=

0;

it

is

generally

necessary

to

\smo

oth"

the

n

r

,

e.g.,

to

adjust

the

n

r

so

that

they

are

all

ab

o

v

e

zero.

Recen

tly

,

Gale

and

Sampson

(		)

ha

v

e

prop

osed

a

simple

and

e�ectiv

e

algorithm

for

smo

othing

these

v

alues.

In

practice,

the

Go

o

d-T

uring

estimate

is

not

used

b

y

itself

for

n-gram

smo

othing,

b

ecause

it

do

es

not

include

the

com

bination

of

higher-order

mo

dels

with

lo

w

er-order

mo

dels

necessary

for

go

o

d

p

erformance,

as

discussed

in

the

follo

wing

sections.

Ho

w

ev

er,

it

is

used

as

a

to

ol

in

sev

eral

smo

othing

tec

hniques.



.

Jelinek-Merce

r

Smo

othing

Consider

the

case

of

constructing

a

bigram

mo

del

on

training

data

where

w

e

ha

v

e

that

c(burnish

the)

=

0

c(burnish

thou

)

=

0

Then,

according

to

b

oth

additiv

e

smo

othing

and

the

Go

o

d-T

uring

estimate,

w

e

will

ha

v

e

p(thejburnish

)

=

p(thou

jburnish

)

Ho

w

ev

er,

in

tuitiv

ely

w

e

should

ha

v

e

p(thejburnish

)

&gt;

p(thou

jburnish

)

b

ecause

the

w

ord

the

is

m

uc

h

more

common

than

the

w

ord

thou.

T

o

capture

this

b

eha

vior,

w

e

can

interp

olate

the

bigram

mo

del

with

a

unigr

am

mo

del.

A

unigr

am

mo

del

(or

-gram

mo

del)

conditions

the

probabilit

y

of

a

w

ord

on

no

other

w

ords,

and

just

re�ects

the

frequency

of

w

ords

in

text.

F

or

example,

the

maxim

um

lik

eliho

o

d

unigram

mo

del

is

p

ML

(w

i

)

=

c(w

i

)



P

w

i

c(w

i

)





One

issue

in

applying

the

Go

o

d-T

uring

estimate

is

deciding

whic

h

distribution

to

apply

it

to.

That

is,

w

e

can

apply

it

to

a

join

t

distribution

on

n-grams,

e.g.,

the

join

t

distribution

on

bigrams

p(w

i�

w

i

).

W

e

can

then

con

v

ert

the

corrected

coun

ts

r

�

in

to

conditional

probabilitie

s

p(w

i

jw

i�

).

Another

c

hoice,

ho

w

ev

er,

is

to

apply

it

to

eac

h

conditional

distribution

separately

,

e.g.,

to

the

distribution

p(w

i

jw

i�

)

for

eac

h

w

i�

.

With

the

former

strategy

,

there

is

plen

t

y

of

data

to

estimate

the

r

�

accurately;

ho

w

ev

er,

r

�

will

only

represen

t

a

go

o

d

aver

age

v

alue

o

v

er

all

conditional

distributio

ns.

The

ideal

adjustmen

t

of

a

coun

t

c

hanges

b

et

w

een

conditiona

l

distributions.

While

taking

the

latter

strategy

can

exhibit

this

b

eha

vior,

data

sparsit

y

is

a

problem

in

estimating

the

r

�

.

In

the

smo

othing

algorithms

to

b

e

describ

ed,

Katz

smo

othing

uses

the

former

strategy

,

while

the

latter

p

ersp

ectiv

e

can

b

e

view

ed

as

motiv

ating

Witten-Bell

smo

othing

and

absolute

discoun

ting

.




W

e

can

linearly

in

terp

olate

a

bigram

mo

del

and

unigram

mo

del

as

follo

ws:

p

in

terp

(w

i

jw

i�

)

=

�

p

ML

(w

i

jw

i�

)

+

(

�

�)

p

ML

(w

i

)

where

0

�

�

�

.

Because

p

ML

(the

jburnish)

=

p

ML

(thou

jburnish

)

=

0

while

presumably

p

ML

(the

)

�

p

ML

(thou

),

w

e

will

ha

v

e

that

p

in

terp

(thejburnish

)

&gt;

p

in

terp

(thou

jburnish

)

as

desired.

In

general,

it

is

useful

to

in

terp

olate

higher-order

n-gram

mo

dels

with

lo

w

er-order

n-gram

mo

dels,

b

ecause

when

there

is

insu�cien

t

data

to

estimate

a

probabilit

y

in

the

higher-order

mo

del,

the

lo

w

er-order

mo

del

can

often

pro

vide

useful

information.

A

general

class

of

in

terp

olated

mo

dels

is

describ

ed

b

y

Jelinek

and

Mercer

(	0).

An

elegan

t

w

a

y

of

p

erforming

this

in

terp

olation

is

giv

en

b

y

Bro

wn

et

al.

(		a)

as

follo

ws

p

in

terp

(w

i

jw

i�

i�n+

)

=

�

w

i�

i�n+

p

ML

(w

i

jw

i�

i�n+

)

+

(

�

�

w

i�

i�n+

)

p

in

terp

(w

i

jw

i�

i�n+

)

()

That

is,

the

nth-order

smo

othed

mo

del

is

de�ned

recursiv

ely

as

a

linear

in

terp

olation

b

et

w

een

the

nth-order

maxim

um

lik

eliho

o

d

mo

del

and

the

(n

�

)th-order

smo

othed

mo

del.

T

o

end

the

recursion,

w

e

can

tak

e

the

smo

othed

st-order

mo

del

to

b

e

the

maxim

um

lik

eliho

o

d

distribution,

or

w

e

can

tak

e

the

smo

othed

0th-order

mo

del

to

b

e

the

uniform

distribution

p

unif

(w

i

)

=





jV

j

Giv

en

�xed

p

ML

,

it

is

p

ossible

to

searc

h

e�cien

tly

for

the

�

w

i�

i�n+

that

maxim

ize

the

probabilit

y

of

some

data

using

the

Baum-W

elc

h

algorithm

(Baum,

	).

T

o

yield

meaningful

results,

the

data

used

to

estimate

the

�

w

i�

i�n+

need

to

b

e

di�eren

t

from

the

data

used

to

calculate

the

p

ML

.



In

held-out

interp

olation,

one

reserv

es

a

section

of

the

training

data

for

this

purp

ose,

where

this

held-

out

data

is

not

used

in

calculating

the

p

ML

.

Alternativ

ely

,

Jelinek

and

Mercer

describ

e

a

tec

hnique

kno

wn

as

delete

d

interp

olation

or

delete

d

estimation

where

di�eren

t

parts

of

the

training

data

rotate

in

training

either

the

p

ML

or

the

�

w

i�

i�n+

;

the

results

are

then

a

v

eraged.

Notice

that

the

optimal

�

w

i�

i�n+

will

b

e

di�eren

t

for

di�eren

t

histories

w

i�

i�n+

.

F

or

example,

for

a

con

text

w

e

ha

v

e

seen

thousands

of

times,

a

high

�

will

b

e

suitable

since

the

higher-order

distribution

will

b

e

v

ery

reliable;

for

a

history

that

has

o

ccurred

only

once,

a

lo

w

er

�

will

b

e

appropriate.

T

raining

eac

h

parameter

�

w

i�

i�n+

indep

enden

tly

is

not

generally

felicitous;

w

e

w

ould

need

an

enormous

amoun

t

of

data

to

train

so

man

y

indep

enden

t

parameters

accurately

.

Instead,

Jelinek

and

Mercer

suggest

dividing

the

�

w

i�

i�n+

in

to

a

mo

derate

n

um

b

er

of

partitions

or

buckets,

and

constraining

all

�

w

i�

i�n+

in

the

same

buc

k

et

to

ha

v

e

the

same

v

alue,

thereb

y

reducing

the

n

um

b

er

of

indep

enden

t

parameters

to

b

e

estimated.

Ideally

,

w

e

should

tie

together

those

�

w

i�

i�n+

that

w

e

ha

v

e

an

a

priori

reason

to

b

eliev

e

should

ha

v

e

similar

v

alues.

Bahl,

Jelinek,

and

Mercer

(	)

suggest

c

ho

osing

these

sets

of

�

w

i�

i�n+

according

to

P

w

i

c(w

i

i�n+

),

the

total

n

um

b

er

of

coun

ts

in

the

higher-order

distribution

b

eing

in

terp

olated

(whic

h

is

equal

to

the

n

um

b

er

of

coun

ts

of

the

corresp

onding

history).

As

touc

hed

on

ab

o

v

e,

this

total

coun

t

should

correlate

with

ho

w

strongly

the

higher-order

distribution

should

b

e

w

eigh

ted;

the

higher

this

coun

t,

the

higher

�

w

i�

i�n+





When

the

same

data

is

used

to

estimate

b

oth,

setting

all

�

w

i�

i�n+

to

one

yields

the

optimal

result.




should

b

e.

More

sp

eci�cally

,

Bahl

et

al.

suggest

partitioning

the

range

of

p

ossible

total

coun

t

v

alues

and

taking

all

�

w

i�

i�n+

asso

ciated

with

the

same

partition

to

b

e

in

the

same

buc

k

et.

In

previous

w

ork

(Chen,

		),

w

e

sho

w

that

buc

k

eting

according

to

the

a

v

erage

n

um

b

er

of

coun

ts

p

er

nonzero

elemen

t

in

a

distribution

P

w

i

c(w

i

i�n+

)



jw

i

:c(w

i

i�n+

)&gt;0j

yields

b

etter

p

erformance

than

using

the

v

alue

P

w

i

c(w

i

i�n+

).

.

Katz

Smo

othing

Katz

smo

othing

(	)

extends

the

in

tuitions

of

the

Go

o

d-T

uring

estimate

b

y

adding

the

com-

bination

of

higher-order

mo

dels

with

lo

w

er-order

mo

dels.

W

e

�rst

describ

e

Katz

smo

othing

for

bigram

mo

dels.

F

or

a

bigram

w

i

i�

with

coun

t

r

=

c(w

i

i�

),

w

e

calculate

its

corrected

coun

t

using

the

equation

c

k

atz

(w

i

i�

)

=

�

d

r

r

if

r

&gt;

0

�(w

i�

)

p

ML

(w

i

)

if

r

=

0

()

That

is,

all

bigrams

with

a

nonzero

coun

t

r

are

discoun

ted

according

to

a

disc

ount

r

atio

d

r

.

The

discoun

t

ratio

d

r

is

appro

ximately

r

�



r

,

the

discoun

t

predicted

b

y

the

Go

o

d-T

uring

estimate,

and

will

b

e

sp

eci�ed

exactly

later.

The

coun

ts

subtracted

from

the

nonzero

coun

ts

are

then

distributed

among

the

zero-coun

t

bigrams

according

to

the

next

lo

w

er-order

distribution,

i.e.,

the

unigram

mo

del.

The

v

alue

�(w

i�

)

is

c

hosen

so

that

the

total

n

um

b

er

of

coun

ts

in

the

distribution

P

w

i

c

k

atz

(w

i

i�

)

is

unc

hanged,

i.e.,

P

w

i

c

k

atz

(w

i

i�

)

=

P

w

i

c(w

i

i�

).

The

appropriate

v

alue

for

�(w

i�

)

is

�(w

i�

)

=



�

P

w

i

:c(w

i

i�

)&gt;0

p

k

atz

(w

i

jw

i�

)



P

w

i

:c(w

i

i�

)=0

p

ML

(w

i

)

=



�

P

w

i

:c(w

i

i�

)&gt;0

p

k

atz

(w

i

jw

i�

)





�

P

w

i

:c(w

i

i�

)&gt;0

p

ML

(w

i

)

T

o

calculate

p

k

atz

(w

i

jw

i�

)

from

the

corrected

coun

t,

w

e

just

normalize:

p

k

atz

(w

i

jw

i�

)

=

c

k

atz

(w

i

i�

)



P

w

i

c

k

atz

(w

i

i�

)

The

d

r

are

calculated

as

follo

ws:

large

coun

ts

are

tak

en

to

b

e

reliable,

so

they

are

not

discoun

ted.

In

particular,

Katz

tak

es

d

r

=



for

all

r

&gt;

k

for

some

k

,

where

Katz

suggests

k

=

.

The

discoun

t

ratios

for

the

lo

w

er

coun

ts

r

�

k

are

deriv

ed

from

the

Go

o

d-T

uring

estimate

applied

to

the

global

bigram

distribution;

that

is,

the

n

r

in

equation

()

denote

the

total

n

um

b

ers

of

bigrams

that

o

ccur

exactly

r

times

in

the

training

data.

These

d

r

are

c

hosen

suc

h

that

the

resulting

discoun

ts

are

prop

ortional

to

the

discoun

ts

predicted

b

y

the

Go

o

d-T

uring

estimate,

and

suc

h

that

the

total

n

um

b

er

of

coun

ts

discoun

ted

in

the

global

bigram

distribution

is

equal

to

the

total

n

um

b

er

of

coun

ts

that

should

b

e

assigned

to

bigrams

with

zero

coun

ts

according

to

the

Go

o

d-T

uring

estimate.



The

former

constrain

t

corresp

onds

to

the

equations



�

d

r

=

�(

�

r

�



r

)





In

the

normal

Go

o

d-T

uring

estimate,

the

total

n

um

b

er

of

coun

ts

discoun

ted

from

n-grams

with

nonzero

coun

ts

happ

ens

to

b

e

equal

to

the

total

n

um

b

er

of

coun

ts

assigned

to

n-grams

with

zero

coun

ts.

Th

us,

the

normalizatio

n

constan

t

for

a

smo

othed

distributio

n

is

iden

tical

to

that

of

the

original

distribution.

In

Katz

smo

othing,

Katz

tries

to

ac

hiev

e

a

similar

e�ect

except

through

discoun

ting

only

coun

ts

r

�

k

.




for

r



f;

:

:

:

;

k

g

for

some

constan

t

�.

The

Go

o

d-T

uring

estimate

predicts

that

the

total

n

um

b

er

of

coun

ts

that

should

b

e

assigned

to

bigrams

with

zero

coun

ts

is

n

0

0

�

=

n

0

n





n

0

=

n



,

so

the

second

constrain

t

corresp

onds

to

the

equation

k

X

r

=

n

r

(

�

d

r

)r

=

n



The

unique

solution

to

these

equations

is

giv

en

b

y

d

r

=

r

�



r

�

(k

+)n

k+



n







�

(k

+)n

k+



n



Katz

smo

othing

for

higher-order

n-gram

mo

dels

is

de�ned

analogously

.

As

w

e

can

see

in

equation

(),

the

bigram

mo

del

is

de�ned

in

terms

of

the

unigram

mo

del;

in

general,

the

Katz

n-gram

mo

del

is

de�ned

in

terms

of

the

Katz

(n

�

)-gram

mo

del,

similar

to

Jelinek-Mercer

smo

othing.

T

o

end

the

recursion,

the

Katz

unigram

mo

del

is

tak

en

to

b

e

the

maxim

um

lik

eliho

o

d

unigram

mo

del.

Recall

that

w

e

men

tioned

in

Section

.

that

it

is

usually

necessary

to

smo

oth

n

r

when

using

the

Go

o

d-T

uring

estimate,

e.g.,

for

those

n

r

that

are

v

ery

lo

w.

Ho

w

ev

er,

in

Katz

smo

othing

this

is

not

essen

tial

b

ecause

the

Go

o

d-T

uring

estimate

is

only

used

for

small

coun

ts

r

�

k

,

and

n

r

is

generally

fairly

high

for

these

v

alues

of

r

.

.

Witten-Bell

Smo

othing

Witten-Bell

smo

othing

(Bell,

Cleary

,

and

Witten,

		0;

Witten

and

Bell,

		)



w

as

dev

elop

ed

for

the

task

of

text

compression,

and

can

b

e

considered

to

b

e

an

instance

of

Jelinek-Mercer

smo

oth-

ing.

In

particular,

the

nth-order

smo

othed

mo

del

is

de�ned

recursiv

ely

as

a

linear

in

terp

olation

b

et

w

een

the

nth-order

maxim

um

lik

eliho

o

d

mo

del

and

the

(n

�

)th-order

smo

othed

mo

del

as

in

equation

():

p

WB

(w

i

jw

i�

i�n+

)

=

�

w

i�

i�n+

p

ML

(w

i

jw

i�

i�n+

)

+

(

�

�

w

i�

i�n+

)

p

WB

(w

i

jw

i�

i�n+

)

()

T

o

compute

the

parameters

�

w

i�

i�n+

for

Witten-Bell

smo

othing,

w

e

will

need

to

use

the

n

um

b

er

of

unique

w

ords

that

follo

w

the

history

w

i�

i�n+

.

W

e

will

write

this

v

alue

as

N

+

(w

i�

i�n+

�),

formally

de�ned

as

N

+

(w

i�

i�n+

�)

=

jfw

i

:

c(w

i�

i�n+

w

i

)

&gt;

0gj

()

The

notation

N

+

is

mean

t

to

ev

ok

e

the

n

um

b

er

of

w

ords

that

ha

v

e

one

or

more

coun

ts,

and

the

�

is

mean

t

to

ev

ok

e

a

free

v

ariable

that

is

summed

o

v

er.

W

e

can

then

assign

the

parameters

�

w

i�

i�n+

for

Witten-Bell

smo

othing

suc

h

that





�

�

w

i�

i�n+

=

N

+

(w

i�

i�n+

�)



N

+

(w

i�

i�n+

�)

+

P

w

i

c(w

i

i�n+

)

()





Witten-Bell

smo

othing

refers

to

metho

d

C

in

these

references.



Di�eren

t

notation

is

used

in

the

original

text

(Bell,

Cleary

,

and

Witten,

		0).

The

order

o

in

the

original

text

corresp

onds

to

our

n

�

,

the

escap

e

probabilit

y

e

o

corresp

onds

to



�

�

w

i�

i�n+

,

q

o

corresp

ond

s

to

N

+

(w

i�

i�n+

�),

and

C

o

corresp

onds

to

P

w

i

c(w

i

i�n+

).




Substituting,

w

e

can

rewrite

equation

()

as

p

WB

(w

i

jw

i�

i�n+

)

=

c(w

i

i�n+

)

+

N

+

(w

i�

i�n+

�)p

WB

(w

i

jw

i�

i�n+

)



P

w

i

c(w

i

i�n+

)

+

N

+

(w

i�

i�n+

�)

()

T

o

motiv

ate

Witten-Bell

smo

othing,

w

e

can

in

terpret

equation

()

as

sa

ying:

with

probabilit

y

�

w

i�

i�n+

w

e

should

use

the

higher-order

mo

del,

and

with

probabilit

y



�

�

w

i�

i�n+

w

e

should

use

the

lo

w

er-order

mo

del.

It

seems

reasonable

that

w

e

should

use

the

higher-order

mo

del

if

the

corresp

onding

n-gram

o

ccurs

in

the

training

data,

and

bac

k

o�

to

the

lo

w

er-order

mo

del

otherwise.

Then,

w

e

should

tak

e

the

term



�

�

w

i�

i�n+

to

b

e

the

probabilit

y

that

a

w

ord

not

observ

ed

after

the

history

w

i�

i�n+

in

the

training

data

o

ccurs

after

that

history

.

T

o

estimate

the

frequency

of

these

no

v

el

w

ords,

imagine

tra

v

ersing

the

training

data

in

order

and

coun

ting

ho

w

man

y

times

the

w

ord

follo

wing

the

history

w

i�

i�n+

di�ers

from

the

w

ords

in

all

suc

h

previous

ev

en

ts.

The

n

um

b

er

of

suc

h

ev

en

ts

is

simply

N

+

(w

i�

i�n+

�),

the

n

um

b

er

of

unique

w

ords

that

follo

w

the

history

w

i�

i�n+

.

Equation

()

can

b

e

view

ed

as

an

appro

ximation

of

this

in

tuition.

The

Go

o

d-T

uring

estimate

pro

vides

another

p

ersp

ectiv

e

on

the

estimation

of

the

probabilit

y

of

no

v

el

w

ords

follo

wing

a

history

.

The

Go

o

d-T

uring

estimate

predicts

that

the

probabilit

y

of

an

ev

en

t

not

seen

in

the

training

data

(using

the

notation

from

Section

.)

is

n





N

,

the

fraction

of

coun

ts

dev

oted

to

items

that

o

ccur

exactly

once.

T

ranslating

this

v

alue

in

to

the

previous

notation,

w

e

get

N



(w

i�

i�n+

�)



P

w

i

c(w

i

i�n+

)

where

N



(w

i�

i�n+

�)

=

jfw

i

:

c(w

i�

i�n+

w

i

)

=

gj

Equation

()

can

b

e

seen

as

an

appro

ximation

to

the

Go

o

d-T

uring

estimate,

where

the

n

um

b

er

of

w

ords

with

at

le

ast

one

coun

t

is

used

in

place

of

the

n

um

b

er

of

w

ords

with

exactly

one

coun

t.

Extensiv

e

comparisons

b

et

w

een

Witten-Bell

smo

othing

and

other

smo

othing

tec

hniques

for

text

compression

are

presen

ted

in

(Bell,

Cleary

,

and

Witten,

		0)

and

(Witten

and

Bell,

		);

ho

w

ev

er,

comparisons

with

smo

othing

tec

hniques

used

in

language

mo

deling

are

not

rep

orted.

T

ext

compression

applications

ha

v

e

requiremen

ts,

suc

h

as

the

abilit

y

to

build

mo

dels

v

ery

e�cien

tly

and

incremen

tally

,

that

w

e

do

not

consider

in

this

w

ork.

.

Absolute

Discoun

ting

Absolute

discoun

ting

(Ney

,

Essen,

and

Kneser,

		),

lik

e

Jelinek-Mercer

smo

othing,

in

v

olv

es

the

in

terp

olation

of

higher-

and

lo

w

er-order

mo

dels.

Ho

w

ev

er,

instead

of

m

ultiplying

the

higher-order

maxim

um

-l

ik

eli

ho

o

d

distribution

b

y

a

factor

�

w

i�

i�n+

,

the

higher-order

distribution

is

created

b

y

subtracting

a

�xed

discoun

t

D

�



from

eac

h

nonzero

coun

t.

That

is,

instead

of

equation

():

p

in

terp

(w

i

jw

i�

i�n+

)

=

�

w

i�

i�n+

p

ML

(w

i

jw

i�

i�n+

)

+

(

�

�

w

i�

i�n+

)

p

in

terp

(w

i

jw

i�

i�n+

)

w

e

ha

v

e

p

abs

(w

i

jw

i�

i�n+

)

=

max

fc(w

i

i�n+

)

�

D

;

0g



P

w

i

c(w

i

i�n+

)

+

(

�

�

w

i�

i�n+

)

p

abs

(w

i

jw

i�

i�n+

)

()




T

o

mak

e

this

distribution

sum

to

,

w

e

tak

e



�

�

w

i�

i�n+

=

D



P

w

i

c(w

i

i�n+

)

N

+

(w

i�

i�n+

�)

(	)

where

N

+

(w

i�

i�n+

�)

is

de�ned

as

in

equation

()

and

where

w

e

assume

0

�

D

�

.

Ney

,

Essen,

and

Kneser

(		)

suggest

setting

D

through

deleted

estimation

on

the

training

data.

They

arriv

e

at

the

estimate

D

=

n





n



+

n



(0)

where

n



and

n



are

the

total

n

um

b

er

of

n-grams

with

exactly

one

and

t

w

o

coun

ts,

resp

ectiv

ely

,

in

the

training

data,

where

n

is

the

order

of

the

higher-order

mo

del

b

eing

in

terp

olated.

W

e

can

motiv

ate

absolute

discoun

ting

using

the

Go

o

d-T

uring

estimate.

Ch

urc

h

and

Gale

(		)

sho

w

empirically

that

the

a

v

erage

Go

o

d-T

uring

discoun

t

(r

�

r

�

)

asso

ciated

with

n-grams

with

larger

coun

ts

(r

�

)

is

largely

constan

t

o

v

er

r

.

F

urther

supp

orting

evidence

is

presen

ted

in

Section

...

F

urthermore,

the

scaling

factor

in

equation

(	)

is

similar

to

the

analogous

factor

for

Witten-Bell

smo

othing

giv

en

in

equation

()

as

describ

ed

in

Section

.,

and

can

b

e

view

ed

as

appro

ximating

the

same

v

alue,

the

probabilit

y

of

a

no

v

el

w

ord

follo

wing

a

history

.

.

Kneser-Ney

Smo

othing

Kneser

and

Ney

(		)

ha

v

e

in

tro

duced

an

extension

of

absolute

discoun

ting

where

the

lo

w

er-

order

distribution

that

one

com

bines

with

a

higher-order

distribution

is

built

in

a

no

v

el

manner.

In

previous

algorithms,

the

lo

w

er-order

distribution

is

generally

tak

en

to

b

e

a

smo

othed

v

ersion

of

the

lo

w

er-order

maxim

um

lik

eliho

o

d

distribution.

Ho

w

ev

er,

a

lo

w

er-order

distribution

is

a

signi�can

t

factor

in

the

com

bined

mo

del

only

when

few

or

no

coun

ts

are

presen

t

in

the

higher-

order

distribution.

Consequen

tly

,

they

should

b

e

optimized

to

p

erform

w

ell

in

these

situations.

T

o

giv

e

a

concrete

example,

consider

building

a

bigram

mo

del

on

data

where

there

exists

a

w

ord

that

is

v

ery

common,

sa

y

Francisco,

that

o

ccurs

only

after

a

single

w

ord,

sa

y

San.

Since

c(Francisco)

is

high,

the

unigram

probabilit

y

p(Francisco)

will

b

e

high

and

an

algorithm

suc

h

as

absolute

discoun

ting

will

assign

a

relativ

ely

high

probabilit

y

to

the

w

ord

Francisco

o

ccurring

after

no

v

el

bigram

histories.

Ho

w

ev

er,

in

tuitiv

ely

this

probabilit

y

should

not

b

e

high

since

in

the

training

data

the

w

ord

Francisco

follo

ws

only

a

single

history

.

That

is,

p

erhaps

the

w

ord

Francisco

should

receiv

e

a

lo

w

unigram

probabilit

y

b

ecause

the

only

time

the

w

ord

o

ccurs

is

when

the

last

w

ord

is

San,

in

whic

h

case

the

bigram

probabilit

y

mo

dels

its

probabilit

y

w

ell.

Extending

this

line

of

reasoning,

p

erhaps

the

unigram

probabilit

y

used

should

not

b

e

prop

or-

tional

to

the

n

um

b

er

of

o

ccurrences

of

a

w

ord,

but

instead

to

the

n

um

b

er

of

di�eren

t

w

ords

that

it

follo

ws.

T

o

giv

e

an

in

tuitiv

e

argumen

t,

imagine

tra

v

ersing

the

training

data

in

order

and

building

a

bigram

mo

del

on

the

preceding

data

to

predict

the

curren

t

w

ord.

Then,

whenev

er

the

curren

t

bigram

do

es

not

o

ccur

in

the

preceding

data,

the

unigram

probabilit

y

will

b

e

a

large

factor

in

the

curren

t

bigram

probabilit

y

.

If

w

e

assign

a

coun

t

to

the

corresp

onding

unigram

whenev

er

suc

h

an

ev

en

t

o

ccurs,

then

the

n

um

b

er

of

coun

ts

assigned

to

eac

h

unigram

will

simply

b

e

the

n

um

b

er

of

di�eren

t

w

ords

that

it

follo

ws.

In

fact,

in

Kneser-Ney

smo

othing

the

unigram

probabilit

y

in

a

bigram

mo

del

is

calculated

in

this

manner;

ho

w

ev

er,

this

calculation

is

motiv

ated

in

an

en

tirely

di�eren

t

manner

in

the

original

pap

er.

The

motiv

ation

giv

en

in

the

original

text

is

that

w

e

should

select

the

lo

w

er-order

distribution

suc

h

that

the

marginals

of

the

higher-order

smo

othed

distribution

should

matc

h

the

marginals

of

the

training

data.

F

or

example,

for

a

bigram

mo

del

w

e

w

ould

lik

e

to

select

a

smo

othed

distribution




p

KN

that

satis�es

the

follo

wing

constrain

t

on

unigram

marginals

for

all

w

i

:

X

w

i�

p

KN

(w

i�

w

i

)

=

c(w

i

)



P

w

i

c(w

i

)

()

The

left-hand

side

of

this

equation

is

the

unigram

marginal

for

w

i

of

the

smo

othed

bigram

distri-

bution

p

KN

,

and

the

righ

t-hand

side

is

the

unigram

frequency

of

w

i

found

in

the

training

data.

Here,

w

e

presen

t

a

di�eren

t

deriv

ation

of

the

resulting

distribution

than

is

presen

ted

b

y

Kneser

and

Ney

(		).

W

e

assume

that

the

mo

del

has

the

form

giv

en

in

equation

()

p

KN

(w

i

jw

i�

i�n+

)

=

max

fc(w

i

i�n+

)

�

D

;

0g



P

w

i

c(w

i

i�n+

)

+

D



P

w

i

c(w

i

i�n+

)

N

+

(w

i�

i�n+

�)

p

KN

(w

i

jw

i�

i�n+

)

()

as

opp

osed

to

the

form

used

in

the

original

pap

er

p

KN

(w

i

jw

i�

i�n+

)

=



&lt;

:

maxfc(w

i

i�n+

)�D

;0g



P

w

i

c(w

i

i�n+

)

if

c(w

i

i�n+

)

&gt;

0

�

(w

i�

i�n+

)p

KN

(w

i

jw

i�

i�n+

)

if

c(w

i

i�n+

)

=

0

where

�

(w

i�

i�n+

)

is

c

hosen

to

mak

e

the

distribution

sum

to

.

That

is,

w

e

in

terp

olate

the

lo

w

er-

order

distribution

with

all

w

ords,

not

just

with

w

ords

that

ha

v

e

zero

coun

ts

in

the

higher-order

distribution.

(Using

the

terminology

to

b

e

de�ned

in

Section

.,

w

e

use

an

interp

olate

d

mo

del

instead

of

a

b

acko�

mo

del.)

W

e

use

this

form

ulation

b

ecause

it

leads

to

a

cleaner

deriv

ation

of

essen

tially

the

same

form

ula;

no

appro

ximations

are

required

as

in

the

original

deriv

ation.

In

addition,

as

will

b

e

sho

wn

later

in

this

pap

er,

the

former

form

ulation

generally

yields

b

etter

p

erformance.

No

w,

our

aim

is

to

�nd

a

unigram

distribution

p

KN

(w

i

)

suc

h

that

the

constrain

ts

giv

en

b

y

equation

()

are

satis�ed.

Expanding

equation

(),

w

e

get

c(w

i

)



P

w

i

c(w

i

)

=

X

w

i�

p

KN

(w

i

jw

i�

)p(w

i�

)

F

or

p(w

i�

),

w

e

simply

tak

e

the

distribution

found

in

the

training

data

p(w

i�

)

=

c(w

i�

)



P

w

i�

c(w

i�

)

Substituting

and

simplifying,

w

e

ha

v

e

c(w

i

)

=

X

w

i�

c(w

i�

)p

KN

(w

i

jw

i�

)

Substituting

in

equation

(),

w

e

ha

v

e

c(w

i

)

=

X

w

i�

c(w

i�

)

"

max

fc(w

i�

w

i

)

�

D

;

0g



P

w

i

c(w

i�

w

i

)

+

D



P

w

i

c(w

i�

w

i

)

N

+

(w

i�

�)

p

KN

(w

i

)

#

=

X

w

i�

:c(w

i�

w

i

)&gt;0

c(w

i�

)

c(w

i�

w

i

)

�

D



c(w

i�

)

+

X

w

i�

c(w

i�

)

D



c(w

i�

)

N

+

(w

i�

�)

p

KN

(w

i

)

=

c(w

i

)

�

N

+

(�w

i

)D

+

D

p

KN

(w

i

)

X

w

i�

N

+

(w

i�

�)

=

c(w

i

)

�

N

+

(�w

i

)D

+

D

p

KN

(w

i

)N

+

(��)








algorithm



�(w

i

jw

i�

i�n+

)



�

(w

i�

i�n+

)



p

smo

oth

(w

i

jw

i�

i�n+

)







additiv

e



c(w

i

i�n+

)+�



P

w

i

c(w

i

i�n+

)+�

jV

j



0



n.a.







Jelinek-Mercer



�

w

i�

i�n+

p

ML

()

+

:

:

:



(

�

�

w

i�

i�n+

)



p

in

terp

(w

i

jw

i�

i�n+

)







Katz



d

r

r



P

w

i

c(w

i

i�n+

)



�

P

w

i

:c(w

i

i�n+

)&gt;0

p

k

atz

(w

i

jw

i�

i�n+

)



P

w

i

:c(w

i

i�n+

)=0

p

k

atz

(w

i

jw

i�

i�n+

)



p

k

atz

(w

i

jw

i�

i�n+

)







Witten-Bell



(

�

�

(w

i�

i�n+

))p

ML

()

+

:

:

:



N

+

(w

i�

i�n+

�)



N

+

(w

i�

i�n+

�)+

P

w

i

c(w

i

i�n+

)



p

WB

(w

i

jw

i�

i�n+

)







absolute

disc.



maxfc(w

i

i�n+

)�D

;0g



P

w

i

c(w

i

i�n+

)

+

:

:

:



D



P

w

i

c(w

i

i�n+

)

N

+

(w

i�

i�n+

�)



p

abs

(w

i

jw

i�

i�n+

)







Kneser-Ney

(in

terp

olated)



maxfc(w

i

i�n+

)�D

;0g



P

w

i

c(w

i

i�n+

)

+

:

:

:



D



P

w

i

c(w

i

i�n+

)

N

+

(w

i�

i�n+

�)



N

+

(�w

i

i�n+

)



N

+

(�w

i�

i�n+

�)





T

able

:

Summary

of

smo

othing

algorithms

using

notation

from

equation

();

the

tok

en

\.

.

.

"

represen

ts

the

term

�

(w

i�

i�n+

)p

smo

oth

(w

i

jw

i�

i�n+

)

corresp

onding

to

in

terp

olation

with

a

lo

w

er-

order

distribution;

the

notation

p

ML

()

is

an

abbreviation

for

p

ML

(w

i

jw

i�

i�n+

)

where

N

+

(�w

i

)

=

jfw

i�

:

c(w

i�

w

i

)

&gt;

0gj

is

the

n

um

b

er

of

di�eren

t

w

ords

w

i�

that

precede

w

i

in

the

training

data

and

where

N

+

(��)

=

X

w

i�

N

+

(w

i�

�)

=

jf(w

i�

;

w

i

)

:

c(w

i�

w

i

)

&gt;

0gj

=

X

w

i

N

+

(�w

i

)

Solving

for

p

KN

(w

i

),

w

e

get

p

KN

(w

i

)

=

N

+

(�w

i

)



N

+

(��)

Generalizing

to

higher-order

mo

dels,

w

e

ha

v

e

that

p

KN

(w

i

jw

i�

i�n+

)

=

N

+

(�w

i

i�n+

)



N

+

(�w

i�

i�n+

�)

()

where

N

+

(�w

i

i�n+

)

=

jfw

i�n+

:

c(w

i

i�n+

)

&gt;

0gj

N

+

(�w

i�

i�n+

�)

=

jf(w

i�n+

;

w

i

)

:

c(w

i

i�n+

)

&gt;

0gj

=

X

w

i

N

+

(�w

i

i�n+

)

.

Algorithm

Summary

As

noted

b

y

Kneser

and

Ney

(		),

most

existing

smo

othing

algorithms

can

b

e

describ

ed

with

the

follo

wing

equation

p

smo

oth

(w

i

jw

i�

i�n+

)

=

�

�(w

i

jw

i�

i�n+

)

if

c(w

i

i�n+

)

&gt;

0

�

(w

i�

i�n+

)p

smo

oth

(w

i

jw

i�

i�n+

)

if

c(w

i

i�n+

)

=

0

()




That

is,

if

an

n-gram

has

a

nonzero

coun

t

then

w

e

use

the

distribution

�(w

i

jw

i�

i�n+

).

Otherwise,

w

e

b

acko�

to

the

lo

w

er-order

distribution

p

smo

oth

(w

i

jw

i�

i�n+

),

where

the

scaling

factor

�

(w

i�

i�n+

)

is

c

hosen

to

mak

e

the

conditional

distribution

sum

to

one.

W

e

refer

to

algorithms

that

fall

directly

in

this

framew

ork

as

b

acko�

mo

dels.

Katz

smo

othing

is

the

canonical

example

of

bac

k

o�

smo

othing.

Sev

eral

smo

othing

algorithms

are

expressed

as

the

linear

in

terp

olation

of

higher-

and

lo

w

er-

order

n-gram

mo

dels

as

in

equation

()

p

smo

oth

(w

i

jw

i�

i�n+

)

=

�

w

i�

i�n+

p

ML

(w

i

jw

i�

i�n+

)

+

(

�

�

w

i�

i�n+

)

p

smo

oth

(w

i

jw

i�

i�n+

)

W

e

can

rewrite

this

as

p

smo

oth

(w

i

jw

i�

i�n+

)

=

�

0

(w

i

jw

i�

i�n+

)

+

�

(w

i�

i�n+

)p

smo

oth

(w

i

jw

i�

i�n+

)

where

�

0

(w

i

jw

i�

i�n+

)

=

�

w

i�

i�n+

p

ML

(w

i

jw

i�

i�n+

)

and

�

(w

i�

i�n+

)

=



�

�

w

i�

i�n+

.

Then,

b

y

taking

�(w

i

jw

i�

i�n+

)

=

�

0

(w

i

jw

i�

i�n+

)

+

�

(w

i�

i�n+

)p

smo

oth

(w

i

jw

i�

i�n+

)

()

w

e

see

that

these

mo

dels

can

b

e

placed

in

the

form

of

equation

().

W

e

refer

to

mo

dels

of

this

form

as

interp

olate

d

mo

dels,

The

k

ey

di�erence

b

et

w

een

bac

k

o�

and

in

terp

olated

mo

dels

is

that

in

determining

the

prob-

abilit

y

of

n-grams

with

nonzer

o

coun

ts,

in

terp

olated

mo

dels

use

information

from

lo

w

er-order

distributions

while

bac

k

o�

mo

dels

do

not.

In

b

oth

bac

k

o�

and

in

terp

olated

mo

dels,

lo

w

er-order

distributions

are

used

in

determining

the

probabilit

y

of

n-grams

with

zer

o

coun

ts.

In

T

able

,

w

e

summarize

all

of

the

smo

othing

algorithms

describ

ed

earlier

in

terms

of

equa-

tion

().

F

or

in

terp

olated

mo

dels,

w

e

use

the

notation

\..."

as

shorthand

for

the

last

term

in

equation

().

W

e

note

that

it

is

easy

to

create

a

bac

k

o�

v

ersion

of

an

in

terp

olated

algorithm.

Instead

of

using

equation

(),

w

e

can

just

tak

e

�(w

i

jw

i�

i�n+

)

=

�

0

(w

i

jw

i�

i�n+

)

and

then

adjust

�

(w

i�

i�n+

)

appropriately

so

that

probabilities

sum

to

one.

As

describ

ed

later,

w

e

ha

v

e

implem

en

ted

the

in

terp

olated

and

bac

k

o�

v

ersions

of

sev

eral

algorithms.

.	

Other

Smo

othing

T

ec

hniques

In

this

section,

w

e

brie�y

describ

e

sev

eral

smo

othing

algorithms

that

are

not

widely

used,

but

whic

h

are

in

teresting

from

a

theoretical

p

ersp

ectiv

e.

The

algorithms

in

this

section

w

ere

not

re-implemen

ted

in

this

researc

h,

while

all

preceding

algorithms

w

ere.

.	.

Ch

urc

h-Gale

Smo

othing

Ch

urc

h

and

Gale

(		)

describ

e

a

smo

othing

metho

d

that

lik

e

Katz's,

com

bines

the

Go

o

d-T

uring

estimate

with

a

metho

d

for

merging

the

information

from

lo

w

er-

and

higher-order

mo

dels.

W

e

describ

e

this

metho

d

for

bigram

mo

dels.

T

o

motiv

ate

this

metho

d,

consider

using

the

Go

o

d-T

uring

estimate

directly

to

build

a

bigram

distribution.

F

or

eac

h

bigram

with

coun

t

r

,

w

e

w

ould

assign

a

corrected

coun

t

of

r

�

=

(r

+

)

n

r

+



n

r

.

As

noted

in

Section

.,

this

has

the

undesirable

	


e�ect

of

giving

all

bigrams

with

zero

coun

t

the

same

corrected

coun

t;

instead,

unigram

frequencies

should

b

e

tak

en

in

to

accoun

t.

Consider

the

corrected

coun

t

assigned

b

y

an

in

terp

olativ

e

mo

del

to

a

bigram

w

i

i�

with

zero

coun

ts.

In

suc

h

a

mo

del,

w

e

w

ould

ha

v

e

p(w

i

jw

i�

)

/

p(w

i

)

for

a

bigram

with

zero

coun

ts.

T

o

con

v

ert

this

probabilit

y

to

a

coun

t,

w

e

m

ultiply

b

y

the

total

n

um

b

er

of

coun

ts

in

the

distribution

to

get

p(w

i

jw

i�

)

X

w

i

c(w

i

i�

)

/

p(w

i

)

X

w

i

c(w

i

i�

)

=

p(w

i

)c(w

i�

)

/

p(w

i

)p(w

i�

)

Th

us,

p(w

i�

)p(w

i

)

ma

y

b

e

a

go

o

d

indicator

of

the

corrected

coun

t

of

a

bigram

w

i

i�

with

zero

coun

ts.

In

Ch

urc

h-Gale

smo

othing,

bigrams

w

i

i�

are

partitioned

or

buckete

d

b

y

the

corresp

onding

v

alue

of

p

ML

(w

i�

)p

ML

(w

i

).

That

is,

they

divide

the

range

of

p

ossible

p

ML

(w

i�

)p

ML

(w

i

)

v

alues

in

to

a

n

um

b

er

of

partitions,

and

all

bigrams

asso

ciated

with

the

same

subrange

are

considered

to

b

e

in

the

same

buc

k

et.

Then,

eac

h

buc

k

et

is

treated

as

a

distinct

probabilit

y

distribution

and

Go

o

d-T

uring

estimation

is

p

erformed

within

eac

h.

F

or

a

bigram

in

buc

k

et

b

with

r

b

coun

ts,

w

e

calculate

its

corrected

coun

t

r

�

b

as

r

�

b

=

(r

b

+

)

n

b;r

+



n

b;r

where

the

coun

ts

n

b;r

include

only

those

bigrams

within

buc

k

et

b.

Ch

urc

h

and

Gale

partition

the

range

of

p

ossible

p

ML

(w

i�

)p

ML

(w

i

)

v

alues

in

to

ab

out



buc

k

ets,

with

three

buc

k

ets

in

eac

h

factor

of

0.

T

o

smo

oth

the

n

b;r

for

the

Go

o

d-T

uring

estimate,

they

use

a

smo

other

b

y

Shirey

and

Hastie

(	).

While

extensiv

e

empirical

analysis

is

rep

orted,

they

presen

t

only

a

single

en

trop

y

result,

com-

paring

the

ab

o

v

e

smo

othing

tec

hnique

with

another

smo

othing

metho

d

in

tro

duced

in

their

pap

er,

extende

d

delete

d

estimation.

In

our

previous

w

ork

(Chen,

		),

w

e

presen

t

further

results,

indi-

cating

that

this

smo

othing

w

orks

w

ell

for

bigram

language

mo

dels.

When

extending

this

metho

d

to

trigram

mo

dels,

there

are

t

w

o

options

for

implem

en

tation.

Unfortunately

,

one

of

these

metho

ds

is

computationally

in

tractable,

and

w

e

ha

v

e

demonstrated

that

the

other

p

erforms

p

o

orly

.

.	.

Ba

y

esian

Smo

othing

Sev

eral

smo

othing

tec

hniques

are

motiv

ated

within

a

Ba

y

esian

framew

ork.

A

prior

distribution

o

v

er

smo

othed

distributions

is

selected,

and

this

prior

is

used

to

someho

w

arriv

e

at

a

�nal

smo

othed

distribution.

F

or

example,

Nadas

(	)

selects

smo

othed

probabilities

to

b

e

their

mean

a

p

oste-

riori

v

alue

giv

en

the

prior

distribution.

Nadas

(	)

h

yp

othesizes

a

prior

distribution

from

the

family

of

b

eta

functions.

N�

adas

rep

orts

results

on

a

single

training

set

indicating

that

N�

adas

smo

othing

p

erforms

sligh

tly

w

orse

than

Katz

and

Jelinek-Mercer

smo

othing.

MacKa

y

and

P

eto

(		)

use

Diric

hlet

priors

in

an

attempt

to

motiv

ate

the

linear

in

terp

olation

used

in

Jelinek-Mercer

smo

othing.

They

compare

their

metho

d

with

Jelinek-Mercer

smo

othing

on

a

single

training

set

of

ab

out

t

w

o

million

w

ords;

their

results

indicate

that

MacKa

y-P

eto

smo

othing

p

erforms

sligh

tly

w

orse

than

Jelinek-Mercer

smo

othing.

0




Mo

di�ed

Kneser-Ney

Smo

othing

In

this

section,

w

e

in

tro

duce

a

no

v

el

v

ariation

of

Kneser-Ney

smo

othing,

whic

h

w

e

refer

to

as

mo

di�e

d

Kneser-Ney

smo

othing,

that

w

e

ha

v

e

found

has

excellen

t

p

erformance.

Instead

of

using

a

single

discoun

t

D

for

all

nonzero

coun

ts

as

in

Kneser-Ney

smo

othing,

w

e

ha

v

e

three

di�eren

t

parameters,

D



,

D



,

and

D

+

,

that

are

applied

to

n-grams

with

one,

t

w

o,

and

three

or

more

coun

ts,

resp

ectiv

ely

.

In

other

w

ords,

instead

of

using

equation

()

from

Section

.,

w

e

tak

e

p

KN

(w

i

jw

i�

i�n+

)

=

c(w

i

i�n+

)

�

D

(c(w

i

i�n+

))



P

w

i

c(w

i

i�n+

)

+

�

(w

i�

i�n+

)p

KN

(w

i

jw

i�

i�n+

)

where

D

(c)

=



&gt;

&gt;

&lt;

&gt;

&gt;

:

0

if

c

=

0

D



if

c

=



D



if

c

=



D

+

if

c

�



T

o

mak

e

the

distribution

sum

to

,

w

e

tak

e

�

(w

i�

i�n+

)

=

D



N



(w

i�

i�n+

�)

+

D



N



(w

i�

i�n+

�)

+

D

+

N

+

(w

i�

i�n+

�)



P

w

i

c(w

i

i�n+

)

where

N



(w

i�

i�n+

�)

and

N

+

(w

i�

i�n+

�)

are

de�ned

analogously

to

N



(w

i�

i�n+

�).

This

mo

di�cation

is

motiv

ated

b

y

evidence

to

b

e

presen

ted

in

Section

..

that

the

ideal

a

v

erage

discoun

t

for

n-grams

with

one

or

t

w

o

coun

ts

is

substan

tially

di�eren

t

from

the

ideal

a

v

erage

discoun

t

for

n-grams

with

higher

coun

ts.

Indeed,

w

e

will

see

later

that

mo

di�ed

Kneser-

Ney

smo

othing

signi�can

tly

outp

erforms

regular

Kneser-Ney

smo

othing.

Just

as

Ney

,

Essen,

and

Kneser

(		)

ha

v

e

dev

elop

ed

an

estimate

for

the

optimal

D

for

absolute

discoun

ting

and

Kneser-Ney

smo

othing

as

a

function

of

training

data

coun

ts

(as

giv

en

in

equation

(0)),

it

is

p

ossible

to

create

analogous

equations

to

estimate

the

optimal

v

alues

for

D



,

D



,

and

D



(Ries,

		).

The

analogous

relations

for

mo

di�ed

Kneser-Ney

smo

othing

are

Y

=

n





n



+

n



D



=



�

Y

n





n



D



=



�

Y

n





n



D

+

=



�

Y

n





n



()



Exp

erimen

tal

Metho

dology

In

this

section,

w

e

describ

e

the

details

of

our

smo

othing

algorithm

implem

en

tations,

ho

w

w

e

c

hose

parameter

v

alues

for

algorithms

with

parameters,

the

data

sets

w

e

used,

and

other

asp

ects

of

our

exp

erimen

tal

metho

dology

.

Brie�y

,

w

e

implem

en

ted

all

of

the

most

widely-used

smo

othing

algorithms

for

language

mo

deling:

additiv

e

smo

othing,

Jelinek-Mercer

smo

othing,

Katz

smo

othing,

Witten-Bell

smo

othing,

absolute

discoun

ting,

and

Kneser-Ney

smo

othing.

In

addition,

w

e

selected

a

simple

instance

of

Jelinek-Mercer

smo

othing

to

serv

e

as

a

baseline,

and

w

e

implemen

ted

our

mo

di�ed

v

ersion

of

Kneser-Ney

smo

othing.

W

e

compared

these

smo

othing

algorithms

using

text




from

the

Bro

wn

corpus,

the

North

American

Business

news

corpus,

the

Switc

h

b

oard

corpus,

and

the

Broadcast

News

corpus.

It

should

b

e

noted

that

there

exist

language

mo

deling

to

olkits

(Rosenfeld,

		;

Clarkson

and

Rosenfeld,

		)

whic

h

can

b

e

used

to

build

smo

othed

n-gram

mo

dels

using

a

v

ariet

y

of

smo

othing

algorithms,

including

Katz

smo

othing,

Jelinek-Mercer

smo

othing,

absolute

discoun

ting,

and

Witten-Bell

smo

othing.

These

to

olkits

ha

v

e

found

wide

use,

most

notably

in

the

area

of

sp

eec

h

recognition.

Ho

w

ev

er,

they

cannot

p

erform

parameter

optimization

and

they

do

not

supp

ort

all

of

the

algorithms

w

e

w

an

ted

to

ev

aluate;

th

us,

they

w

ere

not

suitable

for

our

exp

erimen

ts.

.

Smo

othing

Implemen

tati

ons

In

this

section,

w

e

discuss

the

details

of

our

implem

en

tations

of

v

arious

smo

othing

tec

hniques;

often,

the

original

description

of

an

algorithm

is

not

en

tirely

complete

and

unam

biguous.

In

sev

eral

cases,

w

e

implemen

ted

m

ultiple

v

ariations

of

an

algorithm

when

an

am

biguit

y

w

as

presen

t,

and

c

hose

the

v

ersion

that

p

erformed

b

est.

The

titles

of

the

follo

wing

sections

include

the

mnemonic

w

e

use

to

refer

to

the

implemen

tations

in

later

sections.

W

e

use

the

mnemonic

when

w

e

are

referring

to

our

sp

eci�c

implemen

tation

of

a

smo

othing

metho

d,

as

opp

osed

to

the

algorithm

in

general.

F

or

eac

h

metho

d,

w

e

men

tion

the

parameters

that

can

b

e

tuned

to

optimize

p

erformance;

in

general,

an

y

v

ariable

men

tioned

is

a

tunable

parameter.

T

ypically

,

w

e

set

parameter

v

alues

to

optimize

the

p

erplexit

y

of

held-out

data;

for

more

details,

refer

to

Section

..

More

details

ab

out

our

complete

implemen

tation,

including

tec

hniques

for

limiting

memory

us-

age

for

large

data

sets,

are

giv

en

elsewhere

(Chen,

		).

One

observ

ation

that

w

e

tak

e

adv

an

tage

of

is

that

for

some

algorithms,

when

optimizing

the

v

alues

of

parameters

on

a

held-out

set,

it

is

su�cien

t

to

only

consider

a

small

p

ortion

of

the

en

tire

n-gram

mo

del.

That

is,

when

parameter

v

alues

c

hange,

w

e

need

only

recompute

the

p

ortion

of

the

n-gram

mo

del

relev

an

t

to

the

held-out

set.

Th

us,

for

these

algorithms

it

is

p

ossible

to

p

erform

parameter

optimization

e�cien

tly

,

while

for

algorithms

not

falling

in

to

this

category

it

is

generally

necessary

to

tra

v

erse

the

en

tire

training

set

whenev

er

parameters

are

adjusted.

The

implemen

tatio

ns

for

whic

h

parameter

optimization

is

exp

ensiv

e

include

all

bac

k

o�

algorithms

and

the

algorithm

jelinek-mercer-d

elest

;

this

compu-

tational

cost

is

the

reason

w

e

did

not

use

these

algorithms

in

some

of

the

exp

erimen

ts

with

v

ery

large

data

sets.

..

Additiv

e

Smo

othing

(plus-one,

plus-delta)

W

e

consider

t

w

o

v

ersions

of

additiv

e

smo

othing.

Referring

to

equation

()

in

Section

.,

w

e

�x

�

=



in

plus-one

smo

othing.

In

plus-delta,

w

e

consider

an

y

�

.

(The

v

alues

of

parameters

suc

h

as

�

are

determined

through

training

on

held-out

data.)

T

o

impro

v

e

p

erformance,

w

e

p

erform

bac

k

o�

when

a

history

has

no

coun

ts.

That

is,

when

c(w

i�

i�n+

)

=

0

w

e

tak

e

p

add

(w

i

jw

i�

i�n+

)

=

p

add

(w

i

jw

i�

i�n+

)

instead

of

using

equation

().

F

urthermore,

for

metho

d

plus-delta,

instead

of

a

single

�

w

e

ha

v

e

a

separate

�

n

for

eac

h

lev

el

of

the

n-gram

mo

del.

..

Jelinek-Mercer

Smo

othing

(jelinek-mercer,

jelinek-mercer-de

lest

)

Recall

that

higher-order

mo

dels

are

de�ned

recursiv

ely

in

terms

of

lo

w

er-order

mo

dels.

W

e

end

the

recursion

b

y

taking

the

0th-order

distribution

to

b

e

the

uniform

distribution

p

unif

(w

i

)

=

=jV

j.




W

e

buc

k

et

the

�

w

i�

i�n+

according

to

P

w

i

c(w

i

i�n+

)

as

suggested

b

y

Bahl

et

al.

In

tuitiv

ely

,

eac

h

buc

k

et

should

b

e

made

as

small

as

p

ossible,

to

only

group

together

the

most

similar

n-grams,

while

remaining

large

enough

to

accurately

estimate

the

asso

ciated

parameters.

W

e

mak

e

the

assumption

that

whether

a

buc

k

et

is

large

enough

for

accurate

parameter

estimation

dep

ends

only

on

the

n

um

b

er

of

n-grams

that

fall

in

that

buc

k

et

in

the

data

used

to

train

the

�'s.

W

e

assign

buc

k

ets

so

that

a

minim

um

of

c

min

n-grams

fall

in

eac

h

buc

k

et.

W

e

start

from

the

lo

w

est

p

ossible

v

alue

of

P

w

i

c(w

i

i�n+

)

(i.e.,

zero)

and

put

increasing

v

alues

of

P

w

i

c(w

i

i�n+

)

in

to

the

same

buc

k

et

un

til

this

minim

um

coun

t

is

reac

hed.

W

e

rep

eat

this

pro

cess

un

til

all

p

ossible

v

alues

of

P

w

i

c(w

i

i�n+

)

are

buc

k

eted.

If

the

last

buc

k

et

has

few

er

than

c

min

coun

ts,

w

e

merge

it

with

the

preceding

buc

k

et.

W

e

use

separate

buc

k

ets

for

eac

h

n-gram

mo

del

b

eing

in

terp

olated.

In

p

erforming

this

buc

k

eting,

w

e

create

an

arra

y

con

taining

the

n

um

b

er

of

n-grams

that

o

ccur

for

eac

h

v

alue

of

P

w

i

c(w

i

i�n+

)

up

to

some

maxim

um

v

alue,

whic

h

w

e

call

c

top

.

F

or

n-grams

w

i�

i�n+

with

P

w

i

c(w

i

i�n+

)

&gt;

c

top

,

w

e

pretend

P

w

i

c(w

i

i�n+

)

=

c

top

for

buc

k

eting

purp

oses.

As

men

tioned

in

Section

.,

the

�'s

can

b

e

trained

e�cien

tly

using

the

Baum-W

elc

h

algorithm.

Giv

en

initial

v

alues

for

the

�'s,

the

Baum-W

elc

h

algorithm

adjusts

these

parameters

iterativ

ely

to

minim

i

ze

the

en

trop

y

of

some

data.

The

algorithm

generally

decreases

the

en

trop

y

with

eac

h

iteration,

and

guaran

tees

not

to

increase

it.

W

e

set

all

�'s

initially

to

the

v

alue

�

0

.

W

e

terminate

the

algorithm

when

the

en

trop

y

p

er

w

ord

c

hanges

less

than

�

stop

bits

b

et

w

een

iterations.

(Note

that

the

parameters

c

min

,

c

top

,

�

0

,

and

�

stop

are

all

considered

for

optimization,

as

are

all

v

ariables

in

later

sections.)

W

e

implemen

ted

t

w

o

v

ersions

of

Jelinek-Mercer

smo

othing,

one

using

held-out

in

terp

olation

and

one

using

deleted

in

terp

olation.

In

jelinek-mercer,

the

�'s

are

trained

using

held-out

in-

terp

olation

on

a

held-out

set.

In

jelinek-mercer-

deles

t,

the

�'s

are

trained

using

the

r

elaxe

d

delete

d

interp

olation

tec

hnique

describ

ed

b

y

Jelinek

and

Mercer,

where

one

w

ord

is

deleted

at

a

time.

(This

is

also

kno

wn

as

the

le

ave-one-out

metho

d.)

In

jelinek-mercer-de

lest,

w

e

buc

k

et

an

n-gram

according

to

its

coun

t

b

efore

deletion,

as

this

turned

out

to

signi�can

tly

impro

v

e

p

er-

formance.

W

e

h

yp

othesize

that

this

is

b

ecause

an

n-gram

is

then

placed

in

the

same

buc

k

et

during

training

as

in

ev

aluation,

allo

wing

the

�'s

to

b

e

meaningfully

geared

to

w

ard

individual

n-grams.

..

Katz

Smo

othing

(katz)

Referring

to

Section

.,

instead

of

a

single

k

w

e

allo

w

a

di�eren

t

k

n

for

eac

h

n-gram

mo

del

b

eing

com

bined.

Recall

that

higher-order

mo

dels

are

de�ned

recursiv

ely

in

terms

of

lo

w

er-order

mo

dels,

and

that

the

recursion

is

ended

b

y

taking

the

unigram

distribution

to

b

e

the

maxim

um

lik

eliho

o

d

dis-

tribution.

While

using

the

maxim

um

lik

eliho

o

d

unigram

distribution

often

w

orks

w

ell

in

practice,

this

c

hoice

is

not

w

ell-suited

to

our

w

ork.

In

practice,

the

v

o

cabulary

V

is

usually

c

hosen

to

in-

clude

only

those

w

ords

that

o

ccur

in

the

training

data,

so

that

p

ML

(w

i

)

&gt;

0

for

all

w

i



V

.

This

assures

that

the

probabilities

of

all

n-grams

are

nonzero.

Ho

w

ev

er,

in

this

w

ork

not

all

w

ords

in

the

v

o

cabulary

alw

a

ys

o

ccur

in

the

training

data.

W

e

run

exp

erimen

ts

using

man

y

training

set

sizes,

and

w

e

use

a

�xed

v

o

cabulary

across

all

runs

so

that

results

b

et

w

een

sizes

are

comparable.

Not

all

w

ords

in

the

v

o

cabulary

will

o

ccur

in

the

smaller

training

sets.

Th

us,

unless

w

e

smo

oth

the

unigram

distribution

w

e

ma

y

ha

v

e

n-gram

probabilities

that

are

zero,

whic

h

could

lead

to

an

in�nite

cross-en

trop

y

on

test

data.

T

o

address

this

issue,

w

e

smo

oth

the

unigram

distribution

in

Katz

smo

othing

using

additiv

e

smo

othing;

w

e

call

the

additiv

e

constan

t

�

.

 



 

In

Jelinek-Mercer

smo

othing,

w

e

address

this

issue

b

y

ending

the

mo

del

recursion

with

a

0th-order

mo

del

instead

of

a

unigram

mo

del,

and

taking

the

0th-order

mo

del

to

b

e

a

uniform

distribution.

W

e

tried

a

similar

tac

k

with

Katz

smo

othing,

but

applying

the

natural

extension

of

the

Katz

algorithm

to

com

bining

a

unigram

and

uniform




In

the

algorithm

as

describ

ed

in

the

original

pap

er,

no

probabilit

y

is

assigned

to

n-grams

with

zero

coun

ts

in

a

conditional

distribution

p(w

i

jw

i�

i�n+

)

if

there

are

no

n-grams

w

i

i�n+

that

o

ccur

b

et

w

een



and

k

n

times

in

that

distribution.

This

can

lead

to

an

in�nite

cross-en

trop

y

on

test

data.

T

o

address

this,

whenev

er

there

are

no

coun

ts

b

et

w

een



and

k

n

in

a

conditional

distribution,

w

e

giv

e

the

zero-coun

t

n-grams

a

total

of

�

coun

ts,

and

increase

the

normalization

constan

t

appropriately

.

..

Witten-Bell

Smo

othing

(witten-bell-inte

rp,

witten-bell-backof

f)

The

implemen

tation

witten-bell-interp

is

a

faithful

implem

en

tation

of

the

original

algorithm,

where

w

e

end

the

mo

del

recursion

b

y

taking

the

0th-order

distribution

to

b

e

the

uniform

distri-

bution.

The

implemen

tation

witten-bell-backoff

is

a

bac

k

o�

v

ersion

of

the

original

algorithm

(see

Section

.).

..

Absolute

Discoun

tin

g

(abs-disc-interp

,

abs-disc-backof

f)

Referring

to

Section

.,

instead

of

a

single

D

o

v

er

the

whole

mo

del

w

e

use

a

separate

D

n

for

eac

h

n-gram

lev

el.

As

usual,

w

e

terminate

the

mo

del

recursion

with

the

uniform

distribution.

Also,

instead

of

using

equation

(0)

to

calculate

D

n

,

w

e

�nd

the

v

alues

of

D

n

b

y

optimizing

the

p

erplexit

y

of

held-out

data.

The

implemen

tatio

n

abs-disc-backoff

is

a

bac

k

o�

v

ersion

of

abs-

disc-interp

(see

Section

.).

..

Kneser-Ney

Smo

othing

(kneser-ney,

kneser-ney-fix)

Referring

to

Section

.,

instead

of

taking

equation

()

as

is,

w

e

smo

oth

lo

w

er-order

distributions

in

a

similar

fashion

as

the

highest-order

distribution.

That

is,

for

all

n-gram

mo

dels

b

elo

w

the

highest

lev

el

w

e

tak

e

p

KN

(w

i

jw

i�

i�n+

)

=

max

fN

+

(w

i

i�n+

)

�

D

;

0g



P

w

i

N

+

(w

i

i�n+

)

+

D



P

w

i

N

+

(w

i

i�n+

)

N

+

(w

i�

i�n+

�)

p

KN

(w

i

jw

i�

i�n+

)

W

e

end

the

mo

del

recursion

b

y

taking

the

0th-order

distribution

to

b

e

the

uniform

distribution.

Also,

instead

of

a

single

D

o

v

er

the

whole

mo

del

w

e

use

a

separate

D

n

for

eac

h

n-gram

lev

el.

The

algorithm

kneser-ney

sets

the

D

n

parameters

b

y

optimizing

the

p

erplexit

y

of

held-out

data.

The

metho

d

kneser-ney-fix

sets

the

D

n

parameters

using

equation

(0)

as

suggested

in

the

original

pap

er.

..

Mo

di�ed

Kneser-Ney

Smo

othing

(kneser-ney-mod,

kneser-ney-mod-fix

,

kneser-

ney-mod-backoff)

The

implemen

tation

kneser-ney-mod

of

mo

di�ed

Kneser-Ney

smo

othing

(Section

)

is

iden

tical

to

the

implemen

tatio

n

kneser-ney,

with

the

exception

that

three

discoun

t

parameters,

D

n;

,

D

n;

,

and

D

n;+

,

are

used

at

eac

h

n-gram

lev

el

instead

of

just

a

single

discoun

t

D

n

.

The

algorithm

kneser-ney-mod-

fix

is

iden

tical

to

kneser-ney-mod,

except

that

the

discoun

t

parameters

are

set

using

equation

()

instead

of

b

y

b

eing

optimized

on

held-out

data.

The



mo

del

led

to

p

o

or

results.

W

e

tried

additiv

e

smo

othing

instead,

whic

h

is

equiv

alen

t

to

in

terp

olating

with

a

uniform

distribution

using

the

Jelinek-Mercer

paradigm,

and

this

w

ork

ed

w

ell.




-0.4

-0.2

0

0.2

0.4

0.6

0.8

1

1.2

1.4

0.001

0.01

0.1

1

10

100

diff in test cross-entropy from baseline (bits/token)

delta

performance of katz with respect to delta

100 sent

1000 sent

45000 sent

-0.1

-0.09

-0.08

-0.07

-0.06

-0.05

-0.04

1

10

100

1000

10000

diff in test cross-entropy from baseline (bits/token)

minimum number of counts per bucket

performance of jelinek-mercer with respect to c-min

10000 sent

1000000 sent

10000000 sent

Figure

:

P

erformance

relativ

e

to

baseline

algorithm

jelinek-mercer-bas

eline

of

algorithms

katz

and

jelinek-mercer

with

resp

ect

to

parameters

�

and

c

min

,

resp

ectiv

ely

,

o

v

er

sev

eral

training

set

sizes

implemen

tatio

n

kneser-ney-mod-ba

ckof

f

is

the

bac

k

o�

v

ersion

of

the

in

terp

olated

algorithm

kneser-ney-mod.

..

Baseline

Smo

othing

(jelinek-merce

r-bas

eline

)

F

or

our

baseline

smo

othing

metho

d,

w

e

use

a

v

ersion

of

Jelinek-Mercer

smo

othing

with

held-out

in

terp

olation.

Sp

eci�cally

,

for

eac

h

n-gram

mo

del

b

eing

in

terp

olated

w

e

constrain

all

�

w

i�

i�n+

in

equation

()

to

b

e

equal

to

a

single

v

alue

�

n

.

W

e

mak

e

an

exception

when

the

history

w

i�

i�n+

has

nev

er

o

ccurred

in

the

training

data,

in

whic

h

case

w

e

tak

e

�

w

i�

i�n+

to

b

e

zero

as

there

is

no

information

in

the

higher-order

distribution.

This

is

iden

tical

to

jelinek-mercer

where

c

min

is

set

to

,

so

that

there

is

only

a

single

buc

k

et

(for

nonzero

coun

ts)

for

eac

h

n-gram

lev

el.

0

.

P

arameter

Setting

In

this

section,

w

e

discuss

ho

w

the

setting

of

smo

othing

parameters

a�ects

p

erformance,

and

examine

whic

h

parameters

a�ect

o

v

erall

p

erformance

signi�can

tly

.

In

Figure

,

w

e

giv

e

a

couple

of

examples

of

the

sensitivit

y

of

smo

othing

algorithms

to

parameter

v

alues:

w

e

sho

w

ho

w

the

v

alue

of

the

parameter

�

(whic

h

con

trols

unigram

smo

othing)

a�ects

the

p

erformance

of

the

katz

algorithm,

and

ho

w

the

v

alue

of

the

parameter

c

min

(whic

h

determines

buc

k

et

size)

a�ects

the

p

erformance

of

jelinek-mercer.

Notice

that

p

o

or

parameter

setting

can

lead

to

v

ery

signi�can

t

losses

in

p

erformance.

In

Figure

,

w

e

see

di�erences

in

en

trop

y

from

sev

eral

h

undredths

of

a

bit

to

o

v

er

a

bit.

Also,

w

e

see

that

the

optimal

v

alue

of

a

parameter

v

aries

with

training

set

size.

Th

us,

it

is

imp

ortan

t

to

optimize

parameter

v

alues

to

meaningfully

compare

smo

othing

tec

hniques,

and

this

optimization

should

b

e

sp

eci�c

to

the

giv

en

training

set.



0

This

description

di�ers

from

the

description

of

the

baseline

algorithm

giv

en

in

our

previous

w

ork

(Chen

and

Go

o

dman,

		;

Chen,

		).

In

the

other

texts,

w

e

do

not

describ

e

an

exception

for

the

case

where

the

history

has

nev

er

o

ccurred

and

alw

a

ys

set

�

w

i�

i�n+

to

�

n

.

Ho

w

ev

er,

the

other

descriptions

are

inaccurate

:

the

description

presen

ted

here

applies

to

all

of

the

w

ork.








algorithm



signi�can

t

parameters



insigni�can

t

parameters









plus-one



none



none







plus-delta



�

n



none







jelinek-mercer



�

w

i�

i�n+

,

c

min



�

0

=

0:,

�

stop

=

0:00,

c

top

=

00;

000







jelinek-mercer-

deles

t



�

w

i�

i�n+

,

c

min



�

0

=

0:,

�

stop

=

0:00,

c

top

=

00;

000







katz





�



k

n

=

k

max

n

,

�

=









witten-bell-int

erp



none



none







witten-bell-bac

koff



none



none







abs-disc-interp



D

n



none







abs-disc-backof

f



D

n



none







kneser-ney



D

n



none







kneser-ney-fix



none



none







kneser-ney-mod



D

n;

,

D

n;

,

D

n;+



none







kneser-ney-mod-

backo

ff



D

n;

,

D

n;

,

D

n;+



none







kneser-ney-mod-

fix



none



none







jelinek-mercer-

basel

ine



�

n



�

0

=

0:,

�

stop

=

0:00





T

able

:

P

arameters

that

signi�can

tly

a�ect

p

erplexit

y

for

eac

h

smo

othing

algorithm,

and

insignif-

ican

t

parameters

and

their

default

v

alues

In

eac

h

of

our

exp

erimen

ts,

optimal

v

alues

for

the

parameters

of

eac

h

metho

d

w

ere

searc

hed

for

using

P

o

w

ell's

searc

h

algorithm

(Press

et

al.,

	).

P

arameters

w

ere

c

hosen

to

optimize

the

cross-en

trop

y

of

a

held-out

set

asso

ciated

with

eac

h

training

set.

More

sp

eci�cally

,

as

describ

ed

in

Section

.

there

are

three

held-out

sets

asso

ciated

with

eac

h

training

set,

and

parameter

optimization

w

as

p

erformed

using

the

�rst

of

the

three.

F

or

instances

of

Jelinek-Mercer

smo

othing,

the

�'s

w

ere

trained

using

the

Baum-W

elc

h

algo-

rithm

on

the

second

of

the

three

held-out

sets;

all

other

parameters

w

ere

optimized

using

P

o

w

ell's

algorithm

on

the

�rst

set.

In

particular,

to

ev

aluate

the

en

trop

y

asso

ciated

with

a

giv

en

set

of

(non-�)

parameters

in

P

o

w

ell's

searc

h,

w

e

�rst

optimize

the

�'s

on

the

second

held-out

set.

T

o

constrain

the

parameter

searc

h

in

our

main

exp

erimen

ts,

w

e

searc

hed

only

those

parameters

that

w

ere

found

to

a�ect

p

erformance

signi�can

tly

,

as

indicated

through

preliminary

exp

erimen

ts

o

v

er

sev

eral

data

sizes.

In

eac

h

run

of

these

preliminary

exp

erimen

ts,

w

e

�xed

all

(non-�)

param-

eters

but

one

to

some

reasonable

v

alue,

and

used

P

o

w

ell's

algorithm

to

searc

h

on

the

single

free

parameter.

If

the

range

of

test

data

en

tropies

o

v

er

all

parameter

v

alues

considered

b

y

P

o

w

ell's

algorithm

w

as

m

uc

h

smaller

than

the

t

ypical

di�erence

in

en

tropies

b

et

w

een

di�eren

t

algorithms

(i.e.,

0.00

bits),

w

e

c

hose

not

to

p

erform

the

searc

h

o

v

er

this

parameter

in

the

later

exp

erimen

ts,

and

simply

assign

an

arbitrary

reasonable

v

alue

to

the

parameter.

F

or

eac

h

parameter,

w

e

tried

three

di�eren

t

training

sets:

0,000

w

ords

from

the

WSJ

corpus,

one

million

w

ords

from

the

Bro

wn

corpus,

and

three

million

w

ords

from

the

WSJ

corpus.

W

e

summarize

the

results

of

these

exp

erimen

ts

in

T

able

;

Chen

(		)

giv

es

more

details.

F

or

eac

h

algorithm,

w

e

list

the

parameters

w

e

found

to

b

e

signi�can

t

(and

th

us

searc

h

o

v

er

in

eac

h

later

exp

erimen

t);

w

e

also

list

the

insigni�can

t

parameters

and

the

v

alue

w

e

set

them

to.





W

e

found

that

the

larger

the

v

alue

of

eac

h

k

n

,

the

b

etter

the

p

erformance.

Ho

w

ev

er,

for

large

k

n

there

will

b

e

coun

ts

r

suc

h

that

the

asso

ciated

discoun

t

ratio

d

r

tak

es

on

an

unreasonable

v

alue,

suc

h

as

a

nonp

ositiv

e

v

alue

or

a

v

alue

ab

o

v

e

one.

W

e

tak

e

k

n

to

b

e

as

large

as

p

ossible

suc

h

that

all

d

r

tak

e

on

reasonable

v

alues.




.

Data

W

e

used

data

from

the

Bro

wn

corpus,

the

North

American

Business

news

corpus,

the

Switc

h

b

oard

corpus,

and

the

Broadcast

News

corpus.



The

text

of

the

Bro

wn

corpus

(Kucera

and

F

rancis,

	)

w

as

extracted

from

the

tagged

text

in

the

P

enn

T

reebank

(Marcus,

San

torini,

and

Marcinkiewicz,

		)

and

amoun

ted

to

ab

out

one

millio

n

w

ords.

The

v

o

cabulary

w

e

used

with

the

Bro

wn

corpus

exp

erimen

ts

is

the

set

of

all

,0

w

ords

o

ccurring

in

the

corpus.

The

a

v

erage

sen

tence

length

is

ab

out



w

ords.

The

North

American

Business

news

text

w

as

tak

en

from

the

language

mo

deling

data

distributed

for

the

		

ARP

A

con

tin

uous

sp

eec

h

recognition

ev

aluation

(Stern,

		).

The

data

included

0

millio

n

w

ords

of

Asso

ciated

Press

(AP)

text,

	

milli

on

w

ords

of

W

all

Street

Journal

(WSJ)

text,

and



million

w

ords

of

San

Jose

Mercury

News

(SJM)

text.

F

or

these

exp

erimen

ts,

w

e

used

the

0,000

w

ord

v

o

cabulary

supplied

for

the

ev

aluation.

W

e

primarily

used

the

W

all

Street

Journal

text,

and

only

used

the

other

text

if

more

than

	

milli

on

w

ords

of

data

w

as

required.

W

e

refer

to

this

data

as

the

WSJ/NAB

corpus.

The

a

v

erage

sen

tence

lengths

for

the

W

all

Street

Journal,

Asso

ciated

Press,

and

San

Jose

Mercury

News

texts

are

ab

out

,

,

and

0

w

ords,

resp

ectiv

ely

.

The

Switc

h

b

oard

data

is

three

millio

n

w

ords

of

telephone

con

v

ersation

transcriptions

(Go

dfrey

,

Holliman,

and

McDaniel,

		).

The

v

ersion

of

the

data

w

e

used

w

as

pro

cessed

b

y

the

Jan

us

sp

eec

h

recognition

group

(Rogina

and

W

aib

el,

		),

and

in

our

exp

erimen

ts

w

e

used

their

	,00

w

ord

v

o

cabulary

.

The

a

v

erage

sen

tence

length

is

ab

out



w

ords.

The

Broadcast

News

text

w

as

tak

en

from

the

language

mo

deling

data

distributed

for

the

		

D

ARP

A

Hub



con

tin

uous

sp

eec

h

recognition

ev

aluation

(Rudnic

ky

,

		).

The

data

consists

of

0

millio

n

w

ords

of

transcriptions

of

television

and

radio

news

sho

ws.

F

or

these

exp

erimen

ts,

w

e

used

the

0,000

w

ord

v

o

cabulary

dev

elop

ed

b

y

the

Sphinx

sp

eec

h

recognition

group

(Placew

a

y

et

al.,

		)

for

the

ev

aluation.

The

a

v

erage

sen

tence

length

is

ab

out



w

ords.

F

or

eac

h

exp

erimen

t,

w

e

selected

three

segmen

ts

of

held-out

data

along

with

the

segmen

t

of

training

data.

These

four

segmen

ts

w

ere

c

hosen

to

b

e

adjacen

t

in

the

original

corpus

and

disjoin

t,

the

held-out

segmen

ts

follo

wing

the

training.

The

�rst

t

w

o

held-out

segmen

ts

w

ere

used

to

select

the

parameters

of

eac

h

smo

othing

algorithm,

and

the

last

held-out

segmen

t

w

as

used

as

the

test

data

for

p

erformance

ev

aluation.

The

reason

that

t

w

o

segmen

ts

w

ere

reserv

ed

for

parameter

selection

instead

of

one

is

describ

ed

in

Section

..

F

or

exp

erimen

ts

o

v

er

m

ultiple

training

set

sizes,

the

di�eren

t

training

sets

share

the

same

held-out

sets.

In

exp

erimen

ts

with

m

ultiple

runs

on

the

same

training

set

size,

the

data

segmen

ts

of

eac

h

run

are

completely

disjoin

t.

Eac

h

piece

of

held-out

data

w

as

c

hosen

to

b

e

,00

sen

tences,

or

roughly

0,000

w

ords.

This

decision

do

es

not

necessarily

re�ect

practice

w

ell.

F

or

example,

if

the

training

set

size

is

less

than

0,000

w

ords

then

it

is

not

realistic

to

ha

v

e

this

m

uc

h

held-out

data

a

v

ailable.

Ho

w

ev

er,

w

e

made

this

c

hoice

to

a

v

oid

considering

the

training

v

ersus

held-out

data

tradeo�

for

eac

h

data

size.

In

addition,

the

held-out

data

is

used

to

optimize

t

ypically

v

ery

few

parameters,

so

in

practice

small

held-out

sets

are

generally

adequate,

and

p

erhaps

can

b

e

a

v

oided

altogether

with

tec

hniques

suc

h

as

deleted

estimation.

Another

tec

hnique

is

to

use

some

held-out

data

to

�nd

smo

othing

parameter

v

alues,

and

then

to

fold

that

held-out

data

bac

k

in

to

the

training

data

and

to

rebuild

the

mo

dels.

T

o

giv

e

some

�a

v

or

ab

out

ho

w

the

strategy

used

to

select

a

held-out

set

a�ects

p

erformance,

w

e

ran

t

w

o

small

sets

of

exp

erimen

ts

in

v

estigating

ho

w

held-out

set

size

and

ho

w

folding

bac

k

the

held-out

set

in

to

the

training

set

a�ects

cross-en

trop

y

.

In

Figure

,

w

e

displa

y

the

e�ect

of

held-out

set

size

on

the

p

erformance

of

t

w

o

smo

othing

algorithms,

jelinek-mercer

and

kneser-ney-mod,

o

v

er

three

training

set

sizes

on

the

Broadcast

News

corpus.

P

erformance

is

calculated

relativ

e

to

the

cross-en

trop

y

yielded

b

y

using

a

,00

sen

tence

(ab

out

0,000

w

ord)

held-out

set

for

that





All

of

this

data

is

a

v

ailable

from

the

Linguistic

Data

Consortium.




-0.01

0

0.01

0.02

0.03

100

1000

10000

diff in test cross-entropy from baseline (bits/token)

held-out set size (sentences)

performance of jelinek-mercer over varying held-out set sizes

5000 train sent

50000 train sent

500000 train sent

-0.002

0

0.002

0.004

0.006

0.008

100

1000

10000

diff in test cross-entropy from baseline (bits/token)

held-out set size (sentences)

performance of kneser-ney-mod over varying held-out set sizes

5000 train sent

50000 sent

500000 train sent

Figure

:

P

erformance

relativ

e

to

baseline

held-out

set

size

(,00

sen

tences)

of

jelinek-mercer

and

kneser-ney-mod

o

v

er

sev

eral

held-out

set

sizes;

held-out

set

is

used

to

optimize

smo

othing

algorithm

parameters

training

set

size.

F

or

jelinek-mercer

smo

othing,

whic

h

can

ha

v

e

h

undreds

of

�

parameters

or

more,

the

size

of

the

held-out

set

can

ha

v

e

a

mo

derate

e�ect.

F

or

held-out

sets

m

uc

h

smaller

than

the

baseline

size,

test

cross-en

trop

y

can

b

e

up

to

0.0

bits/w

ord

higher,

whic

h

is

appro

ximately

equiv

alen

t

to

a

%

p

erplexit

y

di�erence.

Ho

w

ev

er,

ev

en

when

the

held-out

set

is

a

factor

of

four

larger

than

the

baseline

size

of

,00

sen

tences,

w

e

see

an

impro

v

emen

t

of

at

most

0.0

bits/w

ord.

As

w

e

will

see

later,

these

di�erences

are

m

uc

h

smaller

than

the

t

ypical

di�erence

in

p

erformance

b

et

w

een

smo

othing

algorithms.

F

or

kneser-ney-mod

smo

othing

whic

h

has

ab

out

0

parameters,

held-out

set

size

has

little

e�ect,

t

ypically

less

than

0.00

bits/w

ord.

In

Figure

,

w

e

displa

y

ho

w

folding

bac

k

the

held-out

set

in

to

the

training

set

after

smo

othing

parameter

optimization

a�ects

p

erformance

o

v

er

di�eren

t

training

set

sizes

for

jelinek-mercer

and

kneser-ney-mod.

P

erformance

is

calculated

relativ

e

to

the

cross-en

trop

y

of

our

default

metho

dology

of

not

folding

the

held-out

set

bac

k

in

to

the

training

set

after

parameter

optimiza-

tion.

The

fold-b

ack

line

corresp

onds

to

the

case

where

w

e

fold

the

held-out

data

bac

k

in

to

the

training,

and

the

extr

a

line

corresp

onds

to

the

case

where

after

folding

the

held-out

data

bac

k

in

to

the

training,

w

e

use

an

additional

held-out

set

to

re-optimize

the

smo

othing

parameters.

As

w

ould

b

e

exp

ected,

for

small

training

set

sizes

p

erformance

is

augmen

ted

signi�can

tly

when

the

held-out

data

is

folded

bac

k

in,

as

this

increases

the

training

set

size

noticeably

.

Ho

w

ev

er,

for

training

set

sizes

of

00,000

sen

tences

or

more,

this

impro

v

emen

t

b

ecomes

negligible.

The

di�erence

b

et

w

een

the

fold-b

ack

and

extr

a

lines

represen

ts

the

b

ene�t

of

using

a

held-out

set

disjoin

t

from

the

train-

ing

set

to

optimize

parameters.

This

b

ene�t

is

insigni�can

t

for

kneser-ney-mod,

but

is

larger

for

jelinek-mercer,

esp

ecially

for

smaller

training

sets.



Results

In

this

section,

w

e

presen

t

the

results

of

our

main

exp

erimen

ts.

In

Section

.,

w

e

presen

t

the

p

erformance

of

v

arious

algorithms

for

di�eren

t

training

set

sizes

on

di�eren

t

corp

ora

for

b

oth

bigram

and

trigram

mo

dels.

W

e

demonstrate

that

the

relativ

e

p

erformance

of

di�eren

t

smo

othing

metho

ds

can

v

ary

signi�can

tly

as

conditions

v

ary;

ho

w

ev

er,

Kneser-Ney

smo

othing

and

v

ariations




-1

-0.8

-0.6

-0.4

-0.2

0

1000

10000

100000

1e+06

diff in test cross-entropy from baseline (bits/token)

training data (sentences)

performance of jelinek-mercer over varying held-out set strategies

fold-back

extra

-1

-0.8

-0.6

-0.4

-0.2

0

1000

10000

100000

1e+06

diff in test cross-entropy from baseline (bits/token)

training data (sentences)

performance of kneser-ney-mod over varying held-out set strategies

fold-back

extra

Figure

:

P

erformance

relativ

e

to

baseline

held-out

metho

dology

of

jelinek-mercer

and

kneser-ney-mod;

fold-b

ack

corresp

onds

to

case

where

held-out

set

used

to

optimize

parameters

is

later

folded

bac

k

in

to

training

set;

extr

a

corresp

onds

to

case

where

training

set

is

augmen

ted

b

y

original

held-out

set,

but

additional

held-out

set

is

used

to

optimize

parameters

consisten

tly

outp

erform

all

other

metho

ds.

In

Section

.,

w

e

presen

t

a

more

detailed

analysis

of

p

erformance,

rating

di�eren

t

tec

hniques

on

ho

w

w

ell

they

p

erform

on

n-grams

with

a

particular

coun

t

in

the

training

data,

e.g.,

n-grams

that

ha

v

e

o

ccurred

exactly

once

in

the

training

data.

W

e

�nd

that

katz

most

accurately

smo

oths

n-grams

with

large

coun

ts,

while

kneser-ney-mod

is

b

est

for

small

coun

ts.

W

e

then

sho

w

the

relativ

e

impact

on

p

erformance

of

small

coun

ts

and

large

coun

ts

for

di�eren

t

training

set

sizes

and

n-gram

orders,

and

use

this

data

to

explain

the

v

ariation

in

p

erformance

of

di�eren

t

algorithms

in

di�eren

t

situations.

In

Section

.,

w

e

presen

t

exp

erimen

ts

with

-gram

and

-gram

mo

dels,

with

n-gram

mo

dels

with

coun

t

cuto�s

(i.e.,

mo

dels

that

ignore

n-grams

with

few

er

than

some

n

um

b

er

of

coun

ts

in

the

training

data),

and

exp

erimen

ts

that

examine

ho

w

cross-en

trop

y

is

related

to

w

ord-error

rate

in

sp

eec

h

recognition.

.

Ov

erall

Results

As

men

tioned

earlier,

w

e

ev

aluate

smo

othing

metho

ds

through

their

cross-en

trop

y

on

test

data,

as

giv

en

in

equation

().

In

Figures



and

,

w

e

displa

y

the

cross-en

trop

y

of

our

baseline

smo

othing

metho

d,

jelinek-mercer-base

line

,

o

v

er

a

v

ariet

y

of

training

set

sizes

for

b

oth

bigram

and

trigram

mo

dels

on

all

four

corp

ora

describ

ed

in

Section

..

W

e

see

that

cross-en

trop

y

decreases

steadily

as

the

training

set

used

gro

ws

in

size;

this

decrease

is

somewhat

slo

w

er

than

linear

in

the

logarithm

of

the

training

set

size.

F

urthermore,

w

e

see

that

the

en

tropies

of

di�eren

t

corp

ora

can

b

e

v

ery

di�eren

t,

and

that

trigram

mo

dels

p

erform

signi�can

tly

b

etter

than

bigram

mo

dels

only

for

larger

training

sets.

In

the

follo

wing

discussion,

w

e

will

primarily

rep

ort

the

p

erformance

of

a

smo

othing

algorithm

as

the

di�erence

of

its

cross-en

trop

y

on

a

test

set

from

the

cross-en

trop

y

of

jelinek-mercer-

baseline

with

the

same

training

set.

T

o

see

ho

w

these

cross-en

trop

y

di�erences

translate

to

	


6.5

7

7.5

8

8.5

9

9.5

10

10.5

11

100

1000

10000

100000

1e+06

1e+07

cross-entropy of test data (bits/token)

training set size (sentences)

cross-entropy of baseline for WSJ/NAB corpus

NAB 2-gram

NAB 3-gram

7

7.5

8

8.5

9

9.5

10

10.5

11

11.5

100

1000

10000

100000

1e+06

cross-entropy of test data (bits/token)

training set size (sentences)

cross-entropy of baseline for Broadcast News corpus

BN 2-gram

BN 3-gram

Figure

:

Cross-en

trop

y

of

baseline

algorithm

jelinek-mercer-ba

selin

e

on

test

set

o

v

er

v

arious

training

set

sizes

on

WSJ/NAB

and

Broadcast

News

corp

ora

6.5

7

7.5

8

8.5

9

9.5

10

100

1000

10000

100000

cross-entropy of test data (bits/token)

training set size (sentences)

cross-entropy of baseline for Brown and Switchboard corpora

Brown 2-gram

Brown 3-gram

SWB 2-gram

SWB 3-gram

Figure

:

Cross-en

trop

y

of

baseline

algorithm

jelinek-mercer-ba

selin

e

on

test

set

o

v

er

v

arious

training

set

sizes

on

Bro

wn

and

Switc

h

b

oard

corp

ora

0


p

erplexit

y

,

recall

that

p

erplexit

y

PP

m

(T

)

is

related

to

cross-en

trop

y

H

m

(T

)

as

PP

m

(T

)

=



H

m

(T

)

Hence,

�xed

di�erences

in

cross-en

trop

y

are

equiv

alen

t

to

�xed

ratios

in

p

erplexit

y

.

F

or

example,

a

%

decrease

in

p

erplexit

y

is

equiv

alen

t

to

a

�

log



(

�

0:0)

�

0:0

bits/w

ord

decrease

in

en

trop

y

,

and

a

0%

decrease

in

p

erplexit

y

is

equiv

alen

t

to

a

�

log



(

�

0:)

�

0:

bits/w

ord

decrease

in

en

trop

y

.

Unless

noted,

all

of

the

p

oin

ts

in

eac

h

graph

represen

t

a

single

run

on

a

single

training

and

test

set.

T

o

giv

e

some

idea

ab

out

the

magnitude

of

the

error

in

our

results,

w

e

ran

a

set

of

exp

erimen

ts

where

for

eac

h

training

set

size,

w

e

ran

ten

exp

erimen

ts

on

completely

disjoin

t

data

sets

(training

and

test).

W

e

calculated

the

empirical

mean

and

the

standard

deviation

(of

the

mean)

o

v

er

these

ten

runs;

these

v

alues

are

displa

y

ed

in

Figures



and

.

In

Figure

,

w

e

displa

y

the

absolute

cross-

en

trop

y

of

the

baseline

algorithm,

jelinek-mercer-base

line

,

on

the

Switc

h

b

oard

and

Broadcast

News

corp

ora

for

bigram

and

trigram

mo

dels

o

v

er

a

range

of

training

set

sizes.

The

standard

deviation

on

the

Switc

h

b

oard

runs

w

as

v

ery

small;

on

Broadcast

News,

the

v

ariation

w

as

relativ

ely

large,

comparable

to

the

di�erences

in

p

erformance

b

et

w

een

smo

othing

algorithms.

In

Figure

,

w

e

displa

y

the

p

erformance

of

a

n

um

b

er

of

smo

othing

algorithms

relativ

e

to

the

baseline

algorithm

on

the

Broadcast

News

and

Switc

h

b

oard

corp

ora

for

trigram

mo

dels

on

a

range

of

training

set

sizes.

W

e

see

that

the

v

ariation

in

cross-en

trop

y

relativ

e

to

the

baseline

is

generally

fairly

small,

m

uc

h

smaller

than

the

di�erence

in

p

erformance

b

et

w

een

algorithms.

Hence,

while

the

v

ariation

in

absolute

cross-en

tropies

is

large,

the

v

ariation

in

relativ

e

cross-en

tropies

is

small

and

w

e

can

mak

e

meaningful

statemen

ts

ab

out

the

relativ

e

p

erformance

of

algorithms

in

this

domain.

Ho

w

ev

er,

in

later

graphs

eac

h

p

oin

t

will

represen

t

a

single

run

instead

of

an

a

v

erage

o

v

er

ten

runs,

and

the

standard

deviation

for

a

single

run

will

b

e

a

factor

of

ab

out

p



0

larger

than

the

v

alues

plotted

in

Figure

.

With

these

larger

deviations,

the

relativ

e

p

erformance

of

t

w

o

algorithms

with

similar

p

erformance

ma

y

b

e

di�cult

to

determine

from

a

single

pair

of

p

oin

ts.

Ho

w

ev

er,

w

e

b

eliev

e

that

an

accurate

and

precise

picture

of

relativ

e

p

erformance

can

b

e

gleaned

from

the

graphs

to

b

e

presen

ted

later

due

to

the

v

ast

o

v

erall

n

um

b

er

of

exp

erimen

ts

p

erformed:

most

exp

erimen

ts

are

carried

out

o

v

er

a

v

ariet

y

of

training

set

sizes

and

on

eac

h

of

four

indep

enden

t

corp

ora.

Relativ

e

p

erformance

trends

are

largely

consisten

t

o

v

er

these

runs.

Nev

ertheless,

there

is

one

phenomenon

that

seems

to

adv

ersely

and

signi�can

tly

a�ect

the

p

erformance

of

a

certain

group

of

algorithms

on

a

small

n

um

b

er

of

data

sets,

e.g.,

see

the

p

oin

ts

corresp

onding

to

a

training

set

size

of

0,000

sen

tences

in

the

graphs

in

Figure

0.

W

e

presen

t

an

analysis

of

this

anomaly

in

Section

..;

the

algorithms

that

this

phenomenon

a�ects

corresp

ond

to

the

algorithms

with

the

largest

v

ariance

in

Figure

.

..

Ov

erall

P

erformance

Di�erences

In

Figures

{,

w

e

displa

y

the

p

erformance

of

v

arious

algorithms

relativ

e

to

the

baseline

algorithm

jelinek-mercer-ba

selin

e

o

v

er

a

v

ariet

y

of

training

set

sizes,

for

bigram

and

trigram

mo

dels,

and

for

eac

h

of

the

four

corp

ora

describ

ed

in

Section

..

These

graphs

do

not

displa

y

all

of

the

algorithms

w

e

implem

en

ted,

as

placing

all

of

the

algorithms

on

a

single

graph

w

ould

lead

to

to

o

m

uc

h

clutter;

instead,

the

algorithms

c

hosen

are

mean

t

to

giv

e

an

o

v

erall

picture

of

the

relativ

e

p

erformance

of

di�eren

t

algorithms.

Comparisons

b

et

w

een

the

displa

y

ed

algorithms

and

the

algorithms

omitted

from

the

graphs

are

pro

vided

in

follo

wing

sections.

F

rom

these

graphs,

w

e

see

that

the

metho

ds

kneser-ney

and

kneser-ney-mod

consisten

tly

outp

erform

all

other

algorithms,

o

v

er

all

training

set

sizes

and

corp

ora,

and

for

b

oth

bigram

and

trigram

mo

dels.

These

metho

ds

also

outp

erform

all

algorithms

not

sho

wn

in

the

graphs,

except




7

7.5

8

8.5

9

9.5

10

10.5

11

100

1000

10000

100000

cross-entropy of test data (bits/token)

training set size (sentences)

cross-entropy of baseline for Switchboard and Broadcast News corpora

SWB 2-gram

SWB 3-gram

BN 2-gram

BN 3-gram

Figure

:

Cross-en

trop

y

of

baseline

algorithm

jelinek-mercer-ba

selin

e

on

test

set

o

v

er

v

arious

training

set

sizes

on

Switc

h

b

oard

and

Broadcast

News

corp

ora;

eac

h

p

oin

t

displa

ys

mean

and

standard

deviation

o

v

er

ten

runs

on

disjoin

t

data

sets

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

difference in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Broadcast News corpus, trigram

baseline

katz

kneser-ney-fix

kneser-ney-mod

abs-disc-interp

jelinek-mercer

-0.2

-0.15

-0.1

-0.05

0

0.05

100

1000

10000

difference in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Switchboard corpus, trigram

jelinek-mercer-baseline

katz

kneser-ney-fix

kneser-ney-mod

abs-disc-interp

jelinek-mercer

Figure

:

P

erformance

relativ

e

to

baseline

of

v

arious

algorithms

on

Broadcast

News

and

Switc

h-

b

oard

corp

ora,

trigram

mo

del;

eac

h

p

oin

t

displa

ys

mean

and

standard

deviation

o

v

er

ten

runs

on

disjoin

t

data

sets




-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 2-gram

jelinek-mercer-baseline

katz

k-n

kneser-ney-mod

abs-disc-int

j-m

witten-bell-backoff

-0.3

-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 3-gram

jelinek-mercer-baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

j-m

witten-bell-backoff

Figure

:

P

erformance

relativ

e

to

baseline

of

v

arious

algorithms

on

WSJ/NAB

corpus

o

v

er

v

arious

training

set

sizes,

bigram

and

trigram

mo

dels

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

1e+06

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Broadcast News corpus, 2-gram

jelinek-mercer-baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

j-m

witten-bell-backoff

-0.3

-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

1e+06

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Broadcast News corpus, 3-gram

baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

jelinek-mercer

witten-bell-backoff

Figure

	:

P

erformance

relativ

e

to

baseline

of

v

arious

algorithms

on

Broadcast

News

corpus

o

v

er

v

arious

training

set

sizes,

bigram

and

trigram

mo

dels




-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Switchboard corpus, 2-gram

jelinek-mercer-baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

j-m

witten-bell-backoff

-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Switchboard corpus, 3-gram

baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

jelinek-mercer

witten-bell-backoff

Figure

0:

P

erformance

relativ

e

to

baseline

of

v

arious

algorithms

on

Switc

h

b

oard

corpus

o

v

er

v

arious

training

set

sizes,

bigram

and

trigram

mo

dels

-0.3

-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Brown corpus, 2-gram

jelinek-mercer-baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

jelinek-mercer

witten-bell-backoff

-0.35

-0.3

-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Brown corpus, 3-gram

jelinek-mercer-baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

jelinek-mercer

witten-bell-backoff

Figure

:

P

erformance

relativ

e

to

baseline

of

v

arious

algorithms

on

Bro

wn

corpus

o

v

er

v

arious

training

set

sizes,

bigram

and

trigram

mo

dels




for

other

v

ariations

of

Kneser-Ney

smo

othing.

In

Section

.,

w

e

will

sho

w

that

this

excellen

t

p

erformance

is

due

to

the

mo

di�ed

bac

k

o�

distributions

that

Kneser-Ney

smo

othing

emplo

ys,

as

describ

ed

in

Section

..

The

algorithms

katz

and

jelinek-mercer

generally

yield

the

next

b

est

p

erformance.

Both

p

erform

signi�can

tly

b

etter

than

the

baseline

metho

d

in

almost

all

situations,

except

for

cases

with

v

ery

little

training

data.

The

algorithm

jelinek-mercer

p

erforms

b

etter

than

katz

in

sparse

data

situations,

and

the

rev

erse

is

true

when

there

is

m

uc

h

data.

F

or

example,

katz

p

erforms

b

etter

on

Broadcast

News

and

WSJ/NAB

trigram

mo

dels

for

training

sets

larger

than

0,000{

00,000

sen

tences;

for

bigram

mo

dels

the

cross-o

v

er

p

oin

t

is

generally

lo

w

er.

In

Section

.,

w

e

will

explain

this

v

ariation

in

p

erformance

relativ

e

to

training

set

size

b

y

sho

wing

that

katz

is

b

etter

at

smo

othing

larger

coun

ts;

these

coun

ts

are

more

prev

alen

t

in

larger

data

sets.

The

w

orst

of

the

displa

y

ed

algorithms

(not

including

the

baseline)

are

the

algorithms

abs-

disc-interp

and

witten-bell-backoff

.

The

metho

d

abs-disc-interp

generally

outp

erforms

the

baseline

algorithm,

though

not

for

v

ery

small

data

sets.

The

metho

d

witten-bell-backo

ff

p

erforms

p

o

orly

,

m

uc

h

w

orse

than

the

baseline,

for

smaller

data

sets.

Both

of

these

algorithms

are

signi�can

tly

sup

erior

to

the

baseline

for

v

ery

large

data

sets;

in

these

situations,

they

are

comp

etitiv

e

with

the

algorithms

katz

and

jelinek-mercer.

These

graphs

mak

e

it

apparen

t

that

the

relativ

e

p

erformance

of

smo

othing

tec

hniques

can

v

ary

dramatically

o

v

er

training

set

size,

n-gram

order,

and

training

corpus.

F

or

example,

the

metho

d

witten-bell-backo

ff

p

erforms

atro

ciously

for

small

training

sets

but

comp

etitiv

ely

on

v

ery

large

training

sets.

There

are

n

umerous

instances

where

the

relativ

e

p

erformance

of

t

w

o

metho

ds

rev

erse

o

v

er

di�eren

t

training

set

sizes,

and

this

cross-o

v

er

p

oin

t

will

v

ary

widely

o

v

er

n-gram

order

or

corpus.

Th

us,

it

is

not

su�cien

t

to

run

exp

erimen

ts

on

one

or

t

w

o

data

sets

for

a

single

training

set

size

to

reasonably

c

haracterize

the

p

erformance

of

a

smo

othing

algorithm,

as

is

the

t

ypical

metho

dology

in

previous

w

ork.

Analysis

of

P

erformance

Anomaly

In

the

graphs

in

Figure

0,

w

e

see

that

sev

eral

algorithms

b

eha

v

e

anomalously

on

the

training

set

of

0,000

sen

tences.

The

algorithms

abs-disc-interp

,

katz,

and

kneser-ney

all

p

erform

substan

tially

w

orse

than

w

ould

b

e

exp

ected

giv

en

their

p

erfor-

mance

on

other

training

sets

for

b

oth

bigram

and

trigram

mo

dels,

while

the

remaining

algorithms

seem

to

b

e

una�ected.

As

can

b

e

seen

later

in

Figures

	

and

,

the

algorithms

kneser-ney-fix

and

kneser-ney-mod-f

ix

are

also

adv

ersely

a�ected.

In

this

section,

w

e

analyze

this

phenomenon

and

sho

w

that

this

particular

training

set

is

indeed

un

usual,

and

explain

wh

y

only

the

listed

algorithms

are

a�ected.

After

in

v

estigation,

w

e

found

that

the

0,000-sen

tence

training

set

had

an

un

usual

distribution

of

coun

ts;

there

w

ere

abnormally

few

trigrams

with

one

coun

t

as

compared

to

trigrams

with

higher

coun

ts.



In

Figure

,

w

e

plot

the

ratio

of

the

n

um

b

er

of

trigrams

with

v

arious

coun

ts

to

the

n

um

b

er

of

trigrams

with

exactly

one

coun

t

o

v

er

v

arious

training

set

sizes

on

the

Switc

h

b

oard

corpus.

This

graph

mak

es

apparen

t

the

un

usual

coun

t

distribution

presen

t

in

the

giv

en

training

set.

This

observ

ation

can

b

e

used

to

explain

wh

y

the

algorithms

katz,

kneser-ney-fix,

and

kneser-ney-mod-fi

x

all

p

erform

un

usually

p

o

orly

.

These

algorithms

share

the

prop

ert

y

that

discoun

ts

are

calculated

based

on

the

coun

ts

of

n-grams

with

a

particular

coun

t

in

the

training

data.

Since

the

training

set

has

an

un

usual

distribution

of

these

coun

ts

and

presumably

the

test

set

has

a

more

t

ypical

distribution,

w

e

ha

v

e

a

mismatc

h

b

et

w

een

the

training

and

test

set,

and

discoun

ts

are

not

set

suitably

for

the

test

set.

Indeed,

in

Figures

	

and



w

e

see

that

the





F

urther

examinatio

n

rev

ealed

that

this

paucit

y

of

one

coun

ts

w

as

presen

t

b

ecause

a

long

segmen

t

of

text

w

as

duplicated

in

the

training

set.




0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

100

1000

10000

100000

training set size (sentences)

n2/n1

n3/n1

n4/n1

Figure

:

Ratio

of

the

n

um

b

er

of

trigrams

with

v

arious

coun

ts

to

the

n

um

b

er

of

trigrams

with

exactly

one

coun

t

o

v

er

v

arious

training

set

sizes

on

the

Switc

h

b

oard

corpus

algorithms

kneser-ney

and

kneser-ney-mod

p

erform

substan

tially

b

etter

than

kneser-ney-fix

and

kneser-ney-mod-fix,

resp

ectiv

ely

,

on

this

training

set.

F

or

kneser-ney

and

kneser-ney-

mod,

discoun

t

parameters

are

set

through

optimization

on

held-out

data,

so

the

mismatc

h

b

et

w

een

training

and

test

data

has

little

e�ect.

This

example

highligh

ts

the

additional

robustness

p

ossible

when

using

held-out

data

to

set

parameters

instead

of

setting

them

deterministically

from

training

data.

T

o

explain

wh

y

the

algorithms

abs-disc-interp

and

kneser-ney

p

erform

p

o

orly

on

this

training

set,

w

e

refer

to

the

optimal

discoun

ts

calculated

on

the

held-out

set

for

kneser-ney-mod

smo

othing

for

a

trigram

mo

del.

W

e

�nd

that

these

discoun

ts

are

spread

un

usually

far

apart

for

the

0,000-sen

tence

training

set:

w

e

�nd

(D



;

D



;

D

+

)

v

alues

of

ab

out

(0.	,

.,

.)

when

v

alues

of

ab

out

(0.	,

.,

.)

is

what

w

ould

b

e

exp

ected

from

in

terp

olating

the

v

alues

found

on

training

sets

of

nearb

y

size.

This

indicates

that

the

ideal

a

v

erage

discoun

t

for

di�eren

t

coun

ts

on

this

training

set

are

un

usually

spread

apart,

and

so

using

a

single

discoun

t

D

for

all

coun

ts

as

is

done

b

y

abs-

disc-interp

and

kneser-ney

is

an

un

usually

p

o

or

appro

ximation

on

this

training

set.

Th

us,

w

e

can

see

ho

w

the

at

ypical

nature

of

the

training

set

leads

to

p

o

or

p

erformance

for

abs-disc-interp

and

kneser-ney.

It

is

in

teresting

to

note

wh

y

the

algorithms

jelinek-mercer,

jelinek-mercer-b

aseli

ne,

and

kneser-ney-mod

do

not

exhibit

anomalous

b

eha

vior

on

the

0,000-sen

tence

training

set.

Because

the

algorithms

jelinek-mercer

and

jelinek-mercer-bas

eline

do

not

utilize

the

coun

ts

of

n-

grams

with

certain

coun

ts

in

the

training

data,

they

are

una�ected

b

y

the

un

usual

distribution

of

these

coun

ts.

The

algorithm

kneser-ney-mod

retains

its

p

erformance

b

ecause

of

a

com

bination

of

t

w

o

reasons:

parameters

are

optimized

on

held-out

data

so

that

the

mismatc

h

b

et

w

een

the

training

and

test

data

can

b

e

comp

ensated

for,

and

it

has

enough

discoun

t

parameters

to

adequately

comp

ensate

for

the

mismatc

h.

..

Additiv

e

Smo

othing

In

Figure

,

w

e

displa

y

the

p

erformance

of

the

plus-one

and

plus-delta

algorithms

rela-

tiv

e

to

the

baseline

algorithm

jelinek-mercer-b

aseli

ne

for

bigram

and

trigram

mo

dels

on

the

WSJ/NAB

corpus

o

v

er

a

range

of

training

set

sizes.

In

general,

these

algorithms

p

erform

m

uc

h

w

orse

than

the

baseline

algorithm,

except

for

situations

with

a

w

ealth

of

data.

F

or

example,




0

0.5

1

1.5

2

2.5

3

3.5

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 2-gram

jelinek-mercer-baseline

plus-one

plus-delta

0

1

2

3

4

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 3-gram

jelinek-mercer-baseline

plus-one

plus-delta

Figure

:

P

erformance

relativ

e

to

baseline

of

plus-one

and

plus-delta

algorithms

on

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

plus-delta

is

comp

etitiv

e

with

the

baseline

metho

d

when

using

a

training

set

of

0,000,000

sen-

tences

for

a

bigram

mo

del

on

WSJ/NAB

data.

Though

not

sho

wn,

these

algorithms

ha

v

e

similar

p

erformance

on

the

other

three

corp

ora.

Gale

and

Ch

urc

h

(		0;

		)

further

discuss

the

p

erfor-

mance

of

these

algorithms.

..

Bac

k

o�

vs.

In

terp

olatio

n

In

this

section,

w

e

compare

the

p

erformance

b

et

w

een

the

bac

k

o�

and

in

terp

olated

v

ersions

of

sev

eral

smo

othing

algorithms.

(F

or

the

de�nitions

of

these

t

yp

es

of

mo

dels,

refer

to

Section

..)

W

e

imple-

men

ted

three

pairs

of

algorithms

that

di�er

only

in

the

bac

k

o�

strategy

used:

witten-bell-inte

rp

and

witten-bell-backo

ff,

abs-disc-interp

and

abs-disc-backof

f,

and

kneser-ney-mod

and

kneser-ney-mod-ba

ckoff

.

In

Figure

,

w

e

displa

y

the

p

erformance

of

witten-bell-interp

and

witten-bell-backo

ff

relativ

e

to

the

baseline

algorithm

jelinek-mercer-base

line

for

bigram

and

trigram

mo

dels

on

the

WSJ/NAB

corpus

o

v

er

a

range

of

training

set

sizes.

W

e

see

that

witten-bell-backo

ff

consisten

tly

outp

erforms

witten-bell-interp

o

v

er

all

training

set

sizes

and

for

b

oth

bigram

and

trigram

mo

dels.

While

not

sho

wn,

these

algorithms

ha

v

e

similar

p

erformance

on

the

other

three

corp

ora.

In

Figures



and

,

w

e

displa

y

the

p

erformance

of

the

bac

k

o�

and

in

terp

olated

v

ersions

of

absolute

discoun

ting

and

mo

di�ed

Kneser-Ney

smo

othing

for

bigram

and

trigram

mo

dels

on

the

WSJ/NAB

and

Broadcast

News

corp

ora

o

v

er

a

range

of

training

set

sizes.

W

e

see

that

kneser-ney-mod

consisten

tly

outp

erforms

kneser-ney-mod-backo

ff.

On

small

data

sets,

abs-

disc-interp

outp

erforms

abs-disc-backoff,

and

the

rev

erse

holds

for

large

data

sets.

W

e

see

that

the

cross-o

v

er

p

oin

t

v

aries

with

corpus

and

n-gram

order.

While

not

sho

wn,

these

algorithms

ha

v

e

similar

p

erformance

on

the

other

t

w

o

corp

ora.

In

Section

.,

w

e

presen

t

an

analysis

that

partially

explains

the

relativ

e

p

erformance

of

bac

k

o�

and

in

terp

olated

algorithms.




-0.05

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 2-gram

jelinek-mercer-baseline

witten-bell-backoff

witten-bell-interp

-0.1

0

0.1

0.2

0.3

0.4

0.5

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 3-gram

jelinek-mercer-baseline

witten-bell-backoff

witten-bell-interp

Figure

:

P

erformance

relativ

e

to

baseline

of

witten-bell-backoff

and

witten-bell-inte

rp

algorithms

on

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

..

Kneser-Ney

Smo

othing

and

V

ariations

In

this

section,

w

e

compare

the

p

erformance

of

the

di�eren

t

v

ariations

of

Kneser-Ney

smo

othing

that

w

e

implemen

ted:

kneser-ney,

kneser-ney-mod,

kneser-ney-fix,

and

kneser-ney-mod-

fix.

W

e

do

not

discuss

the

p

erformance

of

metho

d

kneser-ney-mod-back

off

here,

as

this

w

as

presen

ted

in

Section

...

In

Figure

,

w

e

displa

y

the

p

erformance

of

kneser-ney

and

kneser-ney-mod

relativ

e

to

the

baseline

algorithm

jelinek-mercer-base

line

for

bigram

and

trigram

mo

dels

on

the

WSJ/NAB

corpus

o

v

er

a

range

of

training

set

sizes.

Recall

that

these

algorithms

di�er

in

that

for

eac

h

n-

gram

lev

el,

kneser-ney

has

a

single

discoun

t

D

n

for

eac

h

coun

t

while

kneser-ney-mod

has

three

discoun

ts

D

n;

,

D

n;

,

and

D

n;+

for

n-grams

with

one

coun

t,

t

w

o

coun

ts,

and

three

or

more

coun

ts,

resp

ectiv

ely

,

as

describ

ed

in

Section

..

W

e

see

that

kneser-ney-mod

consisten

tly

outp

erforms

kneser-ney

o

v

er

all

training

set

sizes

and

for

b

oth

bigram

and

trigram

mo

dels.

While

not

sho

wn,

these

algorithms

ha

v

e

similar

b

eha

vior

on

the

other

three

corp

ora.

Their

di�erence

in

p

erformance

is

generally

signi�can

t,

though

is

smaller

for

v

ery

large

data

sets.

In

Section

.,

w

e

explain

this

di�erence

b

y

sho

wing

that

the

correct

a

v

erage

discoun

t

for

n-grams

with

one

coun

t

or

t

w

o

coun

ts

deviates

signi�can

tly

from

the

correct

a

v

erage

discoun

t

for

larger

coun

ts.

In

Figures



and

	,

w

e

displa

y

the

p

erformance

of

kneser-ney

and

kneser-ney-fix

for

bigram

and

trigram

mo

dels

on

the

WSJ/NAB

and

Switc

h

b

oard

corp

ora

o

v

er

a

range

of

training

set

sizes.

Recall

that

these

algorithms

di�er

in

that

for

kneser-ney

w

e

set

the

parameters

D

n

b

y

optimizing

the

cross-en

trop

y

of

held-out

data,

while

for

kneser-ney-fix

these

parameters

are

set

using

the

form

ula

suggested

b

y

Kneser

and

Ney

(		),

as

describ

ed

in

Section

..

While

their

p

erformances

are

sometimes

v

ery

close,

esp

ecially

for

large

data

sets,

w

e

see

that

kneser-ney

con-

sisten

tly

outp

erforms

kneser-ney-fix.

While

not

sho

wn,

these

algorithms

ha

v

e

similar

b

eha

vior

on

the

other

t

w

o

corp

ora.

(F

or

a

discussion

of

the

anomalous

p

oin

ts

in

Figures

	

and



for

the

0,000-sen

tence

training

set,

refer

to

Section

...)

In

Figures

0

and

,

w

e

displa

y

the

p

erformance

of

kneser-ney-mod

and

kneser-ney-mod-

fix

for

bigram

and

trigram

mo

dels

on

the

WSJ/NAB

and

Switc

h

b

oard

corp

ora

o

v

er

a

range

of

training

set

sizes.

As

with

kneser-ney

and

kneser-ney-fix,

these

algorithms

di�er

in

whether

the

discoun

ts

are

set

using

held-out

data

or

using

a

form

ula

based

on

training

set

coun

ts.

W

e

see

similar




-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 2-gram

jelinek-mercer-baseline

abs-disc-backoff

abs-disc-interp

kneser-ney-mod

kneser-ney-mod-backoff

-0.3

-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 3-gram

jelinek-mercer-baseline

abs-disc-backoff

abs-disc-interp

kneser-ney-mod

kneser-ney-mod-backoff

Figure

:

P

erformance

relativ

e

to

baseline

of

bac

k

o�

and

in

terp

olated

v

ersions

of

absolute

dis-

coun

ting

and

mo

di�ed

Kneser-Ney

smo

othing

on

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

-0.2

-0.15

-0.1

-0.05

0

0.05

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Broadcast News corpus, 2-gram

jelinek-mercer-baseline

abs-disc-backoff

abs-disc-interp

kneser-ney-mod

kneser-ney-mod-backoff

-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Broadcast News corpus, 3-gram

jelinek-mercer-baseline

abs-disc-backoff

abs-disc-interp

kneser-ney-mod

kneser-ney-mod-backoff

Figure

:

P

erformance

relativ

e

to

baseline

of

bac

k

o�

and

in

terp

olated

v

ersions

of

absolute

dis-

coun

ting

and

mo

di�ed

Kneser-Ney

smo

othing

on

Broadcast

News

corpus,

bigram

and

trigram

mo

dels

	


-0.2

-0.15

-0.1

-0.05

0

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 2-gram

jelinek-mercer-baseline

kneser-ney

kneser-ney-mod

-0.25

-0.2

-0.15

-0.1

-0.05

0

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 3-gram

jelinek-mercer-baseline

kneser-ney

kneser-ney-mod

Figure

:

P

erformance

relativ

e

to

baseline

of

kneser-ney

and

kneser-ney-mod

algorithms

on

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

b

eha

vior

as

b

efore:

while

their

p

erformance

is

often

close,

esp

ecially

for

large

data

sets,

kneser-

ney-mod

consisten

tly

outp

erforms

kneser-ney-mod-

fix.

While

not

sho

wn,

these

algorithms

ha

v

e

similar

p

erformance

on

the

other

t

w

o

corp

ora.

While

the

-fix

v

ariations

ha

v

e

the

adv

an

tage

of

not

ha

ving

an

y

external

parameters

that

need

to

b

e

optimized,

w

e

see

that

w

e

can

generally

do

a

little

b

etter

b

y

optimizing

parameters

on

held-out

data.

In

addition,

in

situations

where

w

e

ha

v

e

held-out

data

kno

wn

to

b

e

similar

to

the

test

data,

the

v

ariations

with

free

parameters

should

do

w

ell

ev

en

if

the

training

data

do

es

not

exactly

matc

h

the

test

data.

This

robustness

is

highligh

ted

for

the

0,000-sen

tence

training

set

from

the

Switc

h

b

oard

corpus,

as

discussed

in

Section

...

..

Held-out

and

Deleted

Estimation

In

this

section,

w

e

compare

the

held-out

and

deleted

in

terp

olation

v

ariations

of

Jelinek-Mercer

smo

othing.

In

Figure

,

w

e

displa

y

the

p

erformance

of

the

jelinek-mercer

and

jelinek-

mercer-delest

algorithms

on

the

WSJ/NAB

corpus

for

bigram

and

trigram

mo

dels

o

v

er

a

v

ariet

y

of

training

set

sizes.

These

t

w

o

algorithms

di�er

only

in

that

jelinek-mercer

uses

the

held-out

data

to

optimize

the

�

parameters,

while

jelinek-mercer-del

est

optimizes

the

�

parameters

using

deleted

estimation

(i.e.,

the

le

ave-one-out

tec

hnique).

W

e

see

that

jelinek-mercer

p

erforms

signi�can

tly

b

etter

for

smaller

training

sets,

but

for

large

training

sets

jelinek-mercer-dele

st

p

erforms

sligh

tly

b

etter.



Smo

othing

can

b

e

view

ed

as

mo

deling

the

di�erence

in

nature

b

et

w

een

a

training

and

test

set.

Held-out

data

external

to

the

training

data

will

tend

to

b

e

more

di�eren

t

from

the

training

data

than

data

that

is

deleted

from

the

middle

of

the

training

data.

As

our

ev

aluation

test

data

is

also

external

to

the

training

data

(as

is

the

case

in

applications),

�'s

trained

from

held-out

data

ma

y

b

etter

c

haracterize

the

ev

aluation

test

data.

This

ma

y

explain

the

sup

erior

p

erformance

of

jelinek-mercer

on

smaller

data

sets.

W

e

h

yp

othesize

that

the

reason

wh

y

jelinek-mercer-

delest

do

es

w

ell

on

larger

data

sets

is

that

on

larger

training

sets,

data

that

is

deleted

from

the

middle

of

a

training

set

is

su�cien

tly

di�eren

t

from

the

remainder

of

the

data

that

it

is

similar

in





These

results

di�er

sligh

tly

from

those

rep

orted

in

previous

w

ork

(Chen,

		);

in

that

w

ork

w

e

rep

orted

that

held-out

estimation

is

sup

erior.

Ho

w

ev

er,

in

that

w

ork

w

e

did

not

use

training

sets

as

large

as

those

in

this

w

ork,

so

that

w

e

did

not

observ

e

the

cross-o

v

er

p

oin

t

in

p

erformance

.

0


-0.2

-0.15

-0.1

-0.05

0

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 2-gram

jelinek-mercer-baseline

kneser-ney

kneser-ney-fix

-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 3-gram

jelinek-mercer-baseline

kneser-ney

kneser-ney-fix

Figure

:

P

erformance

relativ

e

to

baseline

of

kneser-ney

and

kneser-ney-fix

algorithms

on

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

-0.15

-0.1

-0.05

0

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Switchboard corpus, 2-gram

jelinek-mercer-baseline

kneser-ney

kneser-ney-fix

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

0.15

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Switchboard corpus, 3-gram

jelinek-mercer-baseline

kneser-ney

kneser-ney-fix

Figure

	:

P

erformance

relativ

e

to

baseline

of

kneser-ney

and

kneser-ney-fix

algorithms

on

Switc

h

b

oard

corpus,

bigram

and

trigram

mo

dels




-0.2

-0.15

-0.1

-0.05

0

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 2-gram

jelinek-mercer-baseline

kneser-ney-mod

kneser-ney-mod-fix

-0.25

-0.2

-0.15

-0.1

-0.05

0

100

1000

10000

100000

1e+06

1e+07

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 3-gram

jelinek-mercer-baseline

kneser-ney-mod

kneser-ney-mod-fix

Figure

0:

P

erformance

relativ

e

to

baseline

of

kneser-ney-mod

and

kneser-ney-mod-fix

algo-

rithms

on

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

-0.2

-0.15

-0.1

-0.05

0

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Switchboard corpus, 2-gram

jelinek-mercer-baseline

kneser-ney-mod

kneser-ney-mod-fix

-0.2

-0.15

-0.1

-0.05

0

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on Switchboard corpus, 3-gram

jelinek-mercer-baseline

kneser-ney-mod

kneser-ney-mod-fix

Figure

:

P

erformance

relativ

e

to

baseline

of

kneser-ney-mod

and

kneser-ney-mod-fix

algo-

rithms

on

Switc

h

b

oard

corpus,

bigram

and

trigram

mo

dels




-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 2-gram

jelinek-mercer-baseline

jelinek-mercer

jelinek-mercer-delest

-0.12

-0.1

-0.08

-0.06

-0.04

-0.02

0

0.02

0.04

100

1000

10000

100000

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 3-gram

jelinek-mercer-baseline

jelinek-mercer

jelinek-mercer-delest

Figure

:

P

erformance

relativ

e

to

baseline

of

jelinek-mercer

and

jelinek-mercer-dele

st

algorithms

on

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

nature

to

held-out

data.

Th

us,

the

preceding

e�ect

is

less

pronounced,

and

p

erhaps

b

ecause

of

the

larger

amoun

t

of

data

used

to

optimize

parameters,

the

p

erformance

of

jelinek-mercer-dele

st

b

ecomes

sup

erior.

Ho

w

ev

er,

the

implemen

tation

jelinek-mercer-dele

st

do

es

not

completely

c

haracterize

the

tec

hnique

of

deleted

in

terp

olation

as

w

e

do

not

v

ary

the

size

of

the

c

h

unks

that

are

deleted.

.

Coun

t-b

y-Coun

t

Analysis

In

order

to

pain

t

a

more

detailed

picture

of

the

p

erformance

of

v

arious

algorithms,

instead

of

just

lo

oking

at

the

o

v

erall

cross-en

trop

y

of

a

test

set,

w

e

partition

test

sets

according

to

ho

w

often

eac

h

n-gram

in

the

test

set

o

ccurred

in

the

training

data,

and

examine

p

erformance

within

eac

h

of

these

partitions.

More

sp

eci�cally

,

the

cross-en

trop

y

of

an

n-gram

mo

del

p

of

a

test

set

T

as

expressed

in

equation

()

can

b

e

rewritten

as

H

p

(T

)

=

�





W

T

X

w

i

i�n+

c

T

(w

i

i�n+

)

log



p(w

i

jw

i�

i�n+

)

where

the

sum

ranges

o

v

er

all

n-grams

and

c

T

(w

i

i�n+

)

is

the

n

um

b

er

of

o

ccurrences

of

the

n-

gram

w

i

i�n+

in

the

test

data.

Instead

of

summing

o

v

er

al

l

n-grams,

consider

summing

only

o

v

er

n-grams

with

exactly

r

coun

ts

in

the

training

data,

for

some

r

;

i.e.,

consider

the

v

alue

H

p;r

(T

)

=

�





W

T

X

w

i

i�n+

:c(w

i

i�n+

)=r

c

T

(w

i

i�n+

)

log



p(w

i

jw

i�

i�n+

)

()

Then,

w

e

migh

t

compare

the

v

alues

of

H

p;r

(T

)

b

et

w

een

mo

dels

p

for

eac

h

r

to

yield

a

more

detailed

picture

of

p

erformance.

Ho

w

ev

er,

there

are

t

w

o

orthogonal

comp

onen

ts

that

determine

the

v

alue

H

p;r

(T

),

and

it

is

informativ

e

to

separate

them.

First,

there

is

the

total

probabilit

y

mass

M

p;r

(T

)

that

a

mo

del

p




uses

to

predict

n-grams

with

exactly

r

coun

ts

giv

en

the

histories

in

the

test

set,

i.e.,

the

v

alue

M

p;r

(T

)

=

X

w

i

i�n+

:c(w

i

i�n+

)=r

c

T

(w

i�

i�n+

)p(w

i

jw

i�

i�n+

)

An

in

terpretation

of

the

v

alue

M

p;r

(T

)

is

the

exp

e

cte

d

c

ount

in

the

test

set

T

of

n-grams

with

r

coun

ts

according

to

mo

del

p,

giv

en

the

histories

in

the

test

set.

Ideally

,

the

v

alue

of

M

p;r

(T

)

should

matc

h

the

actual

n

um

b

er

of

n-grams

in

the

test

set

T

that

ha

v

e

r

coun

ts

in

the

training

data,

c

r

(T

),

where

c

r

(T

)

=

X

w

i

i�n+

:c(w

i

i�n+

)=r

c

T

(w

i

i�n+

)

The

v

alue

M

p;r

(T

)

is

prop

ortional

to

the

a

v

erage

probabilit

y

a

mo

del

p

assigns

to

n-grams

with

r

coun

ts;

an

algorithm

with

a

larger

M

p;r

(T

)

will

tend

to

ha

v

e

a

lo

w

er

H

p;r

(T

).

No

w,

consider

a

metric

similar

to

H

p;r

(T

)

where

w

e

factor

out

the

con

tribution

of

M

p;r

(T

),

so

that

algorithms

with

a

larger

M

p;r

(T

)

will

not

tend

to

receiv

e

a

b

etter

score.

That

is,

consider

a

metric

where

w

e

scale

probabilities

so

that

all

algorithms

dev

ote

the

same

total

probabilit

y

to

n-grams

with

r

coun

ts

for

eac

h

r

.

In

particular,

w

e

use

the

v

alue

H

�

p;r

(T

)

=

�





W

T

X

w

i

i�n+

:c(w

i

i�n+

)=r

c

T

(w

i

i�n+

)

log



c

r

(T

)



M

p;r

(T

)

p(w

i

jw

i�

i�n+

)

This

is

similar

to

de�ning

an

(improp

er)

distribution

p

�

(w

i

jw

i�

i�n+

)

=

c

r

(T

)



M

p;r

(T

)

p(w

i

jw

i�

i�n+

)

where

w

e

are

assured

M

p

�

;r

(T

)

=

c

r

(T

)

as

is

ideal,

and

calculating

the

p

erformance

H

p

�

;r

(T

)

for

this

new

mo

del.

As

the

measure

H

�

p;r

(T

)

assures

that

eac

h

mo

del

predicts

eac

h

coun

t

r

with

the

same

total

mass,

this

v

alue

just

measures

ho

w

w

ell

a

mo

del

distributes

its

probabilit

y

mass

among

n-grams

with

the

same

coun

t.

T

o

recap,

w

e

can

use

the

measure

M

p;r

(T

)

to

determine

ho

w

w

ell

a

smo

othed

mo

del

p

assigns

probabilities

on

a

v

erage

to

n-grams

with

r

coun

ts

in

the

training

data;

in

particular,

w

e

w

an

t

M

p;r

(T

)



c

r

(T

)

(or

the

ratio

b

et

w

een

exp

ected

and

actual

coun

ts

in

the

training

data)

to

b

e

near



for

all

r

.

The

v

alue

H

�

p;r

(T

),

whic

h

w

e

refer

to

as

normalize

d

cr

oss-entr

opy

or

normalize

d

p

erformanc

e,

measures

ho

w

w

ell

a

smo

othed

mo

del

p

distributes

probabilities

b

et

w

een

n-grams

with

the

same

coun

t;

as

with

cross-en

trop

y

,

the

lo

w

er

the

b

etter.

W

e

ran

exp

erimen

ts

with

coun

t-b

y-coun

t

analysis

for

t

w

o

training

set

sizes,

0,000

sen

tences

(ab

out

0,000

w

ords)

and

,00,000

sen

tences

(ab

out



milli

on

w

ords),

on

the

WSJ/NAB

corpus.

W

e

used

a

test

set

of

ab

out

0

million

w

ords;

a

larger

test

set

w

as

desirable

b

ecause

of

the

sparseness

of

n-grams

with

exactly

r

coun

ts

for

larger

r

.

..

Exp

ected

vs.

Actual

Coun

ts,

Ov

erall

In

Figure

,

w

e

displa

y

the

ratio

of

exp

ected

to

actual

coun

ts

M

p;r

(T

)



c

r

(T

)

for

v

arious

algorithms

on

the

larger

training

set

for

bigram

and

trigram

mo

dels

for

lo

w

coun

ts

r

�

.

In

Figure

,

w

e

ha

v

e

the

analogous

graphs

for

higher

coun

ts



�

r

&lt;

0.



F

or

lo

w

coun

ts,

w

e

see

that

the

algorithms





F

or

the

zero-coun

t

case,

w

e

exclude

those

n-grams

w

i

i�n+

for

whic

h

the

corresp

ondin

g

history

w

i�

i�n+

has

no

coun

ts,

i.e.,

for

whic

h

P

w

i

c(w

i�

i�n+

w

i

)

=

0.




0.6

0.8

1

1.2

1.4

1.6

1.8

2

0

1

2

3

4

5

expected count / actual count

count

ratio of expected to actual counts, 2-gram, 75M words training

ideal

jelinek-mercer-baseline

jelinek-mercer

witten-bell-backoff

katz

kneser-ney-mod

kneser-ney

abs-disc-interp

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

0

1

2

3

4

5

expected count / actual count

count

ratio of expected to actual counts, 3-gram, 75M words training

ideal

jelinek-mercer-baseline

jelinek-mercer

witten-bell-backoff

katz

kneser-ney-mod

kneser-ney

abs-disc-interp

Figure

:

Ratio

of

exp

ected

n

um

b

er

to

actual

n

um

b

er

in

test

set

of

n-grams

with

a

giv

en

coun

t

in

training

data

for

v

arious

smo

othing

algorithms,

lo

w

coun

ts,

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

0.8

0.9

1

1.1

1.2

1.3

1.4

1.5

5

10

15

20

25

30

35

expected count / actual count

count

ratio of expected to actual counts, 2-gram, 75M words training

ideal

abs-disc-interp

kneser-ney

kneser-ney-mod

katz

jelinek-mercer-baseline

jelinek-mercer

witten-bell-backoff

0.8

0.9

1

1.1

1.2

1.3

1.4

1.5

5

10

15

20

25

30

35

expected count / actual count

count

ratio of expected to actual counts, 3-gram, 75M words training

ideal

abs-disc-interp

kneser-ney

kneser-ney-mod

katz

jelinek-mercer-baseline

jelinek-mercer

witten-bell-backoff

Figure

:

Ratio

of

exp

ected

n

um

b

er

to

actual

n

um

b

er

in

test

set

of

n-grams

with

a

giv

en

coun

t

in

training

data

for

v

arious

smo

othing

algorithms,

high

coun

ts,

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels




0

0.5

1

1.5

2

1

3

5

7

9

11

13

average correct discount

original count

1M words training

trigram

bigram

0

0.5

1

1.5

2

1

3

5

7

9

11

13

average correct discount

original count

200M words training

trigram

bigram

Figure

:

Correct

a

v

erage

discoun

t

for

n-grams

with

a

giv

en

coun

t

in

training

data

on

t

w

o

training

set

sizes,

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

katz

and

kneser-ney-mod

come

closest

to

the

ideal

v

alue

of

.

The

v

alues

farthest

from

the

ideal

are

attained

b

y

the

metho

ds

jelinek-mercer-b

aseli

ne,

jelinek-mercer,

and

witten-bell-

backoff.

These

algorithms

assign

signi�can

tly

to

o

m

uc

h

probabilit

y

on

a

v

erage

to

n-grams

with

lo

w

coun

ts.

F

or

high

coun

ts,

katz

is

nearest

to

the

ideal.

T

o

explain

these

b

eha

viors,

w

e

calculate

the

ide

al

aver

age

disc

ount

for

eac

h

coun

t.

That

is,

consider

all

n-grams

w

i

i�n+

with

coun

t

r

.

Let

us

assume

that

w

e

p

erform

smo

othing

b

y

pretending

that

all

suc

h

n-grams

actually

receiv

e

r

�

coun

ts;

i.e.,

instead

of

the

maxim

um

-l

ik

eli

ho

o

d

distribution

p

ML

(w

i

jw

i�

i�n+

)

=

r



c(w

i�

i�n+

)

w

e

tak

e

p

0

(w

i

jw

i�

i�n+

)

=

r

�



c(w

i�

i�n+

)

Then,

w

e

can

calculate

the

v

alue

of

r

�

suc

h

that

the

ideal

probabilit

y

mass

M

p

0

;r

(T

)

=

c

r

(T

)

is

ac

hiev

ed.

W

e

tak

e

r

�

r

�

for

the

ideal

r

�

to

b

e

the

ide

al

aver

age

disc

ount

for

coun

t

r

.

This

is

an

estimate

of

the

correct

n

um

b

er

of

coun

ts

on

a

v

erage

to

tak

e

a

w

a

y

from

all

n-grams

with

r

coun

ts

in

the

training

data.

In

Figure

,

w

e

graph

the

empirical

estimate

of

this

v

alue

for

r

�



for

bigram

and

trigram

mo

dels

for

a

one

million

and

00

milli

on

w

ord

training

set.

(F

or

v

alues

ab

o

v

e

r

=

,

the

graph

b

ecomes

v

ery

noisy

due

to

data

sparsit

y

.)

W

e

can

see

that

for

v

ery

small

r

the

correct

discoun

t

rises

quic

kly

,

and

then

lev

els

o�.

In

other

w

ords,

it

seems

that

a

sc

heme

that

discoun

ts

di�eren

t

r

uniformly

is

more

appropriate

than

a

sc

heme

that

assigns

discoun

ts

that

are

prop

ortional

to

r

.

Algorithms

that

fall

under

the

former

category

include

abs-disc-interp

and

kneser-ney;

these

algorithms

use

a

�xed

discoun

t

D

n

o

v

er

all

coun

ts.

Algorithms

that

fall

in

the

latter

category

include

all

three

algorithms

that

fared

p

o

orly

in

Figures



and

:

jelinek-mercer-bas

eline

,

jelinek-mercer,

and

witten-

bell-backoff.

These

algorithms

are

all

of

the

form

giv

en

in

equation

()

p

in

terp

(w

i

jw

i�

i�n+

)

=

�

w

i�

i�n+

p

ML

(w

i

jw

i�

i�n+

)

+

(

�

�

w

i�

i�n+

)

p

in

terp

(w

i

jw

i�

i�n+

)




where

the

discoun

t

of

an

n-gram

with

coun

t

r

is

appro

ximately

r

�

�r

.

Because

discoun

ts

are

linear

in

r

when

ideally

they

should

b

e

roughly

constan

t,

discoun

ts

for

these

algorithms

w

ere

to

o

lo

w

for

lo

w

coun

ts

and

to

o

high

for

high

coun

ts.

Katz

smo

othing

c

ho

oses

discoun

ts

according

to

the

Go

o

d-T

uring

discoun

t,

whic

h

theoretically

should

estimate

the

correct

a

v

erage

discoun

t

w

ell,

and

w

e

�nd

this

to

b

e

the

case

empirically

.

While

Katz

assigns

the

correct

total

mass

to

n-grams

with

a

particular

coun

t,

it

do

es

not

p

erform

particularly

w

ell

b

ecause

it

do

es

not

distribute

probabilities

w

ell

b

et

w

een

n-grams

with

the

same

coun

t,

as

w

e

shall

see

when

w

e

examine

its

normalized

cross-en

trop

y

.

The

algorithm

kneser-ney-mod

uses

a

uniform

discoun

t

D

n;+

for

all

coun

ts

three

and

ab

o

v

e,

but

separate

discoun

ts

D

n;

and

D

n;

for

one-

and

t

w

o-coun

ts.

This

mo

di�cation

of

Kneser-Ney

smo

othing

w

as

motiv

ated

b

y

the

observ

ation

in

Figure



that

smaller

coun

ts

ha

v

e

a

signi�can

tly

di�eren

t

ideal

a

v

erage

discoun

t

than

larger

coun

ts.

Indeed,

in

Figure



w

e

see

that

kneser-ney-

mod

is

m

uc

h

closer

to

the

ideal

than

kneser-ney

for

lo

w

coun

ts.

(The

p

erformance

gain

in

using

separate

discoun

ts

for

coun

ts

larger

than

t

w

o

is

marginal.

)

..

Normalized

P

erformance,

Ov

erall

In

Figure

,

w

e

displa

y

the

normalized

cross-en

trop

y

H

�

p;r

(T

)

of

v

arious

algorithms

relativ

e

to

the

normalized

cross-en

trop

y

of

the

baseline

algorithm

on

the



million

w

ord

training

set

for

bigram

and

trigram

mo

dels

for

lo

w

coun

ts

r

�

.

In

Figure

,

w

e

ha

v

e

the

analogous

graphs

for

higher

coun

ts



�

r

&lt;

0.

F

or

the

p

oin

ts

on

the

graph

with

a

coun

t

of

0,

w

e

exclude

those

n-grams

w

i

i�n+

for

whic

h

the

corresp

onding

history

w

i�

i�n+

has

no

coun

ts,

i.e.,

for

whic

h

P

w

i

c(w

i�

i�n+

w

i

)

=

0.

The

asso

ciated

v

alues

for

these

cases

are

displa

y

ed

under

a

coun

t

v

alue

of

-.

W

e

see

that

kneser-ney

and

kneser-ney-mod

signi�can

tly

outp

erform

all

other

algorithms

on

lo

w

coun

ts,

esp

ecially

for

the

p

oin

t

with

a

coun

t

v

alue

of

zero.

W

e

attribute

this

to

the

mo

di�ed

bac

k

o�

distribution

that

is

used

in

Kneser-Ney

smo

othing

as

describ

ed

in

Section

..

As

the

ratio

of

exp

ected

to

actual

coun

ts

for

these

algorithms

is

not

signi�can

tly

sup

erior

to

those

for

all

other

algorithms,

and

as

their

normalized

p

erformance

on

high

coun

ts

is

go

o

d

but

not

remark

able,

w

e

conclude

that

their

excellen

t

normalized

p

erformance

on

lo

w

coun

ts

is

the

reason

for

their

consisten

tly

sup

erior

o

v

erall

p

erformance.

The

algorithms

with

the

w

orst

normalized

p

erformance

on

lo

w

(nonzero)

coun

ts

are

katz

and

witten-bell-backo

ff;

these

are

also

the

only

t

w

o

algorithms

sho

wn

that

use

bac

k

o�

instead

of

in

terp

olation.

Th

us,

it

seems

that

for

lo

w

coun

ts

lo

w

er-order

distributions

pro

vide

v

aluable

information

ab

out

the

correct

amoun

t

to

discoun

t,

and

th

us

in

terp

olation

is

sup

erior

for

these

situations.

Bac

k

o�

mo

dels

do

not

use

lo

w

er-order

distributions

to

help

estimate

the

probabilit

y

of

n-grams

with

lo

w

(nonzero)

coun

ts.

F

or

large

coun

ts,

the

t

w

o

w

orst

p

erforming

algorithms

are

jelinek-mercer

and

jelinek-

mercer-baseline.

W

e

h

yp

othesize

that

this

is

due

to

a

com

bination

of

t

w

o

factors.

First,

b

oth

algorithms

use

linear

discoun

ting,

whic

h

as

men

tioned

in

Section

..

leads

to

large

discoun

ts

for

large

coun

ts.

Second,

these

mo

dels

are

in

terp

olated

as

opp

osed

to

bac

k

o�

mo

dels,

so

that

these

discoun

ts

v

ary

according

to

lo

w

er-order

mo

dels.

Because

of

these

t

w

o

factors,

discoun

ts

for

n-grams

with

large

coun

ts

can

v

ary

widely

from

n-gram

to

n-gram.

Giv

en

that

smo

othing

metho

ds

that

assign

the

same

probabilit

y

to

n-grams

with

a

giv

en

coun

t

across

di�eren

t

distributions

(suc

h

as

Katz)

p

erform

w

ell

on

large

coun

ts,

w

e

h

yp

othesize

that

the

ideal

discoun

t

for

n-grams

with

a

giv

en

high

coun

t

r

should

not

v

ary

m

uc

h.

This

mismatc

h

in

the

v

ariation

of

discoun

ts

could

explain

the

p

o

or

p

erformance

of

jelinek-mercer

and

jelinek-mercer-base

line

in

this

domain.

All

of

the

other

algorithms

are

v

ery

near

to

eac

h

other

in

terms

of

normalized

p

erformance

on

large

coun

ts;

w

e

guess

that

it

do

es

not

matter

m

uc

h

ho

w

large

coun

ts

are

smo

othed

as

long

as

they

are




-0.8

-0.6

-0.4

-0.2

0

-1

0

1

2

3

4

5

diff in test cross-entropy from baseline (bits/token)

count

normalized performance for each count, 2-gram, 75M words training

katz

witten-bell-backoff

abs-disc-interp

jelinek-mercer

kneser-ney

kneser-ney-mod

-0.5

-0.4

-0.3

-0.2

-0.1

0

0.1

0.2

0.3

-1

0

1

2

3

4

5

diff in test cross-entropy from baseline (bits/token)

count

normalized performance for each count, 3-gram, 75M words training

katz

witten-bell-backoff

abs-disc-interp

jelinek-mercer

kneser-ney

kneser-ney-mod

Figure

:

Normalized

cross-en

trop

y

for

n-grams

with

a

giv

en

coun

t

in

training

data

for

v

arious

smo

othing

algorithms,

lo

w

coun

ts,

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

-0.1

-0.08

-0.06

-0.04

-0.02

0

0.02

0.04

0.06

5

10

15

20

25

30

35

diff in test cross-entropy from baseline (bits/token)

count

normalized performance for each count, 2-gram, 75M words training

jelinek-mercer

abs-disc-interp

witten-bell-backoff

kneser-ney

kneser-ney-mod

katz

-0.1

-0.08

-0.06

-0.04

-0.02

0

0.02

0.04

0.06

5

10

15

20

25

30

35

diff in test cross-entropy from baseline (bits/token)

count

normalized performance for each count, 3-gram, 75M words training

jelinek-mercer

abs-disc-interp

witten-bell-backoff

kneser-ney

kneser-ney-mod

katz

Figure

:

Normalized

cross-en

trop

y

for

n-grams

with

a

giv

en

coun

t

in

training

data

for

v

arious

smo

othing

algorithms,

high

coun

ts,

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels




0

0.2

0.4

0.6

0.8

1

100

1000

10000

100000

1e+06

1e+07

cumulative fraction of cross-entropy

training set size (sentences)

bigram model

r=0

r&lt;=1

r&lt;=2

r&lt;=10

r&lt;=infinity

0

0.2

0.4

0.6

0.8

1

100

1000

10000

100000

1e+06

1e+07

cumulative fraction of cross-entropy

training set size (sentences)

trigram model

r=0

r&lt;=1

r&lt;=2

r&lt;=10

r&lt;=infinity

Figure

:

Cum

ulativ

e

fraction

of

cross-en

trop

y

on

test

set

dev

oted

to

n-grams

with

r

or

few

er

coun

ts

in

training

data

for

v

arious

r

on

WSJ/NAB

corpus,

jelinek-mercer-ba

selin

e

smo

othing,

bigram

and

trigram

mo

dels

not

mo

di�ed

to

o

m

uc

h.

..

P

erformance

V

ariation

Ov

er

T

raining

Set

Size

Giv

en

the

preceding

analysis,

it

is

relev

an

t

to

note

what

fraction

of

the

total

en

trop

y

of

the

test

data

is

asso

ciated

with

n-grams

of

di�eren

t

coun

ts,

to

determine

ho

w

the

p

erformance

for

eac

h

coun

t

a�ects

o

v

erall

p

erformance.

In

Figure

,

w

e

displa

y

the

cum

ulativ

e

v

alues

of

H

p;r

(T

)



H

p

(T

)

(see

equation

())

for

di�eren

t

coun

ts

r

for

the

baseline

algorithm

o

v

er

a

range

of

training

set

sizes

for

bigram

and

trigram

mo

dels

on

the

WSJ/NAB

corpus.

A

line

lab

eled

r

�

k

graphs

the

fraction

of

the

en

trop

y

dev

oted

to

n-grams

with

up

to

k

coun

ts,

i.e.,

P

k

r

=0

H

p;r

(T

)



H

p

(T

)

.

Actually

,

this

is

not

quite

accurate,

as

w

e

exclude

from

this

v

alue

the

con

tribution

from

all

n-grams

w

i

i�n+

for

whic

h

the

corresp

onding

history

w

i�

i�n+

has

no

coun

ts.

The

con

tribution

from

these

n-grams

represen

t

the

area

ab

o

v

e

the

r

�



line.

As

w

ould

b

e

exp

ected,

the

prop

ortion

of

the

en

trop

y

dev

oted

to

n-grams

with

high

coun

ts

gro

ws

as

the

size

of

the

training

set

gro

ws.

More

surprising

is

the

fraction

of

the

en

trop

y

dev

oted

to

lo

w

coun

ts

in

trigram

mo

dels

ev

en

for

v

ery

large

training

sets;

for

a

training

set

of

0

million

sen

tences

ab

out

0%

of

the

en

trop

y

comes

from

trigrams

with

zero

coun

ts

in

the

training

data.

This

explains

the

large

impact

that

p

erformance

on

lo

w

coun

ts

has

on

o

v

erall

p

erformance,

and

wh

y

mo

di�ed

Kneser-Ney

smo

othing

has

the

b

est

o

v

erall

p

erformance

ev

en

though

it

excels

mostly

on

lo

w

coun

ts

only

.

In

com

bination

with

the

previous

analysis,

this

data

also

explains

some

of

the

v

ariation

in

the

relativ

e

p

erformance

of

di�eren

t

algorithms

o

v

er

di�eren

t

training

set

sizes

and

b

et

w

een

bigram

and

trigram

mo

dels.

In

particular,

algorithms

that

p

erform

w

ell

on

lo

w

coun

ts

will

p

erform

w

ell

o

v

erall

when

lo

w

coun

ts

form

a

larger

fraction

of

the

total

en

trop

y

(i.e.,

small

data

sets),

and

con

v

ersely

,

algorithms

that

p

erform

w

ell

on

high

coun

ts

will

p

erform

b

etter

on

large

data

sets.

F

or

example,

the

observ

ation

that

jelinek-mercer

outp

erforms

katz

on

small

data

sets

while

katz

is

sup

erior

on

large

data

sets

is

explained

b

y

the

fact

that

katz

is

sup

erior

on

high

coun

ts

while

jelinek-mercer

is

sup

erior

on

lo

w

coun

ts.

Similarly

,

since

bigram

mo

dels

con

tain

more

	


high

coun

ts

than

trigram

mo

dels

on

the

same

size

data,

katz

p

erforms

b

etter

on

bigram

mo

dels

than

on

trigram

mo

dels.

..

Bac

k

o�

vs.

In

terp

olatio

n

In

this

section,

w

e

examine

the

coun

t-b

y-coun

t

p

erformance

of

the

bac

k

o�

and

in

terp

olated

v

ersions

of

sev

eral

smo

othing

algorithms,

namely

the

algorithms:

witten-bell-interp

and

witten-bell-

backoff,

abs-disc-interp

and

abs-disc-backoff,

and

kneser-ney-mod

and

kneser-ney-mod-

backoff.

In

Figures

	

and

0,

w

e

displa

y

the

normalized

p

erformance

of

the

bac

k

o�

and

in

terp

olated

v

ersions

of

Witten-Bell

and

mo

di�ed

Kneser-Ney

smo

othing

o

v

er

a

range

of

coun

ts

for

b

oth

bigram

and

trigram

mo

dels.

W

e

can

see

that

the

in

terp

olated

algorithms

signi�can

tly

outp

erform

the

bac

k

o�

algorithms

on

lo

w

(p

ositiv

e)

coun

ts.

Though

not

sho

wn,

this

holds

for

absolute

discoun

ting

as

w

ell.

As

discussed

in

Section

..,

it

seems

that

for

lo

w

coun

ts

lo

w

er-order

distributions

pro

vide

v

aluable

information

ab

out

the

correct

amoun

t

to

discoun

t,

and

th

us

in

terp

olation

is

sup

erior

for

these

situations.

In

Figures



and

,

w

e

displa

y

the

ratio

of

exp

ected

to

actual

coun

ts

of

the

bac

k

o�

and

in

terp

olated

v

ersions

of

Witten-Bell

and

mo

di�ed

Kneser-Ney

smo

othing

o

v

er

a

range

of

coun

ts

for

b

oth

bigram

and

trigram

mo

dels.

F

or

mo

di�ed

Kneser-Ney

smo

othing,

w

e

see

that

the

bac

k

o�

v

ersion

is

generally

closer

to

the

ideal

according

to

this

criterion.

Though

not

sho

wn,

w

e

see

similar

b

eha

vior

for

absolute

discoun

ting.

F

or

Witten-Bell

smo

othing,

w

e

see

that

the

bac

k

o�

v

ersion

is

closer

to

the

ideal

for

small

coun

ts,

but

not

quite

as

close

for

large

coun

ts.

Ho

w

ev

er,

the

in

terp

olated

v

ersion

is

signi�can

tly

w

orse

for

the

coun

t

of

zero,

b

eing

a

factor

of

.{

a

w

a

y

from

the

ideal.

W

e

h

yp

othesize

that

the

b

etter

p

erformance

of

the

bac

k

o�

mo

del

on

lo

w

coun

ts

on

this

criterion

is

the

reason

for

its

b

etter

o

v

erall

p

erformance.

Th

us,

w

e

see

that

for

these

mo

dels

the

in

terp

olated

v

ersions

generally

ha

v

e

b

etter

normalized

cross-en

tropies,

while

the

bac

k

o�

v

ersions

ha

v

e

more

ideal

exp

ected-to-actual

coun

t

ratios.

W

e

h

yp

othesize

that

the

relativ

e

strength

of

these

t

w

o

in�uences

determine

the

relativ

e

p

erformance

of

the

bac

k

o�

and

in

terp

olated

v

ersions

of

an

algorithm.

Since

the

relativ

e

strengths

of

these

factors

v

ary

,

whether

the

bac

k

o�

or

in

terp

olated

v

ersion

of

an

algorithm

is

sup

erior

dep

ends

on

the

algorithm,

as

w

e

ha

v

e

seen

earlier.

.

Auxiliary

Exp

erime

n

ts

..

Higher

Order

n-Gram

Mo

dels

Due

to

the

increasing

sp

eed

and

memory

of

computers,

there

has

b

een

some

use

of

higher-order

n-gram

mo

dels

suc

h

as

-gram

and

-gram

mo

dels

in

sp

eec

h

recognition

in

recen

t

y

ears

(Seymore

et

al.,

		;

W

eng,

Stolc

k

e,

and

Sank

ar,

		).

In

this

section,

w

e

examine

ho

w

v

arious

smo

othing

algorithms

p

erform

for

these

larger

mo

dels.

In

Figure

,

w

e

displa

y

the

p

erformance

of

-gram

through

-gram

mo

dels

relativ

e

to

a

tri-

gram

mo

del

(all

with

jelinek-mercer-base

line

smo

othing)

on

v

arious

training

set

sizes

on

the

WSJ/NAB

corpus.

As

w

ould

b

e

exp

ected,

the

larger

the

training

set,

the

larger

the

gain

in

using

a

higher-order

mo

del.

F

or

v

ery

large

data

sets,

the

gain

in

using

a

-gram

or

-gram

mo

del

o

v

er

a

trigram

mo

del

can

b

ecome

quite

signi�can

t,

o

v

er

0.

bits/w

ord.

Note

that

all

of

these

mo

dels

w

ere

built

with

no

coun

t

cuto�s;

Chen

(		)

giv

es

a

description

of

our

implemen

tation.

In

Figure

,

w

e

displa

y

the

relativ

e

p

erformance

of

v

arious

smo

othing

algorithms

relativ

e

to

the

baseline

metho

d

for

-gram

and

-gram

mo

dels

o

v

er

a

range

of

training

set

sizes

on

the

WSJ/NAB

corpus.

Again,

w

e

see

kneser-ney

and

kneser-ney-mod

consisten

tly

outp

erforming

0


-0.4

-0.3

-0.2

-0.1

0

0

5

10

15

20

25

30

35

diff in test cross-entropy from baseline (bits/token)

count

normalized performance for each count, 2-gram, 75M words training

jelinek-mercer-baseline

witten-bell-backoff

witten-bell-interp

-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

0.15

0.2

0

5

10

15

20

25

30

35

diff in test cross-entropy from baseline (bits/token)

count

normalized performance for each count, 3-gram, 75M words training

jelinek-mercer-baseline

witten-bell-backoff

witten-bell-interp

Figure

	:

Normalized

cross-en

trop

y

for

n-grams

with

a

giv

en

coun

t

in

training

data

for

witten-

bell-backoff

and

witten-bell-interp

,

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

-0.4

-0.3

-0.2

-0.1

0

0.1

0

5

10

15

20

25

30

35

diff in test cross-entropy from baseline (bits/token)

count

normalized performance for each count, 2-gram, 750k words training

jelinek-mercer-baseline

kneser-ney-mod

kneser-ney-mod-backoff

-0.3

-0.2

-0.1

0

0.1

0.2

0.3

0.4

0

5

10

15

20

25

30

35

diff in test cross-entropy from baseline (bits/token)

count

normalized performance for each count, 3-gram, 750k words training

jelinek-mercer-baseline

kneser-ney-mod

kneser-ney-mod-backoff

Figure

0:

Normalized

cross-en

trop

y

for

n-grams

with

a

giv

en

coun

t

in

training

data

for

kneser-ney-mod

and

kneser-ney-mod-b

ackof

f,

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels




0.6

0.8

1

1.2

1.4

1.6

1.8

0

5

10

15

20

25

30

35

expected count / actual count

count

ratio of expected to actual counts, 2-gram, 75M words training

ideal

witten-bell-backoff

witten-bell-interp

0.5

1

1.5

2

0

5

10

15

20

25

30

35

expected count / actual count

count

ratio of expected to actual counts, 3-gram, 75M words training

ideal

witten-bell-backoff

witten-bell-interp

Figure

:

Ratio

of

exp

ected

n

um

b

er

to

actual

n

um

b

er

in

test

set

of

n-grams

with

a

giv

en

coun

t

in

training

data

for

witten-bell-backo

ff

and

witten-bell-inte

rp,

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels

0.9

0.95

1

1.05

1.1

0

5

10

15

20

25

30

35

expected count / actual count

count

ratio of expected to actual counts, 2-gram, 750k words training

ideal

kneser-ney-mod

kneser-ney-mod-backoff

0.85

0.9

0.95

1

1.05

1.1

1.15

0

5

10

15

20

25

30

35

expected count / actual count

count

ratio of expected to actual counts, 3-gram, 750k words training

ideal

kneser-ney-mod

kneser-ney-mod-backoff

Figure

:

Ratio

of

exp

ected

n

um

b

er

to

actual

n

um

b

er

in

test

set

of

n-grams

with

a

giv

en

coun

t

in

training

data

for

kneser-ney-mod

and

kneser-ney-mod-b

ackof

f,

WSJ/NAB

corpus,

bigram

and

trigram

mo

dels




-0.4

-0.2

0

0.2

0.4

0.6

0.8

100

1000

10000

100000

1e+06

difference in test cross-entropy from trigram (bits/token)

training set size (sentences)

relative performance of n-gram orders on WSJ/NAB corpus

2-gram

3-gram

4-gram

5-gram

Figure

:

P

erformance

relativ

e

to

trigram

mo

del

of

n-gram

mo

dels

of

v

arying

order

on

WSJ/NAB

corpus,

jelinek-mercer-base

line

smo

othing

-0.3

-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

1e+06

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 4-gram

baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

jelinek-mercer

witten-bell-backoff

-0.3

-0.25

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

1e+06

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

relative performance of algorithms on WSJ/NAB corpus, 5-gram

jelinek-mercer-baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

jelinek-mercer

w-b-b

Figure

:

P

erformance

relativ

e

to

baseline

of

v

arious

algorithms

on

WSJ/NAB

corpus,

-gram

and

-gram

mo

dels




the

other

algorithms.

In

addition,

w

e

see

that

algorithms

that

do

not

p

erform

w

ell

on

small

data

sets

for

bigram

and

trigram

mo

dels

p

erform

somewhat

w

orse

on

these

higher-order

mo

dels,

as

the

use

of

a

larger

mo

del

exacerbates

the

sparse

data

problem.

The

metho

ds

katz,

abs-disc-interp

,

and

witten-bell-backo

ff

p

erform

ab

out

as

w

ell

or

w

orse

than

the

baseline

algorithm

except

for

the

largest

data

sets.

On

the

other

hand,

jelinek-mercer

consisten

tly

outp

erforms

the

baseline

algorithm.

..

Coun

t

Cuto�s

F

or

large

data

sets,

c

ount

cuto�s

are

often

used

to

restrict

the

size

of

the

n-gram

mo

del

constructed.

With

coun

t

cuto�s,

all

n-grams

of

a

certain

length

with

few

er

than

a

giv

en

n

um

b

er

of

o

ccurrences

in

the

training

data

are

ignored

in

some

fashion.

Ho

w

coun

ts

are

\ignored"

is

algorithm-sp

eci�c,

and

has

not

generally

b

een

sp

eci�ed

in

the

original

descriptions

of

previous

smo

othing

algorithms.

In

these

exp

erimen

ts,

w

e

implem

en

ted

what

w

e

felt

w

as

the

most

\natural"

w

a

y

to

add

cuto�s

to

v

arious

algorithms.

The

general

strategy

w

e

to

ok

w

as:

for

n-grams

with

coun

ts

b

elo

w

the

cuto�s,

w

e

pretended

they

o

ccurred

zero

times

and

assigned

probabilities

through

bac

k

o�/in

terp

olation;

for

n-grams

with

coun

ts

ab

o

v

e

the

cuto�s,

w

e

assigned

similar

probabilities

as

in

the

non-cuto�

case;

and

w

e

adjusted

the

bac

k

o�/in

terp

olation

scaling

factors

so

that

distributions

w

ere

correctly

normalized.

F

or

instance,

for

Katz

smo

othing

(Section

.)

w

e

use

the

iden

tical

d

r

as

in

the

non-cuto�

case,

but

instead

of

equation

()

w

e

use

the

follo

wing

equation

c

k

atz

(w

i

i�

)

=

�

d

r

r

if

r

&gt;

r

cut

�(w

i�

)

p

ML

(w

i

)

if

r

�

r

cut

where

r

cut

is

the

corresp

onding

cuto�,

and

where

�(w

i�

)

is

still

c

hosen

so

that

the

total

n

um

b

er

of

coun

ts

in

the

distribution

is

unc

hanged.

Later

in

this

section,

w

e

brie�y

describ

e

our

coun

t

cuto�

implemen

tations

for

v

arious

algorithms.

T

o

in

tro

duce

the

terminology

w

e

use

to

describ

e

cuto�

mo

dels,

w

e

use

an

example:

0-0-

cuto�s

for

a

trigram

mo

del

signals

that

all

unigrams

with

0

or

few

er

coun

ts

are

ignored,

all

bigrams

with

0

or

few

er

coun

ts

are

ignored,

and

all

trigrams

with



or

few

er

coun

ts

are

ignored.

Mo

dels

with

no

cuto�s

can

b

e

said

to

ha

v

e

0-0-0

cuto�s.

Using

cuto�s

of

one

or

t

w

o

for

bigrams

and

trigrams

can

greatly

decrease

the

size

of

a

mo

del,

while

yielding

only

a

small

degradation

in

p

erformance.

In

Figure

,

w

e

displa

y

the

p

erformance

of

bigram

and

trigram

mo

dels

with

di�eren

t

cuto�s

relativ

e

to

the

corresp

onding

mo

del

with

no

cuto�s

for

jelinek-mercer-ba

selin

e

smo

othing

on

v

arious

training

set

sizes

on

the

WSJ/NAB

corpus.

F

or

bigram

mo

dels,

w

e

see

that

mo

dels

with

higher

cuto�s

tend

to

p

erform

more

p

o

orly

as

w

ould

b

e

exp

ected,

though

for

v

ery

large

training

sets

0-

cuto�s

are

comparable

with

no

cuto�s.

Ho

w

ev

er,

for

trigram

mo

dels

w

e

see

that

mo

dels

with

0-0-

cuto�s

actually

outp

erform

mo

dels

with

no

cuto�s

o

v

er

most

of

the

training

set

sizes.

In

other

w

ords,

it

seems

that

the

algorithm

jelinek-mercer-bas

eline

smo

oths

trigrams

with

one

coun

t

so

p

o

orly

that

using

these

coun

ts

actually

h

urts

p

erformance.

T

o

sho

w

that

this

b

eha

vior

do

es

not

hold

for

all

smo

othing

algorithms,

in

Figure



w

e

displa

y

the

graph

analogous

to

the

graph

on

the

righ

t

of

Figure



except

using

kneser-ney-mod

instead

of

jelinek-mercer-

baseline

smo

othing.

F

or

kneser-ney-mod,

w

e

see

that

mo

dels

with

cuto�s

indeed

p

erform

more

p

o

orly

than

mo

dels

without

cuto�s.

The

decrease

in

p

erformance

is

mo

derate

for

these

cuto�

v

alues,

though,

esp

ecially

for

larger

data

sets

(ab

out

0.0

bits/w

ord).

In

Figures



and

,

w

e

displa

y

the

p

erformance

of

v

arious

smo

othing

algorithms

for

bigram

and

trigram

mo

dels,

resp

ectiv

ely

,

for

di�eren

t

cuto�s

o

v

er

a

range

of

training

set

sizes

on

the

WSJ/NAB

corpus.

Ov

erall,

w

e

see

that

the

ordering

of

algorithms

b

y

p

erformance

is

largely




0

0.02

0.04

0.06

0.08

0.1

0.12

100

1000

10000

100000

1e+06

difference in test cross-entropy from no cutoffs (bits/token)

training set size (sentences)

relative performance of cutoffs on WSJ/NAB corpus, bigram

no cutoffs

0-1 cutoffs

0-2 cutoffs

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

1e+06

difference in test cross-entropy from no cutoffs (bits/token)

training set size (sentences)

relative performance of cutoffs on WSJ/NAB corpus, trigram

no cutoffs

0-0-1 cutoffs

0-0-2 cutoffs

0-1-1 cutoffs

Figure

:

P

erformance

relativ

e

to

mo

del

with

no

coun

t

cuto�s

of

mo

dels

with

cuto�s

on

WSJ/NAB

corpus,

jelinek-mercer-base

line

smo

othing,

bigram

and

trigram

mo

dels

0

0.05

0.1

0.15

0.2

100

1000

10000

100000

1e+06

difference in test cross-entropy from no cutoffs (bits/token)

training set size (sentences)

relative performance of cutoffs on WSJ/NAB corpus, trigram

no cutoffs

0-0-1 cutoffs

0-0-2 cutoffs

0-1-1 cutoffs

Figure

:

P

erformance

relativ

e

to

mo

del

with

no

coun

t

cuto�s

of

mo

dels

with

cuto�s

on

WSJ/NAB

corpus,

kneser-ney-mod

smo

othing,

trigram

mo

del




-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

1e+06

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

rel. perf. of algs. on WSJ/NAB corpus, 2-gram, 0-1 cutoffs

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

jelinek-mercer

witten-bell-backoff

-0.08

-0.06

-0.04

-0.02

0

0.02

0.04

0.06

0.08

0.1

100

1000

10000

100000

1e+06

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

rel. perf. of algs. on WSJ/NAB corpus, 2-gram, 0-2 cutoffs

jelinek-mercer-baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

jelinek-mercer

witten-bell-backoff

Figure

:

P

erformance

relativ

e

to

baseline

of

v

arious

algorithms

on

WSJ/NAB

corpus,

bigram

mo

del

with

0-

and

0-

cuto�s

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

1e+06

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

rel. perf. of algs. on WSJ/NAB corpus, 3-gram, 0-0-1 cutoffs

jelinek-mercer-baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

jelinek-mercer

witten-bell-backoff

-0.1

-0.05

0

0.05

0.1

100

1000

10000

100000

1e+06

diff in test cross-entropy from baseline (bits/token)

training set size (sentences)

rel. perf. of algs. on WSJ/NAB corpus, 3-gram, 0-1-1 cutoffs

jelinek-mercer-baseline

katz

kneser-ney

kneser-ney-mod

abs-disc-interp

j-m

witten-bell-backoff

Figure

:

P

erformance

relativ

e

to

baseline

of

v

arious

algorithms

on

WSJ/NAB

corpus,

trigram

mo

del

with

0-0-

and

0--

cuto�s




unc

hanged

from

the

non-cuto�

case;

kneser-ney

and

kneser-ney-mod

still

yield

the

b

est

p

er-

formance.

The

most

signi�can

t

di�erence

is

that

our

implemen

tatio

n

abs-disc-interp

p

erforms

more

p

o

orly

relativ

e

to

the

other

algorithms;

it

generally

p

erforms

w

orse

than

the

baseline

algo-

rithm,

unlik

e

in

the

non-cuto�

case.

In

addition,

the

magnitudes

of

the

di�erences

in

p

erformance

seem

to

b

e

less

when

cuto�s

are

used.

F

or

example,

for

trigram

mo

dels

with

cuto�s,

kneser-ney-

mod

p

erforms

up

to

0.

bits/w

ord

b

etter

than

jelinek-mercer-base

line;

for

mo

dels

with

no

cuto�s,

this

v

alue

is

around

0.

bits/w

ord.

Cuto�

Implemen

tat

io

ns

In

this

section,

w

e

brie�y

describ

e

our

implemen

tatio

ns

for

coun

t

cuto�s

for

v

arious

algorithms.

As

men

tioned

earlier,

the

general

strategy

w

e

to

ok

w

as:

for

n-grams

with

coun

ts

b

elo

w

the

cuto�s,

w

e

pretended

they

o

ccurred

zero

times

and

assigned

probabilities

through

bac

k

o�/in

terp

olation;

for

n-grams

with

coun

ts

ab

o

v

e

the

cuto�s,

w

e

assigned

similar

probabilities

as

in

the

non-cuto�

case;

and

w

e

adjusted

the

bac

k

o�/in

terp

olation

scaling

factors

so

that

distributions

w

ere

correctly

normalized.

F

or

the

implemen

tation

of

cuto�s

for

Katz

smo

othing,

refer

to

the

b

eginning

of

Section

...

F

or

jelinek-mercer

and

jelinek-mercer-base

line,

w

e

use

an

equation

analogous

to

equa-

tion

()

from

Section

.:

p

in

terp

(w

i

jw

i�

i�n+

)

=

�

0

w

i�

i�n+

p

cut

(w

i

jw

i�

i�n+

)

+

(

�

�

0

w

i�

i�n+

)

p

in

terp

(w

i

jw

i�

i�n+

)

()

where

p

cut

(w

i

jw

i�

i�n+

)

=

c

cut

(w

i

i�n+

)



P

w

i

c

cut

(w

i

i�n+

)

and

where

c

cut

(w

i

i�n+

)

=

�

c(w

i

i�n+

)

if

c(w

i

i�n+

)

&gt;

c

n

0

otherwise

where

c

n

is

the

coun

t

cuto�

for

that

n-gram

lev

el.

F

or

�

0

w

i�

i�n+

,

w

e

tak

e

�

0

w

i�

i�n+

=

P

w

i

c

cut

(w

i

i�n+

)



P

w

i

c(w

i

i�n+

)

�

w

i�

i�n+

Then,

the

left

term

on

the

righ

t-hand

side

of

equation

()

is

equiv

alen

t

to

the

corresp

onding

term

in

equation

();

i.e.,

n-grams

with

coun

ts

ab

o

v

e

the

cuto�

are

assigned

similar

probabilities

in

the

cuto�

and

non-cuto�

cases.

F

or

Witten-Bell

smo

othing,

instead

of

equation

()

in

Section

.

w

e

tak

e

p

WB

(w

i

jw

i�

i�n+

)

=

c

cut

(w

i

i�n+

)

+

N

0

+

(w

i�

i�n+

�)p

WB

(w

i

jw

i�

i�n+

)



P

w

i

c(w

i

i�n+

)

+

N

+

(w

i�

i�n+

�)

where

c

cut

(w

i

i�n+

)

is

de�ned

as

b

efore

and

N

0

+

(w

i�

i�n+

�)

is

c

hosen

so

that

probabilities

sum

to

;

i.e.,

w

e

tak

e

N

0

+

(w

i�

i�n+

�)

=

N

+

(w

i�

i�n+

�)

+

X

w

i

c(w

i

i�n+

)

�

X

w

i

c

cut

(w

i

i�n+

)

F

or

absolute

discoun

ting,

w

e

use

an

equation

analogous

to

equation

()

in

Section

.

p

abs

(w

i

jw

i�

i�n+

)

=

max

fc

cut

(w

i

i�n+

)

�

D

;

0g



P

w

i

c(w

i

i�n+

)

+

(

�

�

w

i�

i�n+

)

p

abs

(w

i

jw

i�

i�n+

)




7

7.5

8

8.5

9

9.5

10

1000

10000

100000

1e+06

cross-entropy of test data (bits/token)

training set size (sentences)

abs-disc-interp

katz

kneser-ney-fix

kneser-ney-mod

34

36

38

40

42

44

46

48

50

52

1000

10000

100000

1e+06

word-error rate on test set

training set size (sentences)

abs-disc-interp

katz

kneser-ney-fix

kneser-ney-mod

Figure

	:

On

left,

cross-en

trop

y

of

v

arious

algorithms

on

Broadcast

News

sp

eec

h

recognition

test

set

o

v

er

v

arious

training

set

sizes,

trigram

mo

del;

on

righ

t,

sp

eec

h

recognition

w

ord-error

rate

for

same

mo

dels

on

same

test

set

T

o

ha

v

e

this

distribution

sum

to

,

instead

of

equation

(	)

w

e

tak

e



�

�

w

i�

i�n+

=

D

N

+

(w

i�

i�n+

�)

+

P

w

i

c(w

i

i�n+

)

�

P

w

i

c

cut

(w

i

i�n+

)



P

w

i

c(w

i

i�n+

)

F

or

Kneser-Ney

smo

othing,

w

e

mak

e

the

same

adjustmen

ts

as

in

absolute

discoun

ting.

..

Cross-En

trop

y

and

Sp

eec

h

Recognition

In

this

section,

w

e

brie�y

examine

ho

w

the

p

erformance

of

a

language

mo

del

measured

in

terms

of

cross-en

trop

y

correlates

with

sp

eec

h

recognition

p

erformance

using

the

language

mo

del.

Sp

eec

h

recognition

is

p

erhaps

the

most

prev

alen

t

application

of

language

mo

dels,

and

w

e

p

erform

this

study

to

giv

e

an

example

of

ho

w

cross-en

trop

y

correlates

with

an

application-sp

eci�c

measure

of

p

erformance.

Sp

eec

h

recognition

p

erformance

is

generally

measured

in

terms

of

w

ord-error

rate,

whic

h

is

the

n

um

b

er

of

w

ord

errors

made

divided

b

y

the

n

um

b

er

of

w

ords

in

the

correct

transcript.

It

has

b

een

sho

wn

previously

that

there

is

some

linear

correlation

b

et

w

een

the

w

ord-error

rate

pro

duced

using

a

language

mo

del

and

the

cross-en

trop

y

of

the

mo

del

on

the

corresp

onding

text

(Chen,

Beeferman,

and

Rosenfeld,

		).

Ho

w

ev

er,

the

strength

of

the

correlation

dep

ends

on

the

nature

of

the

mo

dels

b

eing

compared.

F

or

these

exp

erimen

ts,

w

e

used

Broadcast

News

sp

eec

h

data

(D

ARP

A,

		).

W

e

generated

narro

w-b

eam

lattices

with

the

Sphinx-I

I

I

sp

eec

h

recognition

system

(Placew

a

y

et

al.,

		)

using

a

Katz-smo

othed

trigram

mo

del

trained

on

0

million

w

ords

of

Broadcast

News

text;

trigrams

o

ccurring

only

once

in

the

training

data

w

ere

excluded

from

the

mo

del.

W

e

calculated

w

ord-error

rates

for

the

language

mo

dels

in

this

exp

erimen

t

b

y

rescoring

these

lattices

with

the

giv

en

language

mo

del.

W

e

constructed

trigram

language

mo

dels

for

eac

h

of

four

smo

othing

algorithms

for

�v

e

di�eren

t

training

set

sizes

(ranging

from

,000

to

,00,000

sen

tences).

Listed

from

b

est

to

w

orst

in

terms

of

cross-en

trop

y

,

these

algorithms

are

kneser-ney-mod,

kneser-ney-fix,

katz,

and

abs-disc-

interp.

All

mo

dels

w

ere

built

with

no

coun

t

cuto�s

except

for

the

largest

training

set,

for

whic

h




34

36

38

40

42

44

46

48

50

52

7

7.5

8

8.5

9

9.5

10

10.5

word-error rate on test set

cross-entropy of test set

abs-disc-interp

katz

kneser-ney-fix

kneser-ney-mod

Figure

0:

Relation

b

et

w

een

p

erplexit

y

and

sp

eec

h

recognition

w

ord-error

rate

on

test

set

for

0

language

mo

dels

trigrams

o

ccurring

only

once

in

the

training

data

w

ere

excluded.

The

cross-en

trop

y

on

the

test

data

of

these

0

mo

dels

are

displa

y

ed

on

the

left

in

Figure

	

b

y

training

set

size.

Then,

w

e

calculated

w

ord-error

rates

for

eac

h

of

these

0

mo

dels

on

the

test

data

using

the

pro

cedure

describ

ed

earlier.

On

the

righ

t

in

Figure

	,

w

e

plot

the

w

ord-error

rates

of

these

0

mo

dels

b

y

training

set

size.

In

Figure

0,

w

e

plot

the

cross-en

trop

y

vs.

the

w

ord-error

rate

for

eac

h

of

the

0

mo

dels.

W

e

can

see

that

the

linear

correlation

b

et

w

een

cross-en

trop

y

and

w

ord-error

rate

is

v

ery

strong

for

this

set

of

mo

dels.

Th

us,

it

seems

that

smo

othing

algorithms

with

lo

w

er

cross-en

tropies

will

generally

lead

to

lo

w

er

w

ord-error

rates

when

plugged

in

to

sp

eec

h

recognition

systems.

F

or

our

particular

data

set,

w

e

see

a

reduction

of

ab

out

.%

absolute

in

w

ord-error

rate

for

ev

ery

bit

of

reduction

in

cross-en

trop

y

.

As

seen

in

Section

.,

the

di�erence

in

cross-en

trop

y

b

et

w

een

the

b

est

smo

othing

algorithm

and

a

medio

cre

smo

othing

algorithm

can

b

e

0.

bits

or

more,

corresp

onding

to

ab

out

a

%

absolute

di�erence

in

w

ord-error

rate.

Hence,

the

c

hoice

of

smo

othing

algorithm

can

mak

e

a

signi�can

t

di�erence

in

sp

eec

h

recognition

p

erformance.



Discussion

Smo

othing

is

a

fundamen

tal

tec

hnique

for

statistical

mo

deling,

imp

ortan

t

not

only

for

language

mo

deling

but

for

man

y

other

applications

as

w

ell,

e.g.,

prep

ositional

phrase

attac

hmen

t

(Collins

and

Bro

oks,

		),

part-of-sp

eec

h

tagging

(Ch

urc

h,

	),

and

sto

c

hastic

parsing

(Magerman,

		;

Go

o

dman,

		).

Whenev

er

data

sparsit

y

is

an

issue,

smo

othing

can

help

p

erformance,

and

data

sparsit

y

is

almost

alw

a

ys

an

issue

in

statistical

mo

deling.

In

the

extreme

case

where

there

is

so

m

uc

h

training

data

that

all

parameters

can

b

e

accurately

trained

without

smo

othing,

one

can

almost

alw

a

ys

expand

the

mo

del,

suc

h

as

b

y

mo

ving

to

a

higher-order

n-gram

mo

del,

to

ac

hiev

e

impro

v

ed

p

erformance.

With

more

parameters

data

sparsit

y

b

ecomes

an

issue

again,

but

with

prop

er

smo

othing

the

mo

dels

are

usually

more

accurate

than

the

original

mo

dels.

Th

us,

no

matter

ho

w

m

uc

h

data

one

has,

smo

othing

can

almost

alw

a

ys

help

p

erformance,

and

for

a

relativ

ely

small

e�ort.

In

this

w

ork,

w

e

ha

v

e

measured

the

p

erformance

of

smo

othing

algorithms

primarily

through

the

cross-en

trop

y

of

test

data,

and

w

e

ha

v

e

also

p

erformed

exp

erimen

ts

measuring

the

w

ord-

error

rate

of

sp

eec

h

recognition.

Cross-en

trop

y

do

es

not

alw

a

ys

correlate

w

ell

with

w

ord-error

	


rate,

esp

ecially

when

the

mo

dels

compared

are

created

using

v

ery

di�eren

t

tec

hniques

(Chen,

Beeferman,

and

Rosenfeld,

		).

Ho

w

ev

er,

in

our

exp

erimen

ts

w

e

found

that

when

the

only

di�erence

b

et

w

een

mo

dels

is

smo

othing,

the

correlation

b

et

w

een

the

t

w

o

measures

is

quite

strong.

It

is

certainly

p

ossible

that

in

other

domains,

impro

v

ed

cross-en

trop

y

from

b

etter

smo

othing

will

not

correlate

with

impro

v

ed

application

p

erformance,

but

w

e

exp

ect

that

in

most

cases

it

will.

F

or

sp

eec

h

recognition,

b

etter

smo

othing

algorithms

ma

y

lead

to

up

to

a

%

absolute

impro

v

emen

t

in

w

ord-error

rate.

T

o

our

kno

wledge,

this

is

the

�rst

empirical

comparison

of

smo

othing

tec

hniques

in

language

mo

deling

of

suc

h

scop

e:

no

other

study

has

systematically

examined

m

ultiple

training

data

sizes,

di�eren

t

corp

ora,

or

has

p

erformed

automatic

parameter

optimization.

W

e

sho

w

that

in

order

to

completely

c

haracterize

the

relativ

e

p

erformance

of

t

w

o

tec

hniques,

it

is

necessary

to

consider

m

ultiple

training

set

sizes

and

to

try

b

oth

bigram

and

trigram

mo

dels.

W

e

sho

w

that

sub-optimal

parameter

selection

can

signi�can

tly

a�ect

relativ

e

p

erformance.

W

e

ha

v

e

also

dev

elop

ed

a

no

v

el

smo

othing

algorithm

that

outp

erforms

all

previous

tec

hniques,

b

y

applying

insigh

ts

gleaned

from

using

the

to

ols

that

w

e

ha

v

e

created

for

the

detailed

analysis

of

smo

othing

algorithms.

Multiple

runs

should

b

e

p

erformed

whenev

er

p

ossible

to

disco

v

er

whether

an

y

calculated

di�er-

ences

are

statistically

signi�can

t;

it

is

unclear

whether

man

y

of

the

previously

rep

orted

results

in

the

literature

are

conclusiv

e

giv

en

that

they

are

based

on

single

runs

and

giv

en

the

v

ariances

found

in

this

w

ork.

F

or

example,

w

e

estimated

that

the

standard

deviation

of

the

p

erformance

of

Katz

smo

othing

relativ

e

to

the

baseline

metho

d

for

a

single

run

is

ab

out

0:0

bits,

whic

h

translates

to

ab

out

a

%

di�erence

in

p

erplexit

y

.

This

standard

deviation

is

comparable

to

previously

rep

orted

di�erences

in

p

erformance.

F

or

instance,

in

the

N�

adas

and

Katz

pap

ers,

di�erences

in

p

erplexit

y

b

et

w

een

algorithms

of

ab

out

%

are

rep

orted

for

a

single

test

set

of

00

sen

tences.

MacKa

y

and

P

eto

presen

t

p

erplexit

y

di�erences

b

et

w

een

algorithms

of

signi�can

tly

less

than

%.

W

e

p

oin

t

out

that

b

ecause

of

the

v

ariation

in

the

p

erformance

of

di�eren

t

smo

othing

meth-

o

ds

and

the

v

ariation

in

the

p

erformance

of

di�eren

t

implemen

tatio

ns

of

the

same

smo

othing

metho

d

(e.g.,

from

parameter

setting),

it

is

vital

to

sp

ecify

the

exact

smo

othing

tec

hnique

and

implemen

tatio

n

of

that

tec

hnique

used

when

referencing

the

p

erformance

of

an

n-gram

mo

del.

F

or

example,

the

Katz

and

N�

adas

pap

ers

describ

e

comparisons

of

their

algorithms

with

\Jelinek-

Mercer"

smo

othing,

but

they

do

not

sp

ecify

the

buc

k

eting

sc

heme

used

or

the

gran

ularit

y

used

in

deleted

in

terp

olation.

Without

this

information,

it

is

imp

ossible

to

determine

the

imp

ort

of

their

comparisons.

More

generally

,

there

has

b

een

m

uc

h

w

ork

comparing

the

p

erformance

of

v

arious

mo

dels

with

that

of

n-gram

mo

dels

where

the

t

yp

e

of

smo

othing

used

is

not

sp

eci�ed.

Again,

without

this

information

w

e

cannot

tell

if

the

comparisons

are

signi�can

t.

Of

the

tec

hniques

studied,

w

e

ha

v

e

found

that

Kneser-Ney

smo

othing

and

v

ariations

consis-

ten

tly

outp

erform

all

other

algorithms.

In

particular,

our

no

v

el

algorithm

kneser-ney-mod

con-

sisten

tly

had

the

b

est

p

erformance.

This

algorithm

di�ers

in

sev

eral

w

a

ys

from

Kneser

and

Ney's

original

algorithm:

in

terp

olation

is

used

instead

of

bac

k

o�,

w

e

use

a

separate

discoun

t

for

one-

and

t

w

o-coun

ts

instead

of

a

single

discoun

t

for

all

coun

ts,

and

w

e

estimate

discoun

ts

on

held-out

data

instead

of

using

a

form

ula

based

on

training

data

coun

ts.

Our

exp

erimen

tal

results

sho

w

that

all

three

of

these

c

hoices

impro

v

e

p

erformance.

P

erforming

just

sligh

tly

w

orse

is

the

algorithm

kneser-ney-mod-fi

x;

this

algorithm

di�ers

from

kneser-ney-mod

in

that

discoun

ts

are

set

using

a

form

ula

based

on

training

data

coun

ts.

This

algorithm

has

the

practical

adv

an

tage

that

no

external

parameters

need

to

b

e

optimized

on

held-out

data.

W

e

pro

vide

tec

hniques

for

analyzing

the

coun

t-b

y-coun

t

p

erformance

of

di�eren

t

smo

othing

tec

hniques.

This

detailed

analysis

helps

explain

the

relativ

e

p

erformance

of

v

arious

algorithms,

and

can

help

predict

ho

w

di�eren

t

algorithms

will

p

erform

in

no

v

el

situations.

These

analysis

to

ols

help

ed

us

design

our

mo

di�cations

to

Kneser-Ney

smo

othing.

0


F

rom

our

exp

erimen

ts

and

analysis,

w

e

found

sev

eral

factors

that

had

a

consisten

t

e�ect

on

the

p

erformance

of

smo

othing

algorithms.

�

The

factor

with

the

largest

in�uence

is

the

use

of

a

mo

di�ed

bac

k

o�

distribution

as

in

Kneser-

Ney

smo

othing.

This

seemed

to

b

e

the

primary

reason

that

the

v

ariations

of

Kneser-Ney

smo

othing

p

erformed

so

w

ell

relativ

e

to

the

remaining

algorithms.

�

Absolute

discoun

ting

is

sup

erior

to

linear

discoun

ting.

As

w

as

sho

wn

earlier,

the

ideal

a

v

erage

discoun

t

for

coun

ts

rises

quic

kly

for

v

ery

lo

w

coun

ts

but

is

basically

�at

for

larger

coun

ts.

Ho

w

ev

er,

the

Go

o

d-T

uring

estimate

can

b

e

used

to

predict

this

a

v

erage

discoun

t

ev

en

b

etter

than

absolute

discoun

ting,

as

w

as

demonstrated

b

y

Katz

smo

othing.

�

In

terms

of

normalized

p

erformance,

in

terp

olated

mo

dels

are

signi�can

tly

sup

erior

to

bac

k-

o�

mo

dels

for

lo

w

(nonzero)

coun

ts.

This

is

b

ecause

lo

w

er-order

mo

dels

pro

vide

v

aluable

information

in

determining

the

correct

discoun

t

for

n-grams

with

lo

w

coun

ts.

�

Adding

free

parameters

to

an

algorithm

and

optimizing

these

parameters

on

held-out

data

can

impro

v

e

the

p

erformance

of

an

algorithm,

e.g.,

kneser-ney-mod

vs.

kneser-ney-mod-

fix.

Our

algorithm

kneser-ney-mod

gets

its

sup

erior

p

erformance

from

a

com

bination

of

all

of

these

factors.

While

w

e

ha

v

e

systematically

explored

smo

othing

for

n-gram

language

mo

dels,

there

remain

man

y

directions

that

need

to

b

e

explored.

Almost

an

y

statistical

mo

del,

not

just

n-gram

mo

dels,

can

and

should

b

e

smo

othed,

and

further

w

ork

will

b

e

needed

to

determine

ho

w

w

ell

the

tec

hniques

describ

ed

here

transfer

to

other

domains.

Ho

w

ev

er,

the

tec

hniques

w

e

ha

v

e

dev

elop

ed,

b

oth

for

smo

othing

and

for

analyzing

smo

othing

algorithm

p

erformance,

should

pro

v

e

useful

not

only

for

language

mo

deling

researc

h

but

for

other

tasks

as

w

ell.

Ac

kno

wledgemen

ts

The

authors

w

ould

lik

e

to

thank

Stuart

Shieb

er

and

the

anon

ymous

review

ers

for

their

commen

ts

on

previous

v

ersions

of

this

pap

er.

This

researc

h

w

as

supp

orted

in

part

b

y

the

National

Science

F

oundation

under

Gran

t

No.

IRI-	-0	

and

Gran

t

No.

CD

A-	-00.

The

second

author

w

as

also

supp

orted

b

y

Gran

t

No.

IRI-	-0

and

a

National

Science

F

oundation

Graduate

Studen

t

F

ello

wship.

References

Bahl,

Lalit

R.,

P

eter

F.

Bro

wn,

P

eter

V.

de

Souza,

and

Rob

ert

L.

Mercer.

		.

A

tree-based

statistical

language

mo

del

for

natural

language

sp

eec

h

recognition.

IEEE

T

r

ansactions

on

A

c

oustics,

Sp

e

e

ch

and

Signal

Pr

o

c

essing,

:00{00,

July

.

Bahl,

Lalit

R.,

F

rederic

k

Jelinek,

and

Rob

ert

L.

Mercer.

	.

A

maxim

um

lik

eliho

o

d

approac

h

to

con

tin

uous

sp

eec

h

recognition.

IEEE

T

r

ansactions

on

Pattern

A

nalysis

and

Machine

Intel-

ligenc

e,

P

AMI-():	{	0,

Marc

h.

Baum,

L.E.

	.

An

inequalit

y

and

asso

ciated

maximi

zation

tec

hnique

in

statistical

estimation

of

probabilistic

functions

of

a

Mark

o

v

pro

cess.

Ine

qualities,

:{.




Bell,

Timoth

y

C.,

John

G.

Cleary

,

and

Ian

H.

Witten.

		0.

T

ext

Compr

ession.

Pren

tice

Hall,

Englew

o

o

d

Cli�s,

N.J.

Bro

wn,

P

eter

F.,

John

Co

c

k

e,

Stephen

A.

Della

Pietra,

Vincen

t

J.

Della

Pietra,

F

rederic

k

Jelinek,

John

D.

La�ert

y

,

Rob

ert

L.

Mercer,

and

P

aul

S.

Ro

ossin.

		0.

A

statistical

approac

h

to

mac

hine

translation.

Computational

Linguistics,

():	{,

June.

Bro

wn,

P

eter

F.,

Stephen

A.

Della

Pietra,

Vincen

t

J.

Della

Pietra,

Jennifer

C.

Lai,

and

Rob

ert

L.

Mercer.

		a.

An

estimate

of

an

upp

er

b

ound

for

the

en

trop

y

of

English.

Computational

Linguistics,

():{0,

Marc

h.

Bro

wn,

P

eter

F.,

Vincen

t

J.

Della

Pietra,

P

eter

V.

deSouza,

Jennifer

C.

Lai,

and

Rob

ert

L.

Mercer.

		b.

Class-based

n-gram

mo

dels

of

natural

language.

Computational

Linguistics,

():{

	,

Decem

b

er.

Chen,

Stanley

F.

		.

Building

Pr

ob

abilistic

Mo

dels

for

Natur

al

L

anguage.

Ph.D.

thesis,

Harv

ard

Univ

ersit

y

,

June.

Chen,

Stanley

F.,

Douglas

Beeferman,

and

Ronald

Rosenfeld.

		.

Ev

aluation

metrics

for

lan-

guage

mo

dels.

In

D

ARP

A

Br

o

adc

ast

News

T

r

anscription

and

Understanding

Workshop.

Chen,

Stanley

F.

and

Josh

ua

T.

Go

o

dman.

		.

An

empirical

study

of

smo

othing

tec

hniques

for

language

mo

deling.

In

Pr

o

c

e

e

dings

of

the

th

A

nnual

Me

eting

of

the

A

CL,

pages

0{,

San

ta

Cruz,

California,

June.

Ch

urc

h,

Kenneth.

	.

A

sto

c

hastic

parts

program

and

noun

phrase

parser

for

unrestricted

text.

In

Pr

o

c

e

e

dings

of

the

Se

c

ond

Confer

enc

e

on

Applie

d

Natur

al

L

anguage

Pr

o

c

essing,

pages

{.

Ch

urc

h,

Kenneth

W.

and

William

A.

Gale.

		.

A

comparison

of

the

enhanced

Go

o

d-Turing

and

deleted

estimation

metho

ds

for

estimating

probabilities

of

English

bigrams.

Computer

Sp

e

e

ch

and

L

anguage,

:	{.

Clarkson,

P

.

and

R.

Rosenfeld.

		.

Statistical

language

mo

deling

using

the

CMU-Cam

bridge

to

olkit.

In

Pr

o

c

e

e

dings

of

Eur

osp

e

e

ch

'	.

Collins,

Mic

hael

and

James

Bro

oks.

		.

Prep

ositional

phrase

attac

hmen

t

through

a

bac

k

ed-o�

mo

del.

In

Da

vid

Y

aro

wsky

and

Kenneth

Ch

urc

h,

editors,

Pr

o

c

e

e

dings

of

the

Thir

d

Workshop

on

V

ery

L

ar

ge

Corp

or

a,

pages

{,

Cam

bridge,

MA,

June.

Co

v

er,

Thomas

M.

and

Jo

y

A.

Thomas.

		.

Elements

of

Information

The

ory.

John

Wiley

.

D

ARP

A.

		.

D

ARP

A

Br

o

adc

ast

News

T

r

anscription

and

Understanding

Workshop.

Gale,

William

A.

and

Kenneth

W.

Ch

urc

h.

		0.

Estimation

pro

cedures

for

language

con

text:

p

o

or

estimates

are

w

orse

than

none.

In

COMPST

A

T,

Pr

o

c

e

e

dings

in

Computational

Statistics,

	th

Symp

osium,

pages

	{,

Dubro

vnik,

Y

ugosla

via,

Septem

b

er.

Gale,

William

A.

and

Kenneth

W.

Ch

urc

h.

		.

What's

wrong

with

adding

one?

In

N.

Oostdijk

and

P

.

de

Haan,

editors,

Corpus-Base

d

R

ese

ar

ch

into

L

anguage.

Ro

dolpi,

Amsterdam.

Gale,

William

A.

and

Geo�rey

Sampson.

		.

Go

o

d-Turing

frequency

estimation

without

tears.

Journal

of

Quantitative

Linguistics,

().

T

o

app

ear.




Go

dfrey

,

J.J.,

E.C.

Holliman,

and

J.

McDaniel.

		.

SWITCHBO

ARD:

T

elephone

sp

eec

h

corpus

for

researc

h

and

dev

elopmen

t.

In

Pr

o

c

e

e

dings

of

ICASSP-	,

v

olume

I,

pages

{0.

Go

o

d,

I.J.

	.

The

p

opulation

frequencies

of

sp

ecies

and

the

estimation

of

p

opulation

parame-

ters.

Biometrika,

0(

and

):{.

Go

o

dman,

Josh

ua.

		.

Probabilistic

feature

grammars.

In

Pr

o

c

e

e

dings

of

the

International

Workshop

on

Parsing

T

e

chnolo

gies

		.

Hull,

Jonathon.

		.

Com

bining

syn

tactic

kno

wledge

and

visual

text

recognition:

A

hidden

Mark

o

v

mo

del

for

part

of

sp

eec

h

tagging

in

a

w

ord

recognition

algorithm.

In

AAAI

Symp

osium:

Pr

ob

abilistic

Appr

o

aches

to

Natur

al

L

anguage,

pages

{.

Je�reys,

H.

	.

The

ory

of

Pr

ob

ability.

Clarendon

Press,

Oxford,

second

edition.

Jelinek,

F

rederic

k

and

Rob

ert

L.

Mercer.

	0.

In

terp

olated

estimation

of

Mark

o

v

source

param-

eters

from

sparse

data.

In

Pr

o

c

e

e

dings

of

the

Workshop

on

Pattern

R

e

c

o

gnition

in

Pr

actic

e,

Amsterdam,

The

Netherlands:

North-Holland,

Ma

y

.

Johnson,

W.E.

	.

Probabilit

y:

deductiv

e

and

inductiv

e

problems.

Mind,

:{.

Katz,

Sla

v

a

M.

	.

Estimation

of

probabilities

from

sparse

data

for

the

language

mo

del

com-

p

onen

t

of

a

sp

eec

h

recognizer.

IEEE

T

r

ansactions

on

A

c

oustics,

Sp

e

e

ch

and

Signal

Pr

o

c

essing,

ASSP-():00{0,

Marc

h.

Kernighan,

M.D.,

K.W.

Ch

urc

h,

and

W.A.

Gale.

		0.

A

sp

elling

correction

program

based

on

a

noisy

c

hannel

mo

del.

In

Pr

o

c

e

e

dings

of

the

Thirte

enth

International

Confer

enc

e

on

Computa-

tional

Linguistics,

pages

0{0.

Kneser,

Reinhard

and

Hermann

Ney

.

		.

Impro

v

ed

bac

king-o�

for

m-gram

language

mo

del-

ing.

In

Pr

o

c

e

e

dings

of

the

IEEE

International

Confer

enc

e

on

A

c

oustics,

Sp

e

e

ch

and

Signal

Pr

o

c

essing,

v

olume

,

pages

{.

Kucera,

H.

and

W.N.

F

rancis.

	.

Computational

A

nalysis

of

Pr

esent-Day

A

meric

an

English.

Bro

wn

Univ

ersit

y

Press,

Pro

vidence

R.I.

Lidstone,

G.J.

	0.

Note

on

the

general

case

of

the

Ba

y

es-Laplace

form

ula

for

inductiv

e

or

a

p

osteriori

probabilities.

T

r

ansactions

of

the

F

aculty

of

A

ctuaries,

:{	.

MacKa

y

,

Da

vid

J.

C.

and

Linda

C.

P

eto.

		.

A

hierarc

hical

Diric

hlet

language

mo

del.

Natur

al

L

anguage

Engine

ering,

():{	.

Magerman,

Da

vid

M.

		.

Natur

al

L

anguage

Parsing

as

Statistic

al

Pattern

R

e

c

o

gnition.

Ph.D.

thesis,

Stanford

Univ

ersit

y

,

F

ebruary

.

Marcus,

M.,

B.

San

torini,

and

M.

Marcinkiewicz.

		.

Building

a

large

annotated

corpus

of

English:

the

Penn

Treebac

k.

Computational

Linguistics,

	().

Mark

o

v,

A.A.

	.

An

example

of

statistical

in

v

estigation

in

the

text

of

`Eugene

On

y

egin'

illustrating

coupling

of

tests

in

c

hains.

Pr

o

c

e

e

dings

of

the

A

c

ademy

of

Scienc

e,

St.

Petersbur

g,

:{.




Nadas,

Arth

ur.

	.

Estimation

of

probabilities

in

the

language

mo

del

of

the

IBM

sp

eec

h

recognition

system.

IEEE

T

r

ansactions

on

A

c

oustics,

Sp

e

e

ch

and

Signal

Pr

o

c

essing,

ASSP-

():	{,

August.

Ney

,

Hermann,

Ute

Essen,

and

Reinhard

Kneser.

		.

On

structuring

probabilistic

dep

endences

in

sto

c

hastic

language

mo

deling.

Computer,

Sp

e

e

ch,

and

L

anguage,

:{.

Placew

a

y

,

P

.,

S.

Chen,

M.

Esk

enazi,

U.

Jain,

V.

P

arikh,

B.

Ra

j,

M.

Ra

vishank

ar,

R.

Rosenfeld,

K.

Seymore,

M.

Siegler,

R.

Stern,

and

E.

Tha

y

er.

		.

The

		

Hub-

Sphinx-

system.

In

Pr

o

c

e

e

dings

of

the

D

ARP

A

Sp

e

e

ch

R

e

c

o

gnition

Workshop,

F

ebruary

.

Press,

W.H.,

B.P

.

Flannery

,

S.A.

T

euk

olsky

,

and

W.T.

V

etterling.

	.

Numeric

al

R

e

cip

es

in

C.

Cam

bridge

Univ

ersit

y

Press,

Cam

bridge.

Ries,

Klaus.

		.

p

ersonal

comm

unication.

Rogina,

Ivica

and

Alex

W

aib

el.

		.

The

Jan

us

sp

eec

h

recognizer.

In

ARP

A

SL

T

Workshop.

Rosenfeld,

Ronald.

		.

The

CMU

statistical

language

mo

deling

to

olkit

and

its

use

in

the

		

ARP

A

CSR

ev

aluation.

In

Pr

o

c

e

e

dings

of

the

Sp

oken

L

anguage

Systems

T

e

chnolo

gy

Workshop,

pages

{0,

Austin,

T

exas,

Jan

uary

.

Rudnic

ky

,

A.I.

		.

Hub

:

Business

Broadcast

News.

In

Pr

o

c

e

e

dings

of

the

D

ARP

A

Sp

e

e

ch

R

e

c

o

gnition

Workshop,

pages

{.

Seymore,

K.,

S.

Chen,

M.

Esk

enazi,

and

R.

Rosenfeld.

		.

Language

and

pron

unciation

mo

d-

eling

in

the

CMU

		

Hub



ev

aluation.

In

Pr

o

c

e

e

dings

of

the

D

ARP

A

Sp

e

e

ch

R

e

c

o

gnition

Workshop,

W

ashington,

D.C.,

F

ebruary

.

Srihari,

Rohini

and

Charlotte

Baltus.

		.

Com

bining

statistical

and

syn

tactic

metho

ds

in

recognizing

handwritten

sen

tences.

In

AAAI

Symp

osium:

Pr

ob

abilistic

Appr

o

aches

to

Natur

al

L

anguage,

pages

{.

Stern,

Ric

hard

M.

		.

Sp

eci�cation

of

the

		

ARP

A

h

ub



ev

aluation:

Unlimited

v

o

cabulary

NAB

news

baseline.

In

Pr

o

c

e

e

dings

of

the

D

ARP

A

Sp

e

e

ch

R

e

c

o

gnition

Workshop,

pages

{.

W

eng,

F

uliang,

Andreas

Stolc

k

e,

and

Anan

th

Sank

ar.

		.

Hub

language

mo

deling

using

domain

in

terp

olation

and

data

clustering.

In

Pr

o

c

e

e

dings

of

the

D

ARP

A

Sp

e

e

ch

R

e

c

o

gnition

Workshop,

W

ashington,

D.C.,

F

ebruary

.

Witten,

Ian

H.

and

Timoth

y

C.

Bell.

		.

The

zero-frequency

problem:

Estimating

the

probabil-

ities

of

no

v

el

ev

en

ts

in

adaptiv

e

text

compression.

IEEE

T

r

ansactions

on

Information

The

ory,

():0{0	,

July

.



