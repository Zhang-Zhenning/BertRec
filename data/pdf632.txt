


Published in

Towards Data Science



May 13, 2020

·

13 min read

Save

Word2Vec Implementation

How to implement Word2Vec using numpy and python

Contents:

A — Concept

B — Implementation








Objective

Introduction

Core idea

Aesop

Architecture


Skip-gram 

Fig_1: Skip gram architecture . Credits

Best. 

(way)

way

(Best,to)

to

window size

(way,success)

(Best,way,success,is)

CBOW: 

(Best,way,success,is) 

“to” 

Fig_2: Cbow model architecture

Implementation process


Data preparation






Model training

Fig_3: Training of a skip gram

Forward propagation


Training error:


Line (6,7)

Line(13,14)

Line(15,16)


Back propagation

Finally loss calculation

Fig_4: Loss function. Credits



Hyperparameters

Model inference and analysis

Inference with a single line of text as input


Fig_5:Varying dim

Fig_6: Varying window size

Inference with a relatively larger corpus

Further improvements:

Fig_7: Varying dimension similarity matrix

Fig_8: Stopwords effect


5



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





Data Science

Machine Learning

Word2vec

Numpy

Implementation

