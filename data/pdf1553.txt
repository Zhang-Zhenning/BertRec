


Published in

Towards Data Science



May 19, 2020

·

7 min read

·

Save

ImagPhoto by Johannes Plenio on Unsplash

Evolution of Language Models: N-Grams, Word

Embeddings, Attention &amp; Transformers

In this post, I thought it would be nice to collate some research on the advancements of Natural

Language Processing (NLP) over the years.

Language Models are simply models that assign probabilities to

sequences of words.







Image depicting human evoluation


Before 1948-1980— Birth of N-Grams and Rule Systems

Photo by Tim Bish on Unsplash

1980-1990 — Rise of compute power and the Birth of RNN

A diagram for a one-unit recurrent neural network (RNN), 19 June 2017 by fdeloche (source)

1990-2000— The Rise of NLP Research and the Birth of LSTM

A diagram for a one-unit Long Short-Term Memory (LSTM), 20 June 2017 by fdeloche (source)

2003 — The First Neural Language Model

An infant yawning

Example of a recurrent neural network

Image of a long short term memory cell


Image of a neural network with a single hidden layer

The first neural language model by Bengio et al. 2003 (source)

Today, we know them as Word Embeddings. :)

2013 — Birth of Widespread Pretrained Word Embeddings (Word2Vec by Google)

Word2Vec models. The CBOW architecture predicts the current word based on the context, and the Skip-gram

predicts surrounding words given the current word. By Mikolov et al. 2013 (source)

2014 — Standford: Global Vectors (Glove)

Overall accuracy on the word analogy task Glove vs CBOW vs Skip-Gram by Pennington et al. 2014 (Source)

Single hidden layer neural networks

Two dual line charts adjacent to one another


2015 — The Comeback: SVD and LSA Word Embeddings &amp; The Birth of Attention

Models

Photo by Science in HD on Unsplash

insignificant performance differences

I guess the lesson here is that new shiny toys aren’t always better than

old (not so shiny) toys.

The Birth of the Attention Model

Would you read an entire paragraph in it’s entirety before starting to

translate all of it?

So why should we expect a model to remember everything before

Man writing math equations on a blackboard with chalk


So why should we expect a model to remember everything before

beginning a translation task?

This was the intuition behind creating the attention mechanism.

2016 — From Neural Machine Translation to Image Captioning with Attention

Examples of attending to the correct object (white indicates the attended regions, underlines indicated the

corresponding word) by Xu et al. 2016 (source)

2017 — Birth of the Transformer

Photo by Mitchell Luo on Unsplash

paradigm shift

solely 

The Transformer — model architecture by Vaswani et al. 2017 (source)

2018-Today— Era of Pretrained Language Models

Gif of sesame street characters

“P” GIF by Sesame Street, 23 March 2016 (source)

Pretrained Word Embeddings had evolved to Pretrained Language Models.

Different images of things, animals or people

Image of the text “Google”


Pretrained Word Embeddings had evolved to Pretrained Language Models.

entire model architecture 

2020 — Ending Notes

Gif of a man in a flower suit being happy

Happy So Excited GIF By GIPHY Studios Originals, 18 November 2016 (source)

decades 

Lady saying amazing

Amazing Season 6 GIF By Bachelor In Paradise, 7 August 2019 (source)


4



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





Data Science

NLP

Artificial Intelligence

Deep Learning

Machine Learning

