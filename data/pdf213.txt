
Expectation-Maximization (EM) Algorithm

Adopted from slides by Alexander Ihler


Probabilistic models in 

unsupervised learning

• K-means algorithm

• Assigned each example to exactly one cluster

• What if clusters are overlapping?

• Hard to tell which cluster is right

• Maybe we should try to remain uncertain

• Used Euclidean distance

• What if cluster has a non-circular shape?

• EM algorithm

• Assign data to cluster 

with some probability

• Gives probability model of x!  (“generative”)


Expectation-Maximization (EM) 

Algorithm

• Learning algorithm for latent variable models

• Observed features :  1 ,  2 , ⋯ ,  

• Latent features : (1), (2), ⋯ , ()

• Assume a probabilistic model over , 

 ,  =    ()

• Learning most likely parameters  and  based on 

the observed data

arg max

,   = �



   ()


Expectation-Maximization (EM) 

Algorithm

• Iteratively update  and 

• Initially assume random parameters 

• Iterate following two steps until convergence:

• Expectation (E-step): Compute  () () for each 

example  based on the current parameters 

• Maximization (M-step): Re-estimate the most likely 

parameters  based on the current data , 


Coin tossing example

• Two coins  and  with unknown biases  and 

• Repeat following procedure 5 times:

• Randomly choose one of the two coins

• Perform 10 independent coin tosses with selected coin





̂ =

# of heads using coin 

total # of flips using coin 

̂ =

# of heads using coin 

total # of flips using coin 


EM Algorithm

• Observed feature  ∈ 0,1, ⋯ , 10 : # of heads

• Observed data:

 1 = 5,  2 = 9,  3 = 8,  4 = 4,  5 = 7

• Latent feature  ∈ ,  : identity of the coin

• (1), (2), (3), (4), (5)

• Assume   = 0.5

• Model

• Parameters , 

 | = �10

 ⋅   ⋅ 1 −  10−

if  = ′′

10

 ⋅   ⋅ 1 −  10−

if  = ′′


EM Algorithm



 () () =   

    

∑ ()    |


EM Algorithm



̂ =

# of heads using coin 

total # of flips using coin  =

# of heads × pro of 

total # of flips × pro of 

̂ =

# of heads using coin 

total # of flips using coin  =

# of heads × pro of 

total # of flips × pro of 


EM for Clustering: Mixtures of 

Gaussians

• Start with parameters describing each cluster

• Mean , variance ,  “size” 

• Probability distribution:  

xx xxx                x  x x   x xx    x               x x x




Mixtures of Gaussians

• Start with parameters describing each cluster

• Mean , variance ,  “size” 

• Probability distribution:  

• Equivalent “latent variable” form:



Select a mixture component with probability 

1

2

3

x



Sample from that component’s Gaussian

“Latent assignment” z:

we observe x, but z is hidden

p(x) = marginal over x






We’ll model each cluster 

using one of these Gaussian 

“bells”…

-2

-1

0

1

2

3

4

5

-2

-1

0

1

2

3

4

5

Maximum Likelihood estimates



Multivariate Gaussian models


EM Algorithm

• Observed feature  ∈ ℝ

• Latent feature  ∈ 1, 2, 3

• Model

• Parameters: Mean , variance Σ,  “size”  for each 

 | =  =  ; , Σ

  =  = 

 ,  =  =  | =    =  =  ⋅  ; , Σ


EM Algorithm:  E-step

• Start with clusters: Mean , Covariance Σ, “size” 

• E-step (“Expectation”)

• For each datum (example) xi, 

• Compute “ric”, the probability that it belongs to cluster c

• Compute its probability under model c

• Normalize to sum to one (over clusters c)

1 ( ; 1, Σ1)

x



 () () =


EM Algorithm:  E-step

• Start with clusters: Mean , Covariance Σ, “size” 

• E-step (“Expectation”)

• For each datum (example) xi, 

• Compute “ric”, the probability that it belongs to cluster c

• Compute its probability under model c

• Normalize to sum to one (over clusters c)

• If xi is very likely under the cth Gaussian, it gets high weight

• Denominator just makes r’s sum to one



2 ( ; 2, Σ2)

1 ≈ .33; 2 ≈ .66

x

 () () =


EM Algorithm:  M-step

• Start with assignment probabilities ric

• Update parameters: Mean , Covariance Σ, “size” 

• M-step (“Maximization”)

• For each cluster (Gaussian) z = c, 

• Update its parameters using the (weighted) data points



Total responsibility allocated to cluster c



Fraction of total assigned to cluster c



Weighted mean of  assigned data



Weighted covariance of  assigned data

(use new weighted means here)


3.3

3.4

3.5

3.6

3.7

3.8

3.9

4

3.7

3.8

3.9

4

4.1

4.2

4.3

4.4

ANEMIA PATIENTS AND CONTROLS

Red Blood Cell Volume

Red Blood Cell Hemoglobin Concentration

From P. Smyth

ICML 2001 


3.3

3.4

3.5

3.6

3.7

3.8

3.9

4

3.7

3.8

3.9

4

4.1

4.2

4.3

4.4

Red Blood Cell Volume

Red Blood Cell Hemoglobin Concentration

EM ITERATION 1

From P. Smyth

ICML 2001 


3.3

3.4

3.5

3.6

3.7

3.8

3.9

4

3.7

3.8

3.9

4

4.1

4.2

4.3

4.4

Red Blood Cell Volume

Red Blood Cell Hemoglobin Concentration

EM ITERATION 3

From P. Smyth

ICML 2001 


3.3

3.4

3.5

3.6

3.7

3.8

3.9

4

3.7

3.8

3.9

4

4.1

4.2

4.3

4.4

Red Blood Cell Volume

Red Blood Cell Hemoglobin Concentration

EM ITERATION 5

From P. Smyth

ICML 2001 


3.3

3.4

3.5

3.6

3.7

3.8

3.9

4

3.7

3.8

3.9

4

4.1

4.2

4.3

4.4

Red Blood Cell Volume

Red Blood Cell Hemoglobin Concentration

EM ITERATION 10

From P. Smyth

ICML 2001 


3.3

3.4

3.5

3.6

3.7

3.8

3.9

4

3.7

3.8

3.9

4

4.1

4.2

4.3

4.4

Red Blood Cell Volume

Red Blood Cell Hemoglobin Concentration

EM ITERATION 15

From P. Smyth

ICML 2001 


3.3

3.4

3.5

3.6

3.7

3.8

3.9

4

3.7

3.8

3.9

4

4.1

4.2

4.3

4.4

Red Blood Cell Volume

Red Blood Cell Hemoglobin Concentration

EM ITERATION 25

From P. Smyth

ICML 2001 


0

5

10

15

20

25

400

410

420

430

440

450

460

470

480

490

LOG-LIKELIHOOD AS A FUNCTION OF EM ITERATIONS

EM Iteration

Log-Likelihood

From P. Smyth

ICML 2001 


• EM is a general framework for partially observed data

• “Complete data” xi, zi

– features and assignments

• Assignments zi are missing (unobserved)

• EM corresponds to

• Computing the distribution over all zi given the parameters

• Maximizing the “expected complete” log likelihood

• GMMs = plug in “soft assignments”, but not always so easy

• Alternatives: Stochastic EM, Hard EM

• Instead of expectations, just sample the zi or choose best (often easier)

• Called “imputing” the values of z

• Hard EM: similar to EM, but less “smooth”, more local minima

• Stochastic EM: similar to EM, but with extra randomness

•

Not obvious when it has converged

EM and missing data

