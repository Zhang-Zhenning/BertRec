


Published in

Towards Data Science



Oct 6, 2018

·

9 min read

Save

Entropy is a measure of uncertainty

Eight properties, several examples and one theorem

Uncertainty in the waiting room








Measuring uncertainty

measure of uncertainty

Shannon entropy

Shannon entropy

Basic properties

Basic property 1: Uniform distributions have maximum uncertainty

uniform probability distributions


Uniform distributions have maximum entropy for a given number of outcomes.

Equiprobable outcomes

In the case of Bernoulli trials, entropy reaches its maximum value for p=0.5

Basic property 2: Uncertainty is additive for independent events

Uncertainty is additive for independent events.

The joint entropy (green) for the two independent events is equal to the sum of the individual events (red and blue).

Basic property 3: Adding an outcome with zero probability has no effect


Adding a third outcome with zero probability doesn’t make a difference.

Adding a zero-probability outcome has not effect on entropy.

Basic property 4: The measure of uncertainty is continuous in all its arguments

The Uniqueness Theorem

Functions that satisfy the four basic properties

Uniqueness Theorem

Other properties

Property 5: Uniform distributions with more outcomes have more uncertainty

Fair coin or fair die?


Property 6: Events have non-negative uncertainty

Shannon entropy

Property 7: Events with a certain outcome have zero uncertainty

A magical coin

The entropy for events with a certain outcome is zero.

Property 8: Flipping the arguments has no effect


Flipping arguments

Summary

Thank you for reading! If you’ve enjoyed this article, hit the clap button and follow

me to learn more about natural language processing and machine learning.

Also, let me know if you have a project at the intersection of these two fields that

you would like to discuss.

Data Science

Machine Learning

NLP

Artificial Intelligence

Towards Data Science


21



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





