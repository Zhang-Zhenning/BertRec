


A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

A Short Overview of

Statistical Language Models

Jon Dehdari





Invited Talk at

the Workshop on Data Mining

and its Use and Usability for Linguistic Analysis

March 2015

1 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Overview

What is a Statistical Language Model?













At the broadest level, it is a probability distribution.



































2 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Overview

What is a Statistical Language Model?













At the broadest level, it is a probability distribution.

Input













Natural Language. Usually entire or preﬁx of:



Words in a sentence (eg. for speech recognition,

machine translation)



Characters (eg. for OCR, Dasher)



Paragraph/Document (eg. for information retrieval)

















2 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Overview

What is a Statistical Language Model?













At the broadest level, it is a probability distribution.

Input













Natural Language. Usually entire or preﬁx of:



Words in a sentence (eg. for speech recognition,

machine translation)



Characters (eg. for OCR, Dasher)



Paragraph/Document (eg. for information retrieval)

Output















Probability [0, 1] – all possible outcomes sum to 1



An unnormalized score, for ranking

2 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Incremental Language Models











Incremental statistical language models provide the

probability that a given word will occur next, based on the

preceding words:

P(wi| w1, . . . , wi−1

�

��

�

h

)



















3 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Incremental Language Models











Incremental statistical language models provide the

probability that a given word will occur next, based on the

preceding words:

P(wi| w1, . . . , wi−1

�

��

�

h

)

For Example:















It’s raining cats and





3 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Incremental Language Models











Incremental statistical language models provide the

probability that a given word will occur next, based on the

preceding words:

P(wi| w1, . . . , wi−1

�

��

�

h

)

For Example:















It’s raining cats and



They went on a shopping



3 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Incremental Language Models











Incremental statistical language models provide the

probability that a given word will occur next, based on the

preceding words:

P(wi| w1, . . . , wi−1

�

��

�

h

)

For Example:















It’s raining cats and



They went on a shopping



I cooked the ﬁsh in a

3 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

A Few Uses for LMs











Statistical language models ensure ﬂuency in speech

recognition (like Siri), machine translation (like Google

Translate), on-screen keyboards (smartphones), etc.









4 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

A Few Uses for LMs











Statistical language models ensure ﬂuency in speech

recognition (like Siri), machine translation (like Google

Translate), on-screen keyboards (smartphones), etc.







Sometimes they don’t work so well...



4 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Actually, There’s a Lot of Uses!



Google suggest



Machine translation



Assisting people with motor disabilities. For example, Dasher



Speech Recognition (ASR)



Optical character recognition (OCR) and handwriting

recognition



Information retrieval / search engines



Data compression



Language identiﬁcation, as well as genre, dialect, and idiolect

identiﬁcation (authorship identiﬁcation)



Software keyboards



Surface realization in natural language generation



Password cracking



Cipher cracking

5 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Diﬀerences in LM Uses

Grammatical

Lexical

Local

Long-

Distance

Parsing

Summarization

IR

NLG

MT

Google Suggest

Password Cracking

ASR

Software Keyboards

OCR, Dasher

LangID, Cipher Cracking

6 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

LM Usage

Typical LM Queries in ...













ASR : p(recognize speech) vs. p(wreck a nice beach) vs.

p(wreck an ice peach), ...

Cipher cracking : p(attack at dawn) vs. p(uebvmkdvkdbsqk)

Google Suggest : p(how to cook french fries) vs. p(how to cook

french dictionary)

IR : query(cats and the cradle): doc1(i like cats) vs.

doc2(i like dogs)

MT &amp; NLG : lex: p(use the force) vs. p(use the power);

ordering: p(ready are you) vs. p(are you ready)

OCR : p(today is your day) vs. p(+qdav ls y0ur d4ij)

A good cipher should obey the principle of diﬀusion (Shannon, 1949).

7 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Language Modeling is Interesting!











NLP Task

Avg. Entropy

Language Modeling (=Word Prediction)

7.12

English-Chinese Translation

5.17

English-French Translation

3.92

QA (Open Domain)

3.87

Syntactic Parsing

1.18

QA (Multi-class Classiﬁcation)

1.08

Text Classiﬁcation (20 News)

0.70

Sentiment Analysis

0.58

Part-of-Speech Tagging

0.42

Named Entity Recognition

0.31

From Li &amp; Hovy (2015)

8 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

n-gram Language Models











The simplest statistical language models,

n-gram LMs, base their prediction on the

previous word or two (Markov assumption)

P(wi|w1 . . . wi−1) ≈ P(wi|wi−n+1 . . . wi−1)



History Sux!

























9 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

n-gram Language Models











The simplest statistical language models,

n-gram LMs, base their prediction on the

previous word or two (Markov assumption)

P(wi|w1 . . . wi−1) ≈ P(wi|wi−n+1 . . . wi−1)



History Sux!

For Example:















It’s raining cats and











9 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

n-gram Language Models











The simplest statistical language models,

n-gram LMs, base their prediction on the

previous word or two (Markov assumption)

P(wi|w1 . . . wi−1) ≈ P(wi|wi−n+1 . . . wi−1)



History Sux!

For Example:















It’s raining cats and



It’s raining cats and









9 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

n-gram Language Models











The simplest statistical language models,

n-gram LMs, base their prediction on the

previous word or two (Markov assumption)

P(wi|w1 . . . wi−1) ≈ P(wi|wi−n+1 . . . wi−1)



History Sux!

For Example:















It’s raining cats and



It’s raining cats and



They went on a shopping







9 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

n-gram Language Models











The simplest statistical language models,

n-gram LMs, base their prediction on the

previous word or two (Markov assumption)

P(wi|w1 . . . wi−1) ≈ P(wi|wi−n+1 . . . wi−1)



History Sux!

For Example:















It’s raining cats and



It’s raining cats and



They went on a shopping



They went on a shopping





9 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

n-gram Language Models











The simplest statistical language models,

n-gram LMs, base their prediction on the

previous word or two (Markov assumption)

P(wi|w1 . . . wi−1) ≈ P(wi|wi−n+1 . . . wi−1)



History Sux!

For Example:















It’s raining cats and



It’s raining cats and



They went on a shopping



They went on a shopping



I cooked the ﬁsh in the



9 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

n-gram Language Models











The simplest statistical language models,

n-gram LMs, base their prediction on the

previous word or two (Markov assumption)

P(wi|w1 . . . wi−1) ≈ P(wi|wi−n+1 . . . wi−1)



History Sux!

For Example:















It’s raining cats and



It’s raining cats and



They went on a shopping



They went on a shopping



I cooked the ﬁsh in the



I cooked the ﬁsh in the

9 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

n-gram LMs















In spite of their many, many shortcomings, n-gram LMs

are still widely used



1

They train quickly



2

They require no manual annotation



3

They are incremental

10 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Uniform Distribution (Zero-gram)

Zero-gram Model















In a zero-gram model, all words from the vocabulary

(V ) are equally likely:

p(wi)

=

1

|V |

=

|V |−1



For example, if we were to open a dictionary and

randomly point to a word, then “orangutan” would

have the same probability as “the”:

p(



)

=

p(λP ∈ D⟨e,t⟩.ıx[P(x) ∧ C(x)])

11 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Unigram Model













In a unigram model, using maximum likelihood

estimation, probabilities are based on word counts:

p(wi)

=

count(wi)

count(w)



For example, if we were to open a novel and randomly

point to a word, then “orangutan” would have much

less probability than “the”:

p(



)

≪

p(λP ∈ D⟨e,t⟩.ıx[P(x) ∧ C(x)])

12 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Bigram Model













But what about:

“I gave a banana to a furry orange

”



Here, a unigram model would give too much probability to

“the” and not enough to “orangutan”

















13 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Bigram Model













But what about:

“I gave a banana to a furry orange

”



Here, a unigram model would give too much probability to

“the” and not enough to “orangutan”

















13 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Bigram Model













But what about:

“I gave a banana to a furry orange

”



Here, a unigram model would give too much probability to

“the” and not enough to “orangutan”





In a bigram model, using maximum likelihood estimation,

probabilities are based on bigram and word counts:

p(wi|wi−1)

=

count(wi−1, wi)

count(wi−1)













13 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Bigram Model













But what about:

“I gave a banana to a furry orange

”



Here, a unigram model would give too much probability to

“the” and not enough to “orangutan”





In a bigram model, using maximum likelihood estimation,

probabilities are based on bigram and word counts:

p(wi|wi−1)

=

count(wi−1, wi)

count(wi−1)



w0



w1



w2



w3



w4



wk

13 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

n-gram LMs

Trigram and other n-gram LMs use a longer contiguous history

p(wi|wi−2, wi−1)

=

count(wi−2, wi−1, wi)

count(wi−2, wi−1)





































14 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

n-gram LMs

Trigram and other n-gram LMs use a longer contiguous history

p(wi|wi−2, wi−1)

=

count(wi−2, wi−1, wi)

count(wi−2, wi−1)



w0



w1



w2



w3



w4



wk

























14 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

n-gram LMs

Trigram and other n-gram LMs use a longer contiguous history

p(wi|wi−2, wi−1)

=

count(wi−2, wi−1, wi)

count(wi−2, wi−1)



w0



w1



w2



w3



w4



wk



w0



w1



w2



w3



w4



wk



w0



w1



w2



w3



w4



wk

14 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Using n-gram LMs

Using Multiple n-gram Models













Backoﬀ – Use the highest-order n-gram model that has

enough occurrences in the training set

Interpolation – Use all n-gram models, weighting them

diﬀerently



















15 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Using n-gram LMs

Using Multiple n-gram Models













Backoﬀ – Use the highest-order n-gram model that has

enough occurrences in the training set

Interpolation – Use all n-gram models, weighting them

diﬀerently

Smoothing n-grams















Smoothing allows us to deal with unseen histories



Usually steals some probability mass from seen events

and gives some to unseen events



See:

http://statmt.org/book/slides/07-language-models.pdf

15 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Skip LMs













Skip LMs like n-gram LMs, but allow intervening words between

the predicted word and its conditioning history. These are

combined (interpolated) with n-gram models.















16 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Skip LMs













Skip LMs like n-gram LMs, but allow intervening words between

the predicted word and its conditioning history. These are

combined (interpolated) with n-gram models.



Example skip bigram:

p(wi|wi−2)

=

count(wi−2, wi)

count(wi−2)



w0



w1



w2



w3



w4



wk

16 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Skip LMs













Skip LMs like n-gram LMs, but allow intervening words between

the predicted word and its conditioning history. These are

combined (interpolated) with n-gram models.



Example skip bigram:

p(wi|wi−2)

=

count(wi−2, wi)

count(wi−2)



w0



w1



w2



w3



w4



wk

+ They capture basic word order variation, and are still (more)

useful with large corpora (Goodman, 2001, § 4)

± There’s many possible combinations of histories to use

− They unnecessarily fragment the training data instead of

generalizing it (Rosenfeld, 1994, pg. 16).

16 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Class LMs













Class-based LMs abstract beyond speciﬁc words, so that, eg.

‘Thursday’ and ‘Friday’ are grouped together to function similarly

+ They’re useful for small- and medium-sized corpora (up to a billion

tokens), and easy to use. Words can be automatically clustered.

± They have advantages and disadvantages for morphologically-rich

&amp; freer word order languages

− They’re poor at handling ﬁxed phrases and multi-word expressions:



&lt;s&gt;



PRP



it



VBZ



’s



VBG



raining



NNS



cats



CC



and

17 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Topic LMs













Both class-based and topic-based LMs use a bottleneck variable to

generalize the history



Class-based LMs generalize the short-term grammatical history



Topic-based LMs generalize the long-term lexical history























18 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Topic LMs













Both class-based and topic-based LMs use a bottleneck variable to

generalize the history



Class-based LMs generalize the short-term grammatical history



Topic-based LMs generalize the long-term lexical history



Documents are (soft) clustered into a set of topics automatically



t1



w1



t2



w2



t3



w3



t4



w4



tk



wk

18 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Topic LMs













Both class-based and topic-based LMs use a bottleneck variable to

generalize the history



Class-based LMs generalize the short-term grammatical history



Topic-based LMs generalize the long-term lexical history



Documents are (soft) clustered into a set of topics automatically



t1



w1



t2



w2



t3



w3



t4



w4



tk



wk

+ Useful for domain adaptation. Widely used in information retrieval

− They’re slow and don’t scale up well. They don’t capture local

grammatical info, so they’re combined with other LMs

18 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Neural Net LMs













Like topic-based LMs, neural net LMs reduce

high-dimensional discrete probability distributions to

low-dimensional continuous distributions



Original idea inspired by biological neurons, but

architecture has diverged from biology



Has (multiple) hidden layers, to allow multiple levels of

generalization



w0



w1



o1



h1



h2



o2



w2



h3



o3



w3



h4



o4



w4



hk



ok



wk

19 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Recurrent Neural Net LMs

Elman Networks















Like previous feedforward layout, but also has the

previous hidden state feed into current hidden state



In principle can capture longer dependencies



w0



w1



o1



h1



h2



o2



w2



h3



o3



w3



h4



o4



w4



hk



ok



wk

20 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

RNNLM’s Continued

When training Elman networks the cycle gets unwrapped (called BPTT)











Output

State/Hidden

Input

State/Hidden (t − 1)

Input (t − 1)

State/Hidden (t − 2)

Input (t − 2)

State/Hidden (t − 3)

Output

State/Hidden

Input

Previous State

W

V

U

V

U

V

U

W

V

U

Copy (delayed)

Image derived from Bod´en (2002)

21 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

Comparison











Language Model

Incremental

Lexical

Distance

Speed

n-gram

Y

Y

Short

Fast

Class

Y

N

Medium

Fast

Cache

Y

Y

Long

Fast

Skip

Y

Y

Medium

Fast

PCFG

N

N

Long

Slow

Topic

Y

N

Long

Slow

FF-NN

Y

Y

Medium

Slow

RNN

Y

Y

Medium

Slow

Fine print goes here. No purchase necessary. Void where prohibited. Medium distance here is longer than a typical n-gram history length (eg. 5-gram), but isn’t the full sentence history. Elman network-based LMs aren’t trained using the full sentence history due to BPTT, and a word’s eﬀect decays over time. While advances have been made in improving softmax normalization, models that use it are still orders of magnitude slower at training than a typical generative model. Induced PCFG (eg. Baker (1979), Lari &amp; Young (1990,1991), H¨anig et al (2008), H¨anig (2010)) are costly to train from unannotated texts.

22 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

References I







Baker, J. K. (1979).

Trainable grammars for speech recognition.

In Klatt, D. H. and Wolf, J. J., editors, Speech Communication Papers for the 97th Meeting of

the Acoustical Society of America, pages 547–550, Cambridge, MA, USA.







Bod´en, M. (2002).

A guide to recurrent neural networks and backpropagation.

Technical report, Halmstad University, School of Information Science, Computer and Electrical

Engineering.







Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra, V. J. D., and Lai, J. C. (1992).

Class-based n-gram models of natural language.

Computational Linguistics, 18(4):467–479.







Elman, J. L. (1990).

Finding structure in time.

Cognitive Science, 14(2):179–211.







Gildea, D. and Hofmann, T. (1999).

Topic-based language models using EM.

In Proceedings of EUROSPEECH, pages 2167–2170.







Goodman, J. T. (2001).

A bit of progress in language modeling, extended version.

Technical Report MSR-TR-2001-72, Microsoft Research.







H¨anig, C. (2010).

Improvements in unsupervised co-occurrence based parsing.

In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,

pages 1–8, Uppsala, Sweden. Association for Computational Linguistics.

23 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

References II







H¨anig, C., Bordag, S., and Quasthoﬀ, U. (2008).

UnsuParse: Unsupervised parsing with unsupervised part of speech tagging.

In Calzolari, N., Choukri, K., Maegaard, B., Mariani, J., Odjik, J., Piperidis, S., and Tapias, D.,

editors, Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),

Marrakech, Morocco.







Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, K.-F., and Rosenfeld, R. (1993).

The SPHINX-II speech recognition system: an overview.

Computer Speech and Language, 2:137–148.







Lari, K. and Young, S. J. (1990).

The estimation of stochastic context-free grammars using the inside-outside algorithm.

Computer Speech and Language, 4:35–56.







Li, J. and Hovy, E. (2015).

The NLP engine: A universal Turing machine for NLP.

Arxiv.org Preprint.







Mikolov, T., Karaﬁ´at, M., Burget, L., ˇCernock´y, J., and Khudanpur, S. (2010).

Recurrent neural network based language model.

In Proceedings of the 11th Annual Conference of the International Speech Communication

Association (INTERSPEECH 2010), pages 1045–1048.







Rosenfeld, R. (1994).

Adaptive Statistical Language Modeling: A Maximum Entropy Approach.

PhD thesis, Carnegie Mellon University, Pittsburgh, PA, USA.







Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986).

Learning representations by back-propagating errors.

Nature, 323(6088):533–536.

24 / 25




A Short Overview

of

Statistical

Language Models

Jon Dehdari

Introduction

n-gram LMs

Skip LMs

Class LMs

Topic LMs

Neural Net LMs

Conclusion

References

References III







Werbos, P. J. (1988).

Generalization of backpropagation with application to a recurrent gas market model.

Neural Networks, 1(4):339–356.







Werbos, P. J. (1990).

Backpropagation through time: What it does and how to do it.

Proceedings of the IEEE, 78(10):1550–1560.

25 / 25

