


Log in

Sign up



usεr11852

40.2k

3

87

138



DanielX2010

173

1

7



yep

614

4

6

Laplace smoothing and Dirichlet prior

Asked 9 years, 3 months ago

Modified 5 years, 11 months ago

Viewed 8k times

12

 

 

On the wikipedia article of Laplace smoothing (or additive smoothing), it is said that from a Bayesian point of view,

I'm puzzled about how that is actually true. Could someone help me understand how those two things are equivalent?

Thanks!

Share

Improve this question

edited May 10, 2017 at 20:36

asked Jan 24, 2014 at 5:37

2 Answers

Sorted by:

11

 

+50

Sure. This is essentially the observation that the Dirichlet distribution is a conjugate prior for the multinomial distribution. This means they have the same functional form. The article mentions it, but I'll just emphasize that this

follows from the multinomial sampling model. So, getting down to it...

The observation is about the posterior, so let's introduce some data, x, which are counts of K distinct items. We observe N= ∑K

i=1xi samples total. We'll assume x is drawn from an unknown distribution π (on which we'll put a Dir(α)

prior on the K-simplex).

The posterior probability of π given α and data x is

p(π|x,α) =p(x|π)p(π|α)

The likelihood, p(x|π), is the multinomial distribution. Now let's write out the pdf's:

p(x|π) =

N!

x1!⋯xk!πx11 ⋯πxkk

and

p(π|α) =

1

B(α)

K

∏

i=1πα−1

i

where B(α) =

Γ(α)K

Γ(Kα). Multiplying, we find that,

p(π|α,x) =p(x|π)p(π|α) ∝

K

∏

i=1πxi+α−1

i

.

In other words, the posterior is also Dirichlet. The question was about the posterior mean. Since the posterior is Dirichlet, we can apply the formula for the mean of a Dirichlet to find that,

E[πi|α,x] =

xi +α

N+Kα.

Hope this helps!

Share

Improve this answer

edited Aug 20, 2014 at 10:06

answered Jan 26, 2014 at 22:26

p(π|α,x) =p(x|π)p(π|α)/p(x|α), so isn't it wrong to say that p(π|α,x) =p(x|π)p(π|α)? They are proportional with respect to π, but writing an equality is not true I think.

– michal

May 18, 2016 at 1:18 

I was confused about this for a long time, and I want to share my realization. These folks motivating Laplace smoothing by Dirichlet are using the Posterior Mean, not the MAP. For simplicity, assume the Beta distribution

(simplest case of Dirichlet) The posterior mean is 

α+nsuccess

α+β+nsuccess+nfailures whereas the MAP is 

α+nsuccess−1

α+β+nsuccess+nfailures−2. So if someone says α =β=1 corresponds to adding 1 to numerator and 2 to denominator, it's because they are using

the Posterior Mean.

– RMurphy

Ask Question

this corresponds to the expected value of the posterior distribution, using a symmetric Dirichlet distribution with parameter α as a prior.

bayesian

smoothing dirichlet-distribution

laplace-smoothing

Cite

Follow





Highest score (default)

Cite

Follow




CROSS VALIDATED

Tour

Help

Chat

Contact

Feedback

COMPANY

Stack Overflow

Teams

Advertising

Collectives

Talent

About

Press

Legal

Privacy Policy

Terms of Service

Cookie Settings

Cookie Policy

STACK EXCHANGE NETWORK

Technology

Culture &amp; recreation

Life &amp; arts

Science

Professional

Business

API



omidi

1,019

7

12

Jun 19, 2017 at 0:26 

0

 

 

As a side note, I would also like to add another point to the above derivation, which it's not really concerning the main question. However, talking about Dirichlet priors on multinomial distribution, I thought it worth to mention

that what would be the form of likelihood function if we're going to take probabilities as nuisance variables.

As it's correctly pointed out by by sydeulissie, the p(π|α,x) is proportional to ∏K

i=1πxi+α−1

i

 . Now here I would like to calculate p(x|α).

p(x|α) =∫

K

∏

i=1p(x|πi,α)p(π|α)dπ1dπ2...dπK

Using an integral identity for gamma functions, we have:

p(x|α) =

Γ(Kα)

Γ(N+Kα)

K

∏

i=1

Γ(xi +α)

Γ(α)

The above derivation of the likelihood for categorical data proposes a more robust way of dealing with this data for cases that the sample size N is not so big enough.

Share

Improve this answer

answered Jan 30, 2014 at 11:06

Your Answer

By clicking “Post Your Answer”, you agree to our terms of service, privacy policy and cookie policy

Not the answer you're looking for? Browse other questions tagged bayesian

smoothing dirichlet-distribution

laplace-smoothing  or ask your own question.

Related

9

Bayesian inference for multinomial distribution with asymmetric prior knowledge?

5

What is a Dirichlet prior

35

Why is Lasso penalty equivalent to the double exponential (Laplace) prior?

2

Bayesian smoothing using Dirichlet prior : why not MAP?

4

Posterior computation for Laplace distribution

4

Relationship between laplace and l1 regularization

Hot Network Questions



Did the Golden Gate Bridge 'flatten' under the weight of 300,000 people in 1987?



Order relations on natural number objects in topoi, and symmetry



How do I stop the Flickering on Mode 13h?



Why does contour plot not show point(s) where function has a discontinuity?



Could a subterranean river or aquifer generate enough continuous momentum to power a waterwheel for the purpose of producing electricity?

more hot questions

 Question feed





Cite

Follow



Post Your Answer

Featured on Meta



New blog post from our CEO Prashanth: Community is the future of AI



Improving the copy in the close modal and post notices - 2023 edition


Data

Blog

Facebook

Twitter

LinkedIn

Instagram

Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2023.4.21.43403

Your privacy

By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.

 

Accept all cookies

Necessary cookies only

Customize settings

