








16.6. Fine-Tuning BERT for Sequence-Level and

Token-Level Applications

In the previous sections of this chapter, we have

designed different models for natural language processing applications, such as

based on RNNs, CNNs, attention, and MLPs. These models are helpful when there

is space or time constraint, however, crafting a specific model for every natural

language processing task is practically infeasible. In Section 15.8, we introduced a pretraining model, BERT,

that requires minimal architecture changes for a wide range of natural language processing tasks. On the one

hand, at the time of its proposal, BERT improved the state of the art on various natural language processing

tasks. On the other hand, as noted in Section 15.10, the two versions of the original BERT model come with

110 million and 340 million parameters. Thus, when there are sufficient computational resources, we may

consider fine-tuning BERT for downstream natural language processing applications.

In the following, we generalize a subset of natural language processing applications as sequence-level and

token-level. On the sequence level, we introduce how to transform the BERT representation of the text input to

the output label in single text classification and text pair classification or regression. On the token level, we

will briefly introduce new applications such as text tagging and question answering and shed light on how

BERT can represent their inputs and get transformed into output labels. During fine-tuning, the “minimal

architecture changes” required by BERT across different applications are the extra fully connected layers.

During supervised learning of a downstream application, parameters of the extra layers are learned from

scratch while all the parameters in the pretrained BERT model are fine-tuned.

16.6.1. Single Text Classification

Single text classification takes a single text sequence as input and outputs its classification result. Besides

sentiment analysis that we have studied in this chapter, the Corpus of Linguistic Acceptability (CoLA) is also a

dataset for single text classification, judging whether a given sentence is grammatically acceptable or not

(Warstadt et al., 2019). For instance, “I should study.” is acceptable but “I should studying.” is not.

Fig. 16.6.1  Fine-tuning BERT for single text classification applications, such as sentiment analysis and

testing linguistic acceptability. Suppose that the input single text has six tokens.

Section 15.8 describes the input representation of BERT. The BERT input sequence unambiguously represents

both single text and text pairs, where the special classification token “&lt;cls&gt;” is used for sequence

classification and the special classification token “&lt;sep&gt;” marks the end of single text or separates a pair of

text. As shown in Fig. 16.6.1, in single text classification applications, the BERT representation of the special

classification token “&lt;cls&gt;” encodes the information of the entire input text sequence. As the representation of



 COLAB [PYTORCH]



 COLAB [MXNET]



 COLAB [JAX]



 COLAB [TENSORFLOW]



 SAGEMAKER STUDIO LAB

