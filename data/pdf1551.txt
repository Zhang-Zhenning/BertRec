
Data Analytics

AI, Data, Data Science, Machine Learning,

Blockchain, Digital







Select a page



N-Gram Language

Models Explained

with Examples

February 2, 2018 by Ajitesh Kumar · Leave a

comment

Language models are models which assign

probabilities to a sentence or a sequence of

words or, probability of an upcoming word

given previous set of words. Language

models are used in fields such as speech

recognition, spelling correction, machine

translation etc. Language models are

primarily of two kinds:

N-Gram language models

Grammar-based language models such

as probabilistic context-free grammars

(PCFGs)

In this post, you will learn about some of the

following:

Introduction to Language Models

N-Grams language models



1.

2.

2.1.

2.2.

2.3.

3.

4.

Table of Contents

Introduction to Language Models

N-Grams Language models

Unigram Language Model Example

Bigram Language Model Example

Trigram Language Model Example

Further Reading

Summary


Introduction to Language

Models

Language models, as mentioned above, is

used to determine the probability of

occurrence of a sentence or a sequence of

words. Language models are created based

on following two scenarios:

Scenario 1: The probability of a sequence of

words is calculated based on the product of

probabilities of each word. Let’s say, we need

to calculate the probability of occurrence of

the sentence, “car insurance must be bought

carefully”. The probability of occurrence of

this sentence will be calculated based on

following formula:

P(car insurance must be bought carefully) = 

P(car) � P(insurance) � P(must) � P(be) � P(bought) � P(carefully)

 

In above formula, the probability of each

word can be calculated based on following:

P(car) =

No. of times word car occurred in the corpus)

total no of words in the corpus

 

Generalizing above, the following can be

said:

P(wi) =

c(wi)

c(w)

 

In above formula, wi is any specific word, 

c(wi) is count of specific word, and c(w) is

count of all words.

Scenario 2: The probability of a sequence of

words is calculated based on the product of

probabilities of words given occurrence of

previous words. Let’s say, we need to

calculate the probability of occurrence of the

sentence, “best websites for comparing car

insurances”. The probability of occurrence of

this sentence will be calculated based on

following formula:

P(“best websites for comparing car insurance”) =

P(best/startOfSentence)P(websites/best)P(for/websites)…P(endOfSentence/insurance)

 

In above formula, the probability of a word

given the previous word can be calculated

using the formula such as following:


P(websites/best) =

P(best websites)

P(best)

 

Generalizing above, the following can be

said:

P(

wi

wi−1) =

P(wi−1wi)

P(wi−1)

N-Grams Language

models

As defined earlier, Language models are used

to determine the probability of a sequence of

words. The sequence of words can be 2

words, 3 words, 4 words…n-words etc. N-

grams is also termed as a sequence of n

words. The language model which is based

on determining probability based on the

count of the sequence of words can be called

as N-gram language model. Based on the

count of words, N-gram can be:

Unigram: Sequence of just 1 word

Bigram: Sequence of 2 words

Trigram: Sequence of 3 words

…so on and so forth

Unigram Language Model

Example

Let’s say we want to determine the

probability of the sentence, “Which is the

best car insurance package”. Based on

Unigram language model, probability can be

calculated as following:

P(“Which is best car insurance package”) = 

P(which)P(is)…P(insurance)P(package)

 

Above represents product of probability of

occurrence of each of the words in the

corpus. The probability of any word, wi can

be calcuted as following:

P(wi) =

c(wi)

c(w)

where wi is ith word, c(wi) is count of wi in the

corpus, and c(w) is count of all the words.

Bigram Language Model

Example

Using above sentence as example and


Bigram language model, the probability can

be determined as following:

P(“Which is best car insurance package”) = 

P(

which

startOfSentence)P(

is

which)P(

best

is ). . P(

endOfSentence

package

)

 

The following represents example of how to

calculate each of the probabilities:

P(

car

best) =

P(best car)

P(best)

 

The above can also be calculated as

following:

P(

car

best) = 

c(best car)

c(best)

 

The above could be read as: Probability of

word “car” given word “best” has occurred is

probability of word “best car” divided by

probability of word “best”. Alternatively,

Probability of word “car” given word “best”

has occurred is count of word “best car”

divided by count of word “best”.

Above represents product of probability of

occurrence of each of the word given

earlier/previous word. Generally speaking,

the probability of any word given previous

word, 

wi

wi−1 can be calculated as following:

P(

wi

wi−1) =

P(wi−1,wi)

P(wi−1)

Trigram Language Model

Example

Let’s say we want to determine probability of

the sentence, “Which company provides best

car insurance package”. Using trigram

language model, the probability can be

determined as following:

P(“Which company provides best car insurance package”) =

P(

company

which startOfSentence)P(

provides

which company)P(

best

company provides)…P(

endOfSentence

insurance package)

 

The following represents example of how to

calculate each of the probabilities:

P(

provides

which company) = 

P(which company provides)

P(which company)

 


The above can also be calculated as

following:

P(

provides

which company) = 

c(which company provides)

c(which company)

 

The above could be read as: Probability of

word “provides” given words “which

company” has occurred is probability of

word “which company provides” divided by

probability of word “which company”.

Alternatively, Probability of word “provides”

given words “which company” has occurred

is count of word “which company provides”

divided by count of word “which company”.

Generalizing above, the probability of any

word given two previous words, 

wi

wi−2,wi−1 can

be calculated as following:

P(

wi

wi−2,wi−1) =

P(wi−2,wi−1,wi)

P(wi−2,wi−1)

Further Reading

N-grams

Probabilistic language model

Language model (Statistical Machine

Translation)

Summary

In this post, you learned about different

types of N-grams language models and

also saw examples.

Did you find this article useful? Do you have

any questions or suggestions about this

article or understanding N-grams language

models? Leave a comment and ask your

questions and I shall do my best to address

your queries.





Follow me

Author Recent Posts

Ajitesh Kumar

I have been recently

working in the area of Data

analytics including Data

Science and Machine

Learning / Deep Learning. I

am also passionate about

different technologies

including programming






← Blockchain – Bitcoin Explorer

Relaunched

10+ Key Stages of Data Science Project Life

cycle →

Leave a Reply

Your email address will not be published.

Required fields are marked *

Comment *

Name *

Email *

Website

 

 

 

 



Posted in AI, NLP. Tagged with nlp.



languages such as Java/JEE,

Javascript, Python, R, Julia,

etc, and technologies such

as Blockchain, mobile

computing, cloud-native

technologies, application

security, cloud computing

platforms, big data, etc. For

latest updates and blogs,

follow us on Twitter. I

would love to connect with

you on Linkedin. 

Check out my latest book

titled as First Principles

Thinking: Building winning

products using first

principles thinking

5  + 

 = 





Post Comment

Post Comment




Data Analytics © 2023

Powered by WordPress. Design by

WildWebLab

About Us



Vitalflux.com is dedicated to help software

engineers &amp; data scientists get technology

news, practice tests, tutorials in order to

reskill / acquire newer skills from time-to-

time.

Thank you for visiting our site today. We

welcome all your suggestions in order to

make our website better. Please feel free to

share your thoughts.

Processing math: 100%

