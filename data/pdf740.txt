
Read Paper

See Code

 



 

ELMo

Introduced by Peters et al. in Deep contextualized word representations

Embeddings from Language Models, or ELMo, is a type of deep contextualized word representation that models both (1)

complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model

polysemy). Word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a

large text corpus.

A biLM combines both a forward and backward LM. ELMo jointly maximizes the log likelihood of the forward and backward directions. To add

ELMo to a supervised model, we freeze the weights of the biLM and then concatenate the ELMo vector $\textbf{ELMO}^{task}_k$ with

$\textbf{x}_k$ and pass the ELMO enhanced representation $[\textbf{x}_k; \textbf{ELMO}^{task}_k]$ into the task RNN. Here

$\textbf{x}_k$ is a context-independent token representation for each token position.

Image Source: here

Source: 

 Deep contextualized word representations



Papers

Language Models



 Edit




 



 



 

Paper

Code

Results

Date

Stars

Tasks

Task

Papers

Share

 Language Modelling

35

8.77%

 Sentiment Analysis

25

6.27%

 Named Entity Recognition (NER)

21

5.26%

 NER

19

4.76%

 Text Classification

15

3.76%

 General Classification

15

3.76%

 Question Answering

14

3.51%

 Natural Language Inference

12

3.01%

 Word Sense Disambiguation

11

2.76%

Usage Over Time

Components

Component

Type



 BiLSTM

Deep Tabular Learning



 Softmax

Output Functions

Categories

Contact us on: 

 hello@paperswithcode.com.

Papers With Code is a free resource with all data licensed under CC-BY-SA.

Terms

 Data policy

 Cookies policy

 from 

 Edit



Language Models



Contextualized Word Embeddings



Word Embeddings



