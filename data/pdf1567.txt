


ML Wiki

Statistical Language Models

Contents

Contents

1 Statistical Language Models

1.1 Generative Models

1.2 Usage

2 Building Language Models

3 Unigrams

3.1 Inference

3.2 Usage

4 N-grams

5 Evaluation of Language Models

5.1 Indirect Way

5.2 Direct Way

6 Structured Models

7 Sources

Statistical Language Models

A statistical language model (Language Model for short) is a probability distribution over sequences of words (i.e. over

sentences)

It assigns any sentence a probability it's a probability of seeing a sentence according to the LM

Language Models can be context dependent

For example:

p1 = P("Today is Wednesday") = 0.001

p2 = P("The equation has a solution") = 0.00001

Although both sentences are valid, sentence 1 is more likely: for example, because the LM can model some general

conversations rather than conversations on a math conference

Generative Models

Language models are generative models for text:

given a LM we can sample sentences according to the distribution and obtain a text sample

so we can "generate" text

Usage

LMs:

provide a way to quantify the uncertainly associated with natural languages

allow to answer Information Retrieval related questions e.g.

how likely would a user use a query with word "baseball" if he wants to find about sport

 

Go



Search

Home

 Page Info

Log in



Search


Building Language Models

Can't enumerate all possible sentences, so need to make some assumptions to simplify the model

Notation:

V = {w1,  . . .  , wN}

is the vocabulary, |V| = N

Unigrams

This is the simplest LM

a word sequence is generated by sampling each word independently

Assumption: for a sequence [w1,  . . .  , wn]

, P(w1,  . . .  , wn) = ∏iP(wi)

Let θ

be a Unigram LM

there are |V|

params in θ

condition: ∑p(wi � θ) = 1

so unigram LM specifies a Multinomial Distribution over words

according to unigram models, sentences with high-probability words have higher probability

High probability words also can indicate topics

e.g. θ1

about Text Mining

θ2

about food

if a document D

is a text mining paper, than we expect P(D � θ1) &gt; P(D � θ2)

Inference

Suppose we have D

generated by some unigram model θ

and we want to learn that are the parameters of θ

: estimate P(w � θ)

for each w � D

this is a Parameter Estimation Problem

Maximum Likelihood Estimation (MLE) is a possible solution: ˆθ = arg maxθP(D � θ)

since unigrams is a multinomial probability distribution, MLE is:

P(w � ˆθ) =

c(w,D)

|D|

where c(w, D)

is the count of word w

in document D

and |D|

is the total number of words in the document

How this formula is derived?

consider log likelihood function: logP(D � θ) = ∑w�Vc(w, D)logP(w � θ)


we want to maximize it s.t. P(w � θ)

is a Probability Distribution i.e. ∑w�VP(w � θ) = 1

use Lagrange Multipliers to convert this constrained optimization problem into an unconstrained one

so let L(θ, λ) = logP(D � θ) + λ(1 − ∑P(w � θ)) = ∑w�Vc(w, D)logP(w � θ) + λ(1 − ∑P(w � θ))

by solving it, we get P(w � ˆθ) =

c(w,D)

|D|

Smoothing for Language Models

MLE may overfit the data: it will assign 0 probabilities to words it hasn't seen

What to do with it?

Bayesian Parameter Estimation can both maximize the data likelihood and incorporate the prior belief to "smooth" the

estimate

use MAP: Maximum A Posteriori Estimation:

ˆθ = arg maxθP(θ � D) = arg maxθP(D � θ)P(θ)

so we can define some prior P(θ)

, and depending on the choice of prior, we'd have different estimators

if the prior prefers models that don't assign 0 probability to any w

, then at the end we won't have 0 entries

adjusting MLE to avoid 0 probability is called "smoothing" - it's a form of regularization

Usage

Popular choice in Information Retrieval

N-grams

Unigrams make unrealistic assumptions about word occurrence:

generally word order matters

Bigram models:

Have the Markov Property:

P(w1,  . . .  , wn) = P(w1) � ∏iP(wi � wi−1)

can capture local dependency between two adjacent words

Usage:

better than unigrams in complex NLP tasks such as Machine Translation, Speech Recognition, etc

not much used in IR: the IR task is relatively easy and these models usually don't give much performance gain over

unigrams

Evaluation of Language Models

Indirect Way

Evaluate not the LM itself, but the application where LM is used

if the LM is used in an IR system, evaluate the performance of this IR system using different LMs (e.g. with Precision and

Recall)

Direct Way

Standalone evaluation of LM










Structured Models

There are also models that take into account the structure of natural languages

for example Probabilistic Context-Free Grammars allow to capture remote dependencies

these models are complex and require a lot of data to estimate the parameters

usually simpler models work just as good

Sources

Information Retrieval (UFRT)

Zhai, ChengXiang. "Statistical language models for information retrieval." 2008.

Categories: Information Retrieval NLP

This page was last modified on 27 June 2015, at 13:45.

Machine Learning Bookcamp: learn machine learning by doing projects (get 40% off with code "grigorevpc")

2012 – 2023 by Alexey Grigorev

Powered by MediaWiki. TyrianMediawiki Skin, with Tyrian design by Gentoo.

Privacy policy About ML Wiki Disclaimers

