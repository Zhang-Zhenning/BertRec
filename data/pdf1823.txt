


Figures and Tables from this paper

Figures and Tables from this paper

Figures and Tables

36 Citations

14 References

Related Papers

DOI: 10.1145/2661829.2661900  • Corpus ID: 11850476

Revisiting the Divergence Minimization Feedback Model

Yuanhua Lv, ChengXiang Zhai  • Published 3 November 2014  • Computer Science  • 

Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management

Pseudo-relevance feedback (PRF) has proven to be an effective strategy for improving retrieval accuracy. In this paper, we revisit a PRF method based on

statistical language models, namely the divergence minimization model (DMM). DMM not only has apparently sound theoretical foundation, but also has

been shown to satisfy most of the retrieval constraints. However, it turns out to perform surprisingly poorly in many previous experiments. We investigate the

cause, and reveal that DMM… Expand

View on ACM





sifaka.cs.uiuc.edu











Sign In

By clicking accept or continuing to use the site, you agree to the terms outlined in our Privacy Policy, Terms of Service, and Dataset License



ACCEPT &amp; CONTINUE




Table 1

Figure 1














Table 2

Table 3

36 Citations

36 Citations









Sort by Relevance








Axiomatic Analysis for Improving the Log-Logistic Feedback Model

Axiomatic Analysis for Improving the Log-Logistic Feedback Model

Ali Montazeralghaem, Hamed Zamani, A. Shakery  • Business, Computer Science  • SIGIR  • 2016

 This paper introduces two new PRF constraints, analyzes the log-logistic feedback model and shows that it does not

satisfy these two constraints as well as the previously proposed "relevance effect" constraint, and modifies thelogistic

formulation to satisfy all these constraints.



29

 • 

LiMe: linear methods for pseudo-relevance feedback

LiMe: linear methods for pseudo-relevance feedback

Daniel Valcarce, Javier Parapar, Álvaro Barreiro  • Computer Science  • SAC  • 2018

 This paper presents a novel formulation of the PRF task as a matrix decomposition problem which is called LiMe, and

uses linear least squares regression with regularisation to solve the proposed decomposition with non-negativity constraints.



10



PDF

 • 

Pseudo Relevance Feedback with Deep Language Models and Dense Retrievers: Successes and Pitfalls

Pseudo Relevance Feedback with Deep Language Models and Dense Retrievers: Successes and Pitfalls

Hang Li, Ahmed Mourad, Shengyao Zhuang, B. Koopman, G. Zuccon  • Computer Science  • ACM Transactions on Information Systems  • 2023

 This article investigates methods for integrating PRF signals with rerankers and dense retrievers based on deep

language models and considers text-based, vector-based and hybrid PRF approaches and investigates different ways of

combining and scoring relevance signals.



16



PDF

 • 

LoL: A Comparative Regularization Loss over Query Reformulation Losses for Pseudo-Relevance Feedback

LoL: A Comparative Regularization Loss over Query Reformulation Losses for Pseudo-Relevance Feedback

Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, Xueqi Cheng  • Computer Science  • SIGIR  • 2022

 The Loss-over-Loss (LoL) framework is proposed to compare the reformulation losses between different revisions of the

same query during training, and a differentiable query reformulation method is presented to implement this framework.



1



PDF

 • 

Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback: A Reproducibility Study

Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback: A Reproducibility Study

Hang Li, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy J. Lin, G. Zuccon  • Computer Science  • ECIR  • 2022

 This paper reproduces and studies a recent method for PRF with dense retrievers, called ANCE-PRF, and investigates the

effect of the hyper-parameters that govern the training process and the robustness of the method across these different

settings.



12



PDF

 • 

Axiomatic Analysis of Smoothing Methods in Language Models for Pseudo-Relevance Feedback

Axiomatic Analysis of Smoothing Methods in Language Models for Pseudo-Relevance Feedback

Hussein Hazimeh, ChengXiang Zhai  • Computer Science  • ICTIR  • 2015

Pseudo-Relevance Feedback (PRF) is an important general technique for improving retrieval effectiveness without requiring

any user effort. Several state-of-the-art PRF models are based on the… 



26



PDF

 • 

Pseudo relevance feedback optimization

Pseudo relevance feedback optimization

A. Arampatzis, Georgios Peikos, S. Symeonidis  • Computer Science  • Information Retrieval Journal  • 2021

 This work proposes a method for automatic optimization of pseudo relevance feedback (PRF) in information retrieval

that outperforms state-of-the-art PRF methods from the recent literature, but is still far from the method’s effectiveness ceiling.



1

 • 

Towards Efficient and Effective Query Variant Generation

Towards Efficient and Effective Query Variant Generation

R. Benham, J. Culpepper, L. Gallagher, Xiaolu Lu, J. Mackenzie  • Computer Science  • DESIRES  • 2018

 This work proposes a new sampling-based system which provides significantly better efficiency-effectiveness tradeoffs

while leveraging both relevance modeling and data fusion, and shows how to leverage query expansion andData fusion to

achieve significantly better risk-reward trade-offs than plain relevance modeling approaches.



16



PDF

 • 

Luhn Revisited: Significant Words Language Models

Luhn Revisited: Significant Words Language Models

M. Dehghani, H. Azarbonyad, J. Kamps, D. Hiemstra, maarten marx  • Computer Science  • CIKM  • 2016

TLDR

TLDR

Expand

View 1 excerpt, cites methods

Save

Alert

TLDR

TLDR

Expand

Save

Alert

TLDR

TLDR

Expand

View 2 excerpts, cites methods

Save

Alert

TLDR

TLDR

Expand

View 3 excerpts, cites methods

Save

Alert

TLDR

TLDR

Expand

View 2 excerpts, cites methods

Save

Alert

Expand

Save

Alert

TLDR

TLDR

Expand

View 1 excerpt

Save

Alert

TLDR

TLDR

Expand

View 1 excerpt, cites background

Save

Alert






14 References

14 References







 This work proposes significant words language models of feedback documents that capture all, and only, the

significant shared terms from feedback documents, and presents them as the effective models capturing the essential terms

and their probabilities.



30



PDF

 • 

Towards Efficient and EffectiveQuery Variant Generation

Towards Efficient and EffectiveQuery Variant Generation

R. Benham, J. Culpepper, L. Gallagher, Xiaolu Lu  • Computer Science  • 2018

 This work shows that their new end-to-end search system approaches the state-of-the-art in effectiveness while still

being efficient in practice, and shows how to leverage query expansion and data fusion to achieve significantly better risk-

reward trade-offs than plain relevance modeling approaches.



PDF

 • 

1

2

3

4

TLDR

TLDR

Expand

View 11 excerpts, cites methods and background

Save

Alert

TLDR

TLDR

Expand

View 1 excerpt, cites background

Save

Alert

Sort by Relevance

Regularized estimation of mixture models for robust pseudo-relevance feedback

Regularized estimation of mixture models for robust pseudo-relevance feedback

Tao Tao, ChengXiang Zhai  • Computer Science  • SIGIR  • 2006

 A more robust method for pseudo feedback based on statistical language models to integrate the original query with

feedback documents in a single probabilistic mixture model and regularize the estimation of the language model parameters in

the model so that the information in the feedback documents can be gradually added to the originalquery.



236



PDF

 • 

A Theoretical Analysis of Pseudo-Relevance Feedback Models

A Theoretical Analysis of Pseudo-Relevance Feedback Models

S. Clinchant, Éric Gaussier  • Computer Science  • ICTIR  • 2013

 This study reveals that most models are deficient with respect to at least one condition, and that this deficiency explains

the results of the analysis of the behavior of the models, as well as some of the results reported on the respective performance

of PRF models.



47



PDF

 • 

Model-based feedback in the language modeling approach to information retrieval

Model-based feedback in the language modeling approach to information retrieval

ChengXiang Zhai, J. Lafferty  • Computer Science  • CIKM '01  • 2001

 This paper proposes and evaluates two different approaches to updating a query language model based on feedback

documents, one based on a generative probabilistic model of feedback documents and onebased on minimization of the KL-

divergence over feedback documents.



859



PDF

 • 

Positional relevance model for pseudo-relevance feedback

Positional relevance model for pseudo-relevance feedback

Yuanhua Lv, ChengXiang Zhai  • Computer Science  • SIGIR  • 2010

 The proposed positional relevance model (PRM) is an extension of the relevance model to exploit term positions and

proximity so as to assign more weights to words closer to query words based on the intuition that words closerto query words

are more likely to be related to the query topic.



197



PDF

 • 

A comparative study of methods for estimating query language models with pseudo feedback

A comparative study of methods for estimating query language models with pseudo feedback

Yuanhua Lv, ChengXiang Zhai  • Computer Science  • CIKM  • 2009

 This work systematically compares five representative state-of-the-art methods for estimating query language models

with pseudo feedback in ad hoc information retrieval, and proposes several heuristics that are intuitively related to the good

retrieval performance of an estimation method.



199



PDF

 • 

TLDR

TLDR

Expand

View 7 excerpts, references methods and background

Save

Alert

TLDR

TLDR

Expand

View 4 excerpts, references background and methods

Save

Alert

TLDR

TLDR

Expand

View 10 excerpts, references methods and background

Save

Alert

TLDR

TLDR

Expand

View 2 excerpts, references methods

Save

Alert

TLDR

TLDR

Expand

View 12 excerpts, references methods, results and background

Save

Alert




Related Papers

Related Papers









Latent concept expansion using markov random fields

Latent concept expansion using markov random fields

Donald Metzler, W. Bruce Croft  • Computer Science  • SIGIR  • 2007

 A robust query expansion technique based on the Markov random field model for information retrieval, called latent

concept expansion, provides a mechanism for modeling term dependencies during expansion and the use of arbitrary features

within the model provides a powerful framework for going beyond simple term occurrence features.



263



PDF

 • 

Reducing the risk of query expansion via robust constrained optimization

Reducing the risk of query expansion via robust constrained optimization

K. Collins-Thompson  • Computer Science  • CIKM  • 2009

 Analysis is provided showing that this approach is a natural and effective way to do selective expansion, automatically

reducing or avoiding expansion in risky scenarios, and successfully attenuating noise in poor baseline methods.



134



PDF

 • 

UMass at TREC 2004: Novelty and HARD

UMass at TREC 2004: Novelty and HARD

N. A. Jaleel, James Allan, 

 C. Wade  • Computer Science  • TREC  • 2004

 The primary findings for passage retrieval are that document retrieval methods performed better than passage retrieval

methods on the passage evaluation metric of binary preference at 12,000 characters, and that clarification forms improved

passage retrieval for every retrieval method explored.



265



PDF

 • 

A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval

A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval

ChengXiang Zhai, J. Lafferty  • Computer Science  • SIGIR  • 2001

 This paper examines the sensitivity of retrieval performance to the smoothing parameters and compares several

popular smoothing methods on different test collections.



1,403



PDF

 • 

A unified optimization framework for robust pseudo-relevance feedback algorithms

A unified optimization framework for robust pseudo-relevance feedback algorithms

Joshua V. Dillon, K. Collins-Thompson  • Computer Science  • CIKM  • 2010

 A flexible new optimization framework for finding effective, reliable pseudo-relevance feedback models that unifies

existing complementary approaches in a principled way and allows a rich new space of model search strategies to be

investigated.



23



PDF

 • 

1

2

TLDR

TLDR

Expand

View 2 excerpts, references methods

Save

Alert

TLDR

TLDR

Expand

View 3 excerpts, references methods

Save

Alert

+5 authors

TLDR

TLDR

Expand

View 7 excerpts, references methods

Save

Alert

TLDR

TLDR

Expand

View 3 excerpts, references methods

Save

Alert

TLDR

TLDR

Expand

View 2 excerpts, references methods

Save

Alert










Stay Connected With Semantic Scholar

Stay Connected With Semantic Scholar

Your E-mail Address



Sign Up

What Is Semantic Scholar?

Semantic Scholar is a free, AI-powered research tool for scientific literature, based at the

Allen Institute for AI.

Learn More

About

About Us

Publishers

Blog

AI2 Careers

Product

Product Overview

Beta Program

S2AG API

Semantic Reader

Research

Publications

Team

Research Careers

Resources

Help

FAQ

Librarians

Tutorials

Contact









Proudly built by AI2

Collaborators &amp; Attributions • Terms of Service • Privacy Policy

