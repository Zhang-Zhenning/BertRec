


Published in

Towards Data Science



Jul 11, 2020

·

11 min read

·

Save

Photo by Marvin Ronsdorf on Unsplash

Expectation Maximization Explained

A versatile algorithm for clustering, NLP, and more

Unsupervised Clustering

Expectation Maximization for Old Faithful Eruption Data (Wikipedia)








Expectation

Maximization

The 5 parameters for our model, represented collectively as θ

Probability of observing a point with value x

Likelihood of observing our whole dataset

Log-likelihood of observing our data

latent variable


latent variable

Expectation

Maximization

Responsibility ɣ of each cluster for data point i

Table of Symbols

The Expectation Maximization algorithm for our example


Expectation Maximization in General

The Chain Rule for Probability

Likelihood Formulas from the Gaussian Mixture Model example


Expectation of the Likelihood in the Gaussian Mixture Model example.

Shorthand Notations for the Expected Likelihoods

The General Expectation–Maximization Algorithm

Why it Works

Schematic form of the function R

Suggestive schematic form for the function R in a special case


Improvement in likelihood after an update step

Conclusion

Acknowledgments

Notes


1



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





Towards Data Science

Mathematics

Statistics

Machine Learning

Data Science

