




Evaluation Metrics for Language Modeling

1 8.OCT.201 9 . 20 MIN READ

Recently, neural network trained language models, such as ULMFIT, BERT, and GPT-2, have been remarkably

successful when transferred to other natural language processing tasks. As such, there's been growing interest in

language models.

Traditionally, language model performance is measured by perplexity, cross entropy, and bits-per-character (BPC). As

language models are increasingly being used as pre-trained models for other NLP tasks, they are often also evaluated

based on how well they perform on downstream tasks. The GLUE benchmark score is one example of broader, multi-

task evaluation for language models 

.

Counterintuitively, having more metrics actually makes it harder to compare language models, especially as indicators

of how well a language model will perform on a specific downstream task are often unreliable. One of my favorite

interview questions is to ask candidates to explain perplexity or the difference between cross entropy and BPC. While

almost everyone is familiar with these metrics, there is no consensus: the candidates’ answers differ wildly from each

other, if they answer at all.

One point of confusion is that language models generally aim to minimize perplexity, but what is the lower bound on

perplexity that we can get since we are unable to get a perplexity of zero? If we don’t know the optimal value, how do

we know how good our language model is?



[1]

ABOUT

ART ICLES

NEWSLET T ER

PODCAST

SUBSCRIBE

WRIT E

SUPPORT

