


Log in

Sign up



gung - Reinstate Monica

140k

85

382

680



michal

1,238

3

12

14



Tom Minka

6,750

1

24

35

Why is the Expectation Maximization algorithm guaranteed to converge to a local optimum?

Asked 9 years, 3 months ago

Modified 7 years, 4 months ago

Viewed 14k times

35

 

 

I have read a couple of explanations of EM algorithm (e.g. from Bishop's Pattern Recognition and Machine Learning and from Roger and Gerolami First Course on Machine Learning). The derivation of EM is ok, I understand it. I

also understand why the algorithm coverges to something: at each step we improve the result and the likelihood is bounded by 1.0, so by using a simple fact (if a function increases and is bounded then it converges) we know that the

algorithm converges to some solution.

However, how do we know it is a local minimum? At each step we are considering only one coordinate (either latent variable or parameters), so we might miss something, like that the local minimum requires moving by both

coordinates at once.

This I believe is a similar problem to that of general class of hill climbing algorithms, which EM is an instance of. So for a general hill climbing algorithm we have this problem for function f(x, y) = x*y. If we start from (0, 0) point,

then only by considering both directions at once we are able to move upwards from 0 value.

Share

Improve this question

edited Dec 10, 2015 at 21:22

asked Jan 26, 2014 at 14:09

3

The likelihood is bounded only for fixed variances. That is, in the binomial situation, the variance is p(1−p); or in the Gaussian situation, if the variance is assumed known. If the variance is unknown, and has to be estimated, the

likelihood is not bounded. Also, in the EM algorithm, there is a generic separation of the missing and parameters, at least for the frequentist statisticians, but the surfaces may indeed have saddles.

– StasK

Mar 4, 2014 at 16:45

@Stask I am not sure that likelihood is generally bounded even with fixed variances. Are you restricting to some particular family?

– Glen_b

Dec 11, 2015 at 0:54

2 Answers

Sorted by:

35

 

EM is not guaranteed to converge to a local minimum. It is only guaranteed to converge to a point with zero gradient with respect to the parameters. So it can indeed get stuck at saddle points.

Share

Improve this answer

answered Mar 4, 2014 at 16:17

2

For examples, see pp. 20 and 38 here, p. 85 here -- try "saddle point" in Amazon reader.

– StasK

Mar 4, 2014 at 16:53

20

 

 

First of all, it is possible that EM converges to a local min, a local max, or a saddle point of the likelihood function. More precisely, as Tom Minka pointed out, EM is guaranteed to converge to a point with zero gradient.

I can think of two ways to see this; the first view is pure intuition, and the second view is the sketch of a formal proof. First, I shall, very briefly, explain how EM works:

Expectation Maximization as gradient ascent

In each iteration t, EM requires that the bound bt touches the likelihood function L at the solution of the previous iteration i.e. θt−1 which implies their gradients are the same too; that is g = ∇bt(θt−1) = ∇L(θt−1). So, EM is at least

as good as gradient ascent because θt is at least as good as θt−1+ηg. In other words:

Sketch of a formal proof

One can show that the gap between the bounds and the likelihood function converges to zero; that is

lim

t→∞L(θt)−bt(θt) =0.

One can prove that the gradient of the bound also converges to the gradient of the likelihood function; that is:

Ask Question

missing-data

convergence

expectation-maximization

Cite

Follow





Highest score (default)

Cite

Follow



Expectation Maximization (EM) is a sequential bound optimization technique where in iteration t, we first construct a (lower) bound bt(θ) on the likelihood function L(θ) and then maximize the bound to obtain the new

solution θt =argmaxθbt(θ), and keep doing this until the new solution does not change.

if EM converges to θ∗ then θ∗ is a convergent point for gradient ascent too and EM satisfies any property shared among gradient ascent solutions (including zero gradient value).


CROSS VALIDATED

Tour

Help

Chat

Contact

Feedback

COMPANY

Stack Overflow

Teams

Advertising

Collectives

Talent

About

Press

Legal

Privacy Policy

Terms of Service

Cookie Settings

Cookie Policy

STACK EXCHANGE NETWORK

Technology



CommunityBot

1



Sobi

2,221

14

23

lim

t→∞∇L(θt) = ∇bt(θt).

Because of (1) and (2) and that the bounds used in EM are differentiable, and that θt =argmaxθbt(θ), we have that ∇bt(θt) =0 and, therefore, limt→∞∇L(θt) =0.

Share

Improve this answer

edited Apr 13, 2017 at 12:44

answered Nov 24, 2015 at 19:30

It's confusing to me that "EM converges to a point where grads are 0". Let's use GMM as an example, at M-step, we could take partial derivatives for those params, and set the derivatives to 0, then we can get closed-form

solution for those params. The confusing part is we're setting derivatives to 0 to get solutions, and this looks like a conflict to "EM converges to a point where grads are 0", right? Please help me to understand this, thanks

– avocado

Mar 27, 2021 at 21:06

Your Answer

By clicking “Post Your Answer”, you agree to our terms of service, privacy policy and cookie policy

Not the answer you're looking for? Browse other questions tagged missing-data

convergence

expectation-maximization  or ask your own question.

Linked

0

Can we get a Global Maxima by using EM algorithm?

Related

12

Hidden Markov models and expectation maximization algorithm

27

Why is the expectation maximization algorithm used?

23

Motivation of Expectation Maximization algorithm

6

Issues with using Expectation Maximization algorithm

9

Convergence from the EM Algorithm with bivariate mixture distribution

2

Expectation-Maximization Algorithm for Binomial

4

Details in proof for convergence of Expectation Maximization Algorithm

1

Use Expectation-Maximization algorithm for obtaining maximal likelihood estimator

Hot Network Questions



Is Brownian motion truly random?



Is sawmill necessarily a building?



"Signpost" puzzle from Tatham's collection



What were the most popular text editors for MS-DOS in the 1980s?



Generic Doubly-Linked-Lists C implementation

more hot questions

 Question feed

Cite

Follow



Post Your Answer

Featured on Meta



New blog post from our CEO Prashanth: Community is the future of AI



Improving the copy in the close modal and post notices - 2023 edition


Culture &amp; recreation

Life &amp; arts

Science

Professional

Business

API

Data

Blog

Facebook

Twitter

LinkedIn

Instagram

Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2023.4.21.43403

Your privacy

By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.

 

Accept all cookies

Necessary cookies only

Customize settings

