
The Expectation-Maximization and

Alternating Minimization Algorithms

Shane M. Haas

September 11, 2002

1

Summary

The Expectation-Maximization (EM) algorithm is a hill-climbing approach to

ﬁnding a local maximum of a likelihood function [7, 8].

The EM algorithm

alternates between ﬁnding a greatest lower bound to the likelihood function

(the “E Step”), and then maximizing this bound (the “M Step”).

The EM

algorithm belongs to a broader class of alternating minimization algorithms [6],

which includes the Arimoto-Blahut algorithm for calculating channel capacity

and rate distortion functions [1, 3], and Cover’s portfolio algorithm to maximize

expected log-investment [4].

2

The Expectation-Maximization (EM) Algorithm

The primary purpose of this report is to introduce the EM algorithm and ex-

amine its relationship to other alternating minimization algorithms. For good

tutorials on the EM algorithm, see [10, 2]. Roweis has a good review of linear

Gaussian models, using the EM algorithm to estimate the model parameters

[13]. This paper also provides pseudo-code for many popular EM applications.

The book [9] is a more complete reference on the EM algorithm.

The basic problem of maximum likelihood estimation is to ﬁnd the parameter

θ that maximizes the likelihood function L(θ) ≡ f ( y | θ ) for a given observation

y. Because the logarithm is an increasing function, an equivalent problem is to

ﬁnd the parameter θ that maximizes the log-likelihood, i.e.

ˆθ = argmax

θ

log f ( y | θ ).

(1)

If the likelihood (or log-likelihood) function is suﬃciently well-behaved, then

we can sometimes calculate a closed-form solution for the maximum likelihood

estimator. More often than not, however, a closed form solution does not exist,

and we must ﬁnd the maximum using numerical optimization techniques, such

as gradient-based ascent or the Newton-Raphson method. The EM algorithm

1


is one such hill-climbing algorithm that converges to a local maximum of the

likelihood surface.

As the name suggests, the EM algorithm alternates between an expectation

and a maximization step. The “E step” ﬁnds a lower bound that is equal to

the log-likelihood function at the current parameter estimate θk. The “M step”

generates the next estimate θk+1 as the parameter that maximizes this greatest

lower bound. This alternating process is shown pictorially in Fig. 1.

The EM algorithm, therefore, is a “divide and conquer” approach that breaks

the original optimization problem into two hopefully easier problems. It then

alternates between solving each easier optimization problem, using the solution

of one to solve the other.

����

�

����

��

���������

��

������

������

���

�����

���

�����

�����

������

�����������������������

��

����

��������������������

���������������

Figure 1: The EM algorithm alternates between ﬁnding a greatest lower bound

(“E step”), and maximizing this bound (“M step”).

The set of log-likelihood lower bounds comes from introducing a hidden or

unobserved random variable x that has a joint density with the observation,

q ( x, y | θ ). The likelihood function is then f ( y | θ ) =

�

q ( x, y | θ ) dx. This

hidden variable often has a natural physical meaning such as the hidden state of

a linear dynamical system [14] or hidden Markov model [12], and can simplify

the likelihood expression.

The log-likelihood lower bounds are parameterized by an arbitrary proba-

bility density p ( x ) not necessarily equal to

�

q ( x, y | θ ) dy for this hidden

variable. The lower bounds come from applying Jensen’s inequality:

2


L(θ)

≡

log f ( y | θ )

=

log

�

q ( x, y | θ )dx

=

log

�

p ( x )q ( x, y | θ )

p ( x )

dx

≥

�

p ( x ) log

�q ( x, y | θ )

p ( x )

�

dx

(2)

≡

F [ p, q(θ) ].

As seen in Figure 1, the E step ﬁnds the density p ( x ) that maximizes this

lower bound for the current estimate θk. The M step then ﬁnds the value of

θ that maximizes the lower bound generated by this density. This alternating

process is summarized below:

E Step: pk+1 = argmax

p

F [ p, q(θk) ],

(3)

M Step: θk+1 = argmax

θ

F [ pk+1, q(θ) ].

(4)

2.1

The E Step

We will now focus on ﬁnding the probability density p ( x ) that maximizes the

lower bound F [ p, q(θk) ] while holding θk ﬁxed. But ﬁrst, we will examine three

interpretations of what this lower bound family represents:

Free Energy: One interesting interpretation is that the lower bounds are the

negative of a quantity known in statistical physics as free energy [11].

Deﬁning energy as − log q ( x, y | θ ), the free energy for a given y is the

average energy with respect to p ( x ) minus the entropy of p ( x ), i.e.

−F [ p, q(θ) ] = −E log q ( x, y | θ )

�

��

�

Avg. Energy

− [−E log p ( x )]

�

��

�

Entropy

,

(5)

where the expectations are with respect to p ( x ). The E step, therefore,

chooses p ( x ) to minimize the free energy for the current parameter es-

timate θk. For a ﬁxed p ( x ), the entropy term does not depend on the

parameter θ. Consequently, the M step minimizes that average energy

with respect to the parameter θ, holding constant the density found in

the E step.

KL Divergence: Another interpretation of this lower bound family is in terms

of the Kullback-Leibler (KL) informational divergence between p ( x ) and

q ( x, y | θ ) for a given y, i.e.

F [ p, q(θ) ] = −D[ p ∥ q(θ) ],

(6)

3


where

D[ p ∥ q(θ) ] ≡

�

p ( x ) log

�

p ( x )

q ( x, y | θ )

�

dx.

(7)

Notice that for a ﬁxed y, q ( x, y | θ ) does not integrate to unity over x,

and hence, as deﬁned, this divergence might be negative.

We now have another interpretation of the E step as choosing a conditional

density on x, p ( x ), that minimizes the KL informational divergence be-

tween this density and the joint density of x and y, q ( x, y | θ ), for a ﬁxed

realization of y.

This interpretation will be important when relating the EM algorithm to

other alternating minimization algorithms in [6]. The maximum-likelihood

problem can be viewed as an alternating minimization problem, i.e.

ˆθ = argmin

θ

min

p

D[ p ∥ q(θ) ],

(8)

where the E step performs the minimization over p for a ﬁxed value of θ

and the M step minimizes over θ for a ﬁxed value of p.

KL Divergence: The third interpretation of the lower bound is also in terms

of a divergence, and gives more insight into its relationship to the log-

likelihood and to its maximization. Let w ( x | y, θ ) be the conditional

probability function implied by q ( x, y | θ ), i.e.

w ( x | y, θ ) = q ( x, y | θ )

f ( y | θ ) ,

(9)

where f ( y | θ ) =

�

q ( x, y | θ ) dx. We can then express the lower bound

as

F [ p, q(θ) ]

=

�

p ( x ) log

�w ( x | y, θ )f ( y | θ )

p ( x )

�

dx

=

log f ( y | θ ) − D[ p ∥ w(θ) ].

(10)

Notice that w ( x | y, θ ) does integrate to unity over x, and hence

D[ p ∥ w(θ) ] ≥ 0,

with equality when p ( x ) ≡ w ( x | y, θ ).

We now see that the lower bound is actually the log-likelihood minus the

divergence between the arbitrarily chosen density, p ( x ), and the actual

conditional density, w ( x | y, θ ). Choosing p ( x ) ≡ w ( x | y, θ ), therefore,

maximizes the lower bound for a ﬁxed value of θ. Furthermore, this choice

makes the lower bound equal to the log-likelihood at this particular value

of θ as illustrated in Fig. 1.

4


Based on this third interpretation, the E step is

E Step: pk+1 = argmax

p

F [ p, q(θk) ] = w ( x | y, θk ),

(11)

or using Baye’s Rule

E Step: pk+1 = w ( x | y, θk ) =

h( y | x, θk )π( x | θk )

�

h( y | χ, θk )π( χ | θk )dχ,

(12)

where h( y | x, θ ) = q ( x, y | θ )π( x | θ ) and π( x | θ ) =

�

q ( x, y | θ ) dy. This

step of the EM algorithm is called the expectation step because the lower bound

that it maximizes is expressed in terms of conditional expectations as seen in

(5).

2.2

The M Step

The M step ﬁnds the value of θ that maximizes the greatest lower bound

F [ pk+1, q(θ) ] produced by the E step.

Evaluating the lower bound at this

maximizing distribution, i.e. pk+1(x) = w ( x | y, θk ), results in

F [ pk+1, q(θ) ]

=

�

w ( x | y, θk ) log

� q ( x, y | θ )

w ( x | y, θk )

�

dx

=

�

w ( x | y, θk ) log q ( x, y | θ ) dx

−

�

w ( x | y, θk ) log w ( x | y, θk ) dx.

(13)

Because the second term does not depend on θ, the M step becomes

M Step: θk+1 = argmax

θ

�

w ( x | y, θk ) log q ( x, y | θ ) dx.

(14)

Notice that the M-step maximizes the conditional expectation of the log-joint

observation and the hidden variable density, q ( x, y | θ ). The unknown param-

eter in the conditional density w ( x | y, θ ) is ﬁxed to its previous estimate, θk,

and does not vary in the maximization.

Unlike the E step, the M step is problem speciﬁc. In other words, maximizing

F [ pk+1, q(θ) ] will depend on the structure of the problem under consideration.

3

Example: Mixture Problem

To illustrate the EM algorithm we will examine the mixture problem from the

introductory chapter of [9] and the applications section of [6]. This mixture

problem assumes that the observed data y = {y1, . . . , yN} is generated in the

following manner. For each sample, a group g, 1 ≤ g ≤ G, is randomly cho-

sen with unknown probability πg. The observed sample yn is then randomly

5


generated according to a known probability density, hg(yn), for group g. Fur-

thermore, each sample is generated independently. The maximum-likelihood

problem is to ﬁnd the group probabilities, θ = {π1, . . . , πG−1}, that maximize

the log-likelihood function

L(θ) =

N

�

n=1

log

� G

�

g=1

hg(yn)πg

�

.

(15)

The constraint �G

g=1 πg = 1 is enforced by deﬁning πG = 1 − �G−1

g=1 πg,

and only estimating θ = {π1, . . . , πG−1}. For simplicity, we will not enforce the

constraints that each πg be non-negative. This assumption does not hurt us if

the components of the maximizing parameter are non-negative. Denote the set

of all valid parameters as Θ. Elements of Θ have G − 1 real components whose

sum is less than or equal to one.

Notice that for a given y, this log-likelihood function is concave with respect

to the parameters because it is the sum of concave functions. An example of a

log-likelihood realization is shown in Figure 2 for ﬁve observations (N = 5) and

two groups (G = 2).

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

−6.5

−6

−5.5

−5

−4.5

−4

π1 = Prob( xn = 1 )

Log−Likelihood

Mixture Problem Log−Likelihood

Figure 2: An example of the log-likelihood surface for a mixture of two groups

(G = 2) with ﬁve observations (N = 5)

Even though the log-likelihood is concave, a closed form solution for its

maximum does not exist. We can use the EM algorithm, however, to iteratively

ﬁnd the maximum.

To use the EM algorithm we will introduce the unobserved or hidden variable

xn that takes a value g, if group g produced the sample yn. The joint density

6


of x = {x1, . . . , xN} and y = {y1, . . . , yN} for a given θ is, therefore,

q ( x, y | θ ) =

N

�

n=1

hxn(yn)πxn.

(16)

The EM algorithm proceeds as follows. The E step (11) produces the con-

ditional density (9) based on the current parameter estimate, i.e.

E Step: pk+1 = w ( x | y, θk ) =

N

�

n=1

Pr{ xn | yn, θk},

(17)

where Baye’s rule gives

Pr{ xn = g | yn, θk} =

hg(yn)πk

g

�G

γ=1 hγ(yn)πkγ

≡ mk

n(g),

(18)

and πk

g denotes the estimate of the group g probability from the current param-

eter estimate θk.

The M step (14) then ﬁnds the parameter θ that maximizes

E[ log q ( x, y | θ ) ] =

N

�

n=1

(E[ log hxn(yn)] + E[ log πxn]) ,

(19)

where the expectation is with respect to pk+1(x) = w ( x | y, θk ). We can ignore

the ﬁrst term because it does not depend on θ. The expectation in the second

term is

E[ log πxn]

=

�

x

w ( x | y, θk ) log πxn

=

G

�

g=1

�

x:xn=g

w ( x | y, θk ) log πg

=

G

�

g=1

Pr{ xn = g | yn, θk} log πg

=

G

�

g=1

mk

n(g) log πg.

(20)

This result can also be seen as an iterated expectation,

E[ log πxn] = E[ E[ log πg | xn = g, y, θk] ] = E[ log πg],

where this last expectation is with respect to Pr{ xn = g | yn, θk} ≡ mk

n(g).

Diﬀerentiating E[ log q ( x, y | θ ) ] with respect to πg, gives the necessary

condition for a ﬁxed point:

∂E[ log q ( x, y | θ ) ]

∂πg

=

N

�

n=1

mk

n(g)

πk+1

g

−

N

�

n=1

mk

n(G)

πk+1

G

= 0,

1 ≤ g ≤ G − 1.

(21)

7


This condition implies that

M Step: πk+1

g

=

�N

n=1 mk

n(g)

�N

n=1

�G

g=1 mkn(g)

= 1

N

N

�

n=1

mk

n(g),

(22)

maximizes the log-likelihood conditional expectation.

The EM algorithm for this example has a very appealing interpretation. Had

we observed x, then the maximum likelihood estimate of the group probability

πg would be the number of times xn equals g divided by the total number of

observations. The EM algorithm follows a very similar procedure. The next

estimate of the group probability πk+1

g

is the expected number of times that xn

equals g conditioned on y and the previous estimate θk divided by the total

number of observations, i.e.

πk+1

g

= ( Avg. # of times xn = g given y and θk)/N.

(23)

4

Alternating Minimization

As mentioned previously, the EM algorithm belongs to class of alternating min-

imization procedures that have a nice geometric interpretation. We will now

summarize the main results of [6], and then show their relationship to the EM

algorithm.

Let P and Q be two elements from arbitrary sets P and Q, respectively.

Deﬁne an arbitrary “distance” function1 d(P, Q) that maps elements of P and

Q to the extended real numbers.

We say that the sequences {Pk}∞

k=0 and {Qk}∞

k=0 are obtained by alternating

minimization if for k = 0, 1, 2, . . .

Pk+1 = argmin

P ∈P

d(P, Qk),

(24)

Qk+1 = argmin

Q∈Q

d(Pk+1, Q),

(25)

with the iterations starting at Q0 = argminQ∈Q d(P0, Q), and the starting point

P0 arbitrary. We can describe an alternating minimization sequence using the

notation P0 → Q0 → P1 → Q1 → · · · .

The main theorem (Th. 3) of [6] proves that if P and Q are convex measures

(not necessarily probability measures) and d(P, Q) = D(P∥Q) is the KL infor-

mational divergence, then all alternating minimization divergences converge.

Furthermore, they converge monotonically to a global minimum.

The proof of the theorems in [6] are very general, developing the geomet-

ric properties of P,Q, and d necessary for convergence. The KL informational

divergence happens to satisfy these properties over convex sets of measures. An-

other example that satisﬁes the geometric properties necessary for convergence

is that of closed, convex sets from a Hilbert space with d as the induced norm.

This example of projection onto convex sets is illustrated in Fig. 3.

1The function d is not a true “distance” because it can be negative and asymmetric. Yet,

it is intuitive to still think of it as measuring the “distance” between elements of P and Q.

8


��

��

��

��

��

�

�

Figure 3: Alternating minimization iterates between ﬁnding the minimum of

d(P, Q) holding Q ﬁxed, and the minimum holding P ﬁxed, i.e. P0 → Q0 →

P1 → Q1 → · · · .

5

Relationship Between Alternating Minimiza-

tion Algorithms

We will now examine the relationship between the EM, Arimoto-Blahut, and a

best constant rebalanced portfolio algorithm as alternating minimization pro-

cedures. As mentioned previously, the EM is an alternating minimization al-

gorithm that minimizes D[ p ∥ q(θ) ]. If q(θ) forms a convex set, then the EM

algorithm will converge to a global minimum.

The algorithm to ﬁnd the best constant rebalanced portfolio for a sequence

of stock returns is just the mixture problem with hg(yn) replaced by the return

of stock g at time n, and the group probabilities with the portfolio weights.

This best constant balanced portfolio is the target portfolio in Cover’s universal

portfolio algorithm [5].

The Arimoto-Blahut algorithm to calculate discrete memoryless channel ca-

pacity minimizes D[ p(θ) ∥ q(φ) ], where p(θ) = h( y | x )θ(x) and q(φ) = h( y |

x )φ( x | y ).

Here, θ(x) is the density over the channel inputs, h( y | x ) is

the channel matrix relating inputs x to outputs y, and φ( x | y ) is an arbi-

trary stochastic matrix. Because p(θ) and q(φ) form convex sets parameterized

by θ and φ, respectively, the Arimoto-Blahut algorithm converges to a global

minimum.

9


6

Conclusions

The EM algorithm is an alternating minimization algorithm that iterates be-

tween ﬁnding a greatest lower bound to the log-likelihood function and maxi-

mizing this bound. The EM algorithm converges when the observed and hidden

variable joint density form a convex set over the allowable parameters. Other

alternating minimization algorithms include the Arimoto-Blahut algorithm for

calculating channel capacity and rate-distortion functions, and some optimal

portfolio algorithms.

References

[1] S. Arimoto.

An algorithm for computing the capacity of arbitrary dis-

crete memoryless channels. IEEE Trans. on Inform. Theory, 18(1):14–20,

January 1972.

[2] J.A. Bilmes. A gentle tutorial of the EM algorithm and its applications

to parameter estimation for gaussian mixture and hidden markov mod-

els. Technical Report TR-97-021, International Computer Science Institute,

1947 Center St., Berkeley, CA 94704-1198, April 1998.

[3] R. Blahut. Computation of channel capacity and rate-distortion functions.

IEEE Trans. on Inform. Theory, 18(4):460–473, July 1972.

[4] T.M. Cover. An algorithm for maximizing expected log investment return.

IEEE Transactions on Information Theory, 30(2):369–373, March 1984.

[5] T.M. Cover. Universal portfolios. Mathematical Finance, 1(1):1–29, Jan-

uary 1991.

[6] I. Csiszar and G. Tusnady. Information geometry and alternating minimiza-

tion procedures. Statistics and Decisions, 1:205–237, 1984. Supplement

Issue.

[7] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximimum likelihood from

incomplete data via the EM algorithm. Journal of the Royal Statistical

Society B, 39(1):1–38, 1977.

[8] H. Hartley. Maximum likelihood estimation from incomplete data. Bio-

metrica, 14:174–194, 1958.

[9] G.J. McLachlan and T. Krishnan. The EM algorithm and extensions. Wiley

series in probability and statstics. John Wiley &amp; Sons, 1997.

[10] T.P.

Minka.

Expectation-maximization

as

lower

bound

maximization.

Technical

report,

M.I.T.,

November

1998.

http://www.stat.cmu.edu/ minka/papers/learning.html.

10


[11] R.M. Neal and G.E. Hinton. A view of the EM algorithm that justiﬁes

incremental, sparse, and other variants. In M.I. Jordan, editor, Learning

in Graphical Models, pages 355–368. Kluwer, Dordrecht, MA, 1998.

[12] L.R. Rabiner. A tutorial on hidden Markov models and selected applica-

tions in speech recognition. Proc. of the IEEE, 77(2):257–286, February

1989.

[13] S. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models.

Neural Computation, 11:305–345, 1999.

[14] R.H. Shumway and D.S. Stoﬀer. An approach to time series smoothing

and forecasting using the EM algorithm. Journal of Time Series Analysis,

3(4):253–264, 1982.

11

