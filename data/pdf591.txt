
BART: Denoising Sequence-to-Sequence Pre-training for Natural

Language Generation, Translation, and Comprehension

Mike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad,

Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer

Facebook AI

{mikelewis,yinhanliu,naman}@fb.com

Abstract

We present BART, a denoising autoencoder

for pretraining sequence-to-sequence models.

BART is trained by (1) corrupting text with an

arbitrary noising function, and (2) learning a

model to reconstruct the original text. It uses

a standard Tranformer-based neural machine

translation architecture which, despite its sim-

plicity, can be seen as generalizing BERT (due

to the bidirectional encoder), GPT (with the

left-to-right decoder), and many other more re-

cent pretraining schemes. We evaluate a num-

ber of noising approaches, ﬁnding the best per-

formance by both randomly shufﬂing the or-

der of the original sentences and using a novel

in-ﬁlling scheme, where spans of text are re-

placed with a single mask token.

BART is

particularly effective when ﬁne tuned for text

generation but also works well for compre-

hension tasks. It matches the performance of

RoBERTa with comparable training resources

on GLUE and SQuAD, achieves new state-

of-the-art results on a range of abstractive di-

alogue, question answering, and summariza-

tion tasks, with gains of up to 6 ROUGE.

BART also provides a 1.1 BLEU increase over

a back-translation system for machine transla-

tion, with only target language pretraining. We

also report ablation experiments that replicate

other pretraining schemes within the BART

framework, to better measure which factors

most inﬂuence end-task performance.

1

Introduction

Self-supervised methods have achieved remarkable

success in a wide range of NLP tasks (Mikolov et al.,

2013; Peters et al., 2018; Devlin et al., 2019; Joshi

et al., 2019; Yang et al., 2019; Liu et al., 2019).

The most successful approaches have been variants of

masked language models, which are denoising autoen-

coders that are trained to reconstruct text where a ran-

dom subset of the words has been masked out. Recent

work has shown gains by improving the distribution of

masked tokens (Joshi et al., 2019), the order in which

masked tokens are predicted (Yang et al., 2019), and the

available context for replacing masked tokens (Dong

et al., 2019). However, these methods typically focus

on particular types of end tasks (e.g. span prediction,

generation, etc.), limiting their applicability.

In this paper, we present BART, which pre-trains

a model combining Bidirectional and Auto-Regressive

Transformers. BART is a denoising autoencoder built

with a sequence-to-sequence model that is applicable

to a very wide range of end tasks.

Pretraining has

two stages (1) text is corrupted with an arbitrary nois-

ing function, and (2) a sequence-to-sequence model is

learned to reconstruct the original text. BART uses a

standard Tranformer-based neural machine translation

architecture which, despite its simplicity, can be seen as

generalizing BERT (due to the bidirectional encoder),

GPT (with the left-to-right decoder), and many other

more recent pretraining schemes (see Figure 1).

A key advantage of this setup is the noising ﬂexibil-

ity; arbitrary transformations can be applied to the orig-

inal text, including changing its length. We evaluate

a number of noising approaches, ﬁnding the best per-

formance by both randomly shufﬂing the order of the

original sentences and using a novel in-ﬁlling scheme,

where arbitrary length spans of text (including zero

length) are replaced with a single mask token. This ap-

proach generalizes the original word masking and next

sentence prediction objectives in BERT by forcing the

model to reason more about overall sentence length and

make longer range transformations to the input.

BART is particularly effective when ﬁne tuned for

text generation but also works well for comprehen-

sion tasks. It matches the performance of RoBERTa

(Liu et al., 2019) with comparable training resources

on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar

et al., 2016), and achieves new state-of-the-art results

on a range of abstractive dialogue, question answer-

ing, and summarization tasks.

For example, it im-

proves performance by 6 ROUGE over previous work

on XSum (Narayan et al., 2018).

BART also opens up new ways of thinking about ﬁne

tuning. We present a new scheme for machine transla-

tion where a BART model is stacked above a few ad-

ditional transformer layers. These layers are trained

to essentially translate the foreign language to noised

arXiv:1910.13461v1  [cs.CL]  29 Oct 2019


Bidirectional 

Encoder

A  _  C  _  E 

B       D    

(a) BERT: Random tokens are replaced with masks, and

the document is encoded bidirectionally. Missing tokens

are predicted independently, so BERT cannot easily be

used for generation.

Autoregressive 

Decoder

A  B  C  D  E

&lt;s&gt; A  B  C  D  

(b) GPT: Tokens are predicted auto-regressively, meaning

GPT can be used for generation. However words can only

condition on leftward context, so it cannot learn bidirec-

tional interactions.

Autoregressive 

Decoder

Bidirectional 

Encoder

A  B  C  D  E

A  _  B  _  E      

&lt;s&gt; A  B  C  D  

(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations. Here, a

document has been corrupted by replacing spans of text with mask symbols. The corrupted document (left) is encoded with

a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.

For ﬁne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the ﬁnal

hidden state of the decoder.

Figure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).

English, by propagation through BART, thereby us-

ing BART as a pre-trained target-side language model.

This approach improves performance over a strong

back-translation MT baseline by 1.1 BLEU on the

WMT Romanian-English benchmark.

To better understand these effects, we also report

an ablation analysis that replicates other recently pro-

posed training objectives. This study allows us to care-

fully control for a number of factors, including data

and optimization parameters, which have been shown

to be as important for overall performance as the se-

lection of training objectives (Liu et al., 2019). We ﬁnd

that BART exhibits the most consistently strong perfor-

mance across the full range of tasks we consider.

2

Model

BART is a denoising autoencoder that maps a corrupted

document to the original document it was derived from.

It is implemented as a sequence-to-sequence model

with a bidirectional encoder over corrupted text and a

left-to-right autoregressive decoder. For pre-training,

we optimize the negative log likelihood of the original

document.

2.1

Architecture

BART uses the standard sequence-to-sequence Trans-

former architecture from (Vaswani et al., 2017), ex-

cept, following GPT, that we modify ReLU activa-

tion functions to GeLUs (Hendrycks &amp; Gimpel, 2016)

and initialise parameters from N(0, 0.02).

For our

base model, we use 6 layers in the encoder and de-

coder, and for our large model we use 12 layers in

each. The architecture is closely related to that used in

BERT, with the following differences: (1) each layer of

the decoder additionally performs cross-attention over

the ﬁnal hidden layer of the encoder (as in the trans-

former sequence-to-sequence model); and (2) BERT

uses an additional feed-forward network before word-

prediction, which BART does not. In total, BART con-

tains roughly 10% more parameters than the equiva-

lently sized BERT model.

2.2

Pre-training BART

BART is trained by corrupting documents and then op-

timizing a reconstruction loss—the cross-entropy be-

tween the decoder’s output and the original document.

Unlike existing denoising autoencoders, which are tai-

lored to speciﬁc noising schemes, BART allows us to

apply any type of document corruption. In the extreme

case, where all information about the source is lost,

BART is equivalent to a language model.

We experiment with several previously proposed and

novel transformations, but we believe there is a sig-

niﬁcant potential for development of other new alter-

natives. The transformations we used are summarized

below, and examples are shown in Figure 2.

Token Masking

Following BERT (Devlin et al.,

2019), random tokens are sampled and replaced with

[MASK] elements.

Token Deletion

Random tokens are deleted from the

input. In contrast to token masking, the model must

decide which positions are missing inputs.


A B C . D E .

A . C . E .

A _ . D _ E .

A _C . _ E .

C . D E . A B

Document Rotation

Token Masking

Token Deletion

Text Inﬁlling

D E . A B C .

Sentence Permutation

Figure 2: Transformations for noising the input that we experiment with. These transformations can be composed.

Text Inﬁlling

A number of text spans are sampled,

with span lengths drawn from a Poisson distribution

(λ = 3). Each span is replaced with a single [MASK]

token. 0-length spans correspond to the insertion of

[MASK] tokens.

Text inﬁlling is inspired by Span-

BERT (Joshi et al., 2019), but SpanBERT samples

span lengths from a different (clamped geometric) dis-

tribution, and replaces each span with a sequence of

[MASK] tokens of exactly the same length. Text inﬁll-

ing teaches the model to predict how many tokens are

missing from a span.

Sentence Permutation

A document is divided into

sentences based on full stops, and these sentences are

shufﬂed in a random order.

Document Rotation

A token is chosen uniformly at

random, and the document is rotated so that it begins

with that token. This task trains the model to identify

the start of the document.

3

Fine-tuning BART

The representations produced by BART can be used in

several ways for downstream applications.

3.1

Sequence Classiﬁcation Tasks

For sequence classiﬁcation tasks, the same input is fed

into the encoder and decoder, and the ﬁnal hidden state

of the ﬁnal decoder token is fed into new multi-class

linear classiﬁer. This approach is related to the CLS

token in BERT; however we add the additional token

to the end so that representation for the token in the

decoder can attend to decoder states from the complete

input (Figure 3a).

3.2

Token Classiﬁcation Tasks

For token classiﬁcation tasks, such as answer endpoint

classiﬁcation for SQuAD, we feed the complete doc-

ument into the encoder and decoder, and use the top

hidden state of the decoder as a representation for each

word. This representation is used to classify the token.

3.3

Sequence Generation Tasks

Because BART has an autoregressive decoder, it can be

directly ﬁne tuned for sequence generation tasks such

as abstractive question answering and summarization.

In both of these tasks, information is copied from the

input but manipulated, which is closely related to the

denoising pre-training objective. Here, the encoder in-

put is the input sequence, and the decoder generates

outputs autoregressively.

3.4

Machine Translation

We also explore using BART to improve machine trans-

lation decoders for translating into English. Previous

work Edunov et al. (2019) has shown that models can

be improved by incorporating pre-trained encoders, but

gains from using pre-trained language models in de-

coders have been limited. We show that it is possible

to use the entire BART model (both encoder and de-

coder) as a single pretrained decoder for machine trans-

lation, by adding a new set of encoder parameters that

are learned from bitext (see Figure 3b).

More precisely, we replace BART’s encoder embed-

ding layer with a new randomly initialized encoder.

The model is trained end-to-end, which trains the new

encoder to map foreign words into an input that BART

can de-noise to English. The new encoder can use a

separate vocabulary from the original BART model.

We train the source encoder in two steps, in both

cases backpropagating the cross-entropy loss from the

output of the BART model. In the ﬁrst step, we freeze

most of BART parameters and only update the ran-

domly initialized source encoder, the BART positional

embeddings, and the self-attention input projection ma-

trix of BART’s encoder ﬁrst layer. In the second step,

we train all model parameters for a small number of

iterations.

4

Comparing Pre-training Objectives

BART supports a much wider range of noising schemes

during pre-training than previous work. We compare a

range of options using base-size models (6 encoder and

6 decoder layers, with a hidden size of 768), evaluated

on a representative subset of the tasks we will consider

for the full large scale experiments in §5.

4.1

Comparison Objectives

While many pre-training objectives have been pro-

posed, fair comparisons between these have been dif-

ﬁcult to perform, at least in part due to differences in

training data, training resources, architectural differ-

ences between models, and ﬁne-tuning procedures. We


Pre-trained 

Decoder

Pre-trained 

Encoder

label

A  B  C  D  E 

&lt;s&gt; A  B  C  D  E

(a) To use BART for classiﬁcation problems, the same

input is fed into the encoder and decoder, and the repre-

sentation from the ﬁnal output is used.

Randomly 

Initialized Encoder

 α β γ δ ε

Pre-trained  

Decoder

Pre-trained 

Encoder

A  B  C  D  E

&lt;s&gt; A  B  C  D  

(b) For machine translation, we learn a small additional

encoder that replaces the word embeddings in BART. The

new encoder can use a disjoint vocabulary.

Figure 3: Fine tuning BART for classiﬁcation and translation.

re-implement strong pre-training approaches recently

proposed for discriminative and generation tasks. We

aim, as much as possible, to control for differences un-

related to the pre-training objective. However, we do

make minor changes to the learning rate and usage of

layer normalisation in order to improve performance

(tuning these separately for each objective). For refer-

ence, we compare our implementations with published

numbers from BERT, which was also trained for 1M

steps on a combination of books and Wikipedia data.

We compare the following approaches:

Language Model

Similarly to GPT (Radford et al.,

2018), we train a left-to-right Transformer language

model. This model is equivalent to the BART decoder,

without cross-attention.

Permuted Language Model

Based on XLNet (Yang

et al., 2019), we sample 1/6 of the tokens, and gener-

ate them in a random order autoregressively. For con-

sistency with other models, we do not implement the

relative positional embeddings or attention across seg-

ments from XLNet.

Masked Language Model

Following BERT (Devlin

et al., 2019), we replace 15% of tokens with [MASK]

symbols, and train the model to independently predict

the original tokens.

Multitask Masked Language Model

As in UniLM

(Dong et al., 2019), we train a Masked Language

Model with additional self-attention masks. Self at-

tention masks are chosen randomly in with the follow

proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 un-

masked, and 1/3 with the ﬁrst 50% of tokens unmasked

and a left-to-right mask for the remainder.

Masked Seq-to-Seq

Inspired by MASS (Song et al.,

2019), we mask a span containing 50% of tokens,

and train a sequence to sequence model to predict the

masked tokens.

For the Permuted LM, Masked LM and Multitask

Masked LM, we use two-stream attention (Yang et al.,

2019) to efﬁciently compute likelihoods of the output

part of the sequence (using a diagonal self-attention

mask on the output to predict words left-to-right).

We experiment with (1) treating the task as a stan-

dard sequence-to-sequence problem, where the source

input to the encoder and the target is the decoder out-

put, or (2) adding the source as preﬁx to the target in

the decoder, with a loss only on the target part of the

sequence. We ﬁnd the former works better for BART

models, and the latter for other models.

To most directly compare our models on their ability

to model their ﬁne-tuning objective (the log likelihood

of the human text), we report perplexity in Table 1.

4.2

Tasks

SQuAD

(Rajpurkar et al., 2016)a an extractive ques-

tion answering task on Wikipedia paragraphs. Answers

are text spans extracted from a given document context.

Similar to BERT (Devlin et al., 2019), we use concate-

nated question and context as input to the encoder of

BART, and additionally pass them to the decoder. The

model includes classiﬁers to predict the start and end

indices of each token.

MNLI

(Williams et al., 2017), a bitext classiﬁcation

task to predict whether one sentence entails another.

The ﬁne-tuned model concatenates the two sentences

with appended an EOS token, and passes them to both

the BART encoder and decoder. In contrast to BERT,

the representation of the EOS token is used to classify

the sentences relations.

ELI5

(Fan et al., 2019), a long-form abstractive ques-

tion answering dataset. Models generate answers con-

ditioned on the concatenation of a question and sup-

porting documents.

XSum

(Narayan et al., 2018), a news summarization

dataset with highly abstractive summaries.

ConvAI2

(Dinan et al., 2019), a dialogue response

generation task, conditioned on context and a persona.

CNN/DM

(Hermann et al., 2015), a news summa-

rization dataset. Summaries here are typically closely

related to source sentences.

4.3

Results

Results are shown in Table 1. Several trends are clear:


Model

SQuAD 1.1

MNLI

ELI5

XSum

ConvAI2

CNN/DM

F1

Acc

PPL

PPL

PPL

PPL

BERT Base (Devlin et al., 2019)

88.5

84.3

-

-

-

-

Masked Language Model

90.0

83.5

24.77

7.87

12.59

7.06

Masked Seq2seq

87.0

82.1

23.40

6.80

11.43

6.19

Language Model

76.7

80.1

21.40

7.00

11.51

6.56

Permuted Language Model

89.1

83.7

24.03

7.69

12.23

6.96

Multitask Masked Language Model

89.2

82.4

23.73

7.50

12.39

6.74

BART Base

w/ Token Masking

90.4

84.1

25.05

7.08

11.73

6.10

w/ Token Deletion

90.4

84.1

24.61

6.90

11.46

5.87

w/ Text Inﬁlling

90.8

84.0

24.26

6.61

11.05

5.83

w/ Document Rotation

77.2

75.3

53.69

17.14

19.87

10.59

w/ Sentence Shufﬂing

85.4

81.5

41.87

10.93

16.67

7.89

w/ Text Inﬁlling + Sentence Shufﬂing

90.8

83.8

24.17

6.62

11.12

5.41

Table 1: Comparison of pre-training objectives. All models are of comparable size and are trained for 1M steps

on a combination of books and Wikipedia data. Entries in the bottom two blocks are trained on identical data

using the same code-base, and ﬁne-tuned with the same procedures. Entries in the second block are inspired by

pre-training objectives proposed in previous work, but have been simpliﬁed to focus on evaluation objectives (see

§4.1). Performance varies considerably across tasks, but the BART models with text inﬁlling demonstrate the most

consistently strong performance.

Performance of pre-training methods varies signiﬁ-

cantly across tasks

The effectiveness of pre-training

methods is highly dependent on the task. For exam-

ple, a simple language model achieves the best ELI5

performance, but the worst SQUAD results.

Token masking is crucial

Pre-training objectives

based on rotating documents or permuting sentences

perform poorly in isolation. The successful methods

either use token deletion or masking, or self-attention

masks.

Deletion appears to outperform masking on

generation tasks.

Left-to-right

pre-training

improves

generation

The Masked Language Model and the Permuted

Language Model perform less well than others on

generation, and are the only models we consider that

do not include left-to-right auto-regressive language

modelling during pre-training.

Bidirectional encoders are crucial for SQuAD

As

noted in previous work (Devlin et al., 2019), just

left-to-right decoder performs poorly on SQuAD, be-

cause future context is crucial in classiﬁcation deci-

sions. However, BART achieves similar performance

with only half the number of bidirectional layers.

The pre-training objective is not the only important

factor

Our Permuted Language Model performs less

well than XLNet (Yang et al., 2019). Some of this dif-

ference is likely due to not including other architectural

improvements, such as relative-position embeddings or

segment-level recurrence.

Pure language models perform best on ELI5

The

ELI5 dataset is an outlier, with much higher perplex-

ities than other tasks, and is the only generation task

where other models outperform BART. A pure lan-

guage model performs best, suggesting that BART is

less effective when the output is only loosely con-

strained by the input.

BART achieves the most consistently strong perfor-

mance.

With the exception of ELI5, BART models

using text-inﬁlling perform well on all tasks.

5

Large-scale Pre-training Experiments

Recent work has shown that downstream performance

can dramatically improve when pre-training is scaled

to large batch sizes (Yang et al., 2019; Liu et al., 2019)

and corpora. To test how well BART performs in this

regime, and to create a useful model for downstream

tasks, we trained BART using the same scale as the

RoBERTa model.

5.1

Experimental Setup

We pre-train a large model with 12 layers in each of the

encoder and decoder, and a hidden size of 1024. Fol-

lowing RoBERTa (Liu et al., 2019), we use a batch size

of 8000, and train the model for 500000 steps. Docu-

ments are tokenized with the same byte-pair encoding

as GPT-2 (Radford et al., 2019). Based on the results in

Section §4, we use a combination of text inﬁlling and

sentence permutation. We mask 30% of tokens in each

document, and permute all sentences. Although sen-

tence permutation only shows signiﬁcant additive gains


SQuAD 1.1

SQuAD 2.0

MNLI

SST

QQP

QNLI

STS-B

RTE

MRPC

CoLA

EM/F1

EM/F1

m/mm

Acc

Acc

Acc

Acc

Acc

Acc

Mcc

BERT

84.1/90.9

79.0/81.8

86.6/-

93.2

91.3

92.3

90.0

70.4

88.0

60.6

UniLM

-/-

80.5/83.4

87.0/85.9

94.5

-

92.7

-

70.9

-

61.1

XLNet

89.0/94.5

86.1/88.8

89.8/-

95.6

91.8

93.9

91.8

83.8

89.2

63.6

RoBERTa

88.9/94.6

86.5/89.4

90.2/90.2

96.4

92.2

94.7

92.4

86.6

90.9

68.0

BART

88.8/94.6

86.1/89.2

89.9/90.1

96.6

92.5

94.9

91.2

87.0

90.4

62.8

Table 2: Results for large models on SQuAD and GLUE tasks. BART performs comparably to RoBERTa and

XLNet, suggesting that BART’s uni-directional decoder layers do not reduce performance on discriminative tasks.

CNN/DailyMail

XSum

R1

R2

RL

R1

R2

RL

Lead-3

40.42

17.62

36.67

16.30

1.60

11.95

PTGEN (See et al., 2017)

36.44

15.66

33.42

29.70

9.21

23.24

PTGEN+COV (See et al., 2017)

39.53

17.28

36.38

28.10

8.02

21.72

UniLM

43.33

20.21

40.51

-

-

-

BERTSUMABS (Liu &amp; Lapata, 2019)

41.72

19.39

38.76

38.76

16.33

31.15

BERTSUMEXTABS (Liu &amp; Lapata, 2019)

42.13

19.60

39.18

38.81

16.50

31.27

BART

44.16

21.28

40.90

45.14

22.27

37.25

Table 3: Results on two standard summarization datasets. BART outperforms previous work on summarization on

two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.

on the CNN/DM summarization dataset, we hypothe-

sised that larger pre-trained models may be better able

to learn from this task. To help the model better ﬁt the

data, we disabled dropout for the ﬁnal 10% of training

steps. We use the same pre-training data as Liu et al.

(2019), consisting of 160Gb of news, books, stories,

and web text.

5.2

Discriminative Tasks

Table 2 compares the performance of BART with sev-

eral recent approaches on the well-studied SQuAD and

GLUE tasks (Warstadt et al., 2018; Socher et al., 2013;

Dolan &amp; Brockett, 2005; Agirre et al., 2007; Williams

et al., 2018; Dagan et al., 2006; Levesque et al., 2011).

The most directly comparable baseline is RoBERTa,

which was pre-trained with the same resources, but

a different objective. Overall, BART performs simi-

larly, with only small differences between the models

on most tasks. suggesting that BART’s improvements

on generation tasks do not come at the expense of clas-

siﬁcation performance.

5.3

Generation Tasks

We also experiment with several text generation tasks.

BART is ﬁne-tuned as a standard sequence-to-sequence

model from the input to the output text. During ﬁne-

tuning we use a label smoothed cross entropy loss

(Pereyra et al., 2017), with the smoothing parameter

set to 0.1. During generation, we set beam size as 5,

remove duplicated trigrams in beam search, and tuned

the model with min-len, max-len, length penalty on the

validation set (Fan et al., 2017).

ConvAI2

Valid F1

Valid PPL

Seq2Seq + Attention

16.02

35.07

Best System

19.09

17.51

BART

20.72

11.85

Table 4: BART outperforms previous work on conver-

sational response generation.

Perplexities are renor-

malized based on ofﬁcial tokenizer for ConvAI2.

Summarization

To provide a comparison with the

state-of-the-art in summarization, we present results

on two summarization datasets, CNN/DailyMail and

XSum, which have distinct properties.

Summaries in the CNN/DailyMail tend to resemble

source sentences. Extractive models do well here, and

even the baseline of the ﬁrst-three source sentences is

highly competitive. Nevertheless, BART outperforms

all existing work.

In contrast, XSum is highly abstractive, and extrac-

tive models perform poorly. BART outperforms the

best previous work, which leverages BERT, by roughly

6.0 points on all ROUGE metrics—representing a sig-

niﬁcant advance in performance on this problem. Qual-

itatively, sample quality is high (see §6).

Dialogue

We evaluate dialogue response generation

on CONVAI2 (Dinan et al., 2019), in which agents

must generate responses conditioned on both the pre-

vious context and a textually-speciﬁed persona. BART

outperforms previous work on two automated metrics.


ELI5

R1

R2

RL

Best Extractive

23.5

3.1

17.5

Language Model

27.8

4.7

23.1

Seq2Seq

28.3

5.1

22.8

Seq2Seq Multitask

28.9

5.4

23.1

BART

30.6

6.2

24.3

Table 5:

BART achieves state-of-the-art results on

the challenging ELI5 abstractive question answering

dataset. Comparison models are from Fan et al. (2019).

RO-EN

Baseline

36.80

Fixed BART

36.29

Tuned BART

37.96

Table 6: The performance (BLEU) of baseline and

BART on WMT’16 RO-EN augmented with back-

translation data. BART improves over a strong back-

translation (BT) baseline by using monolingual English

pre-training.

Abstractive QA

We use the recently proposed ELI5

dataset to test the model’s ability to generate long free-

form answers. We ﬁnd BART outperforms the best pre-

vious work by 1.2 ROUGE-L, but the dataset remains

a challenging, because answers are only weakly speci-

ﬁed by the question.

5.4

Translation

We also evaluated performance on WMT16 Romanian-

English,

augmented

with

back-translation

data

from Sennrich et al. (2016).

We use a 6-layer

transformer source encoder to map Romanian into

a representation that BART is able to de-noise into

English, following the approach introduced in §3.4.

Experiment results are presented in Table 6.

We

compare our results against a baseline Transformer

architecture (Vaswani et al., 2017) with Transformer-

large settings (the baseline row).

We show the

performance of both steps of our model in the ﬁxed

BART and tuned BART rows.

For each row we

experiment on the original WMT16 Romanian-English

augmented with back-translation data.

We use a

beam width of 5 and a length penalty of α = 1.

Preliminary results suggested that our approach was

less effective without back-translation data, and prone

to overﬁtting—future work should explore additional

regularization techniques.

6

Qualitative Analysis

BART shows large improvements on summarization

metrics, of up to 6 points over the prior state-of-the-art.

To understand BART’s performance beyond automated

metrics, we analyse its generations qualitatively.

Table 7 shows example summaries generated by

BART. Examples are taken from WikiNews articles

published after the creation of the pre-training corpus,

to eliminate the possibility of the events described be-

ing present in the model’s training data.

Following

Narayan et al. (2018), we remove the ﬁrst sentence of

the article prior to summarizing it, so there is no easy

extractive summary of the document.

Unsurprisingly, model output is ﬂuent and grammat-

ical English. However, model output is also highly ab-

stractive, with few phrases copied from the input. The

output is also generally factually accurate, and inte-

grates supporting evidence from across the input doc-

ument with background knowledge (for example, cor-

rectly completing names, or inferring that PG&amp;E oper-

ates in California). In the ﬁrst example, inferring that

ﬁsh are protecting reefs from global warming requires

non-trivial inference from the text. However, the claim

that the work was published in Science is not supported

by the source.

These samples demonstrate that the BART pretrain-

ing has learned a strong combination of natural lan-

guage understanding and generation.

7

Related Work

Early methods for pretraining were based on language

models. GPT (Radford et al., 2018) only models left-

ward context, which is problematic for some tasks.

ELMo (Peters et al., 2018) concatenates left-only and

right-only representations, but does not pre-train inter-

actions between these features. Radford et al. (2019)

demonstrated that very large language models can act

as unsupervised multitask models.

BERT (Devlin et al., 2019) introduced masked lan-

guage modelling, which allows pre-training to learn in-

teractions between left and right context words. Re-

cent work has shown that very strong performance can

be achieved by training for longer (Liu et al., 2019),

by tying parameters across layers (Lan et al., 2019),

and by masking spans instead of words (Joshi et al.,

2019). Predictions are not made auto-regressively, re-

ducing the effectiveness of BERT for generation tasks.

UniLM (Dong et al., 2019) ﬁne-tunes BERT with an

ensemble of masks, some of which allow only leftward

context. Like BART, this allows UniLM to be used for

both generative and discriminative tasks. A difference

is that UniLM predictions are conditionally indepen-

dent, whereas BART’s are autoregressive. BART re-

duces the mismatch between pre-training and genera-

tion tasks, because the decoder is always trained on un-

corrupted context.

MASS (Song et al., 2019) is perhaps the most similar

model to BART. An input sequence where a contiguous

span of tokens is masked is mapped to a sequence con-

sisting of the missing tokens. MASS is less effective

for discriminative tasks, because disjoint sets of tokens

are fed into the encoder and decoder.

XL-Net (Yang et al., 2019) extends BERT by pre-


Source Document (abbreviated)

BART Summary

The researchers examined three types of coral in reefs off the

coast of Fiji ... The researchers found when ﬁsh were plentiful,

they would eat algae and seaweed off the corals, which appeared

to leave them more resistant to the bacterium Vibrio coralliilyti-

cus, a bacterium associated with bleaching. The researchers sug-

gested the algae, like warming temperatures, might render the

corals’ chemical defenses less effective, and the ﬁsh were pro-

tecting the coral by removing the algae.

Fisheries off the coast of Fiji are protect-

ing coral reefs from the effects of global

warming, according to a study in the jour-

nal Science.

Sacoolas, who has immunity as a diplomat’s wife, was involved

in a trafﬁc collision ... Prime Minister Johnson was questioned

about the case while speaking to the press at a hospital in Wat-

ford. He said, “I hope that Anne Sacoolas will come back ...

if we can’t resolve it then of course I will be raising it myself

personally with the White House.”

Boris Johnson has said he will raise the is-

sue of US diplomat Anne Sacoolas’ diplo-

matic immunity with the White House.

According to Syrian state media, government forces began de-

ploying into previously SDF controlled territory yesterday. ...

On October 6, US President Donald Trump and Turkish Presi-

dent Recep Tayyip Erdoan spoke on the phone. Then both na-

tions issued statements speaking of an imminent incursion into

northeast Syria ... . On Wednesday, Turkey began a military

offensive with airstrikes followed by a ground invasion.

Syrian government forces have entered

territory held by the US-backed Syrian

Democratic Forces (SDF) in response to

Turkey’s incursion into the region.

This is the ﬁrst time anyone has been recorded to run a full

marathon of 42.195 kilometers (approximately 26 miles) under

this pursued landmark time. It was not, however, an ofﬁcially

sanctioned world record, as it was not an ”open race” of the

IAAF. His time was 1 hour 59 minutes 40.2 seconds. Kipchoge

ran in Vienna, Austria. It was an event speciﬁcally designed to

help Kipchoge break the two hour barrier.

Kenyan runner Eliud Kipchoge has run a

marathon in less than two hours.

PG&amp;E stated it scheduled the blackouts in response to forecasts

for high winds amid dry conditions. The aim is to reduce the risk

of wildﬁres. Nearly 800 thousand customers were scheduled to

be affected by the shutoffs which were expected to last through

at least midday tomorrow.

Power has been turned off to millions of

customers in California as part of a power

shutoff plan.

Table 7: Example summaries from the XSum-tuned BART model on WikiNews articles. For clarity, only relevant

excerpts of the source are shown. Summaries combine information from across the article and prior knowledge.

dicting masked tokens auto-regressively in a permuted

order. This objective allows predictions to condition on

both left and right context. In contrast, the BART de-

coder works left-to-right during pre-training, matching

the setting during generation.

Several papers have explored using pre-trained rep-

resentations to improve machine translation.

The

largest improvements have come from pre-training on

both source and target languages (Song et al., 2019;

Lample &amp; Conneau, 2019), but this requires pre-

training on all languages of interest. Other work has

shown that encoders can be improved using pre-trained

representations (Edunov et al., 2019), but gains in de-

coders are more limited. We show how BART can be

used to improve machine translation decoders.

8

Conclusions

We introduced BART, a pre-training approach that

learns to map corrupted documents to the original.

BART achieves similar performance to RoBERTa on

discriminative tasks, while achieving new state-of-the-

art results on a number of text generation tasks. Fu-

ture work should explore new methods for corrupting

documents for pre-training, perhaps tailoring them to

speciﬁc end tasks.


References

Eneko Agirre, Llu’is M‘arquez, and Richard Wicen-

towski (eds.).

Proceedings of the Fourth Interna-

tional Workshop on Semantic Evaluations (SemEval-

2007). Association for Computational Linguistics,

Prague, Czech Republic, June 2007.

Ido Dagan, Oren Glickman, and Bernardo Magnini.

The PASCAL recognising textual entailment chal-

lenge.

In Machine learning challenges. evaluat-

ing predictive uncertainty, visual object classiﬁca-

tion, and recognising tectual entailment, pp. 177–

190. Springer, 2006.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

Kristina Toutanova.

BERT: Pre-training of deep

bidirectional transformers for language understand-

ing. In Proceedings of the 2019 Conference of the

North American Chapter of the Association for Com-

putational Linguistics: Human Language Technolo-

gies, Volume 1 (Long and Short Papers), pp. 4171–

4186, Minneapolis, Minnesota, June 2019. Associa-

tion for Computational Linguistics. doi: 10.18653/

v1/N19-1423.

URL https://www.aclweb.

org/anthology/N19-1423.

Emily Dinan, Varvara Logacheva, Valentin Malykh,

Alexander Miller, Kurt Shuster, Jack Urbanek,

Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan

Lowe,

et al.

The second conversational in-

telligence challenge (convai2).

arXiv preprint

arXiv:1902.00098, 2019.

William B Dolan and Chris Brockett. Automatically

constructing a corpus of sentential paraphrases. In

Proceedings of the International Workshop on Para-

phrasing, 2005.

Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-

aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,

and Hsiao-Wuen Hon. Uniﬁed language model pre-

training for natural language understanding and gen-

eration. arXiv preprint arXiv:1905.03197, 2019.

Sergey Edunov, Alexei Baevski, and Michael Auli.

Pre-trained language model representations for lan-

guage generation. In Proceedings of the 2019 Con-

ference of the North American Chapter of the Asso-

ciation for Computational Linguistics: Human Lan-

guage Technologies, Volume 1 (Long and Short Pa-

pers), 2019.

Angela Fan, David Grangier, and Michael Auli. Con-

trollable abstractive summarization. arXiv preprint

arXiv:1711.05217, 2017.

Angela Fan, Yacine Jernite, Ethan Perez, David

Grangier, Jason Weston, and Michael Auli.

Eli5:

Long form question answering.

arXiv preprint

arXiv:1907.09190, 2019.

Dan Hendrycks and Kevin Gimpel. Gaussian error lin-

ear units (gelus). arXiv preprint arXiv:1606.08415,

2016.

Karl Moritz Hermann,

Tomas Kocisky,

Edward

Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-

leyman, and Phil Blunsom. Teaching machines to

read and comprehend. In Advances in neural infor-

mation processing systems, pp. 1693–1701, 2015.

Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,

Luke Zettlemoyer, and Omer Levy. Spanbert: Im-

proving pre-training by representing and predicting

spans. arXiv preprint arXiv:1907.10529, 2019.

Guillaume Lample and Alexis Conneau.

Cross-

lingual language model pretraining. arXiv preprint

arXiv:1901.07291, 2019.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman,

Kevin Gimpel, Piyush Sharma, and Radu Sori-

cut.

Albert: A lite bert for self-supervised learn-

ing of language representations.

arXiv preprint

arXiv:1909.11942, 2019.

Hector J Levesque, Ernest Davis, and Leora Morgen-

stern. The Winograd schema challenge. In AAAI

Spring Symposium: Logical Formalizations of Com-

monsense Reasoning, volume 46, pp. 47, 2011.

Yang Liu and Mirella Lapata.

Text summariza-

tion with pretrained encoders.

arXiv preprint

arXiv:1908.08345, 2019.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-

dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,

Luke Zettlemoyer, and Veselin Stoyanov. Roberta:

A robustly optimized bert pretraining approach.

arXiv preprint arXiv:1907.11692, 2019.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey

Dean. Efﬁcient estimation of word representations

in vector space.

arXiv preprint arXiv:1301.3781,

2013.

Shashi Narayan, Shay B Cohen, and Mirella Lapata.

Don’t give me the details, just the summary! topic-

aware convolutional neural networks for extreme

summarization.

arXiv preprint arXiv:1808.08745,

2018.

Gabriel Pereyra,

George Tucker,

Jan Chorowski,

Łukasz Kaiser, and Geoffrey Hinton. Regularizing

neural networks by penalizing conﬁdent output dis-

tributions. arXiv preprint arXiv:1701.06548, 2017.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt

Gardner, Christopher Clark, Kenton Lee, and Luke

Zettlemoyer. Deep contextualized word representa-

tions. arXiv preprint arXiv:1802.05365, 2018.

Alec Radford, Karthik Narasimhan, Tim Salimans,

and Ilya Sutskever.

Improving language un-

derstanding by generative pre-training.

URL

https://s3-us-west-2.

amazonaws.

com/openai-

assets/researchcovers/languageunsupervised/language

understanding paper. pdf, 2018.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,

Dario Amodei, and Ilya Sutskever. Language mod-

els are unsupervised multitask learners.

OpenAI

Blog, 1(8), 2019.


Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,

and Percy Liang.

Squad: 100,000+ questions for

machine comprehension of text.

arXiv preprint

arXiv:1606.05250, 2016.

Abigail

See,

Peter

J

Liu,

and

Christopher

D

Manning.

Get to the point:

Summarization

with pointer-generator networks.

arXiv preprint

arXiv:1704.04368, 2017.

Rico Sennrich, Barry Haddow, and Alexandra Birch.

Edinburgh neural machine translation systems for

WMT 16. In Proceedings of the First Conference

on Machine Translation: Volume 2, Shared Task Pa-

pers, 2016.

Richard Socher, Alex Perelygin, Jean Wu, Jason

Chuang, Christopher D Manning, Andrew Ng, and

Christopher Potts.

Recursive deep models for se-

mantic compositionality over a sentiment treebank.

In Proceedings of EMNLP, pp. 1631–1642, 2013.

Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-

Yan Liu. Mass: Masked sequence to sequence pre-

training for language generation.

In International

Conference on Machine Learning, 2019.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob

Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz

Kaiser, and Illia Polosukhin.

Attention is all you

need. In Advances in neural information processing

systems, pp. 5998–6008, 2017.

Alex Wang, Amanpreet Singh, Julian Michael, Felix

Hill, Omer Levy, and Samuel R Bowman.

Glue:

A multi-task benchmark and analysis platform for

natural language understanding.

arXiv preprint

arXiv:1804.07461, 2018.

Alex Warstadt, Amanpreet Singh, and Samuel R.

Bowman. Neural network acceptability judgments.

arXiv preprint 1805.12471, 2018.

Adina Williams, Nikita Nangia, and Samuel R Bow-

man.

A broad-coverage challenge corpus for

sentence understanding through inference.

arXiv

preprint arXiv:1704.05426, 2017.

Adina Williams, Nikita Nangia, and Samuel R. Bow-

man. A broad-coverage challenge corpus for sen-

tence understanding through inference. In Proceed-

ings of NAACL-HLT, 2018.

Zhilin Yang,

Zihang Dai,

Yiming Yang,

Jaime

Carbonell, Ruslan Salakhutdinov, and Quoc V

Le.

Xlnet:

Generalized autoregressive pretrain-

ing for language understanding.

arXiv preprint

arXiv:1906.08237, 2019.

