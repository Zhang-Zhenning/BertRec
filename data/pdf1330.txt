
Dirichlet Distribution, Dirichlet Process and

Dirichlet Process Mixture

Leon Gu

CSD, CMU


Binomial and Multinomial

Binomial distribution: the number of successes in a sequence of

independent yes/no experiments (Bernoulli trials).

P(X = x | n, p) =

� n

x

�

px(1 − p)n−x

Multinomial: suppose that each experiment results in one of k possible

outcomes with probabilities p1, . . . , pk; Multinomial models the

distribution of the histogram vector which indicates how many time each

outcome was observed over N trials of experiments.

P(x1, . . . , xk | n, p1, . . . , pk) =

N!

�k

i=1 xi!

pxi

i ,

�

i

xi = N, xi ≥ 0


Beta Distribution

p(p | α, β) =

1

B(α, β)pα−1(1 − p)β−1

▶ p ∈ [0, 1]: considering p as the parameter of a Binomial distribution,

we can think of Beta is a “distribution over distributions”

(binomials).

▶ Beta function simply deﬁnes binomial coeﬃcient for continuous

variables. (likewise, Gamma function deﬁnes factorial in continuous

domain.)

B(α, β) = Γ(α + β)

Γ(α)Γ(β) ≃

�

α − 1

α + β − 2

�

▶ Beta is the conjugate prior of Binomial.


Dirichlet Distribution

p(P = {pi} | αi) =

�

i Γ(αi)

Γ(�

i αi)

�

i

pαi−1

i

▶ �

i pi = 1, pi ≥ 0

▶ Two parameters: the scale (or concentration) σ = �

i αi, and the

base measure (α′

1, . . . , α′

k), α′

i = αi/σ.

▶ A generalization of Beta:

▶ Beta is a distribution over binomials (in an interval p ∈ [0, 1]);

▶ Dirichlet is a distribution over Multinomials (in the so-called

simplex �

i pi = 1; pi ≥ 0).

▶ Dirichlet is the conjugate prior of multinomial.


Mean and Variance



▶ The base measure determines the mean distribution;

▶ Altering the scale aﬀects the variance.

E(pi) = αi

σ = α′

i

(1)

V ar(pi) = αi(σ − α)

σ2(σ + 1) = α′

i(1 − α′

i)

(σ + 1)

(2)

Cov(pi, pj) =

−αiαj

σ2(σ + 1)

(3)


Another Example



▶ A Dirichlet with small concentration σ favors extreme distributions,

but this prior belief is very weak and is easily overwritten by data.

▶ As σ → ∞, the covariance → 0 and the samples → base measure.


▶ posterior is also a Dirichlet

p(P = {pi} | αi) =

�

i Γ(αi)

Γ(�

i αi)

�

i

pαi−1

i

(4)

P(x1, . . . , xk | n, p1, . . . , pk) =

n!

�k

i=1 xi!

pxi

i

(5)

p({pi}|x1, . . . , xk) =

�

i Γ(αi + xi)

Γ(N + �

i αi)

�

i

pαi+xi−1

i

(6)

▶ marginalizing over parameters (condition on hyper-parameters only)

p(x1, . . . , xk|α1, . . . , αk) =

�

i αxi

i )

σN

▶ prediction (conditional density of new data given previous data)

p(new result = j|x1, . . . , xk, alpha1, . . . , αk) = αj + xj

σ + N


Dirichlet Process

Suppose that we are interested in a simple generative model (monogram)

for English words. If asked “what is the next word in a newly-discovered

work of Shakespeare?”, our model must surely assign non-zero probability

for words that Shakespeare never used before. Our model should also

satisfy a consistency rule called exchangeability: the probability of ﬁnding

a particular word at a given location in the stream of text should be the

same everywhere in thee stream.

Dirichlet process is a model for a stream of symbols that 1) satisﬁes the

exchangeability rule and that 2) allows the vocabulary of symbols to grow

without limit. Suppose that the mode has seen a stream of length F

symbols. We identify each symbol by an unique integer w ∈ [0, ∞) and

Fw is the counts if the symbol. Dirichlet process models

▶ the probability that the next symbol is symbol w is

Fw

F + α

▶ the probability that the next symbol is never seen before is

α

F + α


G ∼ DP(˙|G0, α)



▶ Dirichlet process generalizes Dirichlet distribution.

▶ G is a distribution function in a space of inﬁnite but countable

number of elements.

▶ G0: base measure; α: concentration


▶ pretty much the same as Dirichlet distribution

▶ expectation and variance

▶ the posterior is also a Dirichlet process DP(˙|G0, α)

▶ prediction

▶ integration over G (data conditional on G0 and αonly)

▶

▶ equivalent constructions

▶ Polya Urn Scheme

▶ Chinese Restaurant Process

▶ Stick Breaking Scheme

▶ Gamma Process Construction


Dirichlet Process Mixture



▶ How many clusters?

▶ Which is better?


Graphical Illustrations

Multinomial-Dirichlet Process



Dirichlet Process Mixture






