


Mar 20, 2018

·

18 min read

Hidden Markov Models Simplified

Sanjay Dorairaj

Adapted from HMM/NLP lectures by James Kunz et al @UC Berkeley

Overview

X1, X2 and X3

y1, y2, y3 and y4

the user observes a sequence of

balls y1,y2,y3 and y4 and is attempting to discern the hidden state which is the right

sequence of three urns that these four balls were pulled from.

Figure 1: HMM hidden and observed states

Source: 

Why Hidden, Markov Model?




Figure 2: HMM State Transitions

Intuition behind HMMs

Figure 1

Transition data

Emission data

Initial state information

Figure 2

Figure 3: HMM State Transitions — Weather Example


P(Y) 

Figure 4: HMM — Basic Math (HMM lectures)

n^k

Variations of the Hidden Markov Model — HMM EM

Primer on Dynamic Programming and Summation Rules

Dynamic Programming

cached recursion.

Figure 5: Fibonnacci Series — Tree

Fibonacci Computation without Dynamic Programming

import time

def

if

return

elif

return

else

return

print %-14s %d

print %-14s %.4f


Fibonacci Computation with Dynamic Programming

def

if

return

if

elif

else

if

return

print %-14s %d

print %-14s %.4f

Manipulating Summations

Figure 6: HMM — Manipulation Summations


for in

for in

print

for in

for in

print

Manipulating Maxes

Figure — 7: HMM — Manipulation Maxes


for in

for in

print

print

for in

for in

print \n

print

print

Training an HMM

Figure — 8: HMM — Toy Example — Graph

State transition and emission probabilities

Figure — 9: HMM — Toy Example — Transition Tables

Scoring a known sequence given observed text

Figure — 10: HMM — Toy Example — Graph


Figure — 11: HMM — Toy Example — Scoring Known Sequence

Scoring a set of sequences given some observations

Generate list of unknown sequences

def

def

if

else

for

in

return

Score all possible sequences

def


for

in

for in

if

else

return

from sets import

import pandas as pd

from tabulate import

def

for

in

for in

for in

print

def

return


Compute the best sequence (Viterbi)

Viterbi

Minimum Bayes Risk

Example 1

Figure — 12: HMM — Toy Example — Scoring an Unknown Sequence


print

print

print \n

print \n

print \n

for in

print

%10s

%0.4f

Example 2 — Parts of Speech (POS)Tagging Example


Figure — 13: HMM — Toy Example — Scoring an Unknown Sequence

print

print

print \n

print \n

print \n

for in

print

%-60s

%0.6f

print \n

print



Computing the Minimum Bayes Risk (MBR) Score

for in

if

else

print

for

in

print %-20s %0.8f

for

in

if

print \n

%0.8f \n

%d \n

%s

Optimizing the efficiency of HMM computations using Dynamic Programming

Examining computation complexity as the sequence length increases


import time

if

else

for in

print

%-60s

%0.6f

print \n

print

print \n

%5d

%5d

%0.4f

Dynamic Programming Intuition for HMMs

Intuition behind the Dynamic Programming algorithm for computing MBR scores


Figure — 14: HMM — Dynamic Programming — Finding the MBR Score Source: UC

Berkeley lectures

alpha

Forward/Backward Algorithm for computing the MBR scores


def

if

if

return

if

return

else

for

in

if

if

return

def

if

if

return

if

if

return

for

in

if

if

return

elif

return

else

for

in

if

if

return


Dynamic Programming Performance Improvements

Forward/Backward algorithm for computing MBR score without caching

import time

print

%d

%s

%0.8f

print \n

%0.4f

Forward/Backward algorithm for computing MBR score with caching


import time

print

%d

%s

%0.8f

print \n

%0.4f

Conclusion

8

Machine Learning

NLP

Artificial Intelligence

Machine Intelligence

Tutorial




Follow







