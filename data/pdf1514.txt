
Katz's back-off model

Toggle the table of contents



Article

Talk

Tools From Wikipedia, the free encyclopedia

Katz back-off is a generative n-gram language model that estimates the conditional probability of a word given its history in the n-

gram. It accomplishes this estimation by backing off through progressively shorter history models under certain conditions.[1] By doing

so, the model with the most reliable information about a given history is used to provide the better results.

The model was introduced in 1987 by Slava M. Katz. Prior to that, n-gram language models were constructed by training individual

models for different n-gram orders using maximum likelihood estimation and then interpolating them together.

Method [edit]

The equation for Katz's back-off model is:[2]

where

C(x) = number of times x appears in training

wi = ith word in the given context

Essentially, this means that if the n-gram has been seen more than k times in training, the conditional probability of a word given its

history is proportional to the maximum likelihood estimate of that n-gram. Otherwise, the conditional probability is equal to the back-off

conditional probability of the (n − 1)-gram.

The more difficult part is determining the values for k, d and α.

 is the least important of the parameters. It is usually chosen to be 0. However, empirical testing may find better values for k.

 is typically the amount of discounting found by Good–Turing estimation. In other words, if Good–Turing estimates  as , then 

To compute , it is useful to first define a quantity β, which is the left-over probability mass for the (n − 1)-gram:

Then the back-off weight, α, is computed as follows:

The above formula only applies if there is data for the "(n − 1)-gram". If not, the algorithm skips n-1 entirely and uses the Katz estimate

for n-2. (and so on until an n-gram with data is found)

Discussion [edit]

This model generally works well in practice, but fails in some circumstances. For example, suppose that the bigram "a b" and the

unigram "c" are very common, but the trigram "a b c" is never seen. Since "a b" and "c" are very common, it may be significant (that is,

not due to chance) that "a b c" is never seen. Perhaps it's not allowed by the rules of the grammar. Instead of assigning a more

appropriate value of 0, the method will back off to the bigram and estimate P(c | b), which may be too high.[3]

References [edit]

1. ^ 



"N-gram models"

 (PDF). Cornell.

2. ^ Katz, S. M. (1987). Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE

Transactions on Acoustics, Speech, and Signal Processing, 35(3), 400–401.

3. ^ Manning and Schütze, Foundations of Statistical Natural Language Processing, MIT Press (1999), ISBN 978-0-262-13360-9.

Categories: Language modeling

Statistical natural language processing








Privacy policy About Wikipedia Disclaimers

Contact Wikipedia Mobile view Developers

Statistics

Cookie statement





This page was last edited on 23 January 2023, at 17:04 (UTC).

Text is available under the Creative Commons Attribution-ShareAlike License 3.0; additional terms may apply. By using this site, you agree to the Terms of

Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.



