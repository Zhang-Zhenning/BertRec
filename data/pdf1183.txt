
Paninski, Intro. Math. Stats., December 8, 2005

49

Part III

Estimation theory

We’ve established some solid foundations; now we can get to what is really

the heart of statistics.


Paninski, Intro. Math. Stats., December 8, 2005

50

Point estimation

“Point estimation” refers to the decision problem we were talking about last

class: we observe data Xi drawn i.i.d.

from pθ(x)16, and our goal is to

estimate the parameter θ ∈ Θ from the data. An “estimator” is any decision

rule, that is, any function from the data space X N into the parameter space

Θ. E.g. the sample mean, in the Gaussian case. Or the function that assigns

“2” to every possible observed data sample.

Bias and variance17

There are two important functions associated with any estimator ˆθ that are

useful as a thumbnail sketch of how well the estimator is doing: the “bias”

Bˆθ(θ) =

Eθ(ˆθ−θ) =

� ∞

−∞

...

� ∞

−∞

N

�

i=1

pθ(Xi)

�

ˆθ({X1, X2, ...XN})−θ

� N

�

i=1

dXi = Eθ(ˆθ)−θ

and the variance

Vˆθ(θ) = Vθ(ˆθ)

There is a very useful relationship between the bias, variance, and the

mean-square error (MSE) of any estimator. Using the usual rules for expec-

tations of squares, it’s easy to show that the square error decomposes into a

bias and variance term:

Eθ

�

(θ − ˆθ)2

�

= Bˆθ(θ)2 + Vˆθ(θ)

So the MSE of an estimator can be simply described as a sum of a term

measuring how far oﬀ the estimator is “on average” (not average square) and

a term measuring the variability of the estimator.

16Note that this is a huge assumption, or rather set of assumptions. We assume that

the data is mutually independent — that is, seeing one data point doesn’t aﬀect the other

data points at all — and even more strongly, that the true underlying distribution of

the data happens, rather too conveniently, to some easy-to-analyze family of distributions

pθ(x), where θ is some simple parameter that tells us everything we need to know about

the data. The message is to take all of the following with a grain of salt: this i.i.d. story

is a simple, tractable model of the data — while it’s a very helpful model, as we’ll see, it’s

important to remember that in 99% of cases it’s something of a caricature.

17HMC 4.1


Paninski, Intro. Math. Stats., December 8, 2005

51

Note that both the bias and variance are functions of θ (and are there-

fore usually unknown, although we will see some exceptions to this below);

the bias could be positive for some parameters but negative for others, for

example.

Here’s an example: let xi be i.i.d. coin ﬂips from some coin that has p

probability of coming up heads. We want to estimate p. Then it’s easy to

compute the bias if we take our estimator to be the sample mean number of

heads: we just need to compute

EBN,p(n/N) = p.

Therefore the bias is zero, no matter what p is. The variance is also easy

to compute, if we recall the binomial variance and use our scaling rule for

variance:

VBN,p(n/N) = 1

N p(1 − p).

Here the variance of our estimator depends on the parameter p.


Paninski, Intro. Math. Stats., December 8, 2005

52

Unbiased, minimum-variance estimators

This bias-variance decomposition leads to another possible way to choose

between all possible admissible estimators. (Recall that we discussed two

such principles before, the Bayes — minimum average error — principle and

the minimax — minimum worst-case error.)

Here’s a third: choose the best unbiased estimator. That is, choose an

estimator ˆθ such that

Bˆθ(θ) = 0

∀θ,

and such that the variance is minimized over the class of all such unbiased

estimators. Unbiased estimators are right “on average,” which is not a bad

thing to aim for (although the condition of exactly zero bias turns out to

be pretty strong, ruling out a lot of otherwise good estimators, so it’s much

more questionable whether we should exclude all estimators with any bias

whatsoever).

Exercise 52: Is the sample mean unbiased for Poisson data?

Exercise 53: Provide an unbiased estimator for b if the data is U([0, b]).

Exercise 54: Provide an unbiased estimator for σ2, the variance param-

eter of the Gaussian distribution N(µ, σ2): 1) in the case that µ is known;

2) in the case that µ is unknown.

Note that this unbiasedness condition rules out trivial estimators such

as ˆθ(D) ≡ 2 ∀D, which is nice. In fact, in some situations we’ll see that a

“uniformly minimum-variance unbiased” estimator (UMVUE) exists: such

an estimator satisﬁes

VˆθUMV U(θ) ≤ Vˆθ(θ)

∀θ,

for any unbiased estimator ˆθ; therefore an UMVUE dominates all other un-

biased estimators under the squared-error cost function.

In this case, it

obviously makes sense to use the UMVUE.

One last interesting thing to note: when a UMVUE does exist, it is

automatically unique. Suppose U1 and U2 are both UMVUE, with variance

V (θ), then the average U = (U1 + U2)/2 is also an unbiased estimator. Now

let’s look at the new function W = (U1 − U2)/2. Now

VU + VW = VU1 + VU2

2

= VU1 ≤ VU.

This means VW = 0, i.e., U1 = U2 with probability 1.


Paninski, Intro. Math. Stats., December 8, 2005

53

Exercise 55: Is the sample mean from a Gaussian with known variance

(say, N(µ, 1)) a UMVUE for the mean parameter µ? If not, can you think

of one, or prove that none exists? (Hint: try the case N = 1, then N = 2,

ﬁrst.)

Exercise 56: Is the sample maximum from a uniform distribution U(a, b)

a UMVUE for the maximum parameter b? If not, can you think of one, or

prove that none exists? (Hint: try the case N = 1, then N = 2, ﬁrst.)

Reparameterization

One very important thing to note about the unbiasedness property is that

it is not invariant with respect to reparameterizations. That is, if we relabel

the parameters, the estimator might not remain unbiased. This is, in fact,

one of the strongest arguments that has been raised against the idea of re-

stricting our attention to unbiased estimators. Exercise 57: Is the sample

mean an unbiased estimator of √p, where p is the parameter of a binomial

distribution? What about the square root of the sample mean: is ˆp =

�

n/N

an unbiased estimator of √p?

The other big drawback, as mentioned above, is that unbiased estimators

don’t necessarily exist. Exercise 58: Does an unbiased estimator exist for

log p, where p is the parameter of a binomial distribution? If so, supply one;

if not, prove it.


Paninski, Intro. Math. Stats., December 8, 2005

54

Maximum likelihood estimators (MLE)



Figure 11: Fisher.

The idea of maximum likelihood is perhaps the most important concept

in parametric estimation. It is straightforward to describe (if not always to

implement), it can be applied to any parametric family of distributions (that

is, any class of distributions that is indexed by a ﬁnite set of parameters

θ1, θ2, ..., θd), and it turns out to be optimal in an asymptotic sense we will

develop more fully in a couple lectures.

The basic idea is to choose the parameter ˆθML under which the observed

data, D, was “most likely.” I.e., choose the ˆθML which maximizes the likeli-

hood, pθ(D). Let’s think about this in terms of Bayes’ rule: if we start with

some prior on the parameters, p(θ), then the posterior on the parameters

given the data is

p(θ|D) = 1

Z p(θ)p(D|θ) = 1

Z p(θ)pθ(D),

where Z = p(D) =

�

p(θ)pθ(D)dθ is a constant ensuring the normalization of

the posterior. So if we ignore the prior p(θ) on the right hand side (or assume

p(θ) is roughly constant in θ), then maximizing the likelihood is roughly the

same as maximizing the posterior probability density of θ, given the data.

It’s clear that this can be applied fairly generally. But how do we actu-

ally compute the maximum? Well, we can ask a computer to do it, using a


Paninski, Intro. Math. Stats., December 8, 2005

55

numerical optimization scheme. Sometimes we can ﬁnd the optimum ana-

lytically. For example, assume (as usual) that we have i.i.d. data Xi. This

means that

pθ(D) =

�

i

pθ(Xi).

This suggests that we maximize the log-likelihood instead, since sums are

easier to work with than products:

ˆθML = arg max

θ

pθ(D) = arg max

θ

�

i

pθ(Xi) = arg max

θ

�

i

log pθ(Xi).

Often we can take the gradient of

L(θ) ≡

�

i

log pθ(Xi)

and set it to zero to obtain a local optimum; then we can sometimes addi-

tionally argue that the optimum is unique. Exercise 59: Give a simple con-

dition, in terms of the second derivative of the log-likelihood d2 log pθ(x)/dθ2,

ensuring that the likelihood has a unique global maximum as a function of

θ ∈ Θ.

Here’s an example: let xi be exponential. Then

pθ({xi}) =

�

i

θe−θxi,

so if we set the derivative of the loglikelihood equal to zero, we get

0

=

∂

∂θ

�

N log θ −

N

�

i=1

θxi

� ����

θ=ˆθMLE

=

N

ˆθMLE

−

�

i

xi,

so we have that

ˆθMLE =

�

1

N

�

i

xi

�−1

;

the MLE for θ is just the inverse of the sample mean of xi. This makes sense:

if we see that the sample mean is very close to zero, then it seems likely that

θ is large.


Paninski, Intro. Math. Stats., December 8, 2005

56

Exercise 60: Find the MLE for (µ, σ2) for Gaussian data, given N

i.i.d. data samples xi. Is the MLE biased? If so, compute the bias.

Exercise 61: Find the MLE for (a, b) for uniform U(a, b) data, given N

i.i.d. data samples xi. Is the MLE biased? If so, compute the bias.

Invariance

One more important point about the MLE is that it is invariant with respect

to reparameterizations. That is, if ˆθML is an MLE for θ, then g(ˆθML) is an

MLE for g(θ) whenever g(.) is invertible. For example, the MLE for σ is just

�

ˆσ2

ML. Exercise 62: Prove this.

Regression18

One important application of ML estimation is to regression analysis. Un-

fortunately we don’t have time to go very deeply into this very important

topic; check out W4315 for more information.

The basic model for regression is as follows: we see paired data {Xi, Yi},

and we have reason to believe Xi and Yi are related. In fact, we can hypoth-

esize a model:

Yi = aXi + ei,

where ei is some (unobserved) i.i.d. noise source; i.e., Y is given by aX,

a linearly-scaled version of X, but contaminated by additive noise. Let’s

assume that ei ∼ N(0, σ2). What is the MLE for the parameters (a, σ2)?

Well, ﬁrst we write down the loglikelihood.

L(a, σ2)

=

�

i

log N(0, σ2)(Yi − aXi)

=

�

i

− log(σ

√

2π) − (Yi − aXi)2

2σ2

.

Now if we take the gradient and set it to zero, we get two equations (one for

a and one for σ2:

�

i

Xi(Yi − ˆaMLXi) = 0,

18HMC 12.3


Paninski, Intro. Math. Stats., December 8, 2005

57

and

�

i

− 1

σ2 + (Yi − aXi)2

(σ2)2

= 0.

So

ˆaML =

�

i XiYi

�

i XiXi

and

ˆσ2

ML =

�N

i=1(Yi − aXi)2

N

,

both of which have a fairly intuitive interpretation.

Again, check out W4315 to learn more about what to do when X is multi-

dimensional, when more complicated (nonlinear) relationships hold between

X and Y , when ei is not normal or even i.i.d., etc.

Robustness

One very important point is that the MLE depends strongly on the para-

metric family chosen. For example, if your data is actually Cauchy, but you

apply the MLE assuming Gaussian data, then you’re not going to do very

well. (Exercise 63: Why?) This is an extreme case, but a lot of work on

the “robustness” of the MLE indicates that things can go fairly badly wrong

even when your data is “mostly” Gaussian (e.g., when data are drawn from

a “mixture” distribution

�

aipi(x),

where the mixture weights ai are positive and sum to one; think e.g. of a

Gaussian distribution for p1 mixed with some occasional “outliers,” a2 &lt; a1,

with p2 having heavier tails than p1). Since, of course, we don’t know a priori

what distribution our data is drawn from, this is a bit of a problem. If there’s

time at the end of the semester after developing the basic theory, we’ll return

to this robustness question. If not, of course, feel free to look up this topic

on your own; see 12.1-12.2 in HMC for a start.


Paninski, Intro. Math. Stats., December 8, 2005

58

Suﬃciency19

Let’s look more closely at this likelihood idea. One thing that we saw was

that not every bit of the data really mattered to our estimate. For example,

if we have i.i.d. data, it doesn’t really matter what order the data appeared

in. So we can throw out the order of the data and do inference given the

unordered data just as well.

Similarly, se saw that we didn’t need to remember everything about Gaus-

sian data, just the sample mean and sample variance (or equivalently, the

sample mean and sample mean square, since we can derive one from the

other), since the likelihood depends on the data only through these statis-

tics:

L(µ, σ2)

=

�

i

log N(µ, σ2)(Xi)

=

�

i

− log(σ

√

2π) − (Xi − µ)2

2σ2

=

−N log(σ

√

2π) −

1

2σ2

��

i

X2

i − 2µ

�

i

Xi + Nµ2

�

.

This is quite a savings: we’ve compressed N data points into just two.

This turns out to be a pretty common phenomenon, if we think about

it. We’re led to a deﬁnition: Any function T(D) of the data D (and only

of the data D) is called a “statistic.” A statistic is called suﬃcient for the

parameter θ if we can split the data into two parts: 1) the suﬃcient statistic,

and 2) the aspects of the data that have nothing to do with estimating θ.

More mathematically,

pθ(D) = F(θ, T(D))G(D),

for some functions F(.) and G(.). This is equivalent (although we’ll skip the

proof) to saying that the conditional distribution of the data D, given T(D),

does not depend on θ at all. That is, the function

pθ(D|T) = pθ(T(D)|D)pθ(D)

pθ(T(D))

=

pθ(D)

pθ(T(D))

19HMC 7.


Paninski, Intro. Math. Stats., December 8, 2005

59

does not depend on θ.

(Yet another way of saying this: θ — T — D is a “Markov chain”: D is

conditionally independent of θ given T, for any prior distribution on θ, i.e.

p(θ, D|T) = p(θ|T)p(D|T).)

Here are some more examples:

• Binomial data: if xi = 1 or 0 depending on if the i-th i.i.d. coin ﬂip

came up heads or tails, then

pθ({xi}) =

� N

�

i xi

�

p

P

i xi(1 − p)N−P

i xi;

from this we can easily see that n = �

i xi is suﬃcient.

• If we have N i.i.d. Poisson observations, then

pθ({xi}) =

�

i

e−θ θxi

xi! = e−Nθ θ

P

i xi

�

i(xi!),

and once again �

i xi is suﬃcient.

• For uniform U[(0, θ]) data,

pθ({xi}) =

�

i

1[θ≥xi]

1

θ = 1

θN 1[θ≥maxi xi],

i.e., maxi xi is suﬃcient.

• For uniform U[(θ, θ + 1]) data,

pθ({xi}) = 1[θ≤mini xi]1[θ+1≥maxi xi]

i.e., the pair (mini xi, maxi xi) is suﬃcient (even though there is only

one parameter θ).

Exercise 64: What is a suﬃcient statistic for exponential data, xi ∼

exp(λ)?

Exercise 65: What is a suﬃcient statistic for uniform data, xi ∼ U([a, b])?

Exercise 66: What is a suﬃcient statistic for Gaussian data, xi ∼

N(0, θ)? (I.e., the mean is known but the variance is not.)

A couple things to note:


Paninski, Intro. Math. Stats., December 8, 2005

60

• The MLE can only depend on the data through suﬃcient statistics,

since

arg max

θ

pθ(D) = arg max

θ

F(θ, T(D))G(D) = arg max

θ

F(θ, T(D)).

Exercise 67: Prove the following statement (if true), or (if false) give

a counterexample and salvage the statement if possible. “If the MLE

is unique, it must necessarily be a function of any suﬃcient statistic.”

• For similar reasons, Bayes estimators only depend on the data through

suﬃcient statistics. Exercise 68: Show this using Bayes’ rule.

• Suﬃciency is only deﬁned in the context of a parametric family. That

is, a statistic may be suﬃcient for one parametric family but not for

another one. Exercise 69: Give an example of this.

• any invertible function (relabeling) of a suﬃcient statistic is itself suf-

ﬁcient. (Hence suﬃcient statistics are very nonunique.) Exercise 70:

Prove this using the factorization deﬁnition of suﬃciency.

Minimal suﬃciency

This last point leads to another important concept. A suﬃcient statistic is

minimal if it can be written as a function of every other conceivable suﬃcient

statistic. In a sense, minimal statistics have all the redundancy compressed

out — there’s nothing irrelevant left to throw out.

In a sense, anything that doesn’t change the likelihood can be thrown

out, as the following simpliﬁcation of a theorem (which we won’t prove) by

Lehmann-Scheﬀ´e shows:

Theorem 3. T is minimal suﬃcient if and only if the following two state-

ments are equivalent:

1. For any data samples D and D′,

pθ(D)

pθ(D′) is constant in θ

2. T(D) = T(D′).


Paninski, Intro. Math. Stats., December 8, 2005

61

This condition is often much easier to check than whether a given statistic

is a function of every other suﬃcient statistic.

Example: xi ∼ exp(θ). Then

pθ(D) =

N

�

i=1

θ exp(−θxi) = θN exp(−θ

N

�

i=1

xi)

for xi &gt; 0 ∀i. Clearly � xi is suﬃcient; is it minimal? Let’s apply the above

theorem: choose another arbitrary data sample, D′ = {x′

1, x′

2, ..., x′

N}. Now

we can see that

pθ(D)

pθ(D′) = exp

�

θ(

�

x′

i −

�

xi)

�

is constant as a function of θ if and only if � xi = � x′

i. Thus � xi is a

minimal suﬃcient statistic; this was much easier to check than whether � xi

was a function of all other possible suﬃcient stats! Conversely, let’s look at

a suﬃcient statistic which is not minimal, namely the full data D. Here it’s

clear that 2 =⇒ 1 but 1 does not imply 2; hence, the full data D is not

minimal.

Here’s another one. Recall that (mini xi, maxi xi) was suﬃcient for uni-

form U[(θ, θ + 1]) data. It’s clear from this theorem that this statistic is also

minimal.

We’ll see more examples in a moment, when we discuss exponential fam-

ilies.

Exercise 71: Let x ∼ N(0, σ2) (i.e., the mean is known but the variance

is not). Is |x| suﬃcient for σ2? If so, is it minimal?

Exercise 72: Let xi be drawn i.i.d. from a density in a “location family”;

that is, we know f(x) and we know pθ(x) = f(x − θ), we just want to know

θ. Can you come up with a minimal suﬃcient statistic for θ (and, of course,

prove that this statistic is minimal suﬃcient)? (Hint: the order statistics

might be useful here, as it’s intuitively clear that we don’t need to remember

what order the data actually came in.)

Rao-Blackwell theorem20

Not only does restricting our attention to suﬃcient statistics make life easier;

it also improves (or at least can’t hurt) our estimation accuracy:

20HMC 7.3


Paninski, Intro. Math. Stats., December 8, 2005

62





Figure 12: Rao and Blackwell.

Theorem 4 (Rao-Blackwell).

E[g(E(ˆθ|T) − θ)] ≤ E[g(ˆθ − θ)]

for any estimator ˆθ, convex error function g, and suﬃcient statistic T. In

particular,

E[(E(ˆθ|T) − θ)2] ≤ E[(ˆθ − θ)2]

In words, take any estimator ˆθ. Then form the estimator E(ˆθ|T) (note

that E(ˆθ|T) is a bona ﬁde statistic — that is, doesn’t depend on θ — if T is

suﬃcient). The theorem says that the risk of E(ˆθ|T) is never worse (and in

practice, often better) than that of the original estimator ˆθ, as long as the

loss function g is convex. Also note that E(ˆθ|T) is unbiased whenever ˆθ is

(Exercise 73: Why?).

We’ll prove the special (g(u) = u2) case to give a sense of what’s going

on here. Just write out E[(ˆθ − θ)2]:

E[(ˆθ − θ)2]

=

ET

�

E[(ˆθ − θ)2|T]

�

=

ET[(E(ˆθ|T) − θ)2] + ET

�

E[(ˆθ − E(ˆθ|T))2]

�

≥

ET[(E(ˆθ|T) − θ)2].


Paninski, Intro. Math. Stats., December 8, 2005

63

Exercise 74: Prove the general case using Jensen’s inequality and the

rules for combining conditional expectations.

It’s worth noting that a very similar proof establishes the important fact

we’ve used a couple times now, that the optimal Bayesian estimator under

square loss is the conditional expectation of θ given the data. I’ll leave the

proof as an exercise.

Also, the proof doesn’t seem to rely directly on suﬃciency — the inequal-

ities above hold for any statistic T, not just for suﬃcient T. The point to

remember, again, is that if T is not suﬃcient then E(ˆθ|T) is not guaranteed

to even be a valid statistic.

Exponential families21

Let’s talk about a class of statistical families whose suﬃcient statistics are

very easy to describe. We say a parametric family is an “exponential family”

if

pθ(x) =







exp

�

f(θ)k(x) + s(x) + g(θ)

�

if a &lt; x &lt; b

0

otherwise,

for some −∞ ≤ a, b ≤ ∞ (note that a and b don’t depend on θ).

An example: x ∼ exp(θ).

pθ(x) = exp([−θx] + [0] + [log θ]),

from which we can read oﬀ f(θ), k(x), s(x), and g(θ).

Another example: N(θ, σ2) (σ2 known). After a little manipulation, we

can write

pθ(x) = exp

�

[−θ

σ2 x] + [−x2

2σ2 − 1

2 log(2πσ2)] + [−θ2

2σ2 ]

�

.

It’s pretty easy to deﬁne a minimal suﬃcient statistic here: if we write

pθ(x) ∼ exp

�

f(θ)k(x)

�

exp

�

s(x)

�

and recall the suﬃcient statistic factorization, then k(x) is a good candidate.

21HMC 7.5.


Paninski, Intro. Math. Stats., December 8, 2005

64

Now for the really useful part: if we look at multiple i.i.d. samples xi,

then it’s easy to see that �

i k(xi) is minimal suﬃcient for the full data {xi}.

(Exercise 75: Prove this.) This saves a whole lot of work — to come up

with a minimal s.s. (and therefore come up with an improved estimator,

according to Rao-Blackwell), all we need to do is manipulate pθ(x) into the

above form. We’ll get some more practice with this in a moment.

More generally, sometimes we need more than one statistic to adequately

describe the data. In this case, we can deﬁne a k-dimensional exponential

family as a parametric family satisfying

pθ(x) =







exp

� �k

j=1 fj(θ)kj(x) + s(x) + g(θ)

�

if a &lt; x &lt; b

0

otherwise.

Here, {kj(x)}1≤j≤k are minimal suﬃcient together (but not alone).

These concepts give us a canonical parameterization of our parametric

family: fj(θ) (we call {fj(θ)}1≤j≤k the “canonical parameter”), and the nat-

ural parameter space is the set of all θ for which the above form makes sense,

that is, the set of θ such that g(θ), as deﬁned above, is ﬁnite.

Exercise 76:

Write out f(θ), k(x), s(x), and g(θ) for 1) N(µ, θ) (µ

known), 2) B(N, θ), and 3) Poiss(θ). Write out fj(θ), kj(x), s(x), and g(θ)

for the normal family in the case that both µ and σ2 are unknown.

Now to make life simpler assume we’re dealing with the canonical param-

eterization, i.e. f(θ) = θ. Let’s look more closely at g(θ). First, there is

some redundancy here: we know, since

�

pθ(x)dx = 1, that

g(θ) = − log

� �

exp[θk(x) + s(x)]dx

�

.

We can go a little further if we remember some of our facts about moment-

generating functions and recognize that an mgf is hiding in the above deﬁ-

nitions. Now, remember that taking derivatives of mgf’s kicks out moments

(hence the name). In this case, we have

∂g

∂θ = −Eθk(x).


Paninski, Intro. Math. Stats., December 8, 2005

65

This is because

−∂g

∂θ

=

∂

∂θ log

� �

exp[θk(x) + s(x)]dx

�

=

�

k(x) exp[θk(x) + s(x)]dx

exp[−g(θ)]

=

�

pθ(x)k(x)dx = Eθk(x).

Exercise 77: Derive a relationship between g(θ), Eθk(x), and the MLE

by taking the derivative of the loglikelihood and setting it to zero.

We can use similar techniques to show that

∂2g

∂θ2 = −Vθk(x).

This, in turn, proves that the log-likelihood log pθ(x) is a concave function

of θ whenever θ is the canonical parameter of an exponential family, which

you’ll recall is quite handy in the context of ML estimation. Exercise 78:

Prove the above formula, and use this to establish the concavity of the log-

likelihood in the canonical exponential family setting.

Of course, exponential families are a special case; life isn’t always so easy.

Exercise 79: Try writing U(a, b) in the exponential form. What goes wrong?

(Hint: don’t forget to keep track of the support of U(a, b).)

It’s interesting to note (though we won’t pursue this) that exponential

families are the only ones for which a ﬁnite-dimensional suﬃcient statistic ex-

ists for all sample sizes N. This is called the “Koopman - Darmois” theorem,

if you want to read more about it.

Exercise 80: Give a minimal suﬃcient statistic for Cauchy data.

Completeness and uniqueness (time permitting)22

“Completeness” of a statistic, in the context of a given probability family, is

a property that guarantees the uniqueness of the unbiased estimator which

may be written as a function of a suﬃcient statistic; this estimator is then

automatically the UMVUE.

22HMC 7.4.


Paninski, Intro. Math. Stats., December 8, 2005

66

We call the statistic U(x) “complete” if23

Eθ(g(U)) = 0 ∀θ =⇒ g(U) = 0.

Exercise 81: Prove that the completeness of a suﬃcient statistic U, as

deﬁned above, guarantees that if φ(U) is an unbiased estimator for θ, then

φ(U) is the UMVUE. (Hint: think about what the completeness condition

says about the diﬀerence between φ(U) and any other unbiased estimator that

is a function of U. Then think about Rao-Blackwell, and the uniqueness of

UMVUEs.)

Exercise 82: Is the natural suﬃcient statistic in an exponential family

complete?

23The term “complete” is inherited from functional analysis (or, in the case of discrete

data, linear algebra): U(x) is complete if pθ(U) is complete in L2(U), the space of square-

integrable functions on the range of U(x), U. If you’ve taken linear algebra, just think of

functions of U as vectors — you can add them and multiply them by scalars to get new

functions of U, just like ordinary vectors. Now the completeness condition just says that

pθ(U) span the set of all functions of U: if any function is orthogonal to all pθ(U) (where

we interpret Eθg(U) =

�

pθ(u)g(u)du as a dot product), then the function must be zero.


Paninski, Intro. Math. Stats., December 8, 2005

67

Asymptotic ideas

Now we turn to the asymptotic properties of our estimators. We’ll discuss

two questions in particular:

1. Does our estimator work properly asymptotically?

That is, does it

provide us with the correct answer if we give it enough data?

2. How asymptotically eﬃcient is our estimator? For example, can we

come up with an estimator which is at least as good as any other

estimator, in some asymptotic sense?

Consistency24

We say an estimator is consistent (in probability) if it provides us with the

correct answer asymptotically. That is,

ˆθ →P θ.

(More precisely, we’re talking about convergence of a sequence of estimators,

one for each N, i.e.,

ˆθN →P θ.

But usually we’ll suppress this extra notation.)

How can we establish that an estimator is consistent? Well, the easiest

thing to do is to establish that the estimator is asymptotically unbiased,

B(θ, N) →N→∞ 0,

and that the variance goes to zero,

V (θ, N) →N→∞ 0;

then we can just apply our bias-variance decomposition and Chebysheﬀ’s

inequality, and we’re done.

Exercise 83: Use this method to develop a simple consistent estimator

of the binomial parameter p.

It’s worth noting that it’s possible to come up with examples in which

the estimator is consistent but either the bias or the variance does not tend

24HMC p. 206.


Paninski, Intro. Math. Stats., December 8, 2005

68

to zero; in other words, for consistency it is suﬃcient but not necessary that

the bias and variance both tend to 0. For example, an estimator might have

very fat tails, such that the variance is inﬁnite for any N, but nonetheless

most of the mass of p(ˆθN) becomes concentrated around the true θ. I’ll leave

it to you to work out the details of such an example.

Method of moments

One way to generate consistent estimators is to ﬁnd a function U(x) of a

single observation Xi such that Eθ(U(x)) = f(θ), where f(θ) is chosen to

be a one-to-one function with a continuous inverse. Then we can use our

results about convergence in probability of continuous functions to see that

the estimator

ˆθMM = f −1

�

1

N

N

�

i=1

U(xi)

�

is consistent for θ. (To see this, note that

1

N

N

�

i=1

U(xi) →P EθU(x) = f(θ),

by the law of large numbers and the deﬁnition of f(θ), then apply f −1 to both

sides and use what we’ve learned about continuous mappings and convergence

in probability.)

In the case of multiple parameters, we would solve this equation simul-

taneously for several Uj. When Uj are taken to be the ﬁrst j moments of

X (i.e., we choose ˆθ to match the observed moments to the true moments

as a function of θ), this estimation technique is known as the “method of

moments.”

Here’s an example. Let xi ∼ exp(θ). Now let U(x) = x. Then

Eθ(U(x)) = f(θ) = 1/θ.

Therefore

ˆθMM =

�

1

N

N

�

i=1

U(xi)

�−1

=

�

1

N

N

�

i=1

xi

�−1

is consistent for θ.


Paninski, Intro. Math. Stats., December 8, 2005

69

We saw another example of this kind of moment-matching estimator re-

cently in the homework: in an exponential family with the canonical param-

eterization, we saw that

EˆθMLE(U(x)) = 1

N

N

�

i=1

U(xi),

where U(x) = k(x) is the minimal suﬃcient statistic of the exponential fam-

ily. Thus in this special case (but not in general!) the MLE is exactly the

method of moments estimator.

Exercise 84: Develop the “method of moments” estimator for λ, the

parameter of the Poisson distribution, using a) U(x) = x and b) U(x) = x2.

Are these estimators consistent? How are these estimators related to the

MLE?

Exercise 85: Develop the “method of moments” estimator for (µ, σ2),

the parameters of the Gaussian distribution. Is this estimator consistent?

How is this estimator related to the MLE?

Exercise 86: Assume U(x) has some ﬁnite variance, Vθ(U(x)), and that

f −1 is continuously diﬀerentiable, with strictly nonzero derivative. Use the

central limit theorem and the delta method to derive the asymptotic distri-

bution of ˆθMM.

This type of estimator, constructed by ﬁnding the solutions of some

equations that the parameter estimate must satisfy, is often called a “Z-

estimator,” because of the special case of the MLE, when we set the gradient

of the likelihood equal to zero (hence the “Z”) and solve the resulting equa-

tions. We’ll look at some more examples in the next section.

Convergence rates and asymptotic normality

Just as we discussed the CLT as a “ﬁner” result than the LLN, we’d like

to know more about an estimator than just the fact that it converges in

probability. For example, we’d like to know the convergence rate — how

quickly it converges to θ, for example the N −1/2 rate we saw when adding

together i.i.d. r.v.’s — and once the rate is established, what the asymptotic

distribution is on the scale deﬁned by the convergence rate. For example,

can we prove asymptotic normality on the N −1/2 scale,

N 1/2(ˆθ − θ) →D N(0, σ2)?


Paninski, Intro. Math. Stats., December 8, 2005

70

And ﬁnally, what is the variance σ2 of this asymptotic rescaled distribution?

We’ll address these questions for the MLE in the next section.

Conﬁdence intervals25

Before we get too deeply into the question of how to prove these kinds of

results, let’s step back a moment and think about what we’re going to do

with this kind of asymptotic approximation. Probably the most important

application of this idea is in the construction of conﬁdence intervals — “error

bars.”

Let’s imagine we know that

√

N(ˆθ − θ) →D N(0, σ2(θ)),

for some estimator ˆθ.

That is, ˆθ is asymptotically unbiased and normal

about the true parameter θ (for now assume we know the asymptotic vari-

ance coeﬃcient σ2(θ), even though we don’t know θ). How can we use this

information? Well, by deﬁnition of asymptotic normality, we know that

P

�

−2 &lt;

√

N(ˆθ − θ)

σ(θ)

&lt; 2

�

≈ 0.95;

thus

P

�

ˆθ − 2σ(θ)

√

N

&lt; θ &lt; ˆθ + 2σ(θ)

√

N

�

≈ 0.95.

So we’ve gone from just making a guess about θ to something stronger: we’ve

bracketed θ in a set, (ˆθ − 2σ(θ)

√

N , ˆθ + 2σ(θ)

√

N ), into which θ falls with about 95%

probability, assuming N is suﬃciently large (it’s essential to remember — and

unfortunately easy to forget — that this argument only holds asymptotically

as N → ∞). In other words, we’ve given an approximate “95% conﬁdence

interval” for θ.

We left one little problem: how do we get σ(θ) without knowing θ? Well,

the coeﬃcient σ(θ) is a function of the parameter θ. If we can estimate θ,

then we can also estimate a function of θ. So we estimate σ(θ): we know

from the continuity properties of stochastic convergence theory that if an

estimator ˆσ is consistent for σ(θ), and recall we’re already assuming that

√

N(ˆθ − θ) →D N(0, σ2(θ)),

25HMC 5.4


Paninski, Intro. Math. Stats., December 8, 2005

71

then26

√

N

� ˆθ − θ

ˆσ

�

→D N(0, 1).

Note, importantly, that the (unknown) parameter θ no longer appears on the

right hand side. Now, applying the same logic as above, (ˆθ − 2ˆσ/

√

N, ˆθ +

2ˆσ/

√

N) is an approximate 95% conﬁdence interval for θ, and importantly,

we don’t need to know θ to construct this interval.

Exercise 87: Generalize this analysis to get a 99% conﬁdence interval

(instead of 95%). What about the (1 − α) · 100% conﬁdence interval, where

0 &lt; α &lt; 1 is some arbitrary (ﬁxed) error tolerance?

Let’s look at a simple example. Let n ∼ Bin(N, p). Then ˆpMLE = n/N.

We know from the standard CLT and the formula for the variance of n that

√

N (ˆpMLE − p) →D N(0, σ2(p)),

where

σ2(p) = p(1 − p).

A simple estimator for σ is

ˆσ =

�

σ2(ˆpMLE) =

�

ˆpMLE(1 − ˆpMLE);

we can prove that this estimator is consistent by our usual delta method

arguments. Thus

√

N

� ˆpMLE − p

ˆσ

�

→D N(0, 1),

and (ˆpMLE − 2ˆσ/

√

N, ˆpMLE + 2ˆσ/

√

N) is an approximate 95% conﬁdence

interval for p.

26If you’re reading along in HMC, be careful — the corresponding equation on page 255

(the equation just before eq. 5.4.4) is incorrect. Do you see why?


Paninski, Intro. Math. Stats., December 8, 2005

72

MLE asymptotics27

Finally we get to the discussion of the asymptotic properties of the maximum

likelihood estimator. As we said long ago, when we ﬁrst introduced the idea

of ML, really the best justiﬁcation for the MLE is in its asymptotic properties:

it turns out to be asymptotically optimal in a sense we will deﬁne below.

Consistency: identiﬁability and the Kullback-Leibler di-

vergence

Before we talk about the asymptotic optimality of the MLE, though, let’s

ask a more basic question: is the MLE even consistent in general?

The answer is (generally speaking) yes, if the parameters are “identi-

ﬁable,” that is, if the distribution of data under any parameter θ in our

parameter space Θ diﬀers from the distribution of the data under any other

parameter, θ′. That is,

pθ(D) ̸= pθ′(D), ∀ D.

This condition makes intuitive sense — if two parameters, say θ1 and θ2

— were not identiﬁable, of course we wouldn’t be able to distinguish them

based on their likelihoods (because the likelihoods would be equal), and so

the MLE is doomed to be inconsistent.

The interesting thing is that this simple identiﬁability condition is enough

to guarantee consistency in most cases, if the data are i.i.d. Let’s write out

the likelihood and try to manipulate it into a form we can deal with.

log pθ(x1, x2, ...xN) =

N

�

i=1

log pθ(xi).

Let’s say the true parameter is θ0. We don’t know θ0, of course (otherwise we

wouldn’t need to estimate it), but subtracting oﬀ its (unknown) loglikelihood

and dividing by N won’t change the location of the MLE:

ˆθML = arg max

θ

N

�

i=1

log pθ(xi) = arg max

θ

1

N

N

�

i=1

(log pθ(xi) − log pθ0(xi)) ;

27HMC 6.1-6.2


Paninski, Intro. Math. Stats., December 8, 2005

73

remember, log pθ0(xi) is constant in θ, so subtracting it oﬀ doesn’t perturb

the MLE at all.

Now we’re left with something that looks a little more familiar: the log-

likelihood ratio

1

N

N

�

i=1

(log pθ(xi) − log pθ0(xi)) = 1

N

N

�

i=1

log pθ(xi)

pθ0(xi)

— the log of the ratio of the likelihood of the data under θ0 and under θ —

is a sample average of i.i.d. r.v.’s! So, by the LLN,

1

N

N

�

i=1

log pθ(xi)

pθ0(xi) →P Eθ0 log pθ(x)

pθ0(x) = −Eθ0 log pθ0(x)

pθ(x) ;

the expectation is taken under θ0 because, remember, that is the true pa-

rameter (i.e., our data are drawn i.i.d. from pθ0(x)).

Now, this last term has a name you might have encountered before;

Eθ0 log

pθ0(x)

pθ(x) is called the “Kullback-Leibler divergence” between the distri-

butions pθ0(x) and pθ(x). This is called a “divergence” because it measures

the distance between pθ0(x) and pθ(x), in the sense that

DKL(pθ1; pθ2) = Eθ1 log pθ1(x)

pθ2(x) ≥ 0,

with equality only when pθ1(x) = pθ2(x) with pθ1-probability one. Exercise

88: Prove this using Jensen’s inequality.

So what have we learned? We now know that, for any ﬁxed θ, the normal-

ized log-likelihood ratio, 1

N

�N

i=1 log pθ(xi)

pθ0(xi), tends to a function, −DKL(pθ0; pθ),

which has a unique maximum at θ0. (Why is the maximum unique? Identiﬁ-

ability.) So we can argue that the MLE asymptotically picks out the argmax

of −DKL(pθ0; pθ), i.e., is consistent. (Actually, completing this consistency

argument rigorously does require a couple technical conditions — e.g., it is

enough that pθ(x) is continuous in θ with probability one, and

Eθ0

�

max

θ∈Θ | log pθ(x)|

�

&lt; ∞

— but we’ll ignore these technical details. The basic logic — LLN + deﬁni-

tion of K-L divergence + Jensen — should be clear.)


Paninski, Intro. Math. Stats., December 8, 2005

74

Asymptotic normality and the Fisher information

OK, that takes care of consistency: now we know that ˆθMLE →P θ0. But

just as in the LLN case, we want to know more. How fast does the MLE

converge? What is the limiting (rescaled) distribution?

It turns out to be useful and informative to step back and look at the

behavior of the posterior density. We know from the above that

p(θ|x1, x2, ...xN) = 1

Z p(θ)p(x1, x2, ...xN|θ)

=

1

Z p(θ)

N

�

i=1

p(xi|θ)

≈

1

Z exp(−NDKL(p(x|θ0); p(x|θ))).

Note immediately that we’re ignoring the prior p(θ) asymptotically; as N

becomes large the likelihood term dominates the shape of the posterior, be-

cause the likelihood term is growing linearly with N, whereas the prior term

is ﬁxed as a function of N.

Next, remember that −DKL(θ0; θ) has a unique maximum at θ0; this

implies that

∂

∂θDKL(θ0; θ)

����

θ=θ0

= 0.

Now if we make a second-order expansion around θ0,

log p(θ|x1, x2, ...xN)

∼

−NDKL(θ0; θ)

=

−N(0 + 0 + 1

2(θ − θ0)I(θ0)(θ − θ0) + ...),

i.e., the posterior likelihood is well-approximated by a Gaussian with mean

θ0 and variance

1

N I(θ0)−1,

where we have abbreviated the curvature of the DKL function at θ as

I(θ0) = ∂2

∂θ2DKL(θ0; θ)

����

θ=θ0

.

This simple geometric quantity I(θ0) is called the “Fisher information” at θ0;

it’s called “information” because the larger I is, the smaller the asymptotic

variance of the posterior is — thus, in a sense, large values of I indicate that


Paninski, Intro. Math. Stats., December 8, 2005

75

the data tell us a lot about the true underlying θ, and vice versa. (This

number is named after Fisher, the statistician who ﬁrst developed a great

deal of the estimation theory we’ve been talking about in this section.)

What about the mean of this Gaussian? We know it’s close to θ0, be-

cause the posterior decays exponentially everywhere else. (This is another

way of saying that ˆθMLE is consistent.) But how close exactly? We took a

relatively crude approach above: we took the random log-likelihood function

log p(xi|θ) and substituted its average, DKL(θ0; θ). What if we try expanding

the loglikelihood directly: We look at �

i log p(xi|θ):

�

i

log p(xi|θ0)

+

�

i

∂

∂θ log pθ(xi)

����

θ0

(θ − θ0) + 1

2

�

i

∂2 log p(xi|θ)

∂θ2

����

θ0

(θ − θ0)2

≈ KN

+

�

i

∂

∂θ log pθ(xi)

����

θ0

(θ − θ0) − 1

2NI(θ0)(θ − θ0)2

Look at

∂

∂θ log pθ(x)

����

θ0

.

This is an important random variable — albeit one with a somewhat compli-

cated deﬁnition — known as the “score.” Exercise 89: Prove that this r.v.

has mean zero and variance I(θ0) (a nice coincidence!). (Caveat: to prove

this, you’ll need to interchange an integral and a derivative, which isn’t al-

ways legal. We’ll mostly ignore this mathematical delicacy here, but note

that it does lead to problems in some cases, e.g. in the case that pθ(x) is

uniform U(0, θ).)

So we can apply the CLT: if we abbreviate

GN =

�

i

∂

∂θ log pθ(xi)

����

θ0

,

then GN is asymptotically Gaussian, with mean zero and variance NI(θ0).

So log p(D|θ) looks like a random upside-down bowl-shaped function:

�

i

log p(xi|θ) ∼ GN(θ − θ0) − 1

2NI(θ0)(θ − θ0)2.

The curvature of this bowl is −NI(θ0). The top of the bowl (i.e., the MLE)

is random, on the other hand, asymptotically Gaussian with mean θ0 and


Paninski, Intro. Math. Stats., December 8, 2005

76

variance (NI(θ0))−1. Exercise 90: Prove this, using what you know about

the peak of an upside-down quadratic, what you already know about the

mean and variance of GN, and the usual rules for multiplication of variances

and the fact that Gaussians are preserved under addition and multiplication

by scalars.

To sum up, our main result: the posterior likelihood is well-approximated

by a Gaussian shape, with variance (NI(θ0))−1 and mean

√

N

�

ˆθMLE − θ0

�

→D N(0, I(θ0)−1).

Note that, as we’ve seen with our concrete examples, the variance asymp-

totically depends on the underlying true parameter θ0, because the Fisher

information I(θ0) depends on θ0.

Exercise 91: We’ve seen some examples where computing the asymp-

totic variance of the MLE is easy by direct methods. Use direct methods to

compute the asymptotic variance of the MLE, then use the above formula to

derive the Fisher information I(θ0), for a) the Gaussian with unknown mean

and known variance; b) the binomial; c) the Poisson.

Exercise 92: Use the delta method to compute the asymptotic variance

of the MLE for exponential data. Now compute the Fisher information I(θ0),

and from this derive the asymptotic variance. Do your answers agree?

Exercise 93: Compute the score and Fisher information in an exponen-

tial family with the canonical parameterization.

Exercise 94: Compute the MLE for double-exponential data,

pθ(x) = 1

2 exp(−|x − θ|).

Now compute the asymptotic variance of the median under double-exponential

data.

Exercise 95: Compute the Fisher information in N i.i.d. observations.

More generally, if x and y are conditionally independent given θ (i.e., p(x, y|θ) =

p(x|θ)p(y|θ), what is the information in the joint sample (x, y)? (Hint: write

out the score, and take the variance.)

Exercise 96: It might be helpful to step back and look at all this from

a more general viewpoint. We are estimating θ by maximizing a function of

the form

MN(θ) =

N

�

i=1

m(xi, θ);


Paninski, Intro. Math. Stats., December 8, 2005

77

here m(xi, θ) is just the log-likelihood of one sample point, log pθ(xi) (and

as usual, xi are i.i.d. from pθ0(x)). What can you say about the asymptotic

distribution of the “M-estimator” (where “M” stands for “maximization”)

ˆθN = arg max

θ∈Θ MN(θ),

if you know that:

1. Epθ0(x)[m(x, θ)] has a unique maximum at θ0;

2.

∂2

∂θ2Epθ0(x)[m(x, θ)]

����

θ=θ0

= −A, A &gt; 0;

3. Vpθ0(x)[∂m(x,θ)

∂θ

] = B, 0 &lt; B &lt; ∞.

Now how does this result ﬁt in with what we just proved about the MLE

(e.g., what form do A and B take in the MLE case)?

Multiparameter case

A similar asymptotic analysis can be performed when we need to estimate

more than one parameter simultaneously. We’ll leave the details to you, but

the basic result is that if (ˆθMLE,1, ˆθMLE,2) is the MLE for (θ1, θ2), then we

can construct the Fisher information matrix

Iij = E ∂

∂θi

∂

∂θj

logθ1,θ2(x),

and the asymptotic covariance matrix of the MLE is given exactly by (NI)−1,

where here (.)−1 is interpreted as a matrix inverse.

Exercise 97: What is the asymptotic variance of ˆθMLE,1 if θ2 is known?

What if θ2 is unknown?

Exercise 98: Compute the asymptotic covariance of ˆµMLE and ˆσ2

MLE un-

der Gaussian data, in two ways: 1) directly, and 2) using the multiparameter

Fisher information.


Paninski, Intro. Math. Stats., December 8, 2005

78

Cramer-Rao bound, eﬃciency, asymptotic eﬃciency

We just established that the asymptotic variance of the MLE looks like

(NI(θ))−1. It turns out that this is asymptotically optimal, as the following

bound shows:

Theorem 5 (Cramer-Rao lower bound on variance). Let ˆθ be unbiased. Then

V (ˆθ) ≥ (I(θ))−1.

More generally, for any estimator ˆθ,

V (ˆθ) ≥

�

dEθ(ˆθ)/dθ

�2

I(θ)

.

Proof. For the general case, compute the covariance of ˆθ and the score; then

apply Cauchy-Schwartz.

For the special unbiased case, just plug in Eθ(ˆθ) = θ.

Exercise 99: Fill in the gaps in the above proof.

Estimators for which the Cramer-Rao bound is achieved exactly are called

“eﬃcient.” However, such estimators are the exception rather than the rule,

as the following exercise demonstrates.

Nonetheless, eﬃciency is still an

extremely useful concept, if applied in an asymptotic sense: a sequence of

estimators ˆθN is “asymptotically eﬃcient” if it asymptotically meets the C-R

bound, that is,

lim

N→∞[NV (ˆθN)] = I(θ)−1.

Exercise 100: Look at the derivation of the Cramer-Rao bound more

closely. What can you say about the case that the bound is met exactly (i.e.,

equality holds in the bound)? More precisely: if the bound is met precisely,

what does this imply about the parametric family pθ(x)?

Exercise 101: If ˆθ1 and ˆθ2 are two asymptotically eﬃcient estimators,

what can you say about their (rescaled) diﬀerence,

√

N(ˆθ1 − ˆθ2)?

Does

this imply anything about the “asymptotic uniqueness” of the MLE as an

asymptotically eﬃcient estimator?


Paninski, Intro. Math. Stats., December 8, 2005

79

Suﬃciency and information loss

Exercise 102: Compute the Fisher information for a suﬃcient statistic and

compare it to the Fisher information in the full data. Is it necessarily true

that a suﬃcient statistic preserves all the information in the full sample? How

about the converse: if IT(x)(θ) = Ix(θ), then is T(x) automatically suﬃcient?

Can we ever have IT(x)(θ) &gt; Ix(θ)? If yes, give an example; if no, prove it.

One-step estimators

It’s often a hassle to exactly solve the likelihood equation

∂L(θ)

∂θ

= 0.

However, in some cases we can come up with a decent estimator ˆθ1 that at

least gets us close: say, ˆθ1 is

√

N-consistent.

Now, we have established that the loglikelihood surface is asymptoti-

cally (as N → ∞) well-approximated (on a N −1/2 scale) by an upside-down

quadratic. So a natural idea is to use the estimator ˆθ2 derived by apply-

ing one step of “Newton’s algorithm”28 for ﬁnding the local maximum of a

function which looks like an upside-down quadratic:

ˆθ2 = ˆθ1 − l′(ˆθ1)

l′′(ˆθ1)

(where l′ and l′′ are the ﬁrst and second derivative of the loglikelihood with

respect to θ, respectively).

Now the very interesting result is that this “one-step” estimator — which

can be computed analytically whenever ˆθ1 can — is asymptotically eﬃcient,

that is, asymptotically just as good as the MLE, even though the MLE

might be a lot harder to compute exactly. Exercise 103: Prove this; that

is, establish the asymptotic eﬃciency of the one-step estimator. (Hint: the

most direct way to do this uses basically the same logic we used to establish

the optimality and asymptotic distribution of the MLE. So try mimicking

that proof.)

28Recall the logic of Newton’s algorithm: to maximize the function f(x) given an initial

guess x1, approximate f(x) with a second-order Taylor expansion about x1 and then

(analytically) solve for the maximum of this quadratic approximation to obtain x2. Draw

a picture and do the relevant algebra to remind yourself of what’s going on here.

