


Aug 17, 2022

·

9 min read

Entropy, Cross-Entropy, Mutual information and

Kullback-Leibler divergence

An introduction to information theory

Photo by AbsolutVision on Unsplash

Introduction

Information




The more probable the event is the less informative is. And the more

surprising the event is the more informative is.

Entropy


Mutual information

Joint Entropy

Conditional entropy

Mutual information


X and Y are independent

X and Y are dependent: 

Kullback-Leibler divergence

Properties

∞

Interpretation

Cross Entropy


Conclusion

Machine Learning

Entropy

Loss Function

Information Theory


1



Follow



A software and machine learning engineer





Information

