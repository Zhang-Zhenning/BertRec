
Trending Now

DSA

Data Structures

Algorithms

Interview Preparation

Data Science

Topic-wise Practice

C

C++

Java

JavaScript

Python

CSS

Competitive Programming

Machine Learning

Aptitude

Write &amp; Earn

Web Development

Puzzles

Projects



Read

Discuss

Courses

Practice

Video

What are Word Embeddings?

It is an approach for representing words and documents. Word Embedding or Word Vector is a numeric vector

input that represents a word in a lower-dimensional space. It allows words with similar meaning to have a similar

representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique

features.

Word Embeddings in NLP









shristikotaiah


Features: Anything that relates words to one another. Eg: Age, Sports, Fitness, Employed etc. Each word vector

has values corresponding to these features.

Goal of Word Embeddings

To reduce dimensionality

To use a word to predict the words around it

Inter word semantics must be captured

How are Word Embeddings used?

They are used as input to machine learning models.

Take the words —-&gt; Give their numeric representation —-&gt; Use in training or inference

To represent or visualize any underlying patterns of usage in the corpus that was used to train them.

Implementations of Word Embeddings:

Word Embeddings are a method of extracting features out of text so that we can input those features into a

machine learning model to work with text data. They try to preserve syntactical and semantic information. The

methods such as Bag of Words(BOW), CountVectorizer and TFIDF rely on the word count in a sentence but do

not save any syntactical or semantic information. In these algorithms, the size of the vector is the number of

elements in the vocabulary. We can get a sparse matrix if most of the elements are zero. Large input vectors will

mean a huge number of weights which will result in high computation required for training. Word Embeddings give

a solution to these problems.

Let’s take an example to understand how word vector is generated by taking emoticons which are most frequently

used in certain conditions and transform each emoji into a vector and the conditions will be our features.

Happy

????

????

????

Sad

???? 

????

????

Excited ????

????

????

Sick

????

???? 

????


The emoji vectors for the emojis will be:

     [happy,sad,excited,sick]

???? =[1,0,1,0]

???? =[0,1,0,1]

???? =[0,0,1,1]

.....

In a similar way, we can create word vectors for different words as well on the basis of given features. The words

with similar vectors are most likely to have the same meaning or are used to convey the same sentiment.

In this article we will be discussing two different approaches to get Word Embeddings:

1) Word2Vec:

In Word2Vec every word is assigned a vector. We start with either a random vector or one-hot vector.

One-Hot vector: A representation where only one bit in a vector is 1.If there are 500 words in the corpus then the

vector length will be 500. After assigning vectors to each word we take a window size and iterate through the entire

corpus. While we do this there are two neural embedding methods which are used:

1.1) Continuous Bowl of Words(CBOW)

In this model what we do is we try to fit the neighboring words in the window to the central word.



1.2) Skip Gram

In this model, we try to make the central word closer to the neighboring words. It is the complete opposite of the

CBOW model. It is shown that this method produces more meaningful embeddings.




After applying the above neural embedding methods we get trained vectors of each word after many iterations

through the corpus. These trained vectors preserve syntactical or semantic information and are converted to lower

dimensions. The vectors with similar meaning or semantic information are placed close to each other in space.

2) GloVe:

This is another method for creating word embeddings. In this method, we take the corpus and iterate through it and

get the co-occurrence of each word with other words in the corpus. We get a co-occurrence matrix through this.

The words which occur next to each other get a value of 1, if they are one word apart then 1/2, if two words apart

then 1/3 and so on.

Let us take an example to understand how the matrix is created. We have a small corpus:

Corpus:

It is a nice evening.

Good Evening!

Is it a nice evening?

 

it

is

a

nice

evening

good

it

0

 

 

 

 

 

is

1+1

0

 

 

 

 

a

1/2+1

1+1/2

0

 

 

 

nice

1/3+1/2

1/2+1/3

1+1

0

 

 

evening 1/4+1/3

1/3+1/4

1/2+1/2

1+1

0

 

good

0

0

0

0

1

0

The upper half of the matrix will be a reflection of the lower half. We can consider a window frame as well to

calculate the co-occurrences by shifting the frame till the end of the corpus. This helps gather information about the

context in which the word is used.


Similar Reads

1.

Finding the Word Analogy from given words using Word2Vec embeddings

2.

Finding the Odd Word amongst given words using Word2Vec embeddings

Initially, the vectors for each word is assigned randomly. Then we take two pairs of vectors and see how close they

are to each other in space. If they occur together more often or have a higher value in the co-occurrence matrix

and are far apart in space then they are brought close to each other. If they are close to each other but are rarely

or not frequently used together then they are moved further apart in space.

After many iterations of the above process, we’ll get a vector space representation that approximates the

information from the co-occurrence matrix. The performance of GloVe is better than Word2Vec in terms of both

semantic and syntactic capturing.

Pre-trained Word Embedding Models:

People generally use pre-trained models for word embeddings. Few of them are:

SpaCy

fastText

Flair etc.

Common Errors made:

You need to use the exact same pipeline during deploying your model as were used to create the training data

for the word embedding. If you use a different tokenizer or different method of handling white space,

punctuation etc. you might end up with incompatible inputs.

Words in your input that doesn’t have a pre-trained vector. Such words are known as Out of Vocabulary

Word(oov). What you can do is replace those words with “UNK” which means unknown and then handle them

separately.

Dimension mis-match: Vectors can be of many lengths. If you train a model with vectors of length say 400 and

then try to apply vectors of length 1000 at inference time, you will run into errors. So make sure to use the same

dimensions throughout.

Benefits of using Word Embeddings:

It is much faster to train than hand build models like WordNet(which uses graph embeddings)

Almost all modern NLP applications start with an embedding layer

It Stores an approximation of meaning

Drawbacks of Word Embeddings:

It can be memory intensive

It is corpus dependent. Any underlying bias will have an effect on your model

It cannot distinguish between homophones. Eg: brake/break, cell/sell, weather/whether etc.

Last Updated : 26 May, 2022






3.

Overview of Word Embedding using Embeddings from Language Models (ELMo)

4.

NLP | Synsets for a word in WordNet

5.

NLP | Word Collocations

6.

NLP | Part of speech tagged - word corpus

7.

NLP | Likely Word Tags

8.

Pre-trained Word embedding using Glove in NLP models

9.

NLP | Classifier-based Chunking | Set 2

10.

Processing text using NLP | Basics

Related Tutorials

1.

Deep Learning Tutorial

2.

Top 101 Machine Learning Projects with Source Code

3.

Machine Learning Mathematics

4.

Natural Language Processing (NLP) Tutorial

5.

Data Science for Beginners


Courses

 

course-img



 142k+ interested Geeks

Python Programming Foundation -

Self Paced

 Beginner and Intermediate

course-img



 102k+ interested Geeks

Complete Machine Learning &amp;

Data Science Program

 Beginner to Advance

course-img

Previous

Next  

Article Contributed By :

shristikotaiah

@shristikotaiah

Vote for difficulty

Current difficulty : Easy

 

 

 

 





Easy



Normal



Medium



Hard



Expert

Improved By :

nikhatkhan11

Article Tags :

Natural-language-processing,

Machine Learning

Practice Tags :

Machine Learning

Report Issue


 A-143, 9th Floor, Sovereign Corporate Tower,

Sector-136, Noida, Uttar Pradesh - 201305

 feedback@geeksforgeeks.org



































































































Company

About Us

Careers

In Media

Contact Us

Terms and

Conditions

Privacy Policy

Copyright Policy


Third-Party

Copyright Notices

Advertise with us

Languages

Python

Java

C++

GoLang

SQL

R Language

Android Tutorial

Data Structures

Array

String

Linked List

Stack

Queue

Tree

Graph

Algorithms

Sorting

Searching

Greedy

Dynamic

Programming

Pattern Searching

Recursion

Backtracking

Web

Development

HTML

CSS

JavaScript

Bootstrap

ReactJS

AngularJS

NodeJS

Write &amp; Earn

Write an Article

Improve an Article

Pick Topics to Write


Write Interview

Experience

Internships

Video Internship

Computer

Science

GATE CS Notes

Operating Systems

Computer Network

Database

Management

System

Software

Engineering

Digital Logic Design

Engineering Maths

Data Science &amp;

ML

Data Science With

Python

Data Science For

Beginner

Machine Learning

Tutorial

Maths For Machine

Learning

Pandas Tutorial

NumPy Tutorial

NLP Tutorial

Interview

Corner

Company

Preparation

Preparation for SDE

Company Interview

Corner

Experienced

Interview

Internship Interview

Competitive

Programming

Aptitude

Python


Python Tutorial

Python

Programming

Examples

Django Tutorial

Python Projects

Python Tkinter

OpenCV Python

Tutorial

GfG School

CBSE Notes for

Class 8

CBSE Notes for

Class 9

CBSE Notes for

Class 10

CBSE Notes for

Class 11

CBSE Notes for

Class 12

English Grammar

UPSC/SSC/BANKING

SSC CGL Syllabus

SBI PO Syllabus

IBPS PO Syllabus

UPSC Ethics Notes

UPSC Economics

Notes

UPSC History

Notes

@geeksforgeeks , Some rights reserved

