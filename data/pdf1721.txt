
arXiv:1707.08052v1  [cs.CL]  25 Jul 2017

Challenges in Data-to-Document Generation

Sam Wiseman and Stuart M. Shieber and Alexander M. Rush

School of Engineering and Applied Sciences

Harvard University

Cambridge, MA, USA

{swiseman,shieber,srush}@seas.harvard.edu

Abstract

Recent neural models have shown signif-

icant progress on the problem of generat-

ing short descriptive texts conditioned on

a small number of database records.

In

this work, we suggest a slightly more dif-

ﬁcult data-to-text generation task, and in-

vestigate how effective current approaches

are on this task. In particular, we introduce

a new, large-scale corpus of data records

paired with descriptive documents, pro-

pose a series of extractive evaluation meth-

ods for analyzing performance, and ob-

tain baseline results using current neural

generation methods.

Experiments show

that these models produce ﬂuent text, but

fail to convincingly approximate human-

generated documents.

Moreover, even

templated baselines exceed the perfor-

mance of these neural models on some

metrics, though copy- and reconstruction-

based extensions lead to noticeable im-

provements.

1

Introduction

Over the past several years, neural text genera-

tion systems have shown impressive performance

on tasks such as machine translation and summa-

rization. As neural systems begin to move toward

generating longer outputs in response to longer

and more complicated inputs, however, the gener-

ated texts begin to display reference errors, inter-

sentence incoherence, and a lack of ﬁdelity to

the source material. The goal of this paper is to

suggest a particular, long-form generation task in

which these challenges may be fruitfully explored,

to provide a publically available dataset for this

task, to suggest some automatic evaluation met-

rics, and ﬁnally to establish how current, neural

text generation methods perform on this task.

A classic problem in natural-language gener-

ation (NLG) (Kukich, 1983; McKeown, 1992;

Reiter and Dale, 1997) involves taking structured

data, such as a table, as input, and producing text

that adequately and ﬂuently describes this data as

output. Unlike machine translation, which aims

for a complete transduction of the sentence to be

translated, this form of NLG is typically taken

to require addressing (at least) two separate chal-

lenges: what to say, the selection of an appro-

priate subset of the input data to discuss, and

how to say it, the surface realization of a genera-

tion (Reiter and Dale, 1997; Jurafsky and Martin,

2014). Traditionally, these two challenges have

been modularized and handled separately by gen-

eration systems.

However, neural generation

systems, which are typically trained end-to-end

as conditional language models (Mikolov et al.,

2010; Sutskever et al., 2011, 2014), blur this dis-

tinction.

In this context, we believe the problem of

generating multi-sentence summaries of tables or

database records to be a reasonable next-problem

for neural techniques to tackle as they begin to

consider more difﬁcult NLG tasks.

In particu-

lar, we would like this generation task to have the

following two properties: (1) it is relatively easy

to obtain fairly clean summaries and their corre-

sponding databases for dataset construction, and

(2) the summaries should be primarily focused on

conveying the information in the database. This

latter property ensures that the task is somewhat

congenial to a standard encoder-decoder approach,

and, more importantly, that it is reasonable to eval-

uate generations in terms of their ﬁdelity to the

database.

One task that meets these criteria is that of

generating summaries of sports games from as-

sociated box-score data, and there is indeed a


long history of NLG work that generates sports

game summaries (Robin, 1994; Tanaka-Ishii et al.,

1998; Barzilay and Lapata, 2005). To this end, we

make the following contributions:

• We introduce a new large-scale corpus con-

sisting of textual descriptions of basketball

games paired with extensive statistical tables.

This dataset is sufﬁciently large that fully

data-driven approaches might be sufﬁcient.

• We introduce a series of extractive evalua-

tion models to automatically evaluate output

generation performance, exploiting the fact

that post-hoc information extraction is signif-

icantly easier than generation itself.

• We apply a series of state-of-the-art neural

methods, as well as a simple templated gener-

ation system, to our data-to-document gener-

ation task in order to establish baselines and

study their generations.

Our experiments indicate that neural systems

are quite good at producing ﬂuent outputs and

generally score well on standard word-match met-

rics, but perform quite poorly at content selection

and at capturing long-term structure. While the

use of copy-based models and additional recon-

struction terms in the training loss can lead to im-

provements in BLEU and in our proposed extrac-

tive evaluations, current models are still quite far

from producing human-level output, and are sig-

niﬁcantly worse than templated systems in terms

of content selection and realization. Overall, we

believe this problem of data-to-document genera-

tion highlights important remaining challenges in

neural generation systems, and the use of extrac-

tive evaluation reveals signiﬁcant issues hidden by

standard automatic metrics.

2

Data-to-Text Datasets

We consider the problem of generating descriptive

text from database records. Following the notation

in Liang et al. (2009), let s = {rj}J

j=1 be a set of

records, where for each r ∈ s we deﬁne r.t ∈ T to

be the type of r, and we assume each r to be a bi-

narized relation, where r.e and r.m are a record’s

entity and value, respectively.

For example, a

database recording statistics for a basketball game

might have a record r such that r.t = POINTS, r.e

= RUSSELL WESTBROOK, and r.m = 50.

In

this case, r.e gives the player in question, and r.m

gives the number of points the player scored. From

these records, we are interested in generating de-

scriptive text, ˆy1:T = ˆy1, . . . , ˆyT of T words such

that ˆy1:T is an adequate and ﬂuent summary of s.

A dataset for training data-to-document systems

typically consists of (s, y1:T ) pairs, where y1:T is

a document consisting of a gold (i.e., human gen-

erated) summary for database s.

Several

benchmark

datasets

have

been

used

in

recent

years

for

the

text

genera-

tion

task,

the

most

popular

of

these

be-

ing

WEATHERGOV

(Liang et al.,

2009)

and

ROBOCUP (Chen and Mooney, 2008). Recently,

neural generation systems have show strong

results on these datasets, with the system of

Mei et al. (2016) achieving BLEU scores in the

60s and 70s on WEATHERGOV, and BLEU scores

of almost 30 even on the smaller ROBOCUP

dataset.

These results are quite promising, and

suggest that neural models are a good ﬁt for

text generation. However, the statistics of these

datasets, shown in Table 1, indicate that these

datasets

use

relatively

simple

language

and

record structure. Furthermore, there is reason to

believe that WEATHERGOV is at least partially

machine-generated (Reiter, 2017). More recently,

Lebret et al. (2016) introduced

the

WIKIBIO

dataset, which is at least an order of magnitude

larger in terms of number of tokens and record

types. However, as shown in Table 1, this dataset

too only contains short (single-sentence) genera-

tions, and relatively few records per generation.

As such, we believe that early success on these

datasets is not yet sufﬁcient for testing the desired

linguistic capabilities of text generation at a

document-scale.

With this challenge in mind, we introduce a

new dataset for data-to-document text generation,

available at https://github.com/harvar

dnlp/boxscore-data.

The dataset is in-

tended to be comparable to WEATHERGOV in

terms of token count, but to have signiﬁcantly

longer target texts, a larger vocabulary space, and

to require more difﬁcult content selection.

The dataset consists of two sources of arti-

cles summarizing NBA basketball games, paired

with their corresponding box- and line-score ta-

bles. The data statistics of these two sources, RO-

TOWIRE and SBNATION, are also shown in Ta-

ble 1. The ﬁrst dataset, ROTOWIRE, uses profes-

sionally written, medium length game summaries




WIN

LOSS

PTS

FG



PCT

RB

AS . . .

TEAM



Heat

11

12

103

49

47

27

Hawks

7

15

95

43

33

20





AS

RB

PT

FG

FGA

CITY . . .

PLAYER



Tyler Johnson

5

2

27

8

16

Miami

Dwight Howard

4

17

23

9

11

Atlanta

Paul Millsap

2

9

21

8

12

Atlanta

Goran Dragic

4

2

21

8

17

Miami

Wayne Ellington

2

3

19

7

15

Miami

Dennis Schroder

7

4

17

8

15

Atlanta

Rodney McGruder

5

5

11

3

8

Miami

Thabo Sefolosha

5

5

10

5

11

Atlanta

Kyle Korver

5

3

9

3

9

Atlanta

. . .



The Atlanta Hawks defeated the Miami Heat

, 103 - 95 , at Philips Arena on Wednesday

. Atlanta was in desperate need of a win and

they were able to take care of a shorthanded

Miami team here . Defense was key for

the Hawks , as they held the Heat to 42

percent shooting and forced them to commit

16 turnovers . Atlanta also dominated in the

paint , winning the rebounding battle , 47

- 34 , and outscoring them in the paint 58

- 26.The Hawks shot 49 percent from the

ﬁeld and assisted on 27 of their 43 made

baskets . This was a near wire - to - wire

win for the Hawks , as Miami held just one

lead in the ﬁrst ﬁve minutes . Miami ( 7 -

15 ) are as beat - up as anyone right now

and it ’s taking a toll on the heavily used

starters . Hassan Whiteside really struggled

in this game , as he amassed eight points ,

12 rebounds and one blocks on 4 - of - 12

shooting ...

Figure 1: An example data-record and document pair from the ROTOWIRE dataset. We show a subset of the game’s records

(there are 628 in total), and a selection from the gold document. The document mentions only a select subset of the records, but

may express them in a complicated manner. In addition to capturing the writing style, a generation system should select similar

record content, express it clearly, and order it appropriately.



RC

WG

WB

RW

SBN



Vocab

409

394

400K

11.3K

68.6K

Tokens

11K

0.9M

19M

1.6M

8.8M

Examples

1.9K

22.1K

728K

4.9K

10.9K

Avg Len

5.7

28.7

26.1

337.1

805.4

Rec. Types

4

10

1.7K

39

39

Avg Records

2.2

191

19.7

628

628



Table 1: Vocabulary size, number of total tokens, number of

distinct examples, average generation length, total number of

record types, and average number of records per example for

the ROBOCUP (RC), WEATHERGOV (WG), WIKIBIO (WB),

ROTOWIRE (RW), and SBNATION (SBN) datasets.

targeted at fantasy basketball fans. The writing

is colloquial, but relatively well structured, and

targets an audience primarily interested in game

statistics. The second dataset, SBNATION, uses

fan-written summaries targeted at other fans. This

dataset is signiﬁcantly larger, but also much more

challenging, as the language is very informal, and

often tangential to the statistics themselves. We

show some sample text from ROTOWIRE in Fig-

ure 1.

Our primary focus will be on the RO-

TOWIRE data.

3

Evaluating Document Generation

We begin by discussing the evaluation of gener-

ated documents, since both the task we introduce

and the evaluation methods we propose are moti-

vated by some of the shortcomings of current ap-

proaches to evaluation. Text generation systems

are typically evaluated using a combination of au-

tomatic measures, such as BLEU (Papineni et al.,

2002), and human evaluation.

While BLEU is

perhaps a reasonably effective way of evaluating

short-form text generation, we found it to be un-

satisfactory for document generation. In particu-

lar, we note that it primarily rewards ﬂuent text

generation, rather than generations that capture the

most important information in the database, or that

report the information in a particularly coherent

way. While human evaluation, on the other hand,

is likely ultimately necessary for evaluating gen-

erations (Liu et al., 2016; Wu et al., 2016), it is

much less convenient than using automatic met-

rics. Furthermore, we believe that current text gen-

erations are sufﬁciently bad in sufﬁciently obvious

ways that automatic metrics can still be of use in

evaluation, and we are not yet at the point of need-

ing to rely solely on human evaluators.

3.1

Extractive Evaluation

To address this evaluation challenge, we begin

with the intuition that assessing document quality

is easier than document generation. In particular,

it is much easier to automatically extract informa-

tion from documents than to generate documents

that accurately convey desired information.

As

such, simple, high-precision information extrac-

tion models can serve as the basis for assessing

and better understanding the quality of automatic


generations. We emphasize that such an evalua-

tion scheme is most appropriate when evaluating

generations (such as basketball game summaries)

that are primarily intended to summarize informa-

tion. While many generation problems do not fall

into this category, we believe this to be an interest-

ing category, and one worth focusing on because

it is amenable to this sort of evaluation.

To see how a simple information extraction sys-

tem might work, consider the document in Fig-

ure 1.

We may ﬁrst extract candidate entity

(player, team, and city) and value (number and

certain string) pairs r.e, r.m that appear in the

text, and then predict the type r.t (or none) of

each candidate pair. For example, we might ex-

tract the entity-value pair (“Miami Heat”, “95”)

from the ﬁrst sentence in Figure 1, and then pre-

dict that the type of this pair is POINTS, giving us

an extracted record r such that (r.e, r.m, r.t) =

(MIAMI HEAT, 95, POINTS). Indeed, many re-

lation extraction systems reduce relation extrac-

tion to multi-class classiﬁcation precisely in this

way (Zhang, 2004; Zhou et al., 2008; Zeng et al.,

2014; dos Santos et al., 2015).

More concretely, given a document ˆy1:T , we

consider all pairs of word-spans in each sen-

tence that represent possible entities e and val-

ues m. We then model p(r.t | e, m; θ) for each

pair, using r.t = ǫ to indicate unrelated pairs.

We use architectures similar to those discussed in

Collobert et al. (2011) and dos Santos et al. (2015)

to parameterize this probability; full details are

given in the Appendix.

Importantly, we note that the (s, y1:T ) pairs

typically used for training data-to-document sys-

tems are also sufﬁcient for training the informa-

tion extraction model presented above, since we

can obtain (partial) supervision by simply check-

ing whether a candidate record lexically matches

a record in s.1 However, since there may be mul-

tiple records r ∈ s with the same e and m but with

different types r.t, we will not always be able to

determine the type of a given entity-value pair

found in the text.

We therefore train our clas-

siﬁer to minimize a latent-variable loss: for all

document spans e and m, with observed types

t(e, m) = {r.t : r ∈ s, r.e = e, r.m = m} (possi-



1Alternative approaches explicitly align the document

with the table for this task (Liang et al., 2009).

bly {ǫ}), we minimize

L(θ) = −

�

e,m

log

�

t′∈t(e,m)

p(r.t = t′ | e, m; θ).

We ﬁnd that this simple system trained in this way

is quite accurate at predicting relations. On the

ROTOWIRE data it achieves over 90% accuracy on

held-out data, and recalls approximately 60% of

the relations licensed by the records.

3.2

Comparing Generations

With a sufﬁciently precise relation extraction sys-

tem, we can begin to evaluate how well an auto-

matic generation ˆy1:T has captured the information

in a set of records s. In particular, since the pre-

dictions of a precise information extraction system

serve to align entity-mention pairs in the text with

database records, this alignment can be used both

to evaluate a generation’s content selection (“what

the generation says”), as well as content placement

(“how the generation says it”).

We consider in particular three induced metrics:

• Content Selection (CS): precision and re-

call of unique relations r extracted from

ˆy1:T that are also extracted from y1:T. This

measures how well the generated document

matches the gold document in terms of se-

lecting which records to generate.

• Relation Generation (RG): precision and

number of unique relations r extracted from

ˆy1:T that also appear in s. This measures how

well the system is able to generate text con-

taining factual (i.e., correct) records.

• Content

Ordering

(CO):

normal-

ized

Damerau-Levenshtein

Dis-

tance

(Brill and Moore,

2000)2

between

the sequences of records extracted from y1:T

and that extracted from ˆy1:T. This measures

how well the system orders the records it

chooses to discuss.

We note that CS primarily targets the “what to say”

aspect of evaluation, CO targets the “how to say it”

aspect, and RG targets both.

We conclude this section by contrasting the

automatic evaluation we have proposed with



2DLD is a variant of Levenshtein distance that allows

transpositions of elements; it is useful in comparing the or-

dering of sequences that may not be permutations of the same

set (which is a requirement for measures like Kendall’s Tau).


recently proposed adversarial

evaluation ap-

proaches, which also advocate automatic metrics

backed by classiﬁcation (Bowman et al., 2016;

Kannan and Vinyals, 2016; Li et al., 2017). Un-

like adversarial evaluation, which uses a black-

box classiﬁer to determine the quality of a gener-

ation, our metrics are deﬁned with respect to the

predictions of an information extraction system.

Accordingly, our metrics are quite interpretable,

since by construction it is always possible to deter-

mine which fact (i.e., entity-value pair) in the gen-

eration is determined by the extractor to not match

the database or the gold generation.

4

Neural Data-to-Document Models

In this section we brieﬂy describe the neural gener-

ation methods we apply to the proposed task. As a

base model we utilize the now standard attention-

based encoder-decoder model (Sutskever et al.,

2014; Cho et al., 2014; Bahdanau et al., 2015).

We also experiment with several recent extensions

to this model, including copy-based generation,

and training with a source reconstruction term in

the loss (in addition to the standard per-target-

word loss).

Base Model

For our base model, we map each

record r ∈ s into a vector ˜r by ﬁrst embedding r.t

(e.g., POINTS), r.e (e.g., RUSSELL WESTBROOK),

and r.m (e.g., 50), and then applying a 1-layer

MLP (similar to Yang et al. (2016)).3 Our source

data-records are then represented as ˜s = {˜rj}J

j=1.

Given ˜s, we use an LSTM decoder with atten-

tion and input-feeding, in the style of Luong et al.

(2015), to compute the probability of each target

word, conditioned on the previous words and on

s. The model is trained end-to-end to minimize

the negative log-likelihood of the words in the gold

text y1:T given corresponding source material s.

Copying

There

has

been

a

surge

of

re-

cent work involving augmenting encoder-decoder

models to copy words directly from the source

material on which they condition (Gu et al.,

2016; G¨ulc¸ehre et al., 2016; Merity et al., 2016;

Jia and Liang, 2016; Yang et al., 2016).

These

models typically introduce an additional binary

variable zt into the per-timestep target word dis-

tribution, which indicates whether the target word



3We also include an additional feature for whether the

player is on the home- or away-team.

ˆyt is copied from the source or generated:

p(ˆyt | ˆy1:t−1, s) =

�

z∈{0,1}

p(ˆyt, zt = z | ˆy1:t−1, s).

In our case, we assume that target words are

copied from the value portion of a record r; that

is, a copy implies ˆyt = r.m for some r and t.

Joint Copy Model

The models of Gu et al.

(2016) and Yang et al. (2016) parameterize the

joint distribution table over ˆyt and zt directly:

p(ˆyt, zt | ˆy1:t−1, s) ∝











copy(ˆyt, ˆy1:t−1, s)

zt = 1, ˆyt ∈ s

0

zt = 1, ˆyt ̸∈ s

gen(ˆyt, ˆy1:t−1, s)

zt = 0,

where copy and gen are functions parameterized

in terms of the decoder RNN’s hidden state that as-

sign scores to words, and where the notation ˆyt ∈ s

indicates that ˆyt is equal to r.m for some r ∈ s.

Conditional

Copy

Model

G¨ulc¸ehre et al.

(2016), on the other hand, decompose the joint

probability as:

p(ˆyt, zt | ˆy1:t−1, s) =

�

pcopy(ˆyt | zt, ˆy1:t−1, s) p(zt | ˆy1:t−1, s)

zt=1

pgen(ˆyt | zt, ˆy1:t−1, s) p(zt | ˆy1:t−1, s)

zt=0,

where an MLP is used to model p(zt | ˆy1:t−1, s).

Models with copy-decoders may be trained to

minimize the negative log marginal probability,

marginalizing out the latent-variable zt (Gu et al.,

2016; Yang et al., 2016; Merity et al.,

2016).

However, if it is known which target words yt are

copied, it is possible to train with a loss that does

not marginalize out the latent zt. G¨ulc¸ehre et al.

(2016), for instance, assume that any target word

yt that also appears in the source is copied, and

train to minimize the negative joint log-likelihood

of the yt and zt.

In applying such a loss in our case, we again

note that there may be multiple records r such

that r.m appears in ˆy1:T.

Accordingly, we

slightly modify the pcopy portion of the loss of

G¨ulc¸ehre et al. (2016) to sum over all matched

records. In particular, we model the probability

of relations r ∈ s such that r.m = yt and r.e

is in the same sentence as r.m. Letting r(yt) =


{r ∈ s : r.m = yt, same−sentence(r.e, r.m)},

we have:

pcopy(yt | zt, y1:t−1, s) =

�

r∈r(yt)

p(r | zt, y1:t−1, s).

We note here that the key distinction for our pur-

poses between the Joint Copy model and the Con-

ditional Copy model is that the latter conditions on

whether there is a copy or not, and so in pcopy the

source records compete only with each other. In

the Joint Copy model, however, the source records

also compete with words that cannot be copied. As

a result, training the Conditional Copy model with

the supervised loss of G¨ulc¸ehre et al. (2016) can

be seen as training with a word-level reconstruc-

tion loss, where the decoder is trained to choose

the record in s that gives rise to yt.

Reconstruction

Losses

Reconstruction-based

techniques can also be applied at the document-

or sentence-level during training.

One simple

approach to this problem is to utilize the hidden

states of the decoder to try to reconstruct the

database.

A fully differentiable approach using

the decoder hidden states has recently been

successfully applied to neural machine translation

by Tu et al. (2017). Unlike copying, this method

is applied only at training, and attempts to learn

decoder hidden states with broader coverage of

the input data.

In adopting this reconstruction approach we

segment the decoder hidden states ht into ⌈ T



B⌉

contiguous blocks of size at most B. Denoting a

single one of these hidden state blocks as bi, we

attempt to predict each ﬁeld value in some record

r ∈ s from bi. We deﬁne p(r.e, r.m | bi), the prob-

ability of the entity and value in record r given bi,

to be softmax(f(bi)), where f is a parameterized

function of bi, which in our experiments utilize a

convolutional layer followed by an MLP; full de-

tails are given in the Appendix. We further extend

this idea and predict K records in s from bi, rather

than one. We can train with the following recon-

struction loss for a particular bi:

L(θ) = −

K

�

k=1

min

r∈s log pk(r | bi; θ)

= −

K

�

k=1

min

r∈s

�

x∈{e,m,t}

log pk(r.x | bi; θ),

where pk is the k’th predicted distribution over

records, and where we have modeled each com-

ponent of r independently. This loss attempts to

make the most probable record in s given bi more

probable.

We found that augmenting the above

loss with a term that penalizes the total variation

distance (TVD) between the pk to be helpful.4

Both L(θ) and the TVD term are simply added

to the standard negative log-likelihood objective at

training time.

5

Experimental Methods

In this section we highlight a few important de-

tails of our models and methods; full details are

in the Appendix. For our ROTOWIRE models, the

record encoder produces ˜rj in R600, and we use

a 2-layer LSTM decoder with hidden states of the

same size as the ˜rj, and dot-product attention and

input-feeding in the style of Luong et al. (2015).

Unlike past work, we use two identically struc-

tured attention layers, one to compute the standard

generation probabilities (gen or pgen), and one to

produce the scores used in copy or pcopy.

We train the generation models using SGD

and truncated BPTT (Elman, 1990; Mikolov et al.,

2010), as in language modeling. That is, we split

each y1:T into contiguous blocks of length 100,

and backprop both the gradients with respect to

the current block as well as with respect to the en-

coder parameters for each block.

Our extractive evaluator consists of an ensem-

ble of 3 single-layer convolutional and 3 single-

layer bidirectional LSTM models. The convolu-

tional models concatenate convolutions with ker-

nel widths 2, 3, and 5, and 200 feature maps in the

style of (Kim, 2014). Both models are trained with

SGD.

Templatized Generator

In addition to neu-

ral baselines, we also use a problem-speciﬁc,

template-based generator.

The template-based

generator ﬁrst emits a sentence about the teams

playing in the game, using a templatized sentence

taken from the training set:

The &lt;team1&gt; (&lt;wins1&gt;-&lt;losses1&gt;) de-

feated the &lt;team2&gt; (&lt;wins2&gt;-&lt;losses2&gt;)

&lt;pts1&gt;-&lt;pts2&gt;.



4Penalizing the TVD between the pk might be useful if,

for instance, K is too large, and only a smaller number of

records can be predicted from bi. We also experimented with

encouraging, rather than penalizing the TVD between the pk,

which might make sense if we were worried about ensuring

the pk captured different records.


Then, 6 player-speciﬁc sentences of the following

form are emitted (again adapting a simple sentence

from the training set):

&lt;player&gt; scored &lt;pts&gt; points (&lt;fgm&gt;-

&lt;fga&gt;

FG,

&lt;tpm&gt;-&lt;tpa&gt;

3PT,

&lt;ftm&gt;-

&lt;fta&gt; FT) to go with &lt;reb&gt; rebounds.

The 6 highest-scoring players in the game are used

to ﬁll in the above template. Finally, a typical end

sentence is emitted:

The &lt;team1&gt;’ next game will be at home

against

the

Dallas

Mavericks,

while

the

&lt;team2&gt; will travel to play the Bulls.

Code implementing all models can be found

at https://github.com/harvardnlp/d

ata2text.

Our encoder-decoder models are

based on OpenNMT (Klein et al., 2017).

6

Results

We found that all models performed quite poorly

on the SBNATION data, with the best model

achieving a validation perplexity of 33.34 and a

BLEU score of 1.78. This poor performance is

presumably attributable to the noisy quality of the

SBNATION data, and the fact that many docu-

ments in the dataset focus on information not in

the box- and line-scores. Accordingly, we focus

on ROTOWIRE in what follows.

The main results for the ROTOWIRE dataset are

shown in Table 2, which shows the performance

of the models in Section 4 in terms of the metrics

deﬁned in Section 3.2, as well as in terms of per-

plexity and BLEU.

6.1

Discussion

There are several interesting relationships in the

development portion of Table 2. First we note that

the Template model scores very poorly on BLEU,

but does quite well on the extractive metrics, pro-

viding an upper-bound for how domain knowl-

edge could help content selection and generation.

All the neural models make signiﬁcant improve-

ments in terms of BLEU score, with the condi-

tional copying with beam search performing the

best, even though all the neural models achieve

roughly the same perplexity.

The extractive metrics provide further insight

into the behavior of the models.

We ﬁrst note

that on the gold documents y1:T , the extractive

model reaches 92% precision.

Using the Joint

The Utah Jazz ( 38 - 26 ) defeated the Houston Rockets ( 38

- 26 ) 117 - 91 on Wednesday at Energy Solutions Arena in

Salt Lake City . The Jazz got out to a quick start in this one

, out - scoring the Rockets 31 - 15 in the ﬁrst quarter alone

. Along with the quick start , the Rockets were the superior

shooters in this game , going 54 percent from the ﬁeld and

43 percent from the three - point line , while the Jazz went

38 percent from the ﬂoor and a meager 19 percent from deep

. The Rockets were able to out - rebound the Rockets 49 -

49 , giving them just enough of an advantage to secure the

victory in front of their home crowd . The Jazz were led

by the duo of Derrick Favors and James Harden . Favors

went 2 - for - 6 from the ﬁeld and 0 - for - 1 from the three

- point line to score a game - high of 15 points , while also

adding four rebounds and four assists ....

Figure 2: Example document generated by the Conditional

Copy system with a beam of size 5. Text that accurately re-

ﬂects a record in the associated box- or line-score is high-

lighted in blue, and erroneous text is highlighted in red.

Copy model, generation only has a record gen-

eration (RG) precision of 47% indicating that re-

lationships are often generated incorrectly.

The

best Conditional Copy system improves this value

to 71%, a signiﬁcant improvement and potentially

the cause of the improved BLEU score, but still far

below gold.

Notably, content selection (CS) and content or-

dering (CO) seem to have no correlation at all

with BLEU. There is some improvement with CS

for the conditional model or reconstruction loss,

but not much change as we move to beam search.

CO actually gets worse as beam search is utilized,

possibly a side effect of generating more records

(RG#). The fact that these scores are much worse

than the simple templated model indicates that fur-

ther research is needed into better copying alone

for content selection and better long term content

ordering models.

Test results are consistent with development re-

sults, indicating that the Conditional Copy model

is most effective at BLEU, RG, and CS, and that

reconstruction is quite helpful for improving the

joint model.

6.2

Human Evaluation

We also undertook two human evaluation studies,

using Amazon Mechanical Turk. The ﬁrst study

attempted to determine whether generations con-

sidered to be more precise by our metrics were

also considered more precise by human raters. To

accomplish this, raters were presented with a par-

ticular NBA game’s box score and line score, as

well as with (randomly selected) sentences from

summaries generated by our different models for




Development



RG

CS

CO

PPL

BLEU

Beam

Model

P%

#

P%

R%

DLD%



Gold

91.77

12.84

100

100

100

1.00

100

Template

99.35

49.7

18.28

65.52

12.2

N/A

6.87



B=1

Joint Copy

47.55

7.53

20.53

22.49

8.28

7.46

10.41

Joint Copy + Rec

57.81

8.31

23.65

23.30

9.02

7.25

10.00

Joint Copy + Rec + TVD

60.69

8.95

23.63

24.10

8.84

7.22

12.78

Conditional Copy

68.94

9.09

25.15

22.94

9.00

7.44

13.31



B=5

Joint Copy

47.00

10.67

16.52

26.08

7.28

7.46

10.23

Joint Copy + Rec

62.11

10.90

21.36

26.26

9.07

7.25

10.85

Joint Copy + Rec + TVD

57.51

11.41

18.28

25.27

8.05

7.22

12.04

Conditional Copy

71.07

12.61

21.90

27.27

8.70

7.44

14.46



Test



Template

99.30

49.61

18.50

64.70

8.04

N/A

6.78

Joint Copy + Rec (B=5)

61.23

11.02

21.56

26.45

9.06

7.47

10.88

Joint Copy + Rec + TVD (B=1)

60.27

9.18

23.11

23.69

8.48

7.42

12.96

Conditional Copy (B=5)

71.82

12.82

22.17

27.16

8.68

7.67

14.49



Table 2: Performance of induced metrics on gold and system outputs of RotoWire development and test data. Columns indicate

Record Generation (RG) precision and count, Content Selection (CS) precision and recall, Count Ordering (CO) in normalized

Damerau-Levenshtein distance, perplexity, and BLEU. These ﬁrst three metrics are described in Section 3.2. Models com-

pare Joint and Conditional Copy also with addition Reconstruction loss and Total Variation Distance extensions (described in

Section 4).

those games. Raters were then asked to count how

many facts in each sentence were supported by

records in the box or line scores, and how many

were contradicted. We randomly selected 20 dis-

tinct games to present to raters, and a total of 20

generated sentences per game were evaluated by

raters. The left two columns of Table 3 contain the

average numbers of supporting and contradicting

facts per sentence as determined by the raters, for

each model. We see that these results are generally

in line with the RG and CS metrics, with the Con-

ditional Copy model having the highest number of

supporting facts, and the reconstruction terms sig-

niﬁcantly improving the Joint Copy models.

Using a Tukey HSD post-hoc analysis of an

ANOVA with the number of contradicting facts as

the dependent variable and the generating model

and rater id as independent variables, we found

signiﬁcant (p &lt; 0.01) pairwise differences in con-

tradictory facts between the gold generations and

all models except “Copy+Rec+TVD,” as well as a

signiﬁcant difference between “Copy+Rec+TVD”

and “Copy”. We similarly found a signiﬁcant pair-

wise difference between “Copy+Rec+TVD” and

“Copy” for number of supporting facts.

Our second study attempted to determine

whether generated summaries differed in terms of

how natural their ordering of records (as captured,

for instance, by the DLD metric) is. To test this,



# Supp.

# Cont. Order Rat.



Gold

2.04

0.70

5.19

Joint Copy

1.65

2.31

3.90

Joint Copy + Rec

2.33

1.83

4.43

Joint Copy + Rec +TVD

2.43

1.16

4.18

Conditional Copy

3.05

1.48

4.03



Table 3: Average rater judgment of number of box score

ﬁelds supporting (left column) or contradicting (middle col-

umn) a generated sentence, and average rater Likert rating for

the naturalness of a summary’s ordering (right column). All

generations use B=1.

we presented raters with random summaries gen-

erated by our models and asked them to rate the

naturalness of the ordering of facts in the sum-

maries on a 1-7 Likert scale.

30 random sum-

maries were used in this experiment, each rated

3 times by distinct raters. The average Likert rat-

ings are shown in the rightmost column of Table 3.

While it is encouraging that the gold summaries

received a higher average score than the gener-

ated summaries (and that the reconstruction term

again improved the Joint Copy model), a Tukey

HSD analysis similar to the one presented above

revealed no signiﬁcant pairwise differences.

6.3

Qualitative Example

Figure 2 shows a document generated by the Con-

ditional Copy model, using a beam of size 5. This

particular generation evidently has several nice


properties: it nicely learns the colloquial style of

the text, correctly using idioms such as “19 per-

cent from deep.” It is also partially accurate in its

use of the records; we highlight in blue when it

generates text that is licensed by a record in the

associated box- and line-scores.

At the same time, the generation also contains

major logical errors. First, there are basic copy-

ing mistakes, such as ﬂipping the teams’ win/loss

records. The system also makes obvious seman-

tic errors; for instance, it generates the phrase

“the Rockets were able to out-rebound the Rock-

ets.” Finally, we see the model hallucinates fac-

tual statements, such as “in front of their home

crowd,” which is presumably likely according to

the language model, but ultimately incorrect (and

not supported by anything in the box- or line-

scores). In practice, our proposed extractive eval-

uation will pick up on many errors in this pas-

sage. For instance, “four assists” is an RG error,

repeating the Rockets’ rebounds could manifest in

a lower CO score, and incorrectly indicating the

win/loss records is a CS error.

7

Related Work

In this section we note additional related work

not noted throughout.

Natural language gen-

eration has been studied for decades (Kukich,

1983; McKeown, 1992; Reiter and Dale, 1997),

and

generating

summaries

of

sports

games

has been a topic of interest for almost as

long

(Robin,

1994;

Tanaka-Ishii et al.,

1998;

Barzilay and Lapata, 2005).

Historically,

research has focused on both

content

selection

(“what

to

say”)

(Kukich,

1983;

McKeown,

1992;

Reiter and Dale,

1997;

Duboue and McKeown,

2003;

Barzilay and Lapata,

2005),

and

surface

re-

alization

(“how

to

say

it”)

(Goldberg et al.,

1994;

Reiter et al.,

2005)

with

earlier

work

using (hand-built) grammars,

and later work

using SMT-like approaches (Wong and Mooney,

2007) or generating from PCFGs (Belz, 2008)

or other formalisms (Soricut and Marcu, 2006;

White et al., 2007). In the late 2000s and early

2010s,

a number of systems were proposed

that did both (Liang et al., 2009; Angeli et al.,

2010; Kim and Mooney, 2010; Lu and Ng, 2011;

Konstas and Lapata, 2013).

Within the world of neural text generation,

some recent work has focused on conditioning lan-

guage models on tables (Yang et al., 2016), and

generating short biographies from Wikipedia Ta-

bles (Lebret et al., 2016; Chisholm et al., 2017).

Mei et al. (2016) use a neural encoder-decoder

approach on standard record-based generation

datasets, obtaining impressive results, and moti-

vating the need for more challenging NLG prob-

lems.

8

Conclusion and Future Work

This work explores the challenges facing neural

data-to-document generation by introducing a new

dataset, and proposing various metrics for auto-

matically evaluating content selection, generation,

and ordering. We see that recent ideas in copying

and reconstruction lead to improvements on this

task, but that there is a signiﬁcant gap even be-

tween these neural models and templated systems.

We hope to motivate researchers to focus further

on generation problems that are relevant both to

content selection and surface realization, but may

not be reﬂected clearly in the model’s perplexity.

Future work on this task might include ap-

proaches that process or attend to the source

records in a more sophisticated way, generation

models that attempt to incorporate semantic or

reference-related constraints, and approaches to

conditioning on facts or records that are not as ex-

plicit in the box- and line-scores.

Acknowledgments

We gratefully acknowledge the support of a

Google Research Award.

References

Gabor Angeli, Percy Liang, and Dan Klein. 2010. A

simple domain-independent probabilistic approach

to generation. In Proceedings of the 2010 Confer-

ence on Empirical Methods in Natural Language

Processing, pages 502–512. Association for Com-

putational Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015.

Neural machine translation by jointly

learning to align and translate. In ICLR.

Regina Barzilay and Mirella Lapata. 2005. Collective

content selection for concept-to-text generation. In

Proceedings of the conference on Human Language

Technology and Empirical Methods in Natural Lan-

guage Processing, pages 331–338. Association for

Computational Linguistics.


Anja Belz. 2008.

Automatic generation of weather

forecast texts using comprehensive probabilistic

generation-space models. Natural Language Engi-

neering, 14(04):431–455.

Steven Bird. 2006. Nltk: the natural language toolkit.

In Proceedings of the COLING/ACL on Interactive

presentation sessions, pages 69–72. Association for

Computational Linguistics.

Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-

drew M. Dai, Rafal J´ozefowicz, and Samy Ben-

gio. 2016. Generating sentences from a continuous

space. In CoNLL, pages 10–21.

Eric Brill and Robert C Moore. 2000. An improved er-

ror model for noisy channel spelling correction. In

Proceedings of the 38th Annual Meeting on Associa-

tion for Computational Linguistics, pages 286–293.

Association for Computational Linguistics.

David L Chen and Raymond J Mooney. 2008. Learn-

ing to sportscast: a test of grounded language acqui-

sition. In Proceedings of the 25th international con-

ference on Machine learning, pages 128–135. ACM.

Andrew Chisholm, Will Radford, and Ben Hachey.

2017.

Learning to generate one-sentence biogra-

phies from wikidata. CoRR, abs/1702.06235.

KyungHyun Cho, Bart van Merrienboer, Dzmitry Bah-

danau, and Yoshua Bengio. 2014. On the properties

of neural machine translation: Encoder-decoder ap-

proaches. Eighth Workshop on Syntax, Semantics

and Structure in Statistical Translation.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael

Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.

2011. Natural language processing (almost) from

scratch.

Journal of Machine Learning Research,

12:2493–2537.

Pablo A Duboue and Kathleen R McKeown. 2003. Sta-

tistical acquisition of content selection rules for nat-

ural language generation. In EMNLP, pages 121–

128. Association for Computational Linguistics.

Jeffrey L. Elman. 1990.

Finding structure in time.

Cognitive Science, 14(2):179–211.

Eli Goldberg, Norbert Driedger, and Richard I Kit-

tredge. 1994. Using natural-language processing to

produce weather forecasts. IEEE Expert, 9(2):45–

53.

Alex Graves and J¨urgen Schmidhuber. 2005. Frame-

wise phoneme classiﬁcation with bidirectional lstm

and other neural network architectures. Neural Net-

works, 18(5):602–610.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.

Li. 2016.

Incorporating copying mechanism in

sequence-to-sequence learning. In ACL.

C¸ aglar G¨ulc¸ehre, Sungjin Ahn, Ramesh Nallapati,

Bowen Zhou, and Yoshua Bengio. 2016. Pointing

the unknown words. In ACL.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long

short-term memory. Neural Comput., 9:1735–1780.

Robin Jia and Percy Liang. 2016. Data recombination

for neural semantic parsing. In ACL Volume 1: Long

Papers.

Dan Jurafsky and James H Martin. 2014. Speech and

language processing, volume 3. Pearson London.

Anjuli Kannan and Oriol Vinyals. 2016. Adversarial

evaluation of dialogue models. In NIPS 2016 Work-

shop on Adversarial Training.

Joohyun Kim and Raymond J Mooney. 2010.

Gen-

erative alignment and semantic parsing for learn-

ing from ambiguous supervision.

In Proceedings

of the 23rd International Conference on Computa-

tional Linguistics: Posters, pages 543–551. Associ-

ation for Computational Linguistics.

Yoon Kim. 2014. Convolutional neural networks for

sentence classiﬁcation.

In EMNLP, pages 1746–

1751.

Guillaume

Klein,

Yoon

Kim,

Yuntian

Deng,

Jean Senellart, and Alexander M. Rush. 2017.

Opennmt: Open-source toolkit for neural machine translation.

CoRR, abs/1701.02810.

Ioannis Konstas and Mirella Lapata. 2013. A global

model for concept-to-text generation. J. Artif. Intell.

Res.(JAIR), 48:305–346.

Karen Kukich. 1983. Design of a knowledge-based re-

port generator. In ACL, pages 145–150.

R´emi Lebret, David Grangier, and Michael Auli. 2016.

Neural text generation from structured data with

application to the biography domain. In EMNLP,

pages 1203–1213.

Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and

Dan Jurafsky. 2017. Adversarial learning for neural

dialogue generation. CoRR, abs/1701.06547.

Percy Liang, Michael I Jordan, and Dan Klein. 2009.

Learning semantic correspondences with less super-

vision. In ACL, pages 91–99. Association for Com-

putational Linguistics.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Michael

Noseworthy, Laurent Charlin, and Joelle Pineau.

2016. How NOT to evaluate your dialogue system:

An empirical study of unsupervised evaluation met-

rics for dialogue response generation. In EMNLP,

pages 2122–2132.

Wei Lu and Hwee Tou Ng. 2011.

A probabilistic

forest-to-string model for language generation from

typed lambda calculus expressions. In Proceedings

of the Conference on Empirical Methods in Natu-

ral Language Processing, pages 1611–1622. Asso-

ciation for Computational Linguistics.


Thang Luong, Hieu Pham, and Christopher D. Man-

ning. 2015. Effective approaches to attention-based

neural machine translation. In Proceedings of the

2015 Conference on Empirical Methods in Natural

Language Processing, EMNLP 2015, pages 1412–

1421.

Kathleen McKeown. 1992. Text generation - using dis-

course strategies and focus constraints to generate

natural language text. Studies in natural language

processing. Cambridge University Press.

Hongyuan Mei, Mohit Bansal, and Matthew R. Walter.

2016. What to talk about and how? selective gener-

ation using lstms with coarse-to-ﬁne alignment. In

NAACL HLT, pages 720–730.

Stephen Merity, Caiming Xiong, James Bradbury, and

Richard Socher. 2016.

Pointer sentinel mixture

models. CoRR, abs/1609.07843.

T. Mikolov, M. Karaﬁt, L. Burget, J. Cernock, and

S. Khudanpur. 2010.

Recurrent neural network

based language model. In INTERSPEECH.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-

Jing Zhu. 2002. Bleu: a method for automatic eval-

uation of machine translation.

In Proceedings of

the 40th annual meeting on association for compu-

tational linguistics, pages 311–318. Association for

Computational Linguistics.

Ehud Reiter. 2017. You need to understand your cor-

pora! the weathergov example. https://ehudr

eiter.com/2017/05/09/weathergov/.

Ehud Reiter and Robert Dale. 1997. Building applied

natural language generation systems. Natural Lan-

guage Engineering, 3(1):57–87.

Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,

and Ian Davy. 2005. Choosing words in computer-

generated weather forecasts. Artiﬁcial Intelligence,

167(1-2):137–169.

Jacques Robin. 1994.

Revision-based generation of

Natural Language Summaries providing historical

Background. Ph.D. thesis, Citeseer.

C´ıcero Nogueira dos Santos, Bing Xiang, and Bowen

Zhou. 2015. Classifying relations by ranking with

convolutional neural networks. In ACL, pages 626–

634.

Radu Soricut and Daniel Marcu. 2006. Stochastic lan-

guage generation using widl-expressions and its ap-

plication in machine translation and summarization.

In Proceedings of the 21st International Conference

on Computational Linguistics and the 44th annual

meeting of the Association for Computational Lin-

guistics, pages 1105–1112. Association for Compu-

tational Linguistics.

Ilya Sutskever, James Martens, and Geoffrey E Hin-

ton. 2011.

Generating text with recurrent neural

networks. In Proceedings of the 28th International

Conference on Machine Learning (ICML), pages

1017–1024.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.

Sequence to sequence learning with neural net-

works. In Advances in Neural Information Process-

ing Systems (NIPS), pages 3104–3112.

Kumiko Tanaka-Ishii, Kˆoiti Hasida, and Itsuki Noda.

1998. Reactive content selection in the generation

of real-time soccer commentary. In COLING-ACL,

pages 1282–1288.

Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,

and Hang Li. 2017. Neural machine translation with

reconstruction. In AAAI, pages 3097–3103.

Michael White, Rajakrishnan Rajkumar, and Scott

Martin. 2007. Towards broad coverage surface real-

ization with ccg. In Proceedings of the Workshop on

Using Corpora for NLG: Language Generation and

Machine Translation (UCNLG+ MT), pages 267–

276.

Yuk Wah Wong and Raymond J Mooney. 2007. Gen-

eration by inverting a semantic parser that uses sta-

tistical machine translation. In HLT-NAACL, pages

172–179.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V

Le,

Mohammad Norouzi,

Wolfgang Macherey,

Maxim Krikun,

Yuan Cao,

Qin Gao,

Klaus

Macherey, et al. 2016.

Google’s neural ma-

chine translation system: Bridging the gap between

human and machine translation.

arXiv preprint

arXiv:1609.08144.

Zichao Yang, Phil Blunsom, Chris Dyer, and Wang

Ling. 2016.

Reference-aware language models.

CoRR, abs/1611.01628.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,

Jun Zhao, et al. 2014.

Relation classiﬁcation via

convolutional deep neural network.

In COLING,

pages 2335–2344.

Zhu Zhang. 2004. Weakly-supervised relation classiﬁ-

cation for information extraction. In Proceedings of

the thirteenth ACM international conference on In-

formation and knowledge management, pages 581–

588. ACM.

GuoDong Zhou, JunHui Li, LongHua Qian, and

Qiaoming Zhu. 2008.

Semi-supervised learning

for relation extraction.

In Third International

Joint Conference on Natural Language Processing,

page 32.


Appendix

A. Additional Dataset Details

The ROTOWIRE data covers NBA games played

between 1/1/2014 and 3/29/2017; some games

have multiple summaries.

The summaries have

been randomly split into training, validation, and

test sets consisting of 3398, 727, and 728 sum-

maries, respectively.

The SBNATION data covers NBA games played

between 11/3/2006 and 3/26/2017; some games

have multiple summaries.

The summaries have

been randomly split into training, validation, and

test sets consisting of 7633, 1635, and 1635 sum-

maries, respectively.

All numbers in the box- and line-scores (but

not the summaries) are converted to integers; frac-

tional numbers corresponding to percents are mul-

tiplied by 100 to obtain integers in [0, 100]. We

show the types of records in the data in Table 4.

B. Generation Model Details

Encoder

For the ROTOWIRE data, a relation r

is encoded into ˜r by embedding each of r.e, r.t,

r.m and a “home-or-away” indicator feature in

R600, and applying a 1-layer MLP (with ReLU

nonlinearity) to map the concatenation of these

vectors back into R600. To initialize the decoder

LSTMs, we ﬁrst mean-pool over the ˜rj by en-

tity (giving one vector per entity), and then lin-

early transform the concatenation of these pooled

entity-representations so that they can initialize

the cells and hidden states of a 2-layer LSTM with

states also in R600. The SBNATION setup is iden-

tical, except all vectors are in R700.

Decoder

As mentioned in the body of the paper,

we compute two different attention distributions

(i.e., using different parameters) at each decod-

ing step. For the Joint Copy model, one attention

distribution is not normalized, and is normalized

along with all the output-word probabilities.

Within the Conditional Copy model we com-

pute p(zt|ˆy1:t−1, s) by mean-pooling the ˜rj, con-

catenating them with the current (topmost) hidden

state of the LSTM, and then feeding this concate-

nation via a 1-layer ReLU MLP with hidden di-

mension 600, and with a Sigmoid output layer.

For the reconstruction-loss, we feed blocks (of

size at most 100) of the decoder’s LSTM hid-

den states through a (Kim, 2014)-style convolu-

tional model.

We use kernels of width 3 and

5, 200 ﬁlters, a ReLU nonlinearity, and max-

over-time pooling.

To create the pk, these now

400-dimensional features are then mapped via an

MLP with a ReLU nonlinearity into 3 separate

200 dimensional vectors corresponding to the pre-

dicted relation’s entity, value, and type, respec-

tively.

These 200 dimensional vectors are then

fed through (separate) linear decoders and softmax

layers in order to obtain distributions over entities,

values, and types. We use K = 3 distinct pk.

Models are trained with SGD, a learning rate

of 1 (which is divided by 2 every time validation

perplexity fails to decrease), and a batch size of 16.

We use dropout (at a rate of 0.5) between LSTM

layers and before the linear decoder.

C. Information Extraction Details

Data

To form an information extraction dataset,

we ﬁrst sentence-tokenize the gold summary doc-

uments y1:T using NLTK (Bird, 2006). We then

determine which word-spans yi:j could represent

entities (by matching against players, teams, or

cities in the database), and which word-spans

yk:l could represent numbers (using the open

source text2num library5 to convert (strings of)

number-words into numbers).6

We then con-

sider each yi:j, yk:l pair in the same sentence, and

if there is a record r in the database such that

r.e = yi:j and r.m = text2num(yk:l) we annotate

the yi:j, yk:l pair with the label r.t; otherwise, we

give it a label of ǫ.

Model

We predict

relations

by ensembling

3

convolutional

models

and

3

bidirectional

LSTM

(Hochreiter and Schmidhuber,

1997;

Graves and Schmidhuber, 2005) models.

Each

model consumes the words in the sentence, which

are embedded in R200, as well as the distances

of each word in the sentence from both the

entity-word-span

and

the

number-word-spans

(as described above), which are each embedded

in R100. These vectors are concatenated (into a

vector in R500) and fed into either a convolutional

model or a bidirectional LSTM model.

The convolutional model uses 600 total ﬁlters,

with 200 ﬁlters for kernels of width 2, 3, and

5, respectively, a ReLU nonlinearity, and max-

pooling. These features are then mapped via a 1-



5https://github.com/exogen/text2num

6We ignore certain particularly misleading number-

words, such as ”three-point,” where we should not expect a

corresponding value of 3 among the records.




Player Types

POSN

MIN

PTS

FGM

FGA

FG-PCT

FG3M

FG3A

FG3-PCT

FTM

FTA

FT-PCT

OREB

DREB

REB

AST

TOV

STL

BLK

PF

FULL-NAME

NAME1

NAME2

CITY

Team Types

PTS-QTR1

PTS-QTR2

PTS-QTR3

PTS-QTR4

PTS

FG-PCT

FG3-PCT

FT-PCT

REB

AST

TOV

WINS

LOSSES

CITY

NAME



Table 4: Possible Record Types

layer (ReLU) MLP into R500, which predicts one

of the 39 relation types (or ǫ) with a linear decoder

layer and softmax.

The bidirectional LSTM model uses a single

layer with 500 units in each direction, which are

concatenated. The hidden states are max-pooled,

and then mapped via a 1-layer (ReLU) MLP into

R700, which predicts one of the 39 relation types

(or ǫ) with a linear decoder layer and softmax.

