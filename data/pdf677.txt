


May 12, 2020

·

11 min read

Language models and RNN

This story covers two topics: Language models(LM) and RNN. For LM, it includes the N-gram

language model and neural LM; for RNN, this story goes from vanilla RNN to vanishing gradient

problem, and introduce LSTM/GRU and variants of RNN: bidirectional RNN and multi-layer

RNN.

1. Language models

Language modeling is the task of predicting what word comes next. More formally: given

a sequence of words x(1),x(2), …x(t), compute the probability distribution of the next word

x(t+1)

Source: the course slide

You can also think of a Language Model as a system that assigns a probability to a piece

of text. 

1.1 N-gram language models




1.1 N-gram language models

the students opened their ____

The pre- deep learning method is learning an n-gram Language Model.

the

student

opened

their

the students

students

opened

opened their

N-gram LM. Source: the course slide

calculating n-grams and (n-1)-gram probabilities. Source: the course slide

First, we make a simplifying assumption: x(t+1) depends only on the preceding n-1 words.

4-gram LM example. Source: the course slide

as the proctor started the clock, the student opened

their ___

students opened their

students opened their books

P(books|students opened their)=0.4

students opened their exams

P(exams|students opened their)=0.1

sparsity problems with n-gram language models. Source: the course slide

The problem of n-gram LM is sparsity.

generating text with an n-gram LM. Source: the course slide

incoherent

1.2 A neural LM

a fixed-window neural LM. Source: the course slide

A fixed-window neural LM improves over n-gram LM.


2. RNN

2.1 Vanilla RNN

. The

core idea is we can apply the same weight W repeatedly.

this diagram shows the most important features of RNN. Source: the course slide

A RNN LM. Source: the course slide

s 

The loss function for RNN LM training. Source: the course slide

Training an RNN Language Model. Source: the course slide

Stochastic Gradient

Descent (SGD) allows us to compute loss and gradient for a small chunk of data, and

update.

“ The gradient w.r.t. a repeated weight is the sum of the gradient w.r.t each time it appears”. Source: the course

slide

backpropagation through time. Source: the course slide

“ The gradient w.r.t. a repeated weight is the sum of the

gradient w.r.t each time it appears”.


2.2 RNN applications

RNN Language Model to generate text 

RNN LM example. Source: the course slide

Evaluating Language Models. Source: the course slide

Language Modeling is a benchmark task that helps us measure our progress in

understanding language

2.3 vanishing gradient

vanishing gradient proof sketch. Source: the course slide

why is vanishing gradient a problem? Source: the course slide

The gradient can be viewed as a measure of the effect of the past on the future. 

,

The writer of the

book is

is

writer

the writer of the books are

are

books

writer

2.4 exploding gradient

math explanation for backpropagation calculation. Source: the course slide

RNN for text classification. Source: the course slide

RNN as an encoder in question answering (left); RNN applied in speech recognition (right). Source: the course slide


2.4 exploding gradient

exploding gradient 

 If the gradient becomes too big, then the SGD

update step becomes too big.

SGD. Source: the course slide

gradient clipping. Source: the course slide

Gradient clipping 

2.5 Long short-term memory (LSTM)

hidden state in the vanilla RNN where the hidden state is constantly being rewritten. Source: the course slide

The main problem is that it’s too difficult for the RNN

to learn to preserve information over many timesteps

The LSTM architecture makes it easier for the RNN to

preserve information over many timesteps.

LSTM. Source: the course slide


2.5 Gated Recurrent Units (GRU)

GRU. Source: the course slide

What is the difference between GRU and LSTM? 

 GRU is quicker

to compute and has fewer parameters.

2.6 Vanishing/exploding gradient is not just an RNN problem

2.7 bidirectional RNNs

ResNet (left) and DenseNet (right). Source: the course slide

Bidirectional RNNs. Source: the course slide


2.8 Multi-layer RNNs

multi-layer RNN example. Source: the course slide

Language Modeling

NLP

Recurrent Neural Network

Machine Learning




Follow



MSc in Computer Science at UT.





Data Science

