


Published in

Towards Data Science



Feb 28, 2022

·

9 min read

Save

Learning to Rank: A Complete Guide to Ranking

using Machine Learning

Photo by Nick Fewings on Unsplash

Ranking: What and Why?

ranking

sorting documents by relevance

with

respect to a query

Information Retrieval

Search Engines 

Recommender Systems

Travel Agencies

Ranking applications: 1) search engines; 2) recommender systems; 3) travel agencies. (Image by author)

relevance score s = f(x)

x = (q, d)

is a query

d is a document








Ranking models rely on a scoring function. (Image by author)

Vector Space Models

 f(x) = f(q, d)

q

d

Learning to Rank 

s

x = (q, d)

Machine Learning models

for Learning to Rank

Ranking Evaluation Metrics

predicted documents ranking

k-th top retrieved document

s

Mean Average Precision (MAP)

MAP — Mean Average Precision. (Image by author)

binary relevance

0 (non relevant) or 1 (relevant)

precision 

recall 

precision-recall curve

Average Precision (AP)

Mean Average

Precision (MAP)

Discounted Cumulative Gain (DCG)

DCG — Discounted Cumulative Gain. (Image by author)

graded relevance

0 (bad),

1 (fair), 2 (good), 3 (excellent), 4 (perfect)

gain 

discount 


discounted gain 

Discounted Cumulative Gain (DCG)

Normalized Discounted

Cumulative Gain (NDCG)

Machine Learning Models for Learning to Rank

inputs outputs

loss function

Input

q

n 

D = {d

d }

xᵢ

= (q, dᵢ) 

Output

relevance score yᵢ

predicted score sᵢ = f(xᵢ)

loss function

3 approaches

Pointwise Methods

each document

dᵢ 

pointwise

sᵢ

yᵢ

regression problem,

Pairwise Methods

each pair of

documents dᵢ, dⱼ

pairwise

yᵢ &gt; yⱼ

binary classification problem

Listwise Methods

listwise

Machine Learning approaches to Learning to Rank: pointwise, pairwise, listwise. (Image by author)

Pointwise Methods

yᵢ 

sᵢ 

Subset Ranking

MSE loss for pointwise methods as in Subset Ranking. (Image by author)

Pairwise Methods

partial information,


relative

preference

binary classification task 

ⱼ

ⱼ

ⱼ

ⱼ

RankNet

BCE loss for pairwise methods as in RankNet. (Image by author)

more importance to documents in higher ranks

LambdaRank

Gradients of an implicit loss function as in LambdaRank. (Image by author)

LambdaMART

Listwise Methods

directly by maximizing the evaluation

metric

should give the best results

Loss = 1 – NDCG

 

 

sorting is non-differentiable

first approach

ranking metrics are used to re-weight

LambdaRank 

LambdaMART

second approach 

SoftRank

smoothened probabilistic

score

ranks k

predicted scores s

probability distributions for the ranks

SoftNDCG

Uncertainty in scores produce a smooth loss in SoftRank. (Image by author)


third approach 

loss over

space of permutations

ListNet

Plackett-Luce model  

Binary Cross-Entropy

distance 

probability distributions over the space of permutations.

Probability of various permutation using Plackett-Luce model in ListNet. (Image by author)

LambdaLoss 

generalized

framework

state-of-the-art accuracy

mixture model

LambdaLoss loss function. (Image by author)

special configurations 

likelihood p(y | s,

π) 

ranked list distribution p(π | s)

 significantly improve the state-of-the-art on Learningt

to Rank tasks.

Conclusions

References


7



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





Information Retrieval

Machine Learning

Data Science

Learning To Rank

Editors Pick

