


ML Wiki

Probabilistic Retrieval Model

Contents

Contents

1 Probabilistic Retrieval Model

1.1 Probabilistic Ranking Principle

1.2 Assumptions

2 Relevance Function

2.1 Descriptive Model Approach

2.2 Generative Models Approach

3 Binary Independence Retrieval Model

3.1 Ranking

3.2 Estimation of Probabilities

4 BM25 Retrieval Function

5 Other Probabilistic Models

6 Sources

Probabilistic Retrieval Model

Probabilistic Ranking Principle

Due to Robertson 1977

relevance = "what is the probability that document D

is relevant to the query Q

?"

Assumptions

ranking assumption: usefulness of a relevant document depends on the number of relevant documents the user has

already seen

the more documents we see - the less useful they are

independence assumption: relevance of Di

to Q

is independent to other documents Dj

from the collection

therefore we can apply it for each document separately

Relevance Function

R = {r, ¬r}

a binary random variable that indicates relevance

let r

represent the event that D

is relevant

¬r

 

Go



Search

Home

 Page Info

Log in



Search


represent the event that D

is not relevant

We need estimate the probability of relevance of a document D

w.r.t a query Q

Need to find:

P(R = r � D, Q)

- prob that D

is relevant to Q

P(R = ¬r � D, Q)

- prob that D

is not relevant to Q

So we need to estimate (learn) P(R � D, Q)

Descriptive Model Approach

Sometimes referred as a "Machine Learning" approach

It then becomes a Classification Problem:

can learn P(R � D, Q)

with a Descriptive Model

R

depends on features that characterize the relationships between D

and Q

for example, # of matched terms

Suppose

we have k

features Fi(Q, D)

f

is a function with parameter Λ

s.t. f(F1,  . . .  , Fk, Λ) = P(R � D, Q)

then can fit f

with some ML algorithm e.g. OLS Regression, Logistic Regression or Kernel Ridge Regression

Comments:

will have to spend a lot of time engineering good features

also model has to learn to rank rather than just classify

but sometimes it will work even better than Vector Space Models

such models can use scores from other IR techniques and combine them + use some extra ones

Literature:

Joachims, Thorsten, et al. "Learning to rank for information retrieval." 2007. [1]

Liu, Tie-Yan. "Learning to rank for information retrieval." 2009. [2]

Generative Models Approach

Can use the Bayes Rule to infer the probabilities

P(R = r � D, Q) =

P(D,Q � R = r)P(R = r)

P(D,Q)

P(R = ¬r � D, Q) =

P(D,Q � R = ¬r)P(R = ¬r)

P(D,Q)

and then compare P(R = r � D, Q)


and P(R = ¬r � D, Q)

Interpretation:

P(D, Q � R = r)

probability that if we know that a relevant document is retrieved, it's D

P(R = r)

probability of retrieving a relevant document

P(D, Q)

probability of retrieving D

and issuing Q

Log Odds Ratio:

it's the same as using Odds ratio:

if 

P(R = r � D,Q)

P(R = ¬ � D,Q) &gt; 1

or log

P(R = r � D,Q)

P(R = ¬ � D,Q) &gt; 0

then D

is relevant w.r.t Q

so again the formulated the problem as two-category Document Classification

but we're more interested in the scores rather than class outcome

Binary Independence Retrieval Model

BIR Model:

classical probabilistic IR Model

assumes that terms are independent: it's a "Naive Bayes Classifier" for IR

Documents are represented by binary vectors

if a term is present in a document, it's 1, otherwise it's 0

T = {Ti}

with Ti = 1

if wi

is present in the document D

Ranking

Can rank using log odds:

use s(Q, D) = log

P(R = r � D,Q)

P(R = ¬r � D,Q) = log

P(D,Q � R = r)P(R = r)

P(D,Q � R = ¬r)P(R = ¬r)

P(R = r)

and P(R = ¬r)

are just constants and will not change relative positions of documents in the rating, so let's remove them:

s(Q, D) = log

P(D,Q � R = r)

P(D,Q � R = ¬r)

by independence have:

s(Q, D) = log

∏P(Ti � Q,R = r)

∏P(Ti � Q,R = ¬r) = log∏

P(Ti � Q,R = r)

P(Ti � Q,R = ¬r) = ∑log

P(Ti � Q,R = r)

P(Ti � Q,R = ¬r)


can sum only over words present in the query:

s(Q, D) =

∑

i�Qlog

P(Ti � Q,R = r)

P(Ti � Q,R = ¬r)

now let pi = P(T1 = 1 � Q, R = r)

and qi = P(T1 = 1 � Q, R = ¬r)

. then

s(Q, D) =

∑

i�Q∩Dlog

pi (1 −qi)

(1 −pi)qi

let ci = log

pi (1 −qi)

(1 −pi)qi

, so we have

s(Q, D) =

∑

i�Q∩Dci

Comments:

here we take into account only presence/absence of words

we don't care about frequency

we assume a Multiple Bernoulli Event Model: P(Ti = 1 � Q, R) + P(Ti = 0 � Q, R) = 0

Estimation of Probabilities

How to estimate pi

and qi

?

Robertson / Sparck Jones Formula

based on the training IR test corpus

and also can be account relevance feedback

Notation

given a query Q

and a corpus C

let N

be the number of documents in the corpus

let R

be the number of documents relevant to Q

ni

# of docs that have term wi

ri

# of relevant docs that have term wi

Then:

pi =

ri +λ

R +2λ

qi =

ni −ri +λ

N −R +2λ

here we add some distortion value λ

(can be e.g. λ = 0.5

) to avoid getting logs with 0

It's Laplace Smoothing










BM25 Retrieval Function

This is a stub. Edit it.

Use 2-Poisson Mixture Model with a Term Frequency formula

BM25:

tf(t, D)

- how many times t

occurs in document D

df(t � C)

- how many documents in the corpus C

contain t

bm25(Q, D � C) = ∑

t�Q,Dln

|C| − df(t � C) + 0.5

df(t � C) + 0.5

�

(k1 + 1)tf(t, D)

k1 (1 − b) + b |D|/ | ˉD| + tf(t, D)

�

(k3 + 1)tf(t, Q)

k3 + tf(t, Q)

where |D|

the length of D

and | ˉD|

is the average length of a document in C

params: t1 � [1, 2]

, b = 0.75

and k3 � [0, 1000]

Note that BM25 formula is very similar to TF-IDFs

See

Robertson, Stephen E., and Steve Walker. "Some simple effective approximations to the 2-poisson model for probabilistic

weighted retrieval." 1994. [3]

Other Probabilistic Models

Statistical Language Models

Bayesian Networks

Sources

Information Retrieval (UFRT)

Zhai, ChengXiang. "Statistical language models for information retrieval." 2008.

Categories: Stubs Information Retrieval Probabilistic Models

This page was last modified on 27 June 2015, at 13:40.

Machine Learning Bookcamp: learn machine learning by doing projects (get 40% off with code "grigorevpc")

2012 – 2023 by Alexey Grigorev

Powered by MediaWiki. TyrianMediawiki Skin, with Tyrian design by Gentoo.

Privacy policy About ML Wiki Disclaimers

(

)

Processing math: 100%

