




SI485i : NLP 

Set 4 

Smoothing Language Models 

 Fall 2013 : Chambers 












Review: evaluating n-gram models 

â€¢ Best evaluation for an N-gram 

â€¢ Put model A in a speech recognizer 

â€¢ Run recognition, get word error rate (WER) for A 

â€¢ Put model B in speech recognition, get word error 

rate for B 

â€¢ Compare WER for A and B 

â€¢ In-vivo evaluation 














Difficulty of in-vivo evaluations 

â€¢ In-vivo evaluation 

â€¢ Very time-consuming 

 

â€¢ Instead: perplexity 






Perplexity 

â€¢

Perplexity is the probability of the test set 

(assigned by the language model), 

normalized by the number of words: 

 

 

 

â€¢

Chain rule: 

 

 

â€¢

For bigrams: 

 









Minimizing perplexity is the same as maximizing probability 



The best language model is one that best predicts an 

unseen test set 






Lesson 1: the perils of overfitting 

â€¢ N-grams only work well for word prediction if the test 

corpus looks like the training corpus 

â€¢ In real life, it often doesnâ€™t 

â€¢ We need to train robust models, adapt to test set, etc 

 






Lesson 2: zeros or not? 

â€¢ Zipfâ€™s Law: 

â€¢ A small number of events occur with high frequency 

â€¢ A large number of events occur with low frequency 

 

â€¢ Resulting Problem: 

â€¢ You might have to wait an arbitrarily long time to get valid 

statistics on low frequency events 

â€¢ Our estimates are sparse! No counts exist for the vast bulk of 

things we want to estimate! 

 

â€¢ Solution: 

â€¢ Estimate the likelihood of unseen N-grams 




















Smoothing is like Robin Hood: 

Steal from the rich, give to the poor (probability mass) 



Slide from Dan Klein 






Laplace smoothing 

â€¢ Also called â€œadd-one smoothingâ€ 

â€¢ Just add one to all the counts! 

 

â€¢ MLE estimate: 

 

â€¢ Laplace estimate: 

 

â€¢ Reconstructed counts: 

 












Laplace smoothed bigram counts 

 












Laplace-smoothed bigrams 

 










Reconstituted counts 

 










Note big change to counts 

â€¢

C(â€œwant toâ€) went from 609 to 238! 

â€¢

P(to|want) from .66 to .26! 

â€¢

Discounted by d= c* / c 

â€¢ d for â€œchinese foodâ€ =.10!!!  A 10x reduction 

â€¢ This means Laplace is a blunt instrument 

â€¢ Could use more fine-grained method (add-k) 

 

â€¢

Laplace smoothing not often used for N-grams, as we have 

much better methods 

â€¢

Despite its flaws, Laplace (add-k) is however still used to 

smooth other probabilistic models in NLP, especially 

â€¢ For pilot studies 

â€¢ In domains where the number of zeros isnâ€™t so huge. 






Exercise 

Hey, I just met you, And this is crazy, 

But here's my number, So call me, maybe? 

 

It's hard to look right, At you baby, 

But here's my number, So call me, maybe? 

 

â€¢ Using a unigram model and Laplace smoothing (+1) 

â€¢ Calculate P(â€œcall me possiblyâ€) 

â€¢ Assume a vocabulary based on the above, plus the word â€œpossiblyâ€ 

 

â€¢ Now instead of k=1, set k=0.01 

â€¢ Calculate P(â€œcall me possiblyâ€) 






Better discounting algorithms 

â€¢ Intuition: use the count of things weâ€™ve seen once to 

help estimate the count of things weâ€™ve never seen 

 

â€¢ Intuition in many smoothing algorithms: 

â€¢ Good-Turing 

â€¢ Kneser-Ney 

â€¢ Witten-Bell 










Good-Turing: Josh Goodman intuition 

â€¢ Imagine you are fishing 

â€¢ There are 8 species in the lake: carp, perch, whitefish, trout, 

salmon, eel, catfish, bass 

â€¢ You catch: 

â€¢ 10 carp, 3 perch, 2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish 

â€¢ How likely is it the next species is new (catfish or bass)? 

â€¢ 3/18 

â€¢ And how likely is it that the next species is another trout? 

â€¢ Must be less than 1/18 












Good-Turing Counts 

â€¢

N[x] is the frequency-of-frequency-x 

â€¢ So for the fish:  N[10]=1, N[1]=3, etc. 

 

â€¢

To estimate the total number of unseen species: 

â€¢ Use the number of species (words) weâ€™ve seen once 

â€¢ c[0]* = N[1]      p0 = c[0]*/N = N[1]/N = 3/18 

â€¢ PGT(things with frequency zero in training) = 

ğ‘µ[ğŸ]

ğ‘µ  

 

â€¢

All other estimates are adjusted (down) 

 

ğ‘[ğ‘¥]âˆ— = (ğ‘¥ + 1) ğ‘[ğ‘¥ + 1]

ğ‘[ğ‘¥]

 

ğ‘ƒğºğ‘‡ ğ‘œğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘‘ ğ‘¥ ğ‘¡ğ‘–ğ‘šğ‘’ğ‘  = ğ‘[ğ‘¥]âˆ—

ğ‘  


 














Bigram frequencies of frequencies 

and GT re-estimates 








Complications 

â€¢

In practice, assume large counts (c&gt;k for some k) are reliable: 

 

 

â€¢

That complicates c*, making it: 

 

 

 

 

 

â€¢

Also: we assume singleton counts c=1 are unreliable, so treat N-grams 

with count of 1 as if they were count=0 

â€¢

Also, need the Nk to be non-zero, so we need to smooth (interpolate) 

the Nk counts before computing c* from them 












GT smoothed bigram probs 

 








Backoff and Interpolation 

â€¢ Donâ€™t try to account for unseen n-grams, just backoff 

to a simpler model until youâ€™ve seen it. 

 

â€¢ Start with estimating the trigram: P(z | x, y)  

â€¢ but C(x,y,z) is zero! 

â€¢ Backoff and use info from the bigram: P(z | y) 

â€¢ but C(y,z) is zero! 

â€¢ Backoff to the unigram: P(z) 

 

â€¢ How to combine the trigram/bigram/unigram info? 






Backoff versus interpolation 

â€¢ Backoff: use trigram if you have it, otherwise bigram, 

otherwise unigram 

 

â€¢ Interpolation: always mix all three 






Interpolation 

â€¢ Simple interpolation 

 

 

 

â€¢ Lambdas conditional on context: 












How to set the lambdas? 

â€¢ Use a held-out corpus 

â€¢ Choose lambdas which maximize the probability of 

some held-out data 

â€¢ I.e. fix the N-gram probabilities 

â€¢ Then search for lambda values 

â€¢ That when plugged into previous equation 

â€¢ Give largest probability for held-out set 








Katz Backoff 

â€¢ Use the trigram probabilty if the trigram was observed: 

â€¢ P(dog | the, black)  if  C(â€œthe black dogâ€) &gt; 0 

 

â€¢ â€œBackoffâ€ to the bigram if it was unobserved: 

â€¢ P(dog | black) if C(â€œblack dogâ€) &gt; 0  

 

â€¢ â€œBackoffâ€ again to unigram if necessary: 

â€¢ P(dog) 

 








Katz Backoff 

â€¢ Gotcha: You canâ€™t just backoff to the shorter n-gram. 

â€¢ Why not? It is no longer a probability distribution. The 

entire model must sum to one. 

â€¢ The individual trigram and bigram distributions are valid, but 

we canâ€™t just combine them. 

 

â€¢ Each distribution now needs a factor. See the book 

for details. 

â€¢ P(dog|the,black) = alpha(dog,black) * P(dog | black) 

