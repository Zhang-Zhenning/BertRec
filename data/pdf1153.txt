






BY 

· PUBLISHED SEPTEMBER 21, 2020 · UPDATED SEPTEMBER 23, 2020

Introduction

Maximum likelihood is a widely used technique for estimation with applications in many areas including time series

modeling, panel data, discrete data, and even machine learning.

In today's blog, we cover the fundamentals of maximum likelihood estimation.

In particular, we discuss:

1. The basic theory of maximum likelihood.

2. The advantages and disadvantages of maximum likelihood estimation.

3. The log-likelihood function.

4. Modeling applications.

In addition, we consider a simple application of maximum likelihood estimation to a linear regression model.

What is Maximum Likelihood Estimation?

Maximum likelihood estimation is a statistical method for estimating the parameters of a model. In maximum likelihood

estimation, the parameters are chosen to maximize the likelihood that the assumed model results in the observed data.

This implies that in order to implement maximum likelihood estimation we must:

1. Assume a model, also known as a data generating process, for our data.

2. Be able to derive the likelihood function for our data, given our assumed model (we will discuss this more later).

Once the likelihood function is derived, maximum likelihood estimation is nothing more than a simple optimization problem.

What are the Advantages and Disadvantages of Maximum

Likelihood Estimation?

At this point, you may be wondering why you should pick maximum likelihood estimation over other methods such as least

squares regression or the generalized method of moments. The reality is that we shouldn't always choose maximum likelihood

estimation. Like any estimation technique, maximum likelihood estimation has advantages and disadvantages.

Advantages of Maximum Likelihood Estimation

There are many advantages of maximum likelihood estimation:

If the model is correctly assumed, the maximum likelihood estimator is the most ef�cient estimator.

Beginner's Guide To Maximum Likelihood Estimation

Beginner's Guide To Maximum Likelihood Estimation





Data Analytics Blog



ERIC 




If the model is correctly assumed, the maximum likelihood estimator is the most ef�cient estimator.

It provides a consistent but �exible approach which makes it suitable for a wide variety of applications, including cases

where assumptions of other models are violated.

It results in unbiased estimates in larger samples.

Ef�ciency is one measure of the quality of an estimator. An ef�cient estimator is one that has a small variance or mean

squared error.

Disadvantages of Maximum Likelihood Estimation

It relies on the assumption of a model and the derivation of the likelihood function which is not always easy.

Like other optimization problems, maximum likelihood estimation can be sensitive to the choice of starting values.

Depending on the complexity of the likelihood function, the numerical estimation can be computationally expensive.

Estimates can be biased in small samples.

What is the Likelihood Function?

Maximum likelihood estimation hinges on the derivation of the likelihood function. For this reason, it is important to have a

good understanding of what the likelihood function is and where it comes from.

Let's start with the very simple case where we have one series y with 10 independent observations: 5, 0, 1, 1, 0, 3, 2, 3, 4, 1.

The Probability Density

The �rst step in maximum likelihood estimation is to assume a probability distribution for the data. A probability density

function measures the probability of observing the data given a set of underlying model parameters.

In this case, we will assume that our data has an underlying Poisson distribution which is a common assumption, particularly

for data that is nonnegative count data.

The Poisson probability density function for an individual observation, yi, is given by

f(yi|θ) =

e−θθyi

yi!

Because the observations in our sample are independent, the probability density of our observed sample can be found by

taking the product of the probability of the individual observations:

f(y1, y2, …, y10|θ) =

10

∏

i=1

e−θθyi

yi!

=

e−10θθ∑10

i=1yi

∏10

i=1yi!

We can use the probability density to answer the question of how likely it is that our data occurs given speci�c parameters.

The Likelihood Function

The differences between the likelihood function and the probability density function are nuanced but important.

A probability density function expresses the probability of observing our data given the underlying distribution parameters.

It assumes that the parameters are known.

The likelihood function expresses the likelihood of parameter values occurring given the observed data. It assumes that the

parameters are unknown.


Mathematically the likelihood function looks similar to the probability density:

L(θ|y1, y2, …, y10) = f(y1, y2, …, y10|θ)

For our Poisson example, we can fairly easily derive the likelihood function

L(θ|y1, y2, …, y10) =

e−10θθ∑10

i=1yi

∏10

i=1yi!

=

e−10θθ20

207, 360

The maximum likelihood estimate of the unknown parameter, θ, is the value that maximizes this likelihood.

The Log-Likelihood Function

In practice, the joint distribution function can be dif�cult to work with and the ln of the likelihood function is used instead. In

the case of our Poisson dataset the log-likelihood function is:

ln(L(θ|y)) = − nθ + ln

n

∑

i=1yi − lnθ

n

∑

i=1yi! = − 10θ + 20ln(θ) − ln(207, 360)

The log-likelihood is usually easier to optimize than the likelihood function.

The Maximum Likelihood Estimator



A graph of the likelihood and log-likelihood for our dataset shows that the maximum likelihood occurs when θ = 2. This

means that our maximum likelihood estimator, ˆθMLE = 2.

The Conditional Maximum Likelihood

In the simple example above, we use maximum likelihood estimation to estimate the parameters of our data's density. We

can extend this idea to estimate the relationship between our observed data, y, and other explanatory variables, x. In this

case, we work with the conditional maximum likelihood function:

L(θ|y, x)

We will look more closely at this in our next example.


Example Applications of Maximum Likelihood Estimation

The versatility of maximum likelihood estimation makes it useful across many empirical applications. It can be applied to

everything from the simplest linear regression models to advanced choice models.

In this section we will look at two applications:

The linear regression model

The probit model

Maximum Likelihood Estimation and the Linear Model

In linear regression, we assume that the model residuals are identical and independently normally distributed:

ϵ = y − ˆβx � N(0, σ2)

Based on this assumption, the log-likelihood function for the unknown parameter vector, θ = {β, σ2}, conditional on the

observed data, y and x is given by:

lnL(θ|y, x) = −

1

2

n

∑

i=1 lnσ2 + ln(2π) +

y − ˆβx

σ2

The maximum likelihood estimates of β and σ2 are those that maximize the likelihood.

Maximum Likelihood Estimation and the Probit Model

The probit model is a fundamental discrete choice model.

The probit model assumes that there is an underlying latent variable driving the discrete outcome. The latent variables

follow a normal distribution such that:

y� = xθ + ϵ

ϵ � N(0, 1)

where

yi =

0 if y�i ≤ 0

1 if y�i &gt; 0

The probability density

P(yi = 1|Xi) = P(y�i &gt; 0|Xi) = P(xθ + ϵ &gt; 0|Xi) =

P(ϵ &gt; − xθ|Xi) = 1 − Φ( − xθ) = Φ(xθ)

where Φ represents the normal cumulative distribution function.

The log-likelihood for this model is

lnL(θ) =

n

∑

i=1 yilnΦ(xiθ) + (1 − yi)ln(1 − (xiθ))

Conclusions

[

]

{

[

]


Conclusions

Congratulations! After today's blog, you should have a better understanding of the fundamentals of maximum likelihood

estimation. In particular, we've covered:

The basic theory of maximum likelihood estimation.

The advantages and disadvantages of maximum likelihood estimation.

The log-likelihood function.

The conditional maximum likelihood function.

2 thoughts on “Beginner's Guide To Maximum Likelihood Estimation”

There is a typo in the log likelihood function for the normal distribution. The numerator in the last term should read (y-

\hat{\beta}x)^2, so the square is missing.

Log in to Reply ↓



Eric

Eric has been working to build, distribute, and strengthen the GAUSS universe since 2012. He is an economist skilled in data

analysis and software development. He has earned a B.A. and MSc in economics and engineering and has over 18 years of

combined industry and academic experience in data analysis and research.





Was this post helpful?

Let us know if you liked the post. That’s the only way we can improve.





Yes





No



dseu

January 13, 2022 at 6:00 am



Eric

January 13, 2022 at 7:51 am



Post author


Thank you for pointing this out. This has been �xed.

Log in to Reply ↓

Leave a Reply

You must be logged in to post a comment.





Recent Posts

Fundamentals of Tuning Machine Learning Hyperparameters

Predicting The Output Gap With Machine Learning Regression Models

Managing String Data with GAUSS Dataframes

Applications of Principal Components Analysis in Finance

Predicting Recessions with Machine Learning Techniques

Where To Buy



Available across the globe, you can have access to GAUSS no matter where you are.

Where to Buy

Request Pricing

Have a Speci�c Question?

Get a real answer from a real person

Contact Us


Need Support?

Get help from our friendly experts.

Contact Support

Try GAUSS for 14 days for FREE

See what GAUSS can do for your data

Start a FREE trial



GAUSS is the product of decades of innovation and enhancement by Aptech Systems, a supportive team of experts dedicated to

the success of the worldwide GAUSS user community. Aptech helps people achieve their goals by offering products and

applications that de�ne the leading edge of statistical analysis capabilities.

GET IN TOUCH WITH US

 Email:

info@aptech.com

 Hours of Operation (AZ MST):

Monday-Thursday: 8:00 AM - 4:00 PM

Friday: 8:00 AM - 3:00 PM

 Phone: (360) 886-7100

FAX: (360) 886-8922

 Address:

Aptech Systems, Inc

PO Box 618

Higley, AZ 85236

© 2023 Aptech Systems, Inc. All rights reserved.

Privacy Policy

