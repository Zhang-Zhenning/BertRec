






RPubs by RStudio





Sign in

Sign in





Register

Register


Next Word Prediction

using Katz Backoff Model

Part 2: N-gram model, Katz Backoff, and

Good-Turing Discounting

Leo Mak

April 2019

Executive Summary

The Capstone Project of the Johns Hopkins Data Science

Specialization is to build an NLP application, which should predict the

next word of a user text input. In Part 1, we have analysed the data and

found that there are a lot of uncommon words and word combinations

(2- and 3-grams) can be removed from the corpora, in order to reduce

memory usage and speed up the model building time.

Part 2 will discuss the Katz backoff model and Good-Turing discounting

algorithm powering the application.

The Method

The basic idea of word prediction is likely to be something as:

He likes to eat ice ...

You probably think of the word is cream. Such kind of next word

prediction models from the previous N − 1 words is called N-gram

models. A 2-gram/bigram is just a 2-word or 2-token sequence wi

i−1,

e.g. “ice cream”, a 3-gram/trigram is a 3-word sequence wi

i−2, e.g. “ice

cream cone”. A single word, e.g. “ice”, not surprisingly, is called

unigram.

N-gram model

An N-gram model is to predict the last word wi of an N-gram from

previous N − 1 word sequence wi−1

i−(N−1):wi−(N−1),…,wi−1, by

calculating the probability of P(wi|wi−(N−1),…,wi−1). One simple version

of N-gram model is to count the relative frequencies of words regarding

the previous N − 1 words, calculate the probabilities and find out the

most probable next word. For example, suppose we want to predict the

next word of eat ice, we can calculate the probability of:

P(cream|eat ice) =

C(eat ice cream)

C(eat ice)

and then to compare with probabilities of other word sequence, e.g. 

P(skating|eat ice), P(sheet|eat ice), etc. Note that C(eat ice) is the

count of the word combination “eat ice” appearing in the corpus. This

probabilities estimation is called maxmimum likelihood estimation or

MLE. There is more detail discussion on N-gram model and related

topics in chapter 4 of (Jurafsky and Martin 2009).

The major problem with the above MLE for training N-gram model is it

cannot handle N-grams that are not seen in training corpus, which will





