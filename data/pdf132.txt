
Trending Now

DSA

Data Structures

Algorithms

Interview Preparation

Data Science

Topic-wise Practice

C

C++

Java

JavaScript

Python

CSS

Competitive Programming

Machine Learning

Aptitude

Write &amp; Earn

Web Development

Puzzles

Projects



Read

Discuss

Courses

Practice

Video

In the real-world applications of machine learning, it is very common that there are many relevant features available

for learning but only a small subset of them are observable. So, for the variables which are sometimes observable

and sometimes not, then we can use the instances when that variable is visible is observed for the purpose of

learning and then predict its value in the instances when it is not observable.

On the other hand, Expectation-Maximization algorithm can be used for the latent variables (variables that are

not directly observable and are actually inferred from the values of the other observed variables) too in order to

predict their values with the condition that the general form of probability distribution governing those latent variables

ML | Expectation-Maximization Algorithm








is known to us. This algorithm is actually at the base of many unsupervised clustering algorithms in the field of

machine learning.

It was explained, proposed and given its name in a paper published in 1977 by Arthur Dempster, Nan Laird, and

Donald Rubin. It is used to find the local maximum likelihood parameters of a statistical model in the cases where

latent variables are involved and the data is missing or incomplete.

 

Algorithm:

1. Given a set of incomplete data, consider a set of starting parameters.

2. Expectation step (E – step): Using the observed available data of the dataset, estimate (guess) the values of

the missing data.

3. Maximization step (M – step): Complete data generated after the expectation (E) step is used in order to

update the parameters.

4. Repeat step 2 and step 3 until convergence.



The essence of Expectation-Maximization algorithm is to use the available observed data of the dataset to estimate

the missing data and then using that data to update the values of the parameters. Let us understand the EM

algorithm in detail.

Initially, a set of initial values of the parameters are considered. A set of incomplete observed data is given to the

system with the assumption that the observed data comes from a specific model.

The next step is known as “Expectation” – step or E-step. In this step, we use the observed data in order to

estimate or guess the values of the missing or incomplete data. It is basically used to update the variables.

The next step is known as “Maximization”-step or M-step. In this step, we use the complete data generated in

the preceding “Expectation” – step in order to update the values of the parameters. It is basically used to update

the hypothesis.

Now, in the fourth step, it is checked whether the values are converging or not, if yes, then stop otherwise


Related Tutorials

1.

Deep Learning Tutorial

2.

Top 101 Machine Learning Projects with Source Code

3.

Machine Learning Mathematics

4.

repeat step-2 and step-3 i.e. “Expectation” – step and “Maximization” – step until the convergence occurs.

Flow chart for EM algorithm –



Usage of EM algorithm –

It can be used to fill the missing data in a sample.

It can be used as the basis of unsupervised learning of clusters.

It can be used for the purpose of estimating the parameters of Hidden Markov Model (HMM).

It can be used for discovering the values of latent variables.

Advantages of EM algorithm –

It is always guaranteed that likelihood will increase with each iteration.

The E-step and M-step are often pretty easy for many problems in terms of implementation.

Solutions to the M-steps often exist in the closed form.

Disadvantages of EM algorithm –

It has slow convergence.

It makes convergence to the local optima only.

It requires both the probabilities, forward and backward (numerical optimization requires only forward

probability).

Last Updated : 14 May, 2019






Courses

 

course-img



 102k+ interested Geeks

Complete Machine Learning &amp;

Data Science Program

 Beginner to Advance

course-img



 142k+ interested Geeks

Python Programming Foundation -

Self Paced

 Beginner and Intermediate

course-img

Natural Language Processing (NLP) Tutorial

5.

Data Science for Beginners

Previous

Next  

Article Contributed By :



GeeksforGeeks

Vote for difficulty

Current difficulty : Easy

 

 

 

 



Easy



Normal



Medium



Hard



Expert

Article Tags :

Machine Learning

Practice Tags :

Machine Learning

Report Issue


 A-143, 9th Floor, Sovereign Corporate Tower,

Sector-136, Noida, Uttar Pradesh - 201305

 feedback@geeksforgeeks.org



































































































Company

About Us

Careers

In Media

Contact Us

Terms and

Conditions


Privacy Policy

Copyright Policy

Third-Party

Copyright Notices

Advertise with us

Languages

Python

Java

C++

GoLang

SQL

R Language

Android Tutorial

Data Structures

Array

String

Linked List

Stack

Queue

Tree

Graph

Algorithms

Sorting

Searching

Greedy

Dynamic

Programming

Pattern Searching

Recursion

Backtracking

Web

Development

HTML

CSS

JavaScript

Bootstrap

ReactJS

AngularJS

NodeJS

Write &amp; Earn

Write an Article


Improve an Article

Pick Topics to Write

Write Interview

Experience

Internships

Video Internship

Computer

Science

GATE CS Notes

Operating Systems

Computer Network

Database

Management

System

Software

Engineering

Digital Logic Design

Engineering Maths

Data Science &amp;

ML

Data Science With

Python

Data Science For

Beginner

Machine Learning

Tutorial

Maths For Machine

Learning

Pandas Tutorial

NumPy Tutorial

NLP Tutorial

Interview

Corner

Company

Preparation

Preparation for SDE

Company Interview

Corner

Experienced

Interview

Internship Interview

Competitive

Programming

Aptitude


Python

Python Tutorial

Python

Programming

Examples

Django Tutorial

Python Projects

Python Tkinter

OpenCV Python

Tutorial

GfG School

CBSE Notes for

Class 8

CBSE Notes for

Class 9

CBSE Notes for

Class 10

CBSE Notes for

Class 11

CBSE Notes for

Class 12

English Grammar

UPSC/SSC/BANKING

SSC CGL Syllabus

SBI PO Syllabus

IBPS PO Syllabus

UPSC Ethics Notes

UPSC Economics

Notes

UPSC History

Notes

@geeksforgeeks , Some rights reserved

