
ARTICLE

Subset ranking using regression

Authors: 

 

Authors Info &amp; Claims

COLT'06: Proceedings of the 19th annual conference on Learning Theory • June 2006 • Pages 605–619

• https://doi.org/10.1007/11776420_44

Published: 22 June 2006 Publication History

David Cossock,

Tong Zhang



Next 

Pages 605–619

 Previous



COLT'06: Proceedings of the 19th annual conference on Learning Theory

Subset ranking using regression





We study the subset ranking problem, motivated by its important application in web-search. In this context, we consider the standard

DCG criterion (discounted cumulated gain) that measures the quality of items near the top of the rank-list. Similar to error minimization

for binary classification, the DCG criterion leads to a non-convex optimization problem that can be NP-hard. Therefore a

computationally more tractable approach is needed. We present bounds that relate the approximate optimization of DCG to the

approximate minimization of certain regression errors. These bounds justify the use of convex learning formulations for solving the

subset ranking problem. The resulting estimation methods are not conventional, in that we focus on the estimation quality in the top-

portion of the rank-list. We further investigate the generalization ability of these formulations. Under appropriate conditions, the

consistency of the estimation schemes with respect to the DCG metric can be derived.

ABSTRACT

References

1.

2.

Bartlett, P., Jordan, M., McAuliffe, J.: Convexity, classification, and risk bounds. Technical Report 638, Statistics Department, University

of California, Berkeley (2003) to appear in JASA.

Lugosi, G., Vayatis, N.: On the Bayes-risk consistency of regularized boosting methods. The Annals of Statistics 32 (2004) 30-55 with

 







 44  0

 Sign in



Browse




Read More

Read More

Read More

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

discussion.

Zhang, T.: Statistical behavior and consistency of classification methods based on convex risk minimization. The Annals of Statistics 32

(2004) 56-85 with discussion.

Zhang, T.: Statistical analysis of some multi-category large margin classification methods. Journal of Machine Learning Research 5

(2004) 1225-1251. 

Steinwart, I.: Support vector machines are universally consistent. J. Complexity 18 (2002) 768-791. 

Tewari, A., Bartlett, P.: On the consistency of multiclass classification methods. In: COLT. (2005). 

Jarvelin, K., Kekalainen, J.: IR evaluation methods for retrieving highly relevant documents. In: SIGIR'00. (2000) 41-48. 

Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., Hullender, G.: Learning to rank using gradient descent. In:

ICML'05. (2005). 

Hanley, J., McNeil, B.: The meaning and use of the Area under a Receiver Operating Characetristic (ROC) curve. Radiology (1982) 29-

36.

Agarwal, S., Graepel, T., Herbrich, R., Har-Peled, S., Roth, D.: Generalization bounds for the area under the ROC curve. Journal of

Machine Learning Research 6 (2005) 393-425. 

Agarwal, S., Roth, D.: Learnability of bipartite ranking functions. In: Proceedings of the 18th Annual Conference on Learning Theory.

(2005). 

Clemencon, S., Lugosi, G., Vayatis, N.: Ranking and scoring using empirical risk minimization. In: COLT'05. (2005). 

Rosset, S.: Model selection via the AUC. In: ICML'04. (2004). 

Herbrich, R., Graepel, T., Obermayer, K.: Large margin rank boundaries for ordinal regression. In A. Smola, P. Bartlett, B.S.,

Schuurmans, D., eds.: Advances in Large Margin Classifiers. MIT Press (2000) 115-132.

Cossock, D.: Method and apparatus for machine learning a document relevance function. US patent application, 20040215606

(2003).

Blanchard, G., Lugosi, G., Vayatis, N.: On the rate of convergence of regularized boosting classifiers. Journal of Machine Learning

Research 4 (2003) 861-894. 

Mannor, S., Meir, R., Zhang, T.: Greedy algorithms for classification - consistency, convergence rates, and adaptivity. Journal of

Machine Learning Research 4 (2003) 713-741. 

Friedman, J.: Greedy function approximation: A gradient boosting machine. The Annals of Statistics 29 (2001) 1189-1232.

Recommendations

Re-ranking search results using query logs

Rank aggregation using ant colony approach for metasearch

Effective rank aggregation for metasearching


Check if you have access through your login credentials or your institution to get full access on this article.



Sign in

Get this Publication

Information

Contributors

Comments

Login options

Full Access


Bibliometrics

Citations



44

COLT'06: Proceedings of the 19th annual conference on Learning Theory

June 2006 656 pages

ISBN:

3540352945

Editors:

Gábor Lugosi,

Hans Ulrich Simon

Springer-Verlag

Berlin, Heidelberg

Published: 22 June 2006

Article

Published in

Publisher

Publication History

Qualifiers




View Table Of Contents

Figures

Other

https://dl.acm.org/doi/10.1007/11776420_44



 Copy Link

0

0

44

Total

Citations

View Citations

0

Total

Downloads

Downloads (Last 12 months)

Downloads (Last 6 weeks)



View Author Metrics

Article Metrics

Other Metrics

Share this Publication link

Share on Social Media

















0





Categories













About


















Join









Connect













The ACM Digital Library is published by the Association for Computing Machinery. Copyright © 2023 ACM, Inc.

Terms of Usage 

Privacy Policy 

Code of Ethics

 





