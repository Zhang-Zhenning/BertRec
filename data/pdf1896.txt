




CS6200: Information Retrieval

Slides by: Jesse Anderton

Query Likelihood Retrieval

LM, session 6


So far, we’ve focused on language models like P(D = w1, w2, …, wn). Where’s the query? 

Remember the key insight from vector space models: we want to represent queries and 

documents in the same way. The query is just a “short document:” a sequence of 

words. There are three obvious approaches we can use for ranking: 

1. Query likelihood: Train a language model on a document, and estimate the query’s 

probability. 

2. Document likelihood: Train a language model on the query, and estimate the 

document’s probability. 

3. Model divergence: Train language models on the document and the query, and 

compare them.

Retrieval With Language Models


Suppose that the query speciﬁes a 

topic. We want to know the probability 

of a document being generated from 

that topic, or P(D|Q). 

However, the query is very small, and 

documents are long: document 

language models have less variance. 

In the Query Likelihood Model, we use 

Bayes' Rule to rank documents based 

on the probability of generating the 

query from the documents’ language 

models.

Query Likelihood Retrieval

Assuming uniform prior

Naive Bayes unigram model

P(D|Q)

rank

= P(Q|D)P(D)

= P(Q|D)

=

�

w�Q

P(w|D)

rank

=

�

w�Q

log P(w|D) Numerically stable version


Example: Query Likelihood

Wikipedia: WWI

World War I (WWI or WW1 or World War One), 

also known as the First World War or the 

Great War, was a global war centred in Europe 

that began on 28 July 1914 and lasted until 11 

November 1918. More than 9 million 

combatants and 7 million civilians died as a 

result of the war, a casualty rate exacerbated 

by the belligerents' technological and industrial 

sophistication, and tactical stalemate. It was 

one of the deadliest conﬂicts in history, paving 

the way for major political changes, including 

revolutions in many of the nations involved.

Query: “deadliest war in history”

Term

P(w|D)

log P(w|D)

deadliest

1/94 = 0.011

-1.973

war

6/94 = 0.063

-1.195

in

3/94 = 0.032

-1.496

history

1/94 = 0.011

-1.973

Π = 2.30e-7

Σ = -6.637


Example: Query Likelihood

Wikipedia: Taiping Rebellion

The Taiping Rebellion was a massive civil 

war in southern China from 1850 to 1864, 

against the ruling Manchu Qing dynasty. It 

was a millenarian movement led by Hong 

Xiuquan, who announced that he had 

received visions, in which he learned that he 

was the younger brother of Jesus. At least 20 

million people died, mainly civilians, in one of 

the deadliest military conﬂicts in history.

Query: “deadliest war in history”

Term

P(w|D)

log P(w|D)

deadliest

1/56 = 0.017

-1.748

war

1/56 = 0.017

-1.748

in

2/56 = 0.035

-1.447

history

1/56 = 0.017

-1.748

Π = 2.56e-8

Σ = −6.691


There are many ways to move beyond this basic model. 

• Use n-gram or skip-gram probabilities, instead of unigrams. 

• Model document probabilities P(D) based on length, authority, genre, 

etc. instead of assuming a uniform probability. 

• Use the tools from the VSM slides: stemming, stopping, etc. 

Next, we’ll see how to ﬁx a major issue with our probability estimates: 

what happens if a query term doesn’t appear in the document?

Wrapping Up

