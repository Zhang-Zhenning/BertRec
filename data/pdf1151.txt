








Log in

Sign up



amilas

33

3



leonbloy

59.8k

9

68

146

Relation between cross entropy and conditional entropy

Asked 1 year, 8 months ago

Modified 5 months ago

Viewed 855 times

3

 

 

Is there a relationship between cross-entropy and conditional entropy between two categorical variables?

Definition of cross-entropy:

HX(Y) = −

∑

x P(X =x)logP(Y=x)

Definition of conditional entropy:

H(Y|X) = −

∑

(x,y)P(X =x,Y=y)logP(Y=y|X =x)

Here, X and Y are defined over the same finite probability space --- i.e., the possibilities for x and y are a finite shared set {1,2,3,...,n}.

In an optimization problem, can we minimize cross-entropy instead of minimizing conditional entropy? If so, can we derive the relationship between these two?

Share

edited Aug 5, 2021 at 23:09

asked Aug 5, 2021 at 4:29

what does (x,y) mean? If no constrain between x and y then HX(Y) = ∑ylog(y) which is infinite if there are infinitely many y's. I think you means to put something else in place of p(y)

– Oliver Díaz

Aug 5, 2021 at 17:27

Sorry for the confusion. I have updated the spaces of X and Y in the post, which are finite and shared.

– amilas

Aug 5, 2021 at 23:08

2 Answers

Sorted by:

2

 

There is little or no relationship. The cross entropy relates only to the marginal distributions, (the dependence between X and Y do not matter) while the conditional entropy relates to the joint distribution (dependence between X

and Y is essential).

In general you could write

HX(Y) =H(X)+DKL(pX||pY)

=H(X|Y)+I(X;Y)+DKL(pX||pY)

=H(X|Y)+DKL(pX,Y||pXpY)+DKL(pX||pY)

but I doubt that this could be useful or have a nice interpretation.

You can readily conclude that

HX(Y) ≥H(X|Y)

with HX(Y) =H(X|Y) �  X,Y are iid.

Share

edited Aug 8, 2021 at 13:32

answered Aug 7, 2021 at 23:55

Thank you so much.

– amilas

Aug 9, 2021 at 13:40

0

 

 

Conditional entropy is probably best viewed as the difference between two cross entropies: H(Y|X) =H(X,Y)(X,Y)−HX(X). That is, it’s the incremental entropy from the probability given by X to that given by the joint variable

(X,Y).

Share

Ask Question

optimization

conditional-probability

information-theory

coding-theory

entropy

Cite

Follow





Highest score (default)

Cite

Follow






MATHEMATICS

Tour

Help

Chat

Contact

Feedback

COMPANY

Stack Overflow

Teams

Advertising

Collectives

Talent

About

Press

Legal

Privacy Policy

Terms of Service

Cookie Settings

Cookie Policy

STACK EXCHANGE NETWORK

Technology

Culture &amp; recreation

Life &amp; arts

Science

Professional

Business

API

Data

Blog

Facebook

Twitter

LinkedIn

Instagram

Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2023.4.21.43403



John Jiang

524

5

13

answered Nov 19, 2022 at 20:12

You must log in to answer this question.

Not the answer you're looking for? Browse other questions tagged optimization

conditional-probability

information-theory

coding-theory

entropy .

Related

5

Logistic regression and cross-entropy

8

Relation between cross entropy and joint entropy

7

Is there a unified definition of entropy for arbitrary random variables?

8

Proof of sub-additivity for Shannon Entropy

1

conditional entropy proof

0

Conditional Entropy of Lossy Channel Output

6

Towards a consistent notation for entropy and cross-entropy

0

explanation for cross entropy for logistic regression

2

Chemical Entropy vs. Mathematical Entropy

Hot Network Questions



Did the drapes in old theatres actually say "ASBESTOS" on them?



Why in the Sierpiński Triangle is this set being used as the example for the OSC and not a more "natural"?



Which ability is most related to insanity: Wisdom, Charisma, Constitution, or Intelligence?



On whose turn does the fright from a terror dive end?



Unexpected uint64 behaviour 0xFFFF'FFFF'FFFF'FFFF - 1 = 0?

more hot questions

 Question feed

Cite

Follow



Featured on Meta



Improving the copy in the close modal and post notices - 2023 edition



New blog post from our CEO Prashanth: Community is the future of AI

Your privacy

By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.

 

Accept all cookies

Necessary cookies only

Customize settings

