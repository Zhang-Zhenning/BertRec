


Natural Language Processing

with Deep Learning

CS224N/Ling284

Christopher Manning / John Hewitt

Lecture 1: Introduction and Word Vectors


Lecture Plan

Lecture 1: Introduction and Word Vectors

1. The course (10 mins)

2. Human language and word meaning (15 mins)

3. Word2vec introduction (15 mins)

4. Word2vec objective function gradients (25 mins)

5. Optimization basics (5 mins)

6. Looking at word vectors (10 mins or less)

Key learning today: The (astounding!) result that word meaning can be represented rather 

well by a (high-dimensional) vector of real numbers

2


Course logistics in brief

3

‚Ä¢ Instructor: Christopher Manning

‚Ä¢ Head TA: John Hewitt

‚Ä¢ Course Manager: Amelie Byun. Course Coordinator: John Cho

‚Ä¢ TAs: Many wonderful people! See website

‚Ä¢ Time: Tu/Th 4:30‚Äì5:50 Pacific time, Nvidia Aud. (‚Üí video)

‚Ä¢ We‚Äôve put a lot of other important information on the class webpage. Please read it!

‚Ä¢ http://cs224n.stanford.edu/

a.k.a., http://www.stanford.edu/class/cs224n/

‚Ä¢ TAs, syllabus, help sessions/office hours, Ed (for all course questions/discussion)

‚Ä¢ Office hours start Wednesday afternoon!

‚Ä¢ Python/numpy and then PyTorch tutorials: First two Fridays. First is 2:30-3:30, Gates B03.

‚Ä¢ Slide PDFs uploaded before each lecture


4




What do we hope to teach? (A.k.a. ‚Äúlearning goals‚Äù)

5

1. The foundations of the effective modern methods for deep learning applied to NLP

‚Ä¢ Basics first, then key methods used in NLP in 2023: Word vectors, feed-forward 

networks, recurrent networks, attention, encoder-decoder models, transformers, 

large pre-trained language models, etc.

2. A big picture understanding of human languages and the difficulties in understanding 

and producing them via computers

3. An understanding of and ability to build systems (in PyTorch) for some of the major 

problems in NLP:

‚Ä¢ Word meaning, dependency parsing, machine translation, question answering


Course work and grading policy

‚Ä¢ 5 x 1-week Assignments: 6% + 4 x 12%: 54% 

‚Ä¢ HW1 is released today! Due next Tuesday! At 4:30 p.m.

‚Ä¢ Submitted to Gradescope in Canvas (i.e., using @stanford.edu email for your Gradescope account)

‚Ä¢ Final Default or Custom Course Project (1‚Äì3 people): 43%

‚Ä¢ Project proposal: 5%, milestone: 5%, poster or web summary: 3%, report: 30%

‚Ä¢ Participation: 3%

‚Ä¢ Guest lecture reactions, Ed, course evals, karma ‚Äì see website!

‚Ä¢ Late day policy

‚Ä¢ 6 free late days; afterwards, 1% off course grade per day late

‚Ä¢ Assignments not accepted more than 3 days late per assignment unless given permission in advance

‚Ä¢ Collaboration policy: Please read the website and the Honor Code! 

Understand allowed collaboration and how to document it: Don‚Äôt take code off the 

web; acknowledge working with other students; write your own assignment solutions

6


High-Level Plan for Assignments (to be completed individually!)

‚Ä¢ Ass1 is hopefully an easy on ramp ‚Äì a Jupyter/IPython Notebook

‚Ä¢ Ass2 is pure Python (numpy) but expects you to do (multivariate) calculus, so you really 

understand the basics

‚Ä¢ Ass3 introduces PyTorch, building a feed-forward network for dependency parsing

‚Ä¢ Ass4 and Ass5 use PyTorch on a GPU (Microsoft Azure)

‚Ä¢ Libraries like PyTorch, Tensorflow, and Jax are now the standard tools of DL

‚Ä¢ For Final Project, more details presented later, but you either:

‚Ä¢ Do the default project, which is a question answering system

‚Ä¢ Open-ended but an easier start; a good choice for many

‚Ä¢ Propose a custom final project, which we approve

‚Ä¢ You will receive feedback from a mentor (TA/prof/postdoc/PhD)

‚Ä¢ Can work in teams of 1‚Äì3; can use any language/packages

7


Lecture Plan

1. The course (10 mins)

2. Human language and word meaning (15 mins)

3. Word2vec introduction (15 mins)

4. Word2vec objective function gradients (25 mins)

5. Optimization basics (5 mins)

6. Looking at word vectors (10 mins or less)

8








Trained on text data, neural machine translation is quite good!









https://kiswahili.tuko.co.ke/


The SEC said, ‚ÄúMusk, your tweets are a 

blight.

They really could cost you your job,

if you don't stop all this tweeting at night.‚Äù

Then Musk cried, ‚ÄúWhy?

The tweets I wrote are not mean,

I don't use all-caps

and I'm sure that my tweets are clean.‚Äù

‚ÄúBut your tweets can move markets

and that's why we're sore.

You may be a genius and a billionaire,

but it doesn't give you the right to

How many users have signed up since the start of 2020?

SELECT count(id) FROM users 

WHERE created_at &gt; ‚Äò2020-01-01‚Äô

What is the average number of influencers each user is 

subscribed to?

SELECT avg(count) FROM ( SELECT user_id, count(*) 

FROM subscribers GROUP BY user_id ) 

AS avg_subscriptions_per_user

S: I broke the window.

Q: What did I break?

S: I gracefully saved the day.

Q: What did I gracefully save?

S: I gave John flowers.

Q: Who did I give flowers to?

S: I gave her a rose and a guitar.

Q: Who did I give a rose and a guitar to?

GPT-3: A first step on the path to foundation models



a

be

bore!‚Äù


ChatGPT: A recent, intriguing set of capabilities








ChatGPT: A recent, intriguing set of capabilities












ChatGPT: A recent, intriguing set of capabilities






How do we represent the meaning of a word?

16

Definition: meaning (Webster dictionary)

‚Ä¢

the idea that is represented by a word, phrase, etc.

‚Ä¢

the idea that a person wants to express by using words, signs, etc.

‚Ä¢

the idea that is expressed in a work of writing, art, etc.

Commonest linguistic way of thinking of meaning:

signifier (symbol) ‚ü∫ signified (idea or thing)

= denotational semantics

tree ‚ü∫ {üå≥, üå≤, üå¥, ‚Ä¶}


How do we have usable meaning in a computer?

17

Previously commonest NLP solution: Use, e.g., WordNet, a thesaurus containing lists of 

synonym sets and hypernyms (‚Äúis a‚Äù relationships) 



[Synset('procyonid.n.01'), 

Synset('carnivore.n.01'), 

Synset('placental.n.01'), 

Synset('mammal.n.01'), 

Synset('vertebrate.n.01'), 

Synset('chordate.n.01'), 

Synset('animal.n.01'), 

Synset('organism.n.01'), 

Synset('living_thing.n.01'), 

Synset('whole.n.02'), 

Synset('object.n.01'), 

Synset('physical_entity.n.01'), 

Synset('entity.n.01')]



noun: good 

noun: good, goodness 

noun: good, goodness 

noun: commodity, trade_good, good 

adj: good 

adj (sat): full, good 

adj: good 

adj (sat): estimable, good, honorable, respectable 

adj (sat): beneficial, good 

adj (sat): good 

adj (sat): good, just, upright 

‚Ä¶

adverb: well, good 

adverb: thoroughly, soundly, good

e.g., synonym sets containing ‚Äúgood‚Äù:

e.g., hypernyms of ‚Äúpanda‚Äù:

from nltk.corpus import wordnet as wn

poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}

for synset in wn.synsets("good"):

print("{}: {}".format(poses[synset.pos()], 

", ".join([l.name() for l in synset.lemmas()])))

from nltk.corpus import wordnet as wn

panda = wn.synset("panda.n.01")

hyper = lambda s: s.hypernyms()

list(panda.closure(hyper))


Problems with resources like WordNet

18

‚Ä¢

A useful resource but missing nuance:

‚Ä¢

e.g., ‚Äúproficient‚Äù is listed as a synonym for ‚Äúgood‚Äù

This is only correct in some contexts

‚Ä¢

Also, WordNet list offensive synonyms in some synonym sets without any 

coverage of the connotations or appropriateness of words

‚Ä¢

Missing new meanings of words:

‚Ä¢

e.g., wicked, badass, nifty, wizard, genius, ninja, bombest

‚Ä¢

Impossible to keep up-to-date!

‚Ä¢

Subjective

‚Ä¢

Requires human labor to create and adapt

‚Ä¢

Can‚Äôt be used to accurately compute word similarity (see following slides)


Representing words as discrete symbols

19

In traditional NLP, we regard words as discrete symbols: 

hotel, conference, motel ‚Äì a localist representation

Such symbols for words can be represented by one-hot vectors:

motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]

hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]

Vector dimension = number of words in vocabulary (e.g., 500,000+)

Means one 1, the rest 0s


Problem with words as discrete symbols

Example: in web search, if a user searches for ‚ÄúSeattle motel‚Äù, we would like to match 

documents containing ‚ÄúSeattle hotel‚Äù

But:

motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]

hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]

These two vectors are orthogonal

There is no natural notion of similarity for one-hot vectors!

Solution:

‚Ä¢ Could try to rely on WordNet‚Äôs list of synonyms to get similarity?

‚Ä¢ But it is well-known to fail badly: incompleteness, etc.

‚Ä¢ Instead: learn to encode similarity in the vectors themselves

Sec. 9.2.2

20


Representing words by their context

21

‚Ä¢ Distributional semantics: A word‚Äôs meaning is given

by the words that frequently appear close-by

‚Ä¢ ‚ÄúYou shall know a word by the company it keeps‚Äù (J. R. Firth 1957: 11)

‚Ä¢ One of the most successful ideas of modern statistical NLP!

‚Ä¢ When a word w appears in a text, its context is the set of words that appear nearby 

(within a fixed-size window).

‚Ä¢ We use the many contexts of w to build up a representation of w

‚Ä¶government debt problems turning into banking crises as happened in 2009‚Ä¶

‚Ä¶saying that Europe needs unified banking regulation to replace the hodgepodge‚Ä¶

‚Ä¶India has just given its banking system a shot in the arm‚Ä¶

These context words will represent banking




Word vectors

22

We will build a dense vector for each word, chosen so that it is similar to vectors of words 

that appear in similar contexts, measuring similarity as the vector dot (scalar) product

Note: word vectors are also called (word) embeddings or (neural) word representations

They are a distributed representation

banking  =

0.286

0.792

‚àí0.177

‚àí0.107

0.109

‚àí0.542

0.349

0.271

monetary  =

0.413

0.582

‚àí0.007

0.247

0.216

‚àí0.718

0.147

0.051








Word meaning as a neural word vector ‚Äì visualization



0.286

0.792

‚àí0.177

‚àí0.107

0.109

‚àí0.542

0.349

0.271

0.487

expect  =

23


3. Word2vec: Overview

Word2vec (Mikolov et al. 2013) is a framework for learning word vectors

Idea:

‚Ä¢ We have a large corpus (‚Äúbody‚Äù) of text: a long list of words

‚Ä¢ Every word in a fixed vocabulary is represented by a vector

‚Ä¢ Go through each position t in the text, which has a center word c and context 

(‚Äúoutside‚Äù) words o

‚Ä¢ Use the similarity of the word vectors for c and o to calculate the probability of o given 

c (or vice versa)

‚Ä¢ Keep adjusting the word vectors to maximize this probability

24


Word2Vec Overview

Example windows and process for computing ùëÉ ùë§ùë°+ùëó | ùë§ùë°

‚Ä¶

crises

banking

into

turning

problems

‚Ä¶

as

center word

at position t

outside context words

in window of size 2

outside context words

in window of size 2

ùëÉ ùë§ùë°+1 | ùë§ùë°

ùëÉ ùë§ùë°+2 | ùë§ùë°

ùëÉ ùë§ùë°‚àí1 | ùë§ùë°

ùëÉ ùë§ùë°‚àí2 | ùë§ùë°

25


Word2Vec Overview

Example windows and process for computing ùëÉ ùë§ùë°+ùëó | ùë§ùë°

‚Ä¶

crises

banking

into

turning

problems

‚Ä¶

as

center word

at position t

outside context words

in window of size 2

outside context words

in window of size 2

ùëÉ ùë§ùë°+1 | ùë§ùë°

ùëÉ ùë§ùë°+2 | ùë§ùë°

ùëÉ ùë§ùë°‚àí1 | ùë§ùë°

ùëÉ ùë§ùë°‚àí2 | ùë§ùë°

26


Word2vec: objective function

27

For each position ùë° = 1, ‚Ä¶ , ùëá, predict context words within a window of fixed size m, 

given center word ùë§ùë°. Data likelihood:

ùêø ùúÉ = ‡∑ë

ùë°=1

ùëá

‡∑ë

‚àíùëö‚â§ùëó‚â§ùëö

ùëó‚â†0

ùëÉ ùë§ùë°+ùëó | ùë§ùë°; ùúÉ

The objective function ùêΩ ùúÉ is the (average) negative log likelihood:

ùêΩ ùúÉ = ‚àí 1

ùëá log ùêø(ùúÉ) = ‚àí 1

ùëá ‡∑ç

ùë°=1

ùëá

‡∑ç

‚àíùëö‚â§ùëó‚â§ùëö

ùëó‚â†0

log ùëÉ ùë§ùë°+ùëó | ùë§ùë°; ùúÉ

Minimizing objective function ‚ü∫ Maximizing predictive accuracy

Likelihood =

ùúÉ is all variables 

to be optimized

sometimes called a cost or loss function


Word2vec: objective function

28

‚Ä¢

We want to minimize the objective function:

ùêΩ ùúÉ = ‚àí 1

ùëá ‡∑ç

ùë°=1

ùëá

‡∑ç

‚àíùëö‚â§ùëó‚â§ùëö

ùëó‚â†0

log ùëÉ ùë§ùë°+ùëó | ùë§ùë°; ùúÉ

‚Ä¢

Question: How to calculate ùëÉ ùë§ùë°+ùëó | ùë§ùë°; ùúÉ ?

‚Ä¢

Answer: We will use two vectors per word w:

‚Ä¢

ùë£ùë§ when w is a center word

‚Ä¢

ùë¢ùë§ when w is a context word

‚Ä¢

Then for a center word c and a context word o:

ùëÉ ùëú ùëê =

exp(ùë¢ùëúùëáùë£ùëê)

œÉùë§‚ààùëâ exp(ùë¢ùë§ùëá ùë£ùëê)


Word2Vec with Vectors

‚Ä¢

Example windows and process for computing ùëÉ ùë§ùë°+ùëó | ùë§ùë°

‚Ä¢

ùëÉ ùë¢ùëùùëüùëúùëèùëôùëíùëöùë† | ùë£ùëñùëõùë°ùëú short for P ùëùùëüùëúùëèùëôùëíùëöùë† | ùëñùëõùë°ùëú ; ùë¢ùëùùëüùëúùëèùëôùëíùëöùë†, ùë£ùëñùëõùë°ùëú, ùúÉ

‚Ä¶

crises

banking

into

turning

problems

‚Ä¶

as

center word

at position t

outside context words

in window of size 2

outside context words

in window of size 2

ùëÉ ùë¢ùëèùëéùëõùëòùëñùëõùëî |ùë£ùëñùëõùë°ùëú

ùëÉ ùë¢ùëêùëüùëñùë†ùëñùë† |ùë£ùëñùëõùë°ùëú

ùëÉ ùë¢ùë°ùë¢ùëõùëñùëõùëî | ùë£ùëñùëõùë°ùëú

ùëÉ ùë¢ùëùùëüùëúùëèùëôùëíùëöùë† | ùë£ùëñùëõùë°ùëú

29

All words vectors ùúÉ

appear in denominator


Word2vec: prediction function

ùëÉ ùëú ùëê =

exp(ùë¢ùëúùëáùë£ùëê)

œÉùë§‚ààùëâ exp(ùë¢ùë§ùëá ùë£ùëê)

‚Ä¢ This is an example of the softmax function ‚Ñùùëõ ‚Üí (0,1)ùëõ

softmax ùë•ùëñ =

exp(ùë•ùëñ)

œÉùëó=1

ùëõ

exp(ùë•ùëó) = ùëùùëñ

‚Ä¢ The softmax function maps arbitrary values ùë•ùëñ to a probability distribution ùëùùëñ

‚Ä¢ ‚Äúmax‚Äù because amplifies probability of largest ùë•ùëñ

‚Ä¢ ‚Äúsoft‚Äù because still assigns some probability to smaller ùë•ùëñ

‚Ä¢ Frequently used in Deep Learning

‚ë† Dot product compares similarity of o and c.

ùë¢ùëáùë£ = ùë¢. ùë£ = œÉùëñ=1

ùëõ

ùë¢ùëñùë£ùëñ

Larger dot product = larger probability

‚ë¢ Normalize over entire vocabulary 

to give probability distribution

30

‚ë° Exponentiation makes anything positive

Open 

region

But sort of a weird name 

because it returns a distribution!


To train the model: Optimize value of parameters to minimize loss

31

To train a model, we gradually adjust parameters to minimize a loss

‚Ä¢ Recall: ùúÉ represents all the 

model parameters, in one

long vector

‚Ä¢ In our case, with 

d-dimensional vectors and 

V-many words, we have ‚Üí

‚Ä¢ Remember: every word has 

two vectors

‚Ä¢ We optimize these parameters by walking down the gradient (see right figure)

‚Ä¢ We compute all vector gradients!








32

4.




33




34




35


5. Optimization: Gradient Descent

‚Ä¢ We have a cost function ùêΩ ùúÉ we want to minimize

‚Ä¢ Gradient Descent is an algorithm to minimize ùêΩ ùúÉ

‚Ä¢ Idea: for current value of ùúÉ, calculate gradient of ùêΩ ùúÉ , then take small step in direction 

of negative gradient. Repeat.



Note: Our 

objectives

may not 

be convex

like this ÔÅå

But life turns 

out to be 

okay ‚ò∫

36


‚Ä¢ Update equation (in matrix notation):

‚Ä¢ Update equation (for single parameter):

‚Ä¢ Algorithm:

Gradient Descent





ùõº = step size or learning rate



37


Stochastic Gradient Descent

‚Ä¢ Problem: ùêΩ ùúÉ is a function of all windows in the corpus (potentially billions!)

‚Ä¢ So                 is very expensive to compute

‚Ä¢ You would wait a very long time before making a single update!

‚Ä¢ Very bad idea for pretty much all neural nets!

‚Ä¢ Solution: Stochastic gradient descent (SGD)

‚Ä¢

Repeatedly sample windows, and update after each one

‚Ä¢ Algorithm:





38


Lecture Plan

1. The course (10 mins)

2. Human language and word meaning (15 mins)

3. Word2vec introduction (15 mins)

4. Word2vec objective function gradients (25 mins)

5. Optimization basics (5 mins)

6. Looking at word vectors (10 mins or less)

‚Ä¢

See Jupyter Notebook

39


40

