
Speech and Language Processing.

Daniel Jurafsky &amp; James H. Martin.

Copyright © 2023.

All

rights reserved.

Draft of January 7, 2023.

CHAPTER

7

Neural Networks and Neural

Language Models

“[M]achines of this character can behave in a very complicated manner when

the number of units is large.”

Alan Turing (1948) “Intelligent Machines”, page 6

Neural networks are a fundamental computational tool for language process-

ing, and a very old one. They are called neural because their origins lie in the

McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simpliﬁed model of the

human neuron as a kind of computing element that could be described in terms of

propositional logic. But the modern use in language processing no longer draws on

these early biological inspirations.

Instead, a modern neural network is a network of small computing units, each

of which takes a vector of input values and produces a single output value. In this

chapter we introduce the neural net applied to classiﬁcation. The architecture we

introduce is called a feedforward network because the computation proceeds iter-

feedforward

atively from one layer of units to the next. The use of modern neural nets is often

called deep learning, because modern networks are often deep (have many layers).

deep learning

Neural networks share much of the same mathematics as logistic regression. But

neural networks are a more powerful classiﬁer than logistic regression, and indeed a

minimal neural network (technically one with a single ‘hidden layer’) can be shown

to learn any function.

Neural net classiﬁers are different from logistic regression in another way. With

logistic regression, we applied the regression classiﬁer to many different tasks by

developing many rich kinds of feature templates based on domain knowledge. When

working with neural networks, it is more common to avoid most uses of rich hand-

derived features, instead building neural networks that take raw words as inputs

and learn to induce features as part of the process of learning to classify. We saw

examples of this kind of representation learning for embeddings in Chapter 6. Nets

that are very deep are particularly good at representation learning. For that reason

deep neural nets are the right tool for tasks that offer sufﬁcient data to learn features

automatically.

In this chapter we’ll introduce feedforward networks as classiﬁers, and also ap-

ply them to the simple task of language modeling: assigning probabilities to word

sequences and predicting upcoming words. In subsequent chapters we’ll introduce

many other aspects of neural models, such as recurrent neural networks (Chap-

ter 9), the Transformer (Chapter 10), and masked language modeling (Chapter 11).


2

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

7.1

Units

The building block of a neural network is a single computational unit. A unit takes

a set of real valued numbers as input, performs some computation on them, and

produces an output.

At its heart, a neural unit is taking a weighted sum of its inputs, with one addi-

tional term in the sum called a bias term. Given a set of inputs x1...xn, a unit has

bias term

a set of corresponding weights w1...wn and a bias b, so the weighted sum z can be

represented as:

z = b+

�

i

wixi

(7.1)

Often it’s more convenient to express this weighted sum using vector notation; recall

from linear algebra that a vector is, at heart, just a list or array of numbers. Thus

vector

we’ll talk about z in terms of a weight vector w, a scalar bias b, and an input vector

x, and we’ll replace the sum with the convenient dot product:

z = w·x+b

(7.2)

As deﬁned in Eq. 7.2, z is just a real valued number.

Finally, instead of using z, a linear function of x, as the output, neural units

apply a non-linear function f to z. We will refer to the output of this function as

the activation value for the unit, a. Since we are just modeling a single unit, the

activation

activation for the node is in fact the ﬁnal output of the network, which we’ll generally

call y. So the value y is deﬁned as:

y = a = f(z)

We’ll discuss three popular non-linear functions f() below (the sigmoid, the tanh,

and the rectiﬁed linear unit or ReLU) but it’s pedagogically convenient to start with

the sigmoid function since we saw it in Chapter 5:

sigmoid

y = σ(z) =

1

1+e−z

(7.3)

The sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output

into the range (0,1), which is useful in squashing outliers toward 0 or 1. And it’s

differentiable, which as we saw in Section ?? will be handy for learning.



Figure 7.1

The sigmoid function takes a real value and maps it to the range (0,1). It is

nearly linear around 0 but outlier values get squashed toward 0 or 1.

Substituting Eq. 7.2 into Eq. 7.3 gives us the output of a neural unit:

y = σ(w·x+b) =

1

1+exp(−(w·x+b))

(7.4)


7.1

•

UNITS

3

Fig. 7.2 shows a ﬁnal schematic of a basic neural unit. In this example the unit

takes 3 input values x1,x2, and x3, and computes a weighted sum, multiplying each

value by a weight (w1, w2, and w3, respectively), adds them to a bias term b, and then

passes the resulting sum through a sigmoid function to result in a number between 0

and 1.

x1

x2

x3



y

w1

w2

w3

∑

b

σ

+1

z

a

Figure 7.2

A neural unit, taking 3 inputs x1, x2, and x3 (and a bias b that we represent as a

weight for an input clamped at +1) and producing an output y. We include some convenient

intermediate variables: the output of the summation, z, and the output of the sigmoid, a. In

this case the output of the unit y is the same as a, but in deeper networks we’ll reserve y to

mean the ﬁnal output of the entire network, leaving a as the activation of an individual node.

Let’s walk through an example just to get an intuition. Let’s suppose we have a

unit with the following weight vector and bias:

w = [0.2,0.3,0.9]

b = 0.5

What would this unit do with the following input vector:

x = [0.5,0.6,0.1]

The resulting output y would be:

y = σ(w·x+b) =

1

1+e−(w·x+b) =

1

1+e−(.5∗.2+.6∗.3+.1∗.9+.5) =

1

1+e−0.87 = .70

In practice, the sigmoid is not commonly used as an activation function. A function

that is very similar but almost always better is the tanh function shown in Fig. 7.3a;

tanh

tanh is a variant of the sigmoid that ranges from -1 to +1:

y = tanh(z) = ez −e−z

ez +e−z

(7.5)

The simplest activation function, and perhaps the most commonly used, is the rec-

tiﬁed linear unit, also called the ReLU, shown in Fig. 7.3b. It’s just the same as z

ReLU

when z is positive, and 0 otherwise:

y = ReLU(z) = max(z,0)

(7.6)

These activation functions have different properties that make them useful for differ-

ent language applications or network architectures. For example, the tanh function

has the nice properties of being smoothly differentiable and mapping outlier values

toward the mean. The rectiﬁer function, on the other hand, has nice properties that


4

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS





(a)

(b)

Figure 7.3

The tanh and ReLU activation functions.

result from it being very close to linear. In the sigmoid or tanh functions, very high

values of z result in values of y that are saturated, i.e., extremely close to 1, and have

saturated

derivatives very close to 0. Zero derivatives cause problems for learning, because as

we’ll see in Section 7.6, we’ll train networks by propagating an error signal back-

wards, multiplying gradients (partial derivatives) from each layer of the network;

gradients that are almost 0 cause the error signal to get smaller and smaller until it is

too small to be used for training, a problem called the vanishing gradient problem.

vanishing

gradient

Rectiﬁers don’t have this problem, since the derivative of ReLU for high values of z

is 1 rather than very close to 0.

7.2

The XOR problem

Early in the history of neural networks it was realized that the power of neural net-

works, as with the real neurons that inspired them, comes from combining these

units into larger networks.

One of the most clever demonstrations of the need for multi-layer networks was

the proof by Minsky and Papert (1969) that a single neural unit cannot compute

some very simple functions of its input. Consider the task of computing elementary

logical functions of two inputs, like AND, OR, and XOR. As a reminder, here are

the truth tables for those functions:

AND

OR

XOR

x1 x2 y

x1 x2 y

x1 x2 y

0

0

0

0

0

0

0

0

0

0

1

0

0

1

1

0

1

1

1

0

0

1

0

1

1

0

1

1

1

1

1

1

1

1

1

0

This example was ﬁrst shown for the perceptron, which is a very simple neural

perceptron

unit that has a binary output and does not have a non-linear activation function. The

output y of a perceptron is 0 or 1, and is computed as follows (using the same weight

w, input x, and bias b as in Eq. 7.2):

y =

� 0, if w·x+b ≤ 0

1, if w·x+b &gt; 0

(7.7)


7.2

•

THE XOR PROBLEM

5

It’s very easy to build a perceptron that can compute the logical AND and OR

functions of its binary inputs; Fig. 7.4 shows the necessary weights.

x1

x2

+1

-1

1

1

x1

x2

+1

0

1

1

(a)

(b)

Figure 7.4

The weights w and bias b for perceptrons for computing logical functions. The

inputs are shown as x1 and x2 and the bias as a special node with value +1 which is multiplied

with the bias weight b. (a) logical AND, with weights w1 = 1 and w2 = 1 and bias weight

b = −1. (b) logical OR, with weights w1 = 1 and w2 = 1 and bias weight b = 0. These

weights/biases are just one from an inﬁnite number of possible sets of weights and biases that

would implement the functions.

It turns out, however, that it’s not possible to build a perceptron to compute

logical XOR! (It’s worth spending a moment to give it a try!)

The intuition behind this important result relies on understanding that a percep-

tron is a linear classiﬁer. For a two-dimensional input x1 and x2, the perceptron

equation, w1x1 +w2x2 +b = 0 is the equation of a line. (We can see this by putting

it in the standard linear format: x2 = (−w1/w2)x1 + (−b/w2).) This line acts as a

decision boundary in two-dimensional space in which the output 0 is assigned to all

decision

boundary

inputs lying on one side of the line, and the output 1 to all input points lying on the

other side of the line. If we had more than 2 inputs, the decision boundary becomes

a hyperplane instead of a line, but the idea is the same, separating the space into two

categories.

Fig. 7.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn

by one possible set of parameters for an AND and an OR classiﬁer. Notice that there

is simply no way to draw a line that separates the positive cases of XOR (01 and 10)

from the negative cases (00 and 11). We say that XOR is not a linearly separable

linearly

separable

function. Of course we could draw a boundary with a curve, or some other function,

but not a single line.

7.2.1

The solution: neural networks

While the XOR function cannot be calculated by a single perceptron, it can be cal-

culated by a layered network of perceptron units. Rather than see this with networks

of simple perceptrons, however, let’s see how to compute XOR using two layers of

ReLU-based units following Goodfellow et al. (2016). Fig. 7.6 shows a ﬁgure with

the input being processed by two layers of neural units. The middle layer (called

h) has two units, and the output layer (called y) has one unit. A set of weights and

biases are shown for each ReLU that correctly computes the XOR function.

Let’s walk through what happens with the input x = [0, 0]. If we multiply each

input value by the appropriate weight, sum, and then add the bias b, we get the vector

[0, -1], and we then apply the rectiﬁed linear transformation to give the output of the

h layer as [0, 0]. Now we once again multiply by the weights, sum, and add the

bias (0 in this case) resulting in the value 0. The reader should work through the

computation of the remaining 3 possible input pairs to see that the resulting y values

are 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1].


6

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

0

0

1

1

x1

x2

0

0

1

1

x1

x2

0

0

1

1

x1

x2

a)  x1 AND x2

b)  x1 OR x2

c)  x1 XOR x2

?

Figure 7.5

The functions AND, OR, and XOR, represented with input x1 on the x-axis and input x2 on the

y-axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no

way to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig

(2002).

x1

x2

h1

h2

y1

+1

1

-1

1

1

1

-2

0

1

+1

0

Figure 7.6

XOR solution after Goodfellow et al. (2016). There are three ReLU units, in

two layers; we’ve called them h1, h2 (h for “hidden layer”) and y1. As before, the numbers

on the arrows represent the weights w for each unit, and we represent the bias b as a weight

on a unit clamped to +1, with the bias weights/units in gray.

It’s also instructive to look at the intermediate results, the outputs of the two

hidden nodes h1 and h2. We showed in the previous paragraph that the h vector for

the inputs x = [0, 0] was [0, 0]. Fig. 7.7b shows the values of the h layer for all

4 inputs. Notice that hidden representations of the two input points x = [0, 1] and

x = [1, 0] (the two cases with XOR output = 1) are merged to the single point h =

[1, 0]. The merger makes it easy to linearly separate the positive and negative cases

of XOR. In other words, we can view the hidden layer of the network as forming a

representation of the input.

In this example we just stipulated the weights in Fig. 7.6. But for real examples

the weights for neural networks are learned automatically using the error backprop-

agation algorithm to be introduced in Section 7.6. That means the hidden layers will

learn to form useful representations. This intuition, that neural networks can auto-

matically learn useful representations of the input, is one of their key advantages,

and one that we will return to again and again in later chapters.


7.3

•

FEEDFORWARD NEURAL NETWORKS

7

0

0

1

1

x1

x2

a) The original x space

0

0

1

1

h1

h2

2

b) The new (linearly separable) h space

Figure 7.7

The hidden layer forming a new representation of the input. (b) shows the

representation of the hidden layer, h, compared to the original input representation x in (a).

Notice that the input point [0, 1] has been collapsed with the input point [1, 0], making it

possible to linearly separate the positive and negative cases of XOR. After Goodfellow et al.

(2016).

7.3

Feedforward Neural Networks

Let’s now walk through a slightly more formal presentation of the simplest kind of

neural network, the feedforward network. A feedforward network is a multilayer

feedforward

network

network in which the units are connected with no cycles; the outputs from units in

each layer are passed to units in the next higher layer, and no outputs are passed

back to lower layers. (In Chapter 9 we’ll introduce networks with cycles, called

recurrent neural networks.)

For historical reasons multilayer networks, especially feedforward networks, are

sometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer,

multi-layer

perceptrons

MLP

since the units in modern multilayer networks aren’t perceptrons (perceptrons are

purely linear, but modern networks are made up of units with non-linearities like

sigmoids), but at some point the name stuck.

Simple feedforward networks have three kinds of nodes: input units, hidden

units, and output units.

Fig. 7.8 shows a picture. The input layer x is a vector of simple scalar values just

as we saw in Fig. 7.2.

The core of the neural network is the hidden layer h formed of hidden units hi,

hidden layer

each of which is a neural unit as described in Section 7.1, taking a weighted sum of

its inputs and then applying a non-linearity. In the standard architecture, each layer

is fully-connected, meaning that each unit in each layer takes as input the outputs

fully-connected

from all the units in the previous layer, and there is a link between every pair of units

from two adjacent layers. Thus each hidden unit sums over all the input units.

Recall that a single hidden unit has as parameters a weight vector and a bias. We

represent the parameters for the entire hidden layer by combining the weight vector

and bias for each unit i into a single weight matrix W and a single bias vector b for

the whole layer (see Fig. 7.8). Each element Wji of the weight matrix W represents

the weight of the connection from the ith input unit xi to the jth hidden unit hj.

The advantage of using a single matrix W for the weights of the entire layer is

that now the hidden layer computation for a feedforward network can be done very

efﬁciently with simple matrix operations. In fact, the computation only has three


8

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

x1

x2

xn0

…

…

+1

b

…

U

W

input layer

hidden layer

output layer

h1

y1

y2

yn2

h2

h3

hn1

Figure 7.8

A simple 2-layer feedforward network, with one hidden layer, one output layer,

and one input layer (the input layer is usually not counted when enumerating layers).

steps: multiplying the weight matrix by the input vector x, adding the bias vector b,

and applying the activation function g (such as the sigmoid, tanh, or ReLU activation

function deﬁned above).

The output of the hidden layer, the vector h, is thus the following (for this exam-

ple we’ll use the sigmoid function σ as our activation function):

h = σ(Wx+b)

(7.8)

Notice that we’re applying the σ function here to a vector, while in Eq. 7.3 it was

applied to a scalar. We’re thus allowing σ(·), and indeed any activation function

g(·), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].

Let’s introduce some constants to represent the dimensionalities of these vectors

and matrices. We’ll refer to the input layer as layer 0 of the network, and have n0

represent the number of inputs, so x is a vector of real numbers of dimension n0,

or more formally x ∈ Rn0, a column vector of dimensionality [n0,1]. Let’s call the

hidden layer layer 1 and the output layer layer 2. The hidden layer has dimensional-

ity n1, so h ∈ Rn1 and also b ∈ Rn1 (since each hidden unit can take a different bias

value). And the weight matrix W has dimensionality W ∈ Rn1×n0, i.e. [n1,n0].

Take a moment to convince yourself that the matrix multiplication in Eq. 7.8 will

compute the value of each hj as σ

��n0

i=1 W jixi +bj

�

.

As we saw in Section 7.2, the resulting value h (for hidden but also for hypoth-

esis) forms a representation of the input. The role of the output layer is to take

this new representation h and compute a ﬁnal output. This output could be a real-

valued number, but in many cases the goal of the network is to make some sort of

classiﬁcation decision, and so we will focus on the case of classiﬁcation.

If we are doing a binary task like sentiment classiﬁcation, we might have a sin-

gle output node, and its scalar value y is the probability of positive versus negative

sentiment. If we are doing multinomial classiﬁcation, such as assigning a part-of-

speech tag, we might have one output node for each potential part-of-speech, whose

output value is the probability of that part-of-speech, and the values of all the output

nodes must sum to one. The output layer is thus a vector y that gives a probability

distribution across the output nodes.

Let’s see how this happens. Like the hidden layer, the output layer has a weight

matrix (let’s call it U), but some models don’t include a bias vector b in the output


7.3

•

FEEDFORWARD NEURAL NETWORKS

9

layer, so we’ll simplify by eliminating the bias vector in this example. The weight

matrix is multiplied by its input vector (h) to produce the intermediate output z:

z = Uh

There are n2 output nodes, so z ∈ Rn2, weight matrix U has dimensionality U ∈

Rn2×n1, and element Ui j is the weight from unit j in the hidden layer to unit i in the

output layer.

However, z can’t be the output of the classiﬁer, since it’s a vector of real-valued

numbers, while what we need for classiﬁcation is a vector of probabilities. There is

a convenient function for normalizing a vector of real values, by which we mean

normalizing

converting it to a vector that encodes a probability distribution (all the numbers lie

between 0 and 1 and sum to 1): the softmax function that we saw on page ?? of

softmax

Chapter 5. More generally for any vector z of dimensionality d, the softmax is

deﬁned as:

softmax(zi) =

exp(zi)

�d

j=1 exp(zj)

1 ≤ i ≤ d

(7.9)

Thus for example given a vector

z = [0.6,1.1,−1.5,1.2,3.2,−1.1],

(7.10)

the softmax function will normalize it to a probability distribution (shown rounded):

softmax(z) = [0.055,0.090,0.0067,0.10,0.74,0.010]

(7.11)

You may recall that we used softmax to create a probability distribution from a

vector of real-valued numbers (computed from summing weights times features) in

the multinomial version of logistic regression in Chapter 5.

That means we can think of a neural network classiﬁer with one hidden layer

as building a vector h which is a hidden layer representation of the input, and then

running standard multinomial logistic regression on the features that the network

develops in h. By contrast, in Chapter 5 the features were mainly designed by hand

via feature templates. So a neural network is like multinomial logistic regression,

but (a) with many layers, since a deep neural network is like layer after layer of lo-

gistic regression classiﬁers; (b) with those intermediate layers having many possible

activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we’ll

continue to use σ for convenience to mean any activation function); (c) rather than

forming the features by feature templates, the prior layers of the network induce the

feature representations themselves.

Here are the ﬁnal equations for a feedforward network with a single hidden layer,

which takes an input vector x, outputs a probability distribution y, and is parameter-

ized by weight matrices W and U and a bias vector b:

h = σ(Wx+b)

z = Uh

y = softmax(z)

(7.12)

And just to remember the shapes of all our variables, x ∈ Rn0, h ∈ Rn1, b ∈ Rn1,

W ∈ Rn1×n0, U ∈ Rn2×n1, and the output vector y ∈ Rn2. We’ll call this network a 2-

layer network (we traditionally don’t count the input layer when numbering layers,

but do count the output layer). So by this terminology logistic regression is a 1-layer

network.


10

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

7.3.1

More details on feedforward networks

Let’s now set up some notation to make it easier to talk about deeper networks of

depth more than 2. We’ll use superscripts in square brackets to mean layer num-

bers, starting at 0 for the input layer. So W[1] will mean the weight matrix for the

(ﬁrst) hidden layer, and b[1] will mean the bias vector for the (ﬁrst) hidden layer. n j

will mean the number of units at layer j. We’ll use g(·) to stand for the activation

function, which will tend to be ReLU or tanh for intermediate layers and softmax

for output layers. We’ll use a[i] to mean the output from layer i, and z[i] to mean the

combination of weights and biases W[i]a[i−1] + b[i]. The 0th layer is for inputs, so

we’ll refer to the inputs x more generally as a[0].

Thus we can re-represent our 2-layer net from Eq. 7.12 as follows:

z[1] = W[1]a[0] +b[1]

a[1] = g[1](z[1])

z[2] = W[2]a[1] +b[2]

a[2] = g[2](z[2])

ˆy = a[2]

(7.13)

Note that with this notation, the equations for the computation done at each layer are

the same. The algorithm for computing the forward step in an n-layer feedforward

network, given the input vector a[0] is thus simply:

for i in 1,...,n

z[i] = W[i] a[i−1] + b[i]

a[i] = g[i](z[i])

ˆy = a[n]

The activation functions g(·) are generally different at the ﬁnal layer. Thus g[2]

might be softmax for multinomial classiﬁcation or sigmoid for binary classiﬁcation,

while ReLU or tanh might be the activation function g(·) at the internal layers.

The need for non-linear activation functions

One of the reasons we use non-

linear activation functions for each layer in a neural network is that if we did not, the

resulting network is exactly equivalent to a single-layer network. Let’s see why this

is true. Imagine the ﬁrst two layers of such a network of purely linear layers:

z[1] = W[1]x+b[1]

z[2] = W[2]z[1] +b[2]

We can rewrite the function that the network is computing as:

z[2] = W[2]z[1] +b[2]

= W[2](W[1]x+b[1])+b[2]

= W[2]W[1]x+W[2]b[1] +b[2]

= W′x+b′

(7.14)

This generalizes to any number of layers. So without non-linear activation functions,

a multilayer network is just a notational variant of a single layer network with a

different set of weights, and we lose all the representational power of multilayer

networks.


7.4

•

FEEDFORWARD NETWORKS FOR NLP: CLASSIFICATION

11

Replacing the bias unit

In describing networks, we will often use a slightly sim-

pliﬁed notation that represents exactly the same function without referring to an ex-

plicit bias node b. Instead, we add a dummy node a0 to each layer whose value will

always be 1. Thus layer 0, the input layer, will have a dummy node a[0]

0 = 1, layer 1

will have a[1]

0 = 1, and so on. This dummy node still has an associated weight, and

that weight represents the bias value b. For example instead of an equation like

h = σ(Wx+b)

(7.15)

we’ll use:

h = σ(Wx)

(7.16)

But now instead of our vector x having n0 values: x = x1,...,xn0, it will have n0 +

1 values, with a new 0th dummy value x0 = 1: x = x0,...,xn0. And instead of

computing each hj as follows:

hj = σ

� n0

�

i=1

Wji xi +bj

�

,

(7.17)

we’ll instead use:

hj = σ

� n0

�

i=0

Wji xi

�

,

(7.18)

where the value Wj0 replaces what had been bj. Fig. 7.9 shows a visualization.

x1

x2

xn0

…

…

+1

b

…

U

W

h1

y1

y2

yn2

h2

h3

hn1

x1

x2

xn0

…

…

x0=1

…

U

W

h1

y1

y2

yn2

h2

h3

hn1

(a)

(b)

Figure 7.9

Replacing the bias node (shown in a) with x0 (b).

We’ll continue showing the bias as b when we go over the learning algorithm

in Section 7.6, but then we’ll switch to this simpliﬁed notation without explicit bias

terms for the rest of the book.

7.4

Feedforward networks for NLP: Classiﬁcation

Let’s see how to apply feedforward networks to NLP tasks! In this section we’ll

look at classiﬁcation tasks like sentiment analysis; in the next section we’ll introduce

neural language modeling.


12

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

Let’s begin with a simple 2-layer sentiment classiﬁer. You might imagine tak-

ing our logistic regression classiﬁer from Chapter 5, which corresponds to a 1-layer

network, and just adding a hidden layer. The input element xi could be scalar fea-

tures like those in Fig. ??, e.g., x1 = count(words ∈ doc), x2 = count(positive lexicon

words ∈ doc), x3 = 1 if “no” ∈ doc, and so on. And the output layer ˆy could have

two nodes (one each for positive and negative), or 3 nodes (positive, negative, neu-

tral), in which case ˆy1 would be the estimated probability of positive sentiment, ˆy2

the probability of negative and ˆy3 the probability of neutral. The resulting equations

would be just what we saw above for a 2-layer network (as always, we’ll continue

to use the σ to stand for any non-linearity, whether sigmoid, ReLU or other).

x = [x1,x2,...xN]

(each xi is a hand-designed feature)

h = σ(Wx+b)

z = Uh

ˆy = softmax(z)

(7.19)

Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this

hidden layer to our logistic regression classiﬁer allows the network to represent the

non-linear interactions between features. This alone might give us a better sentiment

classiﬁer.

U

W

[n⨉1]

Hidden layer

Output layer

softmax

[dh⨉n]

[dh⨉1]

[3⨉dh]

Input words

p(+)

h1

h2

h3

hdh

…

y1

^

y2

^

y3

^

x

h

y

Input layer 

n=3 features

[3⨉1]

x1

x2

x3

dessert

was

great

positive lexicon

words = 1

count of “no” 

= 0

wordcount

=3

p(-)

p(neut)

Figure 7.10

Feedforward network sentiment analysis using traditional hand-built features

of the input text.

Most applications of neural networks for NLP do something different, however.

Instead of using hand-built human-engineered features as the input to our classiﬁer,

we draw on deep learning’s ability to learn features from the data by representing

words as embeddings, like the word2vec or GloVe embeddings we saw in Chapter 6.

There are various ways to represent an input for classiﬁcation. One simple baseline

is to apply some sort of pooling function to the embeddings of all the words in the

pooling

input. For example, for a text with n input words/tokens w1,...,wn, we can turn the

n embeddings e(w1),...,e(wn) (each of dimensionality d) into a single embedding

also of dimensionality d by just summing the embeddings, or by taking their mean

(summing and then dividing by n):

xmean = 1

n

n

�

i=1

e(wi)

(7.20)


7.4

•

FEEDFORWARD NETWORKS FOR NLP: CLASSIFICATION

13

There are many other options, like taking the element-wise max. The element-wise

max of a set of n vectors is a new vector whose kth element is the max of the kth

elements of all the n vectors. Here are the equations for this classiﬁer assuming

mean pooling; the architecture is sketched in Fig. 7.11:

x = mean(e(w1),e(w2),...,e(wn))

h = σ(Wx+b)

z = Uh

ˆy = softmax(z)

(7.21)

U

W

[d⨉1]

Hidden layer

Output layer

softmax

[dh⨉d]

[dh⨉1]

[3⨉dh]

Input words

p(+)

embedding for

“great”

embedding for

“dessert”

h1

h2

h3

hdh

…

y1

^

y2

^

y3

^

x

h

y

Input layer 

pooled 

embedding

[3⨉1]

pooling

+

dessert

was

great

embedding for

“was”

p(-)

p(neut)

Figure 7.11

Feedforward network sentiment analysis using a pooled embedding of the in-

put words.

While Eq. 7.21 shows how to a classify a single example x, in practice we want

to efﬁciently classify an entire test set of m examples. We do this by vectoring the

process, just as we saw with logistic regression; instead of using for-loops to go

through each example, we’ll use matrix multiplication to do the entire computation

of an entire test set at once. First, we pack all the input feature vectors for each input

x into a single input matrix X, with each row i a row vector consisting of the pooled

embedding for input example x(i) (i.e., the vector x(i)). If the dimensionality of our

pooled input embedding is d, X will be a matrix of shape [m×d].

We will then need to slightly modify Eq. 7.21. X is of shape [m×d] and W is of

shape [dh ×d], so we’ll have to reorder how we multiply X and W and transpose W

so they correctly multiply to yield a matrix H of shape [m × dh]. The bias vector b

from Eq. 7.21 of shape [1×dh] will now have to be replicated into a matrix of shape

[m×dh]. We’ll need to similarly reorder the next step and transpose U. Finally, our

output matrix ˆY will be of shape [m × 3] (or more generally [m × do], where do is

the number of output classes), with each row i of our output matrix ˆY consisting of

the output vector ˆy(i).‘ Here are the ﬁnal equations for computing the output class

distribution for an entire test set:

H = σ(XW⊺ +b)

Z = HU⊺

ˆY = softmax(Z)

(7.22)


14

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

The idea of using word2vec or GloVe embeddings as our input representation—

and more generally the idea of relying on another algorithm to have already learned

an embedding representation for our input words—is called pretraining. Using

pretraining

pretrained embedding representations, whether simple static word embeddings like

word2vec or the much more powerful contextual embeddings we’ll introduce in

Chapter 11, is one of the central ideas of deep learning. (It’s also possible, how-

ever, to train the word embeddings as part of an NLP task; we’ll talk about how to

do this in Section 7.7 in the context of the neural language modeling task.)

7.5

Feedforward Neural Language Modeling

As our second application of feedforward networks, let’s consider language model-

ing: predicting upcoming words from prior word context. Neural language modeling

is an important NLP task in itself, and it plays a role in many important algorithms

for tasks like machine translation, summarization, speech recognition, grammar cor-

rection, and dialogue. We’ll describe simple feedforward neural language models,

ﬁrst introduced by Bengio et al. (2003). While modern neural language models use

more powerful architectures like the recurrent nets or transformer networks to be

introduced in Chapter 9, the feedforward language model introduces many of the

important concepts of neural language modeling.

Neural language models have many advantages over the n-gram language mod-

els of Chapter 3. Compared to n-gram models, neural language models can handle

much longer histories, can generalize better over contexts of similar words, and are

more accurate at word-prediction. On the other hand, neural net language models

are much more complex, are slower and need more energy to train, and are less in-

terpretable than n-gram models, so for many (especially smaller) tasks an n-gram

language model is still the right tool.

A feedforward neural language model (LM) is a feedforward network that takes

as input at time t a representation of some number of previous words (wt−1,wt−2,

etc.) and outputs a probability distribution over possible next words. Thus—like the

n-gram LM—the feedforward neural LM approximates the probability of a word

given the entire prior context P(wt|w1:t−1) by approximating based on the N − 1

previous words:

P(wt|w1,...,wt−1) ≈ P(wt|wt−N+1,...,wt−1)

(7.23)

In the following examples we’ll use a 4-gram example, so we’ll show a neural net to

estimate the probability P(wt = i|wt−3,wt−2,wt−1).

Neural language models represent words in this prior context by their embed-

dings, rather than just by their word identity as used in n-gram language models.

Using embeddings allows neural language models to generalize better to unseen

data. For example, suppose we’ve seen this sentence in training:

I have to make sure that the cat gets fed.

but have never seen the words “gets fed” after the word “dog”. Our test set has the

preﬁx “I forgot to make sure that the dog gets”. What’s the next word? An n-gram

language model will predict “fed” after “that the cat gets”, but not after “that the dog

gets”. But a neural LM, knowing that “cat” and “dog” have similar embeddings, will

be able to generalize from the “cat” context to assign a high enough probability to

“fed” even after seeing “dog”.


7.5

•

FEEDFORWARD NEURAL LANGUAGE MODELING

15

7.5.1

Forward inference in the neural language model

Let’s walk through forward inference or decoding for neural language models.

forward

inference

Forward inference is the task, given an input, of running a forward pass on the

network to produce a probability distribution over possible outputs, in this case next

words.

We ﬁrst represent each of the N previous words as a one-hot vector of length

|V|, i.e., with one dimension for each word in the vocabulary. A one-hot vector is

one-hot vector

a vector that has one element equal to 1—in the dimension corresponding to that

word’s index in the vocabulary— while all the other elements are set to zero. Thus

in a one-hot representation for the word “toothpaste”, supposing it is V5, i.e., index

5 in the vocabulary, x5 = 1, and xi = 0 ∀i ̸= 5, as shown here:

[0 0 0 0 1 0 0 ... 0 0 0 0]

1 2 3 4 5 6 7 ...

... |V|

The feedforward neural language model (sketched in Fig. 7.13) has a moving

window that can see N words into the past. We’ll let N equal 3, so the 3 words

wt−1, wt−2, and wt−3 are each represented as a one-hot vector. We then multiply

these one-hot vectors by the embedding matrix E. The embedding weight matrix E

has a column for each word, each a column vector of d dimensions, and hence has

dimensionality d ×|V|. Multiplying by a one-hot vector that has only one non-zero

element xi = 1 simply selects out the relevant column vector for word i, resulting in

the embedding for word i, as shown in Fig. 7.12.

E

|V|

d

1

|V|

d

1

=

✕

5

5

e5

Figure 7.12

Selecting the embedding vector for word V5 by multiplying the embedding

matrix E with a one-hot vector with a 1 in index 5.

The 3 resulting embedding vectors are concatenated to produce e, the embedding

layer. This is followed by a hidden layer and an output layer whose softmax produces

a probability distribution over words. For example y42, the value of output node 42,

is the probability of the next word wt being V42, the vocabulary word with index 42

(which is the word ‘ﬁsh’ in our example).

Here’s the algorithm in detail for our mini example:

1. Select three embeddings from E: Given the three previous words, we look

up their indices, create 3 one-hot vectors, and then multiply each by the em-

bedding matrix E. Consider wt−3. The one-hot vector for ‘for’ (index 35) is

multiplied by the embedding matrix E, to give the ﬁrst part of the ﬁrst hidden

layer, the embedding layer. Since each column of the input matrix E is an

embedding

layer

embedding for a word, and the input is a one-hot column vector xi for word

Vi, the embedding layer for input w will be Exi = ei, the embedding for word

i. We now concatenate the three embeddings for the three context words to

produce the embedding layer e.

2. Multiply by W: We multiply by W (and add b) and pass through the ReLU

(or other) activation function to get the hidden layer h.


16

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

U

W

embedding

 layer

3d⨉1

hidden

layer

output layer

softmax

dh⨉3d

dh⨉1

|V|⨉dh

input layer

one-hot 

vectors

E

|V|⨉3

d⨉|V|

p(do|…)

p(aardvark|…)

p(zebra|…)

p(fish|…)

|V|⨉1

E

E

h1

h2

y1

h3

hdh

…

…

y34

y|V|

…

00

1

0

0

1

|V|

35

0

0

1

0

0

1

|V|

451

0

0

1

0

0

1

|V|

992

0

0

…

…

y42

y35102

^

^

^

^

^

h

e

x

y

for

all

the

?

thanks

and

…

wt-3

wt-2

wt-1

wt

…

Figure 7.13

Forward inference in a feedforward neural language model. At each timestep

t the network computes a d-dimensional embedding for each context word (by multiplying a

one-hot vector by the embedding matrix E), and concatenates the 3 resulting embeddings to

get the embedding layer e. The embedding vector e is multiplied by a weight matrix W and

then an activation function is applied element-wise to produce the hidden layer h, which is

then multiplied by another weight matrix U. Finally, a softmax output layer predicts at each

node i the probability that the next word wt will be vocabulary word Vi.

3. Multiply by U: h is now multiplied by U

4. Apply softmax: After the softmax, each node i in the output layer estimates

the probability P(wt = i|wt−1,wt−2,wt−3)

In summary, the equations for a neural language model with a window size of 3,

given one-hot input vectors for each input context word, are:

e = [Ext−3;Ext−2;Ext−1]

h = σ(We+b)

z = Uh

ˆy = softmax(z)

(7.24)

Note that we formed the embedding layer e by concatenating the 3 embeddings

for the three context vectors; we’ll often use semicolons to mean concatenation of

vectors.

In the next section we’ll introduce a general algorithm for training neural net-

works, and then return to how to speciﬁcally train the neural language model in

Section 7.7.


7.6

•

TRAINING NEURAL NETS

17

7.6

Training Neural Nets

A feedforward neural net is an instance of supervised machine learning in which we

know the correct output y for each observation x. What the system produces, via

Eq. 7.13, is ˆy, the system’s estimate of the true y. The goal of the training procedure

is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training

observation as close as possible to the true y.

In general, we do all this by drawing on the methods we introduced in Chapter 5

for logistic regression, so the reader should be comfortable with that chapter before

proceeding.

First, we’ll need a loss function that models the distance between the system

output and the gold output, and it’s common to use the loss function used for logistic

regression, the cross-entropy loss.

Second, to ﬁnd the parameters that minimize this loss function, we’ll use the

gradient descent optimization algorithm introduced in Chapter 5.

Third, gradient descent requires knowing the gradient of the loss function, the

vector that contains the partial derivative of the loss function with respect to each

of the parameters. In logistic regression, for each observation we could directly

compute the derivative of the loss function with respect to an individual w or b. But

for neural networks, with millions of parameters in many layers, it’s much harder to

see how to compute the partial derivative of some weight in layer 1 when the loss

is attached to some much later layer. How do we partial out the loss over all those

intermediate layers? The answer is the algorithm called error backpropagation or

backward differentiation.

7.6.1

Loss function

The cross-entropy loss that is used in neural networks is the same one we saw for

cross-entropy

loss

logistic regression. If the neural network is being used as a binary classiﬁer, with

the sigmoid at the ﬁnal layer, the loss function is the same logistic regression loss

we saw in Eq. ??:

LCE(ˆy,y) = −log p(y|x) = −[ylog ˆy+(1−y)log(1− ˆy)]

(7.25)

If we are using the network to classify into 3 or more classes, the loss function is

exactly the same as the loss for multinomial regression that we saw in Chapter 5 on

page ??. Let’s brieﬂy summarize the explanation here for convenience. First, when

we have more than 2 classes we’ll need to represent both y and ˆy as vectors. Let’s

assume we’re doing hard classiﬁcation, where only one class is the correct one.

The true label y is then a vector with K elements, each corresponding to a class,

with yc = 1 if the correct class is c, with all other elements of y being 0. Recall that

a vector like this, with one value equal to 1 and the rest 0, is called a one-hot vector.

And our classiﬁer will produce an estimate vector with K elements ˆy, each element

ˆyk of which represents the estimated probability p(yk = 1|x).

The loss function for a single example x is the negative sum of the logs of the K

output classes, each weighted by their probability yk:

LCE(ˆy,y) = −

K

�

k=1

yk log ˆyk

(7.26)

We can simplify this equation further; let’s ﬁrst rewrite the equation using the func-

tion 1{} which evaluates to 1 if the condition in the brackets is true and to 0 oth-


18

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

erwise. This makes it more obvious that the terms in the sum in Eq. 7.26 will be 0

except for the term corresponding to the true class for which yk = 1:

LCE(ˆy,y) = −

K

�

k=1

1{yk = 1}log ˆyk

In other words, the cross-entropy loss is simply the negative log of the output proba-

bility corresponding to the correct class, and we therefore also call this the negative

log likelihood loss:

negative log

likelihood loss

LCE(ˆy,y) = −log ˆyc

(where c is the correct class)

(7.27)

Plugging in the softmax formula from Eq. 7.9, and with K the number of classes:

LCE(ˆy,y) = −log

exp(zc)

�K

j=1 exp(zj)

(where c is the correct class)

(7.28)

7.6.2

Computing the Gradient

How do we compute the gradient of this loss function? Computing the gradient

requires the partial derivative of the loss function with respect to each parameter.

For a network with one weight layer and sigmoid output (which is what logistic

regression is), we could simply use the derivative of the loss that we used for logistic

regression in Eq. 7.29 (and derived in Section ??):

∂LCE(ˆy,y)

∂wj

= (ˆy−y)xj

= (σ(w·x+b)−y)xj

(7.29)

Or for a network with one weight layer and softmax output (=multinomial logistic

regression), we could use the derivative of the softmax loss from Eq. ??, shown for

a particular weight wk and input xi

∂LCE(ˆy,y)

∂wk,i

= −(yk − ˆyk)xi

= −(yk − p(yk = 1|x))xi

= −

�

yk −

exp(wk ·x+bk)

�K

j=1 exp(wj ·x+bj)

�

xi

(7.30)

But these derivatives only give correct updates for one weight layer: the last one!

For deep networks, computing the gradients for each weight is much more complex,

since we are computing the derivative with respect to weight parameters that appear

all the way back in the very early layers of the network, even though the loss is

computed only at the very end of the network.

The solution to computing this gradient is an algorithm called error backprop-

agation or backprop (Rumelhart et al., 1986). While backprop was invented spe-

error back-

propagation

cially for neural networks, it turns out to be the same as a more general procedure

called backward differentiation, which depends on the notion of computation

graphs. Let’s see how that works in the next subsection.


7.6

•

TRAINING NEURAL NETS

19

7.6.3

Computation Graphs

A computation graph is a representation of the process of computing a mathematical

expression, in which the computation is broken down into separate operations, each

of which is modeled as a node in a graph.

Consider computing the function L(a,b,c) = c(a+2b). If we make each of the

component addition and multiplication operations explicit, and add names (d and e)

for the intermediate outputs, the resulting series of computations is:

d = 2∗b

e = a+d

L = c∗e

We can now represent this as a graph, with nodes for each operation, and di-

rected edges showing the outputs from each operation as the inputs to the next, as

in Fig. 7.14. The simplest use of computation graphs is to compute the value of

the function with some given inputs. In the ﬁgure, we’ve assumed the inputs a = 3,

b = 1, c = −2, and we’ve shown the result of the forward pass to compute the re-

sult L(3,1,−2) = −10. In the forward pass of a computation graph, we apply each

operation left to right, passing the outputs of each computation as the input to the

next node.

e=a+d

d = 2b

L=ce

a=3

b=1

c=-2

e=5

d=2

L=-10

forward pass

a

b

c

Figure 7.14

Computation graph for the function L(a,b,c) = c(a+2b), with values for input

nodes a = 3, b = 1, c = −2, showing the forward pass computation of L.

7.6.4

Backward differentiation on computation graphs

The importance of the computation graph comes from the backward pass, which

is used to compute the derivatives that we’ll need for the weight update. In this

example our goal is to compute the derivative of the output function L with respect

to each of the input variables, i.e., ∂L

∂a, ∂L

∂b, and ∂L

∂c . The derivative ∂L

∂a tells us how

much a small change in a affects L.

Backwards differentiation makes use of the chain rule in calculus, so let’s re-

chain rule

mind ourselves of that. Suppose we are computing the derivative of a composite

function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect

to v(x) times the derivative of v(x) with respect to x:

d f

dx = du

dv · dv

dx

(7.31)

The chain rule extends to more than two functions. If computing the derivative of a

composite function f(x) = u(v(w(x))), the derivative of f(x) is:

d f

dx = du

dv · dv

dw · dw

dx

(7.32)


20

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

The intuition of backward differentiation is to pass gradients back from the ﬁnal

node to all the nodes in the graph. Fig. 7.15 shows part of the backward computation

at one node e. Each node takes an upstream gradient that is passed in from its parent

node to the right, and for each of its inputs computes a local gradient (the gradient

of its output with respect to its input), and uses the chain rule to multiply these two

to compute a downstream gradient to be passed on to the next earlier node.

e

d

L

e

d

∂L

∂d

∂L

∂e

=

∂e

∂d

∂L

∂e

∂e

∂d

upstream

 gradient

downstream

 gradient

local

 gradient

Figure 7.15

Each node (like e here) takes an upstream gradient, multiplies it by the local

gradient (the gradient of its output with respect to its input), and uses the chain rule to compute

a downstream gradient to be passed on to a prior node. A node may have multiple local

gradients if it has multiple inputs.

Let’s now compute the 3 derivatives we need. Since in the computation graph

L = ce, we can directly compute the derivative ∂L

∂c :

∂L

∂c = e

(7.33)

For the other two, we’ll need to use the chain rule:

∂L

∂a = ∂L

∂e

∂e

∂a

∂L

∂b = ∂L

∂e

∂e

∂d

∂d

∂b

(7.34)

Eq. 7.34 and Eq. 7.33 thus require ﬁve intermediate derivatives: ∂L

∂e , ∂L

∂c , ∂e

∂a, ∂e

∂d , and

∂d

∂b, which are as follows (making use of the fact that the derivative of a sum is the

sum of the derivatives):

L = ce :

∂L

∂e = c, ∂L

∂c = e

e = a+d :

∂e

∂a = 1, ∂e

∂d = 1

d = 2b :

∂d

∂b = 2

In the backward pass, we compute each of these partials along each edge of the

graph from right to left, using the chain rule just as we did above. Thus we begin by

computing the downstream gradients from node L, which are ∂L

∂e and ∂L

∂c . For node e,

we then multiply this upstream gradient ∂L

∂e by the local gradient (the gradient of the

output with respect to the input), ∂e

∂d to get the output we send back to node d: ∂L

∂d .

And so on, until we have annotated the graph all the way to all the input variables.

The forward pass conveniently already will have computed the values of the forward

intermediate variables we need (like d and e) to compute these derivatives. Fig. 7.16

shows the backward pass.


7.6

•

TRAINING NEURAL NETS

21

e=d+a

d = 2b

L=ce

a=3

b=1

e=5

d=2

L=-10

 

a

b

c

∂L=5

∂c

∂L =-2

∂e

∂e =1

∂d

∂d =2

∂b

∂e =1

∂a

backward pass

c=-2

∂L =-2

∂e

∂L =5

∂c

∂L

∂d

=-2

∂e

∂d

∂L

∂e

=

∂L

∂a

=-2

∂e

∂a

∂L

∂e

=

∂L

∂b

=-4

∂d

∂b

∂L

∂d

=

Figure 7.16

Computation graph for the function L(a,b,c) = c(a+2b), showing the backward pass computa-

tion of ∂L

∂a , ∂L

∂b , and ∂L

∂c .

Backward differentiation for a neural network

Of course computation graphs for real neural networks are much more complex.

Fig. 7.17 shows a sample computation graph for a 2-layer neural network with n0 =

2, n1 = 2, and n2 = 1, assuming binary classiﬁcation and hence using a sigmoid

output unit for simplicity. The function that the computation graph is computing is:

z[1] = W[1]x+b[1]

a[1] = ReLU(z[1])

z[2] = W[2]a[1] +b[2]

a[2] = σ(z[2])

ˆy = a[2]

(7.35)

For the backward pass we’ll also need to compute the loss L. The loss function

for binary sigmoid output from Eq. 7.25 is

LCE(ˆy,y) = −[ylog ˆy+(1−y)log(1− ˆy)]

(7.36)

Our output ˆy = a[2], so we can rephrase this as

LCE(a[2],y) = −

�

yloga[2] +(1−y)log(1−a[2])

�

(7.37)

The weights that need updating (those for which we need to know the partial

derivative of the loss function) are shown in teal. In order to do the backward pass,

we’ll need to know the derivatives of all the functions in the graph. We already saw

in Section ?? the derivative of the sigmoid σ:

dσ(z)

dz

= σ(z)(1−σ(z))

(7.38)

We’ll also need the derivatives of each of the other activation functions. The

derivative of tanh is:

d tanh(z)

dz

= 1−tanh2(z)

(7.39)


22

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

z[2] = 

+

a[2] = σ

 

a[1] = 

ReLU

z[1] = 

+

b[1]

*

*

*

*

x1

x2

a[1] = 

ReLU

z[1] = 

+

b[1]

*

*

w[2]

11

w[1]

11

w[1]

12

w[1]

21

w[1]

22

b[2]

w[2]

12

L (a[2],y)

1

2

1

1

1

2

2

Figure 7.17

Sample computation graph for a simple 2-layer neural net (= 1 hidden layer) with two input units

and 2 hidden units. We’ve adjusted the notation a bit to avoid long equations in the nodes by just mentioning

the function that is being computed, and the resulting variable name. Thus the * to the right of node w[1]

11 means

that w[1]

11 is to be multiplied by x1, and the node z[1] = + means that the value of z[1] is computed by summing

the three nodes that feed into it (the two products, and the bias term b[1]

i ).

The derivative of the ReLU is

d ReLU(z)

dz

=

� 0 for z &lt; 0

1 for z ≥ 0

(7.40)

We’ll give the start of the computation, computing the derivative of the loss

function L with respect to z, or ∂L

∂z (and leaving the rest of the computation as an

exercise for the reader). By the chain rule:

∂L

∂z = ∂L

∂a[2]

∂a[2]

∂z

(7.41)

So let’s ﬁrst compute

∂L

∂a[2] , taking the derivative of Eq. 7.37, repeated here:

LCE(a[2],y) = −

�

yloga[2] +(1−y)log(1−a[2])

�

∂L

∂a[2] = −

��

y∂ log(a[2])

∂a[2]

�

+(1−y)∂ log(1−a[2])

∂a[2]

�

= −

��

y 1

a[2]

�

+(1−y)

1

1−a[2] (−1)

�

= −

� y

a[2] + y−1

1−a[2]

�

(7.42)

Next, by the derivative of the sigmoid:

∂a[2]

∂z

= a[2](1−a[2])


7.7

•

TRAINING THE NEURAL LANGUAGE MODEL

23

Finally, we can use the chain rule:

∂L

∂z

=

∂L

∂a[2]

∂a[2]

∂z

= −

� y

a[2] + y−1

1−a[2]

�

a[2](1−a[2])

= a[2] −y

(7.43)

Continuing the backward computation of the gradients (next by passing the gra-

dients over b[2]

1 and the two product nodes, and so on, back to all the teal nodes), is

left as an exercise for the reader.

7.6.5

More details on learning

Optimization in neural networks is a non-convex optimization problem, more com-

plex than for logistic regression, and for that and other reasons there are many best

practices for successful learning.

For logistic regression we can initialize gradient descent with all the weights and

biases having the value 0. In neural networks, by contrast, we need to initialize the

weights with small random numbers. It’s also helpful to normalize the input values

to have 0 mean and unit variance.

Various forms of regularization are used to prevent overﬁtting. One of the most

important is dropout: randomly dropping some units and their connections from

dropout

the network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning

of hyperparameters is also important. The parameters of a neural network are the

hyperparameter

weights W and biases b; those are learned by gradient descent. The hyperparameters

are things that are chosen by the algorithm designer; optimal values are tuned on a

devset rather than by gradient descent learning on the training set. Hyperparameters

include the learning rate η, the mini-batch size, the model architecture (the number

of layers, the number of hidden nodes per layer, the choice of activation functions),

how to regularize, and so on. Gradient descent itself also has many architectural

variants such as Adam (Kingma and Ba, 2015).

Finally, most modern neural networks are built using computation graph for-

malisms that make it easy and natural to do gradient computation and parallelization

on vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)

and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested

reader should consult a neural network textbook for further details; some sugges-

tions are at the end of the chapter.

7.7

Training the neural language model

Now that we’ve seen how to train a generic neural net, let’s talk about the architec-

ture for training a neural language model, setting the parameters θ = E,W,U,b.

For some tasks, it’s ok to freeze the embedding layer E with initial word2vec val-

freeze

ues. Freezing means we use word2vec or some other pretraining algorithm to com-

pute the initial embedding matrix E, and then hold it constant while we only modify

W, U, and b, i.e., we don’t update E during language model training. However, often

we’d like to learn the embeddings simultaneously with training the network. This is


24

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

useful when the task the network is designed for (like sentiment classiﬁcation, trans-

lation, or parsing) places strong constraints on what makes a good representation for

words.

Let’s see how to train the entire model including E, i.e. to set all the parameters

θ = E,W,U,b. We’ll do this via gradient descent (Fig. ??), using error backpropa-

gation on the computation graph to compute the gradient. Training thus not only sets

the weights W and U of the network, but also as we’re predicting upcoming words,

we’re learning the embeddings E for each word that best predict upcoming words.

U

W

embedding

 layer

3d⨉1

hidden

layer

output layer

softmax

dh⨉3d

dh⨉1

|V|⨉dh

input layer

one-hot 

vectors

E

|V|⨉3

d⨉|V|

p(do|…)

p(aardvark|…)

p(zebra|…)

p(fish|…)

|V|⨉1

E

E

h1

h2

y1

h3

hdh

…

…

y34

y|V|

…

00

1

0

0

1

|V|

35

0

0

1

0

0

1

|V|

451

0

0

1

0

0

1

|V|

992

0

0

…

…

y42

y35102

^

^

^

^

^

h

e

x

y

for

all

the

fish

thanks

and

…

wt-3

wt-2

wt-1

wt

…

L = −log P(fish | for, all, the)

wt=fish

Figure 7.18

Learning all the way back to embeddings. Again, the embedding matrix E is

shared among the 3 context words.

Fig. 7.18 shows the set up for a window size of N=3 context words. The input x

consists of 3 one-hot vectors, fully connected to the embedding layer via 3 instanti-

ations of the embedding matrix E. We don’t want to learn separate weight matrices

for mapping each of the 3 previous words to the projection layer. We want one single

embedding dictionary E that’s shared among these three. That’s because over time,

many different words will appear as wt−2 or wt−1, and we’d like to just represent

each word with one vector, whichever context position it appears in. Recall that the

embedding weight matrix E has a column for each word, each a column vector of d

dimensions, and hence has dimensionality d ×|V|.

Generally training proceeds by taking as input a very long text, concatenating all

the sentences, starting with random weights, and then iteratively moving through the

text predicting each word wt. At each word wt, we use the cross-entropy (negative

log likelihood) loss. Recall that the general form for this (repeated from Eq. 7.27 is:

LCE(ˆy,y) = −log ˆyi,

(where i is the correct class)

(7.44)

For language modeling, the classes are the words in the vocabulary, so ˆyi here means

the probability that the model assigns to the correct next word wt:

LCE = −log p(wt|wt−1,...,wt−n+1)

(7.45)


7.8

•

SUMMARY

25

The parameter update for stochastic gradient descent for this loss from step s to s+1

is then:

θ s+1 = θ s −η ∂ [−log p(wt|wt−1,...,wt−n+1)]

∂θ

(7.46)

This gradient can be computed in any standard neural network framework which

will then backpropagate through θ = E,W,U,b.

Training the parameters to minimize loss will result both in an algorithm for

language modeling (a word predictor) but also a new set of embeddings E that can

be used as word representations for other tasks.

7.8

Summary

• Neural networks are built out of neural units, originally inspired by human

neurons but now simply an abstract computational device.

• Each neural unit multiplies input values by a weight vector, adds a bias, and

then applies a non-linear activation function like sigmoid, tanh, or rectiﬁed

linear unit.

• In a fully-connected, feedforward network, each unit in layer i is connected

to each unit in layer i+1, and there are no cycles.

• The power of neural networks comes from the ability of early layers to learn

representations that can be utilized by later layers in the network.

• Neural networks are trained by optimization algorithms like gradient de-

scent.

• Error backpropagation, backward differentiation on a computation graph,

is used to compute the gradients of the loss function for a network.

• Neural language models use a neural network as a probabilistic classiﬁer, to

compute the probability of the next word given the previous n words.

• Neural language models can use pretrained embeddings, or can learn embed-

dings from scratch in the process of language modeling.

Bibliographical and Historical Notes

The origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-

loch and Pitts, 1943), a simpliﬁed model of the human neuron as a kind of com-

puting element that could be described in terms of propositional logic. By the late

1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and

Bernard Widrow at Stanford) developed research into neural networks; this phase

saw the development of the perceptron (Rosenblatt, 1958), and the transformation

of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).

The ﬁeld of neural networks declined after it was shown that a single perceptron

unit was unable to model functions as simple as XOR (Minsky and Papert, 1969).

While some small amount of work continued during the next two decades, a major

revival for the ﬁeld didn’t come until the 1980s, when practical tools for building

deeper networks like error backpropagation became widespread (Rumelhart et al.,


26

CHAPTER 7

•

NEURAL NETWORKS AND NEURAL LANGUAGE MODELS

1986). During the 1980s a wide variety of neural network and related architec-

tures were developed, particularly for applications in psychology and cognitive sci-

ence (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart

and McClelland 1986a, Elman 1990), for which the term connectionist or paral-

connectionist

lel distributed processing was often used (Feldman and Ballard 1982, Smolensky

1988). Many of the principles and techniques developed in this period are foun-

dational to modern work, including the ideas of distributed representations (Hinton,

1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality

(Smolensky, 1990).

By the 1990s larger neural networks began to be applied to many practical lan-

guage processing tasks as well, like handwriting recognition (LeCun et al. 1989) and

speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements

in computer hardware and advances in optimization and training techniques made it

possible to train even larger and deeper networks, leading to the modern term deep

learning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in

Chapter 9 and Chapter 16.

There are a number of excellent books on the subject. Goldberg (2017) has

superb coverage of neural networks for natural language processing. For neural

networks in general see Goodfellow et al. (2016) and Nielsen (2015).


Bibliographical and Historical Notes

27

Abadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen,

C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,

S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Is-

ard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Leven-

berg, D. Man´e, R. Monga, S. Moore, D. Murray, C. Olah,

M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Tal-

war, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas,

O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu,

and X. Zheng. 2015. TensorFlow: Large-scale machine

learning on heterogeneous systems. Software available

from tensorﬂow.org.

Bengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003.

A neural probabilistic language model. JMLR, 3:1137–

1155.

Bengio, Y., P. Lamblin, D. Popovici, and H. Larochelle.

2007.

Greedy layer-wise training of deep networks.

NeurIPS.

Elman, J. L. 1990. Finding structure in time. Cognitive sci-

ence, 14(2):179–211.

Feldman, J. A. and D. H. Ballard. 1982. Connectionist mod-

els and their properties. Cognitive Science, 6:205–254.

Goldberg, Y. 2017. Neural Network Methods for Natural

Language Processing, volume 10 of Synthesis Lectures

on Human Language Technologies. Morgan &amp; Claypool.

Goodfellow, I., Y. Bengio, and A. Courville. 2016. Deep

Learning. MIT Press.

Hinton, G. E. 1986. Learning distributed representations of

concepts. COGSCI.

Hinton, G. E., S. Osindero, and Y.-W. Teh. 2006. A fast

learning algorithm for deep belief nets. Neural computa-

tion, 18(7):1527–1554.

Hinton, G. E., N. Srivastava, A. Krizhevsky, I. Sutskever, and

R. R. Salakhutdinov. 2012. Improving neural networks

by preventing co-adaptation of feature detectors. ArXiv

preprint arXiv:1207.0580.

Kingma, D. and J. Ba. 2015. Adam: A method for stochastic

optimization. ICLR 2015.

LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E.

Howard, W. Hubbard, and L. D. Jackel. 1989. Backprop-

agation applied to handwritten zip code recognition. Neu-

ral computation, 1(4):541–551.

McClelland, J. L. and J. L. Elman. 1986. The TRACE model

of speech perception. Cognitive Psychology, 18:1–86.

McCulloch, W. S. and W. Pitts. 1943. A logical calculus of

ideas immanent in nervous activity. Bulletin of Mathe-

matical Biophysics, 5:115–133.

Minsky, M. and S. Papert. 1969. Perceptrons. MIT Press.

Morgan, N. and H. Bourlard. 1990.

Continuous speech

recognition using multilayer perceptrons with hidden

markov models. ICASSP.

Nielsen, M. A. 2015. Neural networks and Deep learning.

Determination Press USA.

Paszke, A., S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-

Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.

2017. Automatic differentiation in pytorch. NIPS-W.

Rosenblatt, F. 1958. The perceptron: A probabilistic model

for information storage and organization in the brain. Psy-

chological review, 65(6):386–408.

Rumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986.

Learning internal representations by error propagation. In

D. E. Rumelhart and J. L. McClelland, editors, Parallel

Distributed Processing, volume 2, pages 318–362. MIT

Press.

Rumelhart, D. E. and J. L. McClelland. 1986a. On learning

the past tense of English verbs. In D. E. Rumelhart and

J. L. McClelland, editors, Parallel Distributed Process-

ing, volume 2, pages 216–271. MIT Press.

Rumelhart, D. E. and J. L. McClelland, editors. 1986b. Par-

allel Distributed Processing. MIT Press.

Russell, S. and P. Norvig. 2002. Artiﬁcial Intelligence: A

Modern Approach, 2nd edition. Prentice Hall.

Smolensky, P. 1988. On the proper treatment of connection-

ism. Behavioral and brain sciences, 11(1):1–23.

Smolensky, P. 1990. Tensor product variable binding and

the representation of symbolic structures in connectionist

systems. Artiﬁcial intelligence, 46(1-2):159–216.

Srivastava, N., G. E. Hinton, A. Krizhevsky, I. Sutskever,

and R. R. Salakhutdinov. 2014.

Dropout:

a simple

way to prevent neural networks from overﬁtting. JMLR,

15(1):1929–1958.

Widrow, B. and M. E. Hoff. 1960. Adaptive switching cir-

cuits. IRE WESCON Convention Record, volume 4.

