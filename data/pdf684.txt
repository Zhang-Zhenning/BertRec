


Published in

Towards Data Science



Nov 10, 2019

·

11 min read

·

Save

Probability Learning VI: Hidden Markov Models

Markov Chains and Markov Models made EASY

Hello again friends!

Bayes’ Theorem

How Bayes’ Theorem is applied in Machine Learning

Maximum Likelihood

 The Math Behind Bayes

Naive Bayes

I deeply encourage you to read them

this article can be

understood 

lay the theoretical

background of hidden Markov models, show how they can be used, and talk about some

of its applications.








Andréi Márkov, 

Markov

lets get to know Markov a little bit.

learning resources

H ow to Learn M achine Learning

professional resources

A Igents.co —  A career community for D ata 

S cientists &amp; M achine Learning E ngineers.

So, who is this Andrei Markov?

Image of Andrei Markov

Russian mathematician

study of stochastic processes

Markov Chains: Probabilistic Sequences

 the Markov Chain

 immediately

“future is independent of the past given the present”

Example of a two state Markov Chain

defined by a set of states and the transition probabilities between

each state

Transition Matrix, also called the

Markov Matrix.

One possible transition matrix for this chain

element ij is the probability of transiting from state j to state i


Diagram of a Markov Chain with the transition probabilities

 from data.

Calculations of transition probabilities from data

However, later in this article we will see just how special they are.

ets carry on and learn about Hidden Markov Models.

Hidden Markov Models: Discovering the unknown

hidden variables

observed variables

The state of a system might only be partially observable, or not observable at all

Using the latter information

the observed variables we would like to infer the former 

the hidden variables

Hidden and observed variables for our problem

emission probabilities

we could try to find out what

the weather of a certain period of time was, knowing in which days John gave us a phone

call.


 For this, we first need to calculate the prior

probabilities 

which we

obtain from the same observations as the transitions probabilities.

Calculations of prior probabilities

Diagram of the process of calculating the probability for one weather scenario

we have calculated the

probability of the whole thing happening by simply multiplying all these aforementioned

probabilities.

We would have to do this for every possible weather scenario

If we wanted to calculate the weather for a full week,

we would have one hundred and twenty eight different scenarios

This is where Markov Chains come in handy.

“future is independent of the past given

the present”.

stores the probabilities of chains of scenarios starting from

a length 1 to the n-1

What does this mean?

This largely simplifies the previous problem


Intuition behind a Hidden Markov Model

no previous information

At each given day we only use the BEST probabilities up to that day

 and picking the best one

for

the first day

Calculations of the probabilities of Monday being sunny and rainy

we

will use the best calculated probability for sunny and for rainy.

Calculations of probabilities of sunny and rainy for Tuesday

If we continue this chain, calculating the probabilities for Wednesday now

Calculations of probabilities for Wednesday


Weather conditions for the whole week

That is it! 

Hidden Markov Models: Applications

are very good when working with sequences. 

Natural Language Processing

translating hand written documents into digital text.

It takes a handwritten text as an

input, breaks it down into different lines and then converts the whole thing into a digital

format.

Pipeline of the Pen to Print app

Conclusion

follow me on

Medium

@jaimezorno

 here

Additional resources


9



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science

Machine Learning

Data Science

Artificial Intelligence

Deep Learning

Startup






