
BERT (language model)

Toggle the table of contents



 14

languages

Article

Talk

Tools From Wikipedia, the free encyclopedia

Bidirectional Encoder Representations from Transformers (BERT) is a family of masked-language models introduced in 2018 by researchers at Google.[1][2] A 2020 literature survey concluded that "in a little over a

year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model."[3]

BERT was originally implemented in the English language at two model sizes:[1] (1) BERTBASE: 12 encoders with 12 bidirectional self-attention heads totaling 110 million parameters, and (2) BERTLARGE: 24 encoders with

16 bidirectional self-attention heads totaling 340 million parameters. Both models were pre-trained on the Toronto BookCorpus[4] (800M words) and English Wikipedia (2,500M words).

Architecture [edit]

BERT is based on the transformer architecture. Specifically, BERT is composed of Transformer encoder layers.

BERT uses WordPiece to convert each English word into an integer code. Its vocabulary has size 30,000. Any token not appearing in its vocabulary is replaced by [UNK] for "unknown".

BERT was pre-trained simultaneously on two tasks:[5]

language modeling: 15% of tokens were selected for prediction, and the training objective was to predict the selected token given its context. The selected token is

replaced with a [MASK] token with probability 80%,

replaced with a random word token with probability 10%,

not replaced with probability 10%.

For example, the sentence "my dog is cute" may have the 4-th token selected for prediction. The model would have input text

"my dog is [MASK]" with probability 80%,

"my dog is happy" with probability 10%,

"my dog is cute" with probability 10%.

After processing the input text, the model's 4-th output vector is passed to a separate neural network, which outputs a probability distribution over its 30,000-large vocabulary.

next sentence prediction: Given two spans of text, the model predicts if these two spans appeared sequentially in the training corpus, outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS]

(for "classify"). The two spans are separated by a special token [SEP] (for "separate"). After processing the two spans, the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the

binary classification into [IsNext] and [NotNext].

For example, given "[CLS] my dog is cute [SEP] he likes playing" the model should output token [IsNext].

Given "[CLS] my dog is cute [SEP] how do magnets work" the model should output token [NotNext].

As a result of this training process, BERT learns latent representations of words and sentences in context. After pre-training, BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance

on specific tasks such as NLP tasks (language inference, text classification) and sequence-to-sequence based language generation tasks (question-answering, conversational response generation).[1][6] The pre-training

stage is significantly more computationally expensive than fine-tuning.

Performance [edit]

When BERT was published, it achieved state-of-the-art performance on a number of natural language understanding tasks:[1]

GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks)

SQuAD (Stanford Question Answering Dataset[7]) v1.1 and v2.0

SWAG (Situations With Adversarial Generations[8])

Analysis [edit]

The reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood.[9][10] Current research has focused on investigating the relationship behind BERT's output

as a result of carefully chosen input sequences,[11][12] analysis of internal vector representations through probing classifiers,[13][14] and the relationships represented by attention weights.[9][10] The high performance of the

BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT, based on the Transformer model architecture, applies its self-attention mechanism to learn information from a text

from the left and right side during training, and consequently gains a deep understanding of the context. For example, the word fine can have two different meanings depending on the context (I feel fine today, She has

fine blond hair). BERT considers the words surrounding the target word fine from the left and right side.

However it comes at a cost: due to encoder-only architecture lacking a decoder, BERT can't be prompted and can't generate text, while bidirectional models in general do not work effectively without the right

side,[clarification needed] thus being difficult to prompt, with even short text generation requiring sophisticated computationally expensive techniques.[15]

In contrast to deep learning neural networks which require very large amounts of data, BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as the

underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content

of the given dataset one uses but also the goal of the task. For example, if the task is a sentiment classification task on financial data, a pre-trained model for the analysis of sentiment of financial text should be chosen.

The weights of the original pre-trained models were released on GitHub.[16]

History [edit]

BERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. The design has its origins from pre-training contextual representations, including semi-

supervised sequence learning,[17] generative pre-training, ELMo,[18] and ULMFit.[19] Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text

corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, where BERT takes into account the context for each occurrence of a given

word. For instance, whereas the vector for "running" will have the same word2vec vector representation for both of its occurrences in the sentences "He is running a company" and "He is running a marathon", BERT will

provide a contextualized embedding that will be different according to the sentence.

On October 25, 2019, Google announced that they had started applying BERT models for English language search queries within the US.[20] On December 9, 2019, it was reported that BERT had been adopted by

Google Search for over 70 languages.[21] In October 2020, almost every single English-based query was processed by a BERT model.[22]

Recognition [edit]

The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).[23]

References [edit]

1. ^ a b c d Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". arXiv:1810.04805v2  [cs.CL ].

2. ^ "Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing" . Google AI Blog. Retrieved 2019-11-27.

3. ^ Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). "A Primer in BERTology: What We Know About How BERT Works" . Transactions of the Association for Computational Linguistics. 8: 842–866. arXiv:2002.12327 .

doi:10.1162/tacl_a_00349 . S2CID 211532403 .

4. ^ Zhu, Yukun; Kiros, Ryan; Zemel, Rich; Salakhutdinov, Ruslan; Urtasun, Raquel; Torralba, Antonio; Fidler, Sanja (2015). "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading

Books". pp. 19–27. arXiv:1506.06724  [cs.CV ].

5. ^ "Summary of the models — transformers 3.4.0 documentation" . huggingface.co. Retrieved 2023-02-16.

6. ^ Horev, Rani (2018). "BERT Explained: State of the art language model for NLP" . Towards Data Science. Retrieved 27 September 2021.

7. ^ Rajpurkar, Pranav; Zhang, Jian; Lopyrev, Konstantin; Liang, Percy (2016-10-10). "SQuAD: 100,000+ Questions for Machine Comprehension of Text". arXiv:1606.05250  [cs.CL ].

8. ^ Zellers, Rowan; Bisk, Yonatan; Schwartz, Roy; Choi, Yejin (2018-08-15). "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference". arXiv:1808.05326  [cs.CL ].

9. ^ a b Kovaleva, Olga; Romanov, Alexey; Rogers, Anna; Rumshisky, Anna (November 2019). "Revealing the Dark Secrets of BERT" . Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the

9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4364–4373. doi:10.18653/v1/D19-1445 . S2CID 201645145 .

10. ^ a b Clark, Kevin; Khandelwal, Urvashi; Levy, Omer; Manning, Christopher D. (2019). "What Does BERT Look at? An Analysis of BERT's Attention" . Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting

Neural Networks for NLP. Stroudsburg, PA, USA: Association for Computational Linguistics: 276–286. doi:10.18653/v1/w19-4828 .

11. ^ Khandelwal, Urvashi; He, He; Qi, Peng; Jurafsky, Dan (2018). "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context". Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics

(Volume 1: Long Papers). Stroudsburg, PA, USA: Association for Computational Linguistics: 284–294. arXiv:1805.04623 . doi:10.18653/v1/p18-1027 . S2CID 21700944 .








v · t · e

(Volume 1: Long Papers). Stroudsburg, PA, USA: Association for Computational Linguistics: 284–294. arXiv:1805.04623 . doi:10.18653/v1/p18-1027 . S2CID 21700944 .

12. ^ Gulordava, Kristina; Bojanowski, Piotr; Grave, Edouard; Linzen, Tal; Baroni, Marco (2018). "Colorless Green Recurrent Networks Dream Hierarchically". Proceedings of the 2018 Conference of the North American Chapter of the

Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Stroudsburg, PA, USA: Association for Computational Linguistics: 1195–1205. arXiv:1803.11138 . doi:10.18653/v1/n18-1108 .

S2CID 4460159 .

13. ^ Giulianelli, Mario; Harding, Jack; Mohnert, Florian; Hupkes, Dieuwke; Zuidema, Willem (2018). "Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information".

Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Stroudsburg, PA, USA: Association for Computational Linguistics: 240–248. arXiv:1808.08079 . doi:10.18653/v1/w18-

5426 . S2CID 52090220 .

14. ^ Zhang, Kelly; Bowman, Samuel (2018). "Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis" . Proceedings of the 2018 EMNLP Workshop BlackboxNLP:

Analyzing and Interpreting Neural Networks for NLP. Stroudsburg, PA, USA: Association for Computational Linguistics: 359–361. doi:10.18653/v1/w18-5448 .

15. ^ Patel, Ajay; Li, Bryan; Rasooli, Mohammad Sadegh; Constant, Noah; Raffel, Colin; Callison-Burch, Chris (2022). "Bidirectional Language Models Are Also Few-shot Learners" . Arxiv. S2CID 252595927 .

16. ^ "BERT" . GitHub. Retrieved 28 March 2023.

17. ^ Dai, Andrew; Le, Quoc (4 November 2015). "Semi-supervised Sequence Learning". arXiv:1511.01432  [cs.LG ].

18. ^ Peters, Matthew; Neumann, Mark; Iyyer, Mohit; Gardner, Matt; Clark, Christopher; Lee, Kenton; Luke, Zettlemoyer (15 February 2018). "Deep contextualized word representations". arXiv:1802.05365v2  [cs.CL ].

19. ^ Howard, Jeremy; Ruder, Sebastian (18 January 2018). "Universal Language Model Fine-tuning for Text Classification". arXiv:1801.06146v5  [cs.CL ].

20. ^ Nayak, Pandu (25 October 2019). "Understanding searches better than ever before" . Google Blog. Retrieved 10 December 2019.

21. ^ Montti, Roger (10 December 2019). "Google's BERT Rolls Out Worldwide" . Search Engine Journal. Search Engine Journal. Retrieved 10 December 2019.

Further reading [edit]

Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). "A Primer in BERTology: What we know about how BERT works". arXiv:2002.12327

 [cs.CL

].

External links [edit]

Official GitHub repository

BERT on Devopedia

Google

Alphabet Inc. · History · List of Android apps · List of Easter eggs (April Fools' Day) · List of mergers and acquisitions

Company

Divisions

Ads · AI (Brain · DeepMind) · Android (booting process · recovery mode · software development · version history) · China (Goojje) · Chrome · Cloud · Glass · Google.org (Crisis Response · Public Alerts · RechargeIT) · Health · Maps ·

Pixel · Search (Timeline) · Sidewalk Labs · Sustainability · YouTube (History · "Me at the zoo" · Social impact · YouTuber)

People

Current

Krishna Bharat · Vint Cerf · Jeff Dean · John Doerr · Sanjay Ghemawat · Al Gore · John L. Hennessy · Urs Hölzle · Salar Kamangar · Ray Kurzweil · Ann Mather · Alan Mulally · Sundar Pichai (CEO) · Ruth Porat (CFO) ·

Rajen Sheth · Hal Varian · Susan Wojcicki · Neal Mohan

Former

Andy Bechtolsheim · Sergey Brin (Founder) · David Cheriton · Matt Cutts · David Drummond · Alan Eustace · Timnit Gebru · Omid Kordestani · Paul Otellini · Larry Page (Founder) · Patrick Pichette · Eric Schmidt ·

Ram Shriram · Amit Singhal · Shirley M. Tilghman · Rachel Whetstone

Real estate

111 Eighth Avenue · Androidland · Barges · Binoculars Building · Central Saint Giles · Chelsea Market · Chrome Zone · Data centers · Modular · Googleplex · Mayfield Mall · Pier 57 · Sidewalk Toronto · St. John's Terminal ·

YouTube Space · YouTube Theater

Design

Fonts (Croscore · Noto · Product Sans · Roboto) · Logo (Doodle (Doodle Champion Island Games · Magic Cat Academy)) · Material Design

Events

Android (Developer Challenge · Developer Day · Developer Lab) · Code-in · Code Jam · Developer Day · Developers Live · Doodle4Google · G-Day · I/O · Jigsaw · Living Stories · Lunar XPRIZE · Mapathon · Science Fair ·

Summer of Code · Talks at Google

YouTube

Awards · CNN/YouTube presidential debates · Comedy Week · Live · Music Awards · Space Lab · Symphony Orchestra

Projects and

initiatives

20% project · Area 120 (Reply · Tables) · ATAP · Business Groups · Computing University Initiative · Data Liberation Front · Data Transfer Project · Developer Expert · Digital Garage · Digital News Initiative · Digital Unlocked · Dragonfly

· Founders' Award · Free Zone · Get Your Business Online · Google for Education · Google for Startups · Labs · Liquid Galaxy · Made with Code · Māori · ML FairnessNative Client · News Lab · Nightingale · OKR · PowerMeter ·

Privacy Sandbox · Quantum Artificial Intelligence Lab · RechargeIT · Shield · Solve for X · Starline · Student Ambassador Program · Submarine communications cables (Dunant · Grace Hopper) · Sunroof · Versus Debates · YouTube

(Creator Awards · Next Lab and Audience Development Group · Original Channel Initiative) · Zero

Criticism

2018 data breach · 2018 walkouts · Alphabet Workers Union · Censorship · DeGoogle · "Did Google Manipulate Search for Hillary?" · Dragonfly · FairSearch · "Ideological Echo Chamber" memo · Litigation · Privacy concerns

(Street View) · San Francisco tech bus protests · Services outages · Smartphone patent wars · Worker organization

YouTube

Back advertisement controversy · Censorship · Copyright issues · Copyright strike · Elsagate · Fantastic Adventures scandal · Headquarters shooting · Kohistan video case · Reactions to Innocence of Muslims ·

Slovenian government incident

Development

Operating systems

Android (Automotive · Glass OS · Go · gLinux · Goobuntu · Things · TV · Wear OS) · ChromeOS (ChromiumOS · Neverware) · Fuchsia · TV

Libraries/

frameworks

ALTS · AMP · Angular (JS) · ARCore · APIs · Blockly · Chart API · Charts · Dialogflow · Exposure Notification · Fast Pair · Federated Learning of Cohorts · File System · FlatBuffers · Flutter · Gears · gRPC · Gson · Guava · Guice ·

Guetzli · JAX · gVisor · MapReduce · Matter · Mobile Services · Neural Machine Translation · OpenSocial · Pack · Polymer · Protocol Buffers · Reqwireless · Shell · Skia Graphics Engine · Tango · TensorFlow · Test · WaveNet ·

Weave · Web Accelerator · WebRTC

Platforms

App Engine · AppJet · Apps Script · Cloud Platform (Anvato) · Firebase (Cloud Messaging · Crashlytics) · Global IP Solutions (Internet Low Bitrate Codec · Internet Speech Audio Codec) · Gridcentric, Inc. · ITA Software ·

Kubernetes · LevelDB · Neatx · SageTV

Apigee

Bigtable · Bitium · Chronicle (VirusTotal) · Compute Engine · Connect · Dataflow · Datastore · Kaggle · Looker · Mandiant · Messaging · Orbitera · Shell · Stackdriver · Storage

Tools

American Fuzzy Lop · Android Cloud to Device Messaging · Android Debug Bridge · Android Studio · App Maker · App Runtime for Chrome · AppSheet · Bazel · Chrome Frame · Closure Tools · Cpplint · Data Protocol · Gadgets ·

Gerrit · GYP · Kythe · Lighthouse · MIT App Inventor · Mashup Editor · Native Client · Optimize · OpenRefine · OR-Tools · PageSpeed · Plugin for Eclipse · Programmable Search Engine · Public DNS · reCAPTCHA · Schema.org ·

Search Console · Sitemaps · Swiffy · Tesseract (software) · Trendalyzer · VisBug · Wave Federation Protocol · Web Toolkit

Search algorithms

Hummingbird · PageRank (applications in biochemistry · Matrix) · Panda · Penguin · Pigeon · RankBrain

Others

BERT · BigQuery · Chrome Experiments · Flutter · Googlebot · Keyhole Markup Language · LaMDA (Bard) · Open Location Code · PaLM · Programming languages (Caja · Carbon · Dart · Go · Sawzall) · Transformer · Viewdle ·

Webdriver Torso · Web Server

File formats

AAB · APK (AV1) · On2 Technologies (VP3 · VP6 · VP8 (libvpx)) · VP9 · WebM · WebP · WOFF2

Products

Entertainment

Currents (news app) · Green Throttle Games · Owlchemy Labs · Oyster · PaperofRecord.com · Podcasts · Quick, Draw! · Santa Tracker · Songza · Stadia (games · Typhoon Studios) · TV · Vevo · Video

Play

Books · Games · most downloaded apps · Music · Newsstand · Pass · Services

YouTube

BandPage · BrandConnect · Content ID · Instant · Kids · Music · Official channel · Preferred · Premium (original programming) · YouTube Rewind · RightsFlow · Shorts · Studio · TV

Communication

Allo · Bump · Buzz · Chat · Contacts · Currents (social app) · Dodgeball · Duo · Fi Wireless · Friend Connect · Gizmo5 · Google+ · Gmail (History · Inbox · Interface) · Groups · Hangouts · Helpouts · IME (Japanese · Pinyin ·

Transliteration) · Jaiku · Marratech · Meebo · Meet · Messages · Moderator · Neotonic Software · Orkut · Postini · Quest Visual (Word Lens) · Schemer · Spaces · Sparrow · Talk · Translate (Translator Toolkit) · Voice ·

Voice Local Search · Wave

Search

Aardvark · Alerts · Answers · Base · BeatThatQuote.com · Blog Search · Books (Ngram Viewer) · Code Search · Data Commons · Dataset Search · Dictionary · Directory · Fast Flip · Flu Trends · Finance · Goggles · Google.by ·

Images (Image Labeler · Image Swirl) · Kaltix · Knowledge Graph (Freebase · Metaweb) · Like.com · News (Archive · Weather) · Patents · People Cards · Personalized Search · Public Data Explorer · Questions and Answers ·

SafeSearch · Scholar · Searchwiki · Shopping · Catalogs (Express) · Squared · Tenor · Travel (Flights) · Trends (Insights for Search) · Voice Search · WDYL

Navigation

Earth · Endoxon · ImageAmerica · Maps (Latitude · Map Maker · Navigation · Pin · Street View (Coverage · Trusted)) · Waze

Business

and finance

Ad Manager · AdMob · Ads · Adscape · AdSense · Attribution · BebaPay · Checkout · Contributor · DoubleClick (Affiliate Network · Invite Media) · Marketing Platform (Analytics · Looker Studio · Urchin) · Pay (mobile app) (Wallet ·

Pay (payment method) · Send · Tez) · PostRank · Primer · Softcard · Wildfire Interactive · Widevine

Organization

and productivity

Bookmarks · Browser Sync · Calendar · Cloud Search · Desktop · Drive · Etherpad · fflick · Files · iGoogle · Jamboard · Notebook · One · Photos · Quickoffice · Quick Search Box · Surveys · Sync · Tasks · Toolbar

Docs Editors

Docs · Drawings · Forms · Fusion Tables · Keep · Sheets · Slides · Sites

Publishing

Apture · Blogger (Pyra Labs) · Domains · FeedBurner · One Pass · Page Creator · Sites · Web Designer

Others

Account (Dashboard · Takeout) · Android Auto · Android Beam · Arts &amp; Culture · Assistant · Authenticator · Body · BufferBox · Building Maker · BumpTop · Cast (List of supported apps) · Classroom · Cloud Print · Crowdsource ·

Expeditions · Family Link · Find My Device · Fit · Google Fonts · Gboard · Gesture Search · Grasshopper · Impermium · Knol · Lively · Live Transcribe · MyTracks · Nearby Share · Now · Offers · Opinion Rewards · Person Finder ·

PlinkArt · Poly · Question Hub · Read Along · Reader · Safe Browsing · Sidewiki · SlickLogin · Socratic · Sound Amplifier · Speech Services · Station · Store · TalkBack · Tilt Brush · URL Shortener · Voice Access · Wavii · Web Light ·

WiFi · Workspace (Marketplace)

Chrome

Apps · Chromium · Dinosaur Game · GreenBorder · Remote Desktop · Web Store · V8

Images and

photography

Camera · Lens · Snapseed (Nik Software) · Panoramio · Photos · Picasa (Web Albums) · Picnik

Hardware

Smartphones

Android Dev Phone · Android One · Nexus (Nexus One · S · Galaxy Nexus · 4 · 5 · 6 · 5X · 6P · Comparison) · Pixel (Pixel · 2 · 3 · 3a · 4 · 4a · 5 · 5a · 6 · 6a · 7 · Comparison) · Play Edition · Project Ara

Laptops and tablets

Chromebook · Nexus (7 (2012) · 7 (2013) · 10 · 9 · Comparison) · Pixel (Chromebook Pixel · Pixelbook · Pixelbook Go · C · Slate · Tablet)

Others

Chromebit · Chromebox · Clips · Digital media players (Chromecast · Nexus Player · Nexus Q) · Dropcam · Fitbit (List of products) · Liquid Galaxy · Nest (Smart Speakers · Thermostat · Wifi) · OnHub · Pixel Buds ·

Pixel Visual Core · Pixel Watch · Search Appliance · Sycamore processor · Tensor · Tensor Processing Unit · Titan Security Key · Virtual reality (Cardboard · Contact Lens · Daydream · Glass)


Privacy policy About Wikipedia Disclaimers

Contact Wikipedia Mobile view Developers

Statistics

Cookie statement





This page was last edited on 27 April 2023, at 16:34 (UTC).

Text is available under the Creative Commons Attribution-ShareAlike License 3.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a

non-profit organization.

v · t · e

v · t · e

Related

Terms and phrases

"Don't be evil" · Gayglers · Google (verb) · Google bombing (2004 U.S. presidential election) · Google effect · Googlefight · Google hacking · Googleshare · Google tax · Googlewhack · Googlization · "Illegal flower tribute" ·

Rooting · Search engine manipulation effect · Sitelink · Site reliability engineering · YouTube poop

Documentaries

AlphaGo · Google: Behind the Screen · Google Current · Google Maps Road Trip · Google and the World Brain · The Creepy Line

Books

Google Hacks · The Google Story · Google Volume One · Googled: The End of the World as We Know It · How Google Works · I'm Feeling Lucky · In the Plex · The Google Book

Popular culture

Google Feud · Google Me (film) · "Google Me" (Kim Zolciak song) · "Google Me" (Teyana Taylor song) · Is Google Making Us Stupid? · Proceratium google · Matt Nathanson: Live at Google · The Billion Dollar Code ·

The Internship · Where on Google Earth is Carmen Sandiego?

Others

elgooG · g.co · .google · Pimp My Search · Predictions of the end · Relationship with Wikipedia · Sensorvault · Stanford Digital Library Project

Italics indicate discontinued products or services.



 Category · 



 Commons · 



 Outline · 



 WikiProject

Natural language processing

General terms

AI-complete · Bag-of-words · n-gram (Bigram · Trigram) · Computational linguistics · Natural-language understanding · Stop words · Text processing

Text analysis

Collocation extraction · Concept mining · Coreference resolution · Deep linguistic processing · Distant reading · Information extraction · Named-entity recognition · Ontology learning · Parsing ·

Part-of-speech tagging · Semantic role labeling · Semantic similarity · Sentiment analysis · Terminology extraction · Text mining · Textual entailment · Truecasing · Word-sense disambiguation ·

Word-sense induction

Text segmentation

Compound-term processing · Lemmatisation · Lexical analysis · Text chunking · Stemming · Sentence segmentation · Word segmentation

Automatic summarization

Multi-document summarization · Sentence extraction · Text simplification

Machine translation

Computer-assisted · Example-based · Rule-based · Statistical · Transfer-based · Neural

Distributional semantics models

BERT · Document-term matrix · Explicit semantic analysis · fastText · GloVe · Language model (large) · Latent semantic analysis · Seq2seq · Word embedding · Word2vec

Language resources,

datasets and corpora

Types and

standards

Corpus linguistics · Lexical resource · Linguistic Linked Open Data · Machine-readable dictionary · Parallel text · PropBank · Semantic network · Simple Knowledge Organization System ·

Speech corpus · Text corpus · Thesaurus (information retrieval) · Treebank · Universal Dependencies

Data

BabelNet · Bank of English · DBpedia · FrameNet · Google Ngram Viewer · UBY · WordNet

Automatic identification

and data capture

Speech recognition · Speech segmentation · Speech synthesis · Natural language generation · Optical character recognition

Topic model

Document classification · Latent Dirichlet allocation · Pachinko allocation

Computer-assisted

reviewing

Automated essay scoring · Concordancer · Grammar checker · Predictive text · Pronunciation assessment · Spell checker · Syntax guessing

Natural language

user interface

Chatbot · Interactive fiction · Question answering · Virtual assistant · Voice user interface

Related

Hallucination · Natural Language Toolkit · spaCy

Differentiable computing

General

Differentiable programming · Information geometry · Statistical manifold

Automatic differentiation · Neuromorphic engineering · Pattern recognition · Tensor calculus · Computational learning theory · Inductive bias

Concepts

Gradient descent (SGD) · Clustering · Regression (Overfitting) · Hallucination · Adversary · Attention · Convolution · Loss functions · Backpropagation · Normalization · Activation (Softmax · Sigmoid · Rectifier) · Regularization ·

Datasets (Augmentation) · Diffusion · Autoregression

Applications

Machine learning (In-context learning) · Artificial neural network (Deep learning) · Scientific computing · Artificial Intelligence · Language model (Large language model)

Hardware

IPU · TPU · VPU · Memristor · SpiNNaker

Software libraries

TensorFlow · PyTorch · Keras · Theano · JAX · LangChain

Implementations

Audio–visual

AlexNet · WaveNet · Human image synthesis · HWR · OCR · Speech synthesis · Speech recognition · Facial recognition · AlphaFold · DALL-E · Midjourney · Stable Diffusion

Verbal

Word2vec · Seq2seq · BERT · LaMDA (Bard) · NMT · Project Debater · IBM Watson · GPT-2 · GPT-3 · ChatGPT · GPT-4 · GPT-J · Chinchilla AI · PaLM · BLOOM · LLaMA

Decisional

AlphaGo · AlphaZero · Q-learning · SARSA · OpenAI Five · Self-driving car · MuZero · Action selection (Auto-GPT) · Robot control

People

Yoshua Bengio · Alex Graves · Ian Goodfellow · Stephen Grossberg · Demis Hassabis · Geoffrey Hinton · Yann LeCun · Fei-Fei Li · Andrew Ng · Jürgen Schmidhuber · David Silver

Organizations

Anthropic · EleutherAI · Google DeepMind · OpenAI · Meta AI · Mila · MIT CSAIL

Architectures

Neural Turing machine · Differentiable neural computer · Transformer · Recurrent neural network (RNN) · Long short-term memory (LSTM) · Gated recurrent unit (GRU) · Echo state network · Multilayer perceptron (MLP) ·

Convolutional neural network · Residual network · Autoencoder · Variational autoencoder (VAE) · Generative adversarial network (GAN) · Graph neural network



 Portals (Computer programming · Technology) · 



 Categories (Artificial neural networks · Machine learning)

Category: Large language models



