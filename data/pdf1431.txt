
Home &gt; AI technologies

Enterprise

AI

DEFINITION

language modeling

Ben Lutkevich, Technical Features Writer

Language modeling (LM) is the use of various statistical and probabilistic techniques to determine the probability of a given

sequence of words occurring in a sentence. Language models analyze bodies of text data to provide a basis for their word

predictions. They are used in natural language processing (NLP) applications, particularly ones that generate text as an output.

Some of these applications include , machine translation and question answering.

Language models determine word probability by analyzing text data. They interpret this data by feeding it through an algorithm

that establishes rules for context in natural language. Then, the model applies these rules in language tasks to accurately

predict or produce new sentences. The model essentially learns the features and characteristics of basic language and uses

those features to understand new phrases.

There are several different probabilistic approaches to modeling language, which vary depending on the purpose of the

language model. From a technical perspective, the various types differ by the amount of text data they analyze and the math

they use to analyze it. For example, a language model designed to generate sentences for an automated Twitter bot may use

different math and analyze text data in a different way than a language model designed for determining the likelihood of a

search query.

Some common statistical language modeling types are:

N-gram. N-grams are a relatively simple approach to language models. They create a probability distribution for a

sequence of n The n can be any number, and defines the size of the "gram", or sequence of words being assigned a

probability. For example, if n = 5, a gram might look like this: "can you please call me." The model then assigns

probabilities using sequences of n size. Basically, n can be thought of as the amount of context the model is told to

consider. Some types of n-grams are unigrams, bigrams, trigrams and so on.

Unigram. The unigram is the simplest type of language model. It doesn't look at any conditioning context in its calculations.

It evaluates each word or term independently. Unigram models commonly handle language processing tasks such as

information retrieval. The unigram is the foundation of a more specific model variant called the query likelihood model,

which uses information retrieval to examine a pool of documents and match the most relevant one to a specific query.

Bidirectional. Unlike n-gram models, which analyze text in one direction (backwards), bidirectional models analyze text in

both directions, backwards and forwards. These models can predict any word in a sentence or body of text by using every

other word in the text. Examining text bidirectionally increases result accuracy. This type is often utilized in machine

learning and speech generation applications. For example, Google uses a bidirectional model to process search queries.

Exponential. Also known as maximum entropy models, this type is more complex than n-grams. Simply put, the model

evaluates text using an equation that combines feature functions and n-grams. Basically, this type specifies features and

parameters of the desired results, and unlike n-grams, leaves analysis parameters more ambiguous -- it doesn't specify

individual gram sizes, for example. The model is based on the principle of entropy, which states that the probability

distribution with the most entropy is the best choice. In other words, the model with the most chaos, and least room for

assumptions, is the most accurate. Exponential models are designed maximize cross entropy, which minimizes the

amount statistical assumptions that can be made. This enables users to better trust the results they get from these

models.

Continuous space. This type of model represents words as a non-linear combination of weights in a neural network. The

process of assigning a weight to a word is also known as word embedding. This type becomes especially useful as data

sets get increasingly large, because larger datasets often include more unique words. The presence of a lot of unique or

rarely used words can cause problems for linear model like an n-gram. This is because the amount of possible word

sequences increases, and the patterns that inform results become weaker. By weighting words in a non-linear, distributed

way, this model can "learn" to approximate words and therefore not be misled by any unknown values. Its "understanding"

of a given word is not as tightly tethered to the immediate surrounding words as it is in n-gram models.

The models listed above are more general statistical approaches from which more specific variant language models are

derived. For example, as mentioned in the n-gram description, the query likelihood model is a more specific or specialized

How language modeling works


model that uses the n-gram approach. Model types may be used in conjunction with one another.

The models listed also vary significantly in complexity. Broadly speaking, more complex language models are better at NLP

tasks, because language itself is extremely complex and always evolving. Therefore, an exponential model or continuous

space model might be better than an n-gram for NLP tasks, because they are designed to account for ambiguity and variation

in language.

A good language model should also be able to process long-term dependencies, handling words that may derive their meaning

from other words that occur in far-away, disparate parts of the text. An LM should be able to understand when a word is

referencing another word from a long distance, as opposed to always relying on proximal words within a certain fixed history.

This requires a more complex model.

Language modeling is crucial in modern NLP applications. It is the reason that machines can understand qualitative

information. Each language model type, in one way or another, turns qualitative information into quantitative information. This

allows people to communicate with machines as they do with each other to a limited extent.

It is used directly in a variety of industries including tech, finance, healthcare, transportation, legal, military and government.

Additionally, it's likely most people reading this have interacted with a language model in some way at some point in the day,

whether it be through Google search, an autocomplete text function or engaging with a voice assistant.

The roots of language modeling as it exists today can be traced back to 1948. That year, Claude Shannon published a paper

titled "A Mathematical Theory of Communication." In it, he detailed the use of a stochastic model called the Markov chain to

create a statistical model for the sequences of letters in English text. This paper had a large impact on the

telecommunications industry, laid the groundwork for information theory and language modeling. The Markov model is still

used today, and n-grams specifically are tied very closely to the concept.

Language models are the backbone of natural language processing (NLP). Below are some NLP tasks that use language

modeling, what they mean, and some applications of those tasks:

Speech recognition -- involves a machine being able to process speech audio. This is commonly used by voice assistants

like Siri and Alexa.

Machine translation -- involves the translation of one language to another by a machine. Google Translate and Microsoft

Translator are two programs that do this. SDL Government is another, which is used to translate foreign social media

feeds in real time for the U.S. government.

Parts-of-speech tagging -- involves the markup and categorization of words by certain grammatical characteristics. This is

utilized in the study of linguistics, first and perhaps most famously in the study of the Brown Corpus, a body of composed

of random English prose that was designed to be studied by computers. This corpus has been used to train several

important language models, including one used by Google to improve search quality.

Parsing -- involves analysis of any string of data or sentence that conforms to formal grammar and syntax rules. In

language modeling, this may take the form of sentence diagrams that depict each word's relationship to the others. Spell

checking applications use language modeling and parsing.

Sentiment analysis -- involves determining the sentiment behind a given phrase. Specifically, it can be used to understand

opinions and attitudes expressed in a text. Businesses can use this to analyze product reviews or general posts about

their product, as well as analyze internal data like employee surveys and customer support chats. Some services that

provide sentiment analysis tools are Repustate and Hubspot's ServiceHub. Google's NLP tool -- called Bidirectional

Encoder Representations from Transformers (BERT) -- is also used for sentiment analysis.

Optical character recognition -- involves the use of a machine to convert images of text into machine encoded text. The

image may be a scanned document or document photo, or a photo with text somewhere in it -- on a sign, for example. It is

often used in data entry when processing old paper records that need to be digitized. In can also be used to analyze and

identify handwriting samples.

Information retrieval -- involves searching in a document for information, searching for documents in general, and

searching for metadata that corresponds to a document. Web browsers are the most common information retrieval

applications.

This was last updated in March 2020

Importance of language modeling

Uses and examples of language modeling

Continue Reading About language modeling


Related Terms

narrow AI (weak AI)

Narrow AI is an application of artificial intelligence technologies to enable a high-functioning system that replicates -- and ... See complete definition

Turing Test

A Turing Test is a method of inquiry in artificial intelligence (AI) for determining whether or not a computer is capable of ... See complete definition

What is generative AI? Everything you need to know

Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery,... See complete definition

natural language generation (NLG)

By: TechTarget Contributor

transformer model



By: George Lawton

large language model (LLM)

How does Salesforce Einstein Language NLP improve productivity?

∙

What do NLP benchmarks like GLUE and SQuAD mean for developers?

∙ Promethium tool taps natural language processing for analytics

∙ A comparison of 6 speech-to-text services

∙ An in-depth explanation of language models with probability equations

∙

Dig Deeper on AI technologies


Latest TechTarget resources

Business Analytics

CIO

Data Management

ERP

Business Analytics

 QlikWorld 2023 recap: The future is bright for Qlik

Qlik celebrated analytics advancements at QlikWorld 2023, highlighting its acquisition of Talend, the evolution of Qlik Cloud, ...

 Sisense's Orad stepping down, Katz named new CEO

With the embedded analytics specialist recently reaching important financial milestones, the vendor's longtime leader is handing ...

 Knime updates Business Hub to ease data science deployment

The vendor is adding a tool that enables continuous integration and deployment of AI and ML models to help organizations better ...

CIO

 AI policy advisory group talks competition in draft report

The National AI Advisory Committee's first draft report points out how investing in AI research and development can help the U.S....

 ChatGPT use policy up to businesses as regulators struggle

As regulators struggle to keep up with emerging AI tech such as ChatGPT, businesses will be responsible for creating use policies...

 Federal agencies promise action against 'AI-driven harm'

Federal enforcement agencies cracked down on artificial intelligence systems Tuesday, noting that the same consumer protection ...

Data Management

 New Starburst, DBT integration eases data transformation

The integration enables data mesh adopters to work with data from multiple sources without having to move it in and out of a ...

 InfluxData update ups speed, power of time series database

The new version of InfluxDB features an architecture built with Apache Arrow and written in Rust to improve the speed and ...



By: Sean Kerner

How to detect AI-generated content



By: Ron Karjian


 IBM acquires Ahana, steward of open source PrestoDB

The purchase not only gives IBM a managed SaaS and AWS marketplace version of the popular open-source Presto database, but ...

ERP

 3D printing has a complex relationship with sustainability

3D printing promises some sustainability benefits, including creating lighter parts and shorter supply chains, but the overall ...

 What adding a decision intelligence platform can do for ERP

Tom Oliver of AI vendor Faculty makes the case for decision intelligence technology as the solution to the data-silo problems of ...

 7 3PL KPIs that can help you evaluate success

Supply chain leaders should look at some particular KPIs to determine whether their company's 3PL provider is meeting their needs...

All Rights Reserved, Copyright 2018 - 2023, TechTarget

Privacy Policy 

Cookie Preferences 

Do Not Sell or Share My Personal Information

Close

About Us

Editorial Ethics Policy

Meet The Editors

Contact Us

Advertisers

Partner with Us

Media Kit

Corporate Site

Contributors

Reprints

Answers

Definitions

E-Products

Events

Features

Guides

Opinions

Photo Stories

Quizzes

Tips

Tutorials

Videos

