
next up 



 



 



 

Next: Extended language modeling approaches Up: Language models for information Previous: Ponte and Croft's Experiments Contents 

Index 

Language modeling versus other approaches in IR

The language modeling approach provides a novel way of looking at the problem of text retrieval, which links it with a lot of recent work in speech

and language processing. As Ponte and Croft (1998) emphasize, the language modeling approach to IR provides a different approach to scoring

matches between queries and documents, and the hope is that the probabilistic language modeling foundation improves the weights that are used,

and hence the performance of the model. The major issue is estimation of the document model, such as choices of how to smooth it effectively.

The model has achieved very good retrieval results. Compared to other probabilistic approaches, such as the BIM from Chapter 11 , the main

difference initially appears to be that the LM approach does away with explicitly modeling relevance (whereas this is the central variable evaluated

in the BIM approach). But this may not be the correct way to think about things, as some of the papers in Section 12.5 further discuss. The LM

approach assumes that documents and expressions of information needs are objects of the same type, and assesses their match by importing the

tools and methods of language modeling from speech and natural language processing. The resulting model is mathematically precise, conceptually

simple, computationally tractable, and intuitively appealing. This seems similar to the situation with XML retrieval (Chapter 10 ): there the

approaches that assume queries and documents are objects of the same type are also among the most successful.

On the other hand, like all IR models, you can also raise objections to the model. The assumption of equivalence between document and

information need representation is unrealistic. Current LM approaches use very simple models of language, usually unigram models. Without an

explicit notion of relevance, relevance feedback is difficult to integrate into the model, as are user preferences. It also seems necessary to move

beyond a unigram model to accommodate notions of phrase or passage matching or Boolean retrieval operators. Subsequent work in the LM

approach has looked at addressing some of these concerns, including putting relevance back into the model and allowing a language mismatch

between the query language and the document language.

The model has significant relations to traditional tf-idf models. Term frequency is directly represented in tf-idf models, and much recent work has

recognized the importance of document length normalization. The effect of doing a mixture of document generation probability with collection

generation probability is a little like idf: terms rare in the general collection but common in some documents will have a greater influence on the

ranking of documents. In most concrete realizations, the models share treating terms as if they were independent. On the other hand, the intuitions

are probabilistic rather than geometric, the mathematical models are more principled rather than heuristic, and the details of how statistics like term

frequency and document length are used differ. If you are concerned mainly with performance numbers, recent work has shown the LM approach

to be very effective in retrieval experiments, beating tf-idf and BM25 weights. Nevertheless, there is perhaps still insufficient evidence that its

performance so greatly exceeds that of a well-tuned traditional vector space retrieval system as to justify changing an existing implementation.

next up 



 



 



 

Next: Extended language modeling approaches Up: Language models for information Previous: Ponte and Croft's Experiments Contents 

Index

Â© 2008 Cambridge University Press

This is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.

2009-04-07

