
Lecture 6a: Introduction to 

Hidden Markov Models  

Introduction to Computational Biology 

Instructor: Teresa Przytycka, PhD 

 Igor Rogozin PhD 

 




First order Markov model (informal) 



C 

T 

G 

A 













transversion 

transition 

β,α -probability of 

given mutation in a 

unit of time

A random walk in this graph will generates a path; 

say AATTCA…. 

For each such path we can compute the probability of the path 

In this graph every path is possible (with different probability) but 

in general this does need to be true. 


First order Markov model (formal)  

Markov model is represented by a graph with set of 

vertices corresponding to the set of states Q and 

probability of going from state i to state j in a 

random walk described by matrix a: 

a – n x n transition probability matrix 

 a(i,j)= P[q t+1=j|q t=i] 

where q t denotes state at time t 

Thus Markov model M is described by Q and a 

M = (Q, a) 

 


Example 



Transition probabilities for general DNA 

 seq. 

 a 



1/4 

Transition probability matrix a: 

Set of states Q: 

{Begin, End, A,T,C,G} 


Probability of a sequence of states 

S=q0,…,qm in model M 

P(S|M) = a(q0,q1) a(q1 , q2) a(q2 , q3) … 

a(qm-1 , qm) 

P(S|M) = probability of visiting sequence of states S 

assuming Markov model M defined by a: 

 n x n transition probability matrix a(i,j) = Pr[q t+1=j|q t=i] 

 Assume a random walk that starts from state q0 and goes 

for m steps. What is the probability that S= q0,…,qm is 

the sequence of states visited? 


Example 



Transition probabilities for general DNA 

 seq. (human) 



Transition probabilities for CpG island 

(Assume that any state can be the  

end  state ) 

 a1 

a2 

Probability of sequence Begin AACGC: 

 in model M1: P(S|M1) =  

¼ x0.3x 0.205 x0.078 x 0.246   

in model M2: P(S|M2) 

¼  x 0.18 x 0.274 x 0.274 x 0.339  



1/4 


Choosing Markov model that  is most likely 

to generate given sequence 

P(Model|Sequence) =  

P(Sequence| Model)P(Model )  / P(Sequence) 

assuming probability of sequence is the same we need to compare: 

P(S|M1 )P(M1)   &lt;  P(S|M2)P(M2)   

 

If  often assume that both models are equally likely and then it boils down to 

comparing 

P(S|M1 )  &lt;  P(S|M2) 

 

But we may know that in a given genome CpG island are less frequent than 

non CpG seqences and we can use this prior knowledge to define P(M1) 

and P(M2) 

 In  is not simply which of the two P(S|M1)  and  P(S|M2) is bigger 

(unless we make additional assumption) 

 


Introducing HMM 

•  Assume that we not only  perform random 

walk on the graph but also in each state 

print out (emit) one of a finite number of 

symbols   


Introducing emission probabilities 

•  Assume that at each state a Markov process emits 

(with some probability distribution) a symbol from 

alphabet Σ.  

•  Hidden Markov Model: Rather than observing a 

sequence of states we observe a sequence of 

emitted symbols. 

Example: 

 Σ ={A,C,T,G}. 

Generate a sequence 

where A,C,T,G have 

frequency p(A) =.33, 

p(G)=.2, p(C)=.2, p(T) 

= .27 respectively 

A  .33 

T  .27 

C  .2 

G  .2  

1.0 

 one state 

emission probabilities 


First, back to Markov 

•  A Markov model of DNA sequence where 

regular sequence is interrupted with CpG 

islands 


generating DNA sequence with CpG 

islands 



Represents transitions between any pair of states 

We do not have a 1-1 mapping between states and generated letters 

thus if we observe a sequence, say GCGCAGC generated by such 

model we cannot be sure which state generated which letter. 

Distinguishable states 


The same with HHM: 

Example CpG island-continued 

Model for general  

 sequence 

Model for CpG island 

A  .33 

T  .27 

C  .2 

G  .2  

1.0-e 

A  .2 

T  .2 

C  .34 

G  .26  

1.0-e1 

emission probabilities 

(the numbers are for illustration only) 

e 

e1 


What is different comparing 

Markov model? 

•  Note that if there each letter can be generated in a 

two different states. 

•  Thus is we have a sequence of letters we cannot 

uniquely identify the path used in the generation  - 

the path of states is “hidden” 

•  Rather than observing sequence of states we 

observe a sequence of emitted letters. 

•  We move it even further by giving each sate a 

choice for emitting one of several possible letters. 


HMM – formal definition 

HMM is a Markov process that at each time step generates a 

symbol from some alphabet, Σ,  according to emission 

probability  that depends on state. 

 M = (Q, Σ, a,e) 

Q – finite set of states, say n states ={1,…n} 

a – n x n transition probability 

            matrix a(i,j) = Pr[q t+1=j|q t=i] 

Σ = {σ1, …,σk} 

 e(i,j) == probability of generating symbol σj in state qi = 

P[at=σj|qt = i]; where at is tth element of generated sequence 

 

qt = state in position t in 

the sequence (tth time step) 


Typical applications of HMM 

Given is a family of bio-sequences. Assume 

that it was generated using a HMM. 

Goal: construct HMM that models these 

sequences. 

 

 


Markov Chain/Hidden Markov Model 

Both are based on the idea of random walk in a 

directed graph, where probability of next step is 

defined by edge weight. 

In HMM additionally, at step a symbol from some 

fixed alphabet is emitted. 

Markov Chain – the result of the experiment (what 

you observe) is a sequence of state visited. 

HMM – the result of the experiment is the sequence 

of symbols emitted. The states are hidden from the 

observer.  




Example from Jones/Pevzner book 

state 

Emitted symbol  

HTHHTHTTTHHHTHHHHTHHHTHH 

You may guess that somewhere here the cassino switched to biased coin 


HMM – formal definition 

HMM is a Markov process that at each time step generates a 

symbol from some alphabet, Σ,  according to emission 

probability  that depends on state. 

 M = (Q, Σ, a,e) 

Q – finite set of states, say n states ={1,…n} 

a – n x n transition probability 

            matrix a(i,j) = Pr[q t+1=j|q t=i] 

Σ = {σ1, …,σk} 

 e(i,j) == probability of generating symbol σj in state qi = 

P[at=σj|qt = i]; where at is tth element of generated sequence 

 

qt = state in position t in 

the sequence (tth time step) 


Recall from Markov chain:  

Probability of a sequence of states S=q0,…,qm in 

Markov model M 

P(S|M) = a(q0,q1) a(q1 , q2) a(q2 , q3) … 

a(qm-1 , qm) 

P(S|M) = probability of visiting sequence of states S 

assuming Markov model M defined by a: 

 n x n transition probability matrix a(i,j) = Pr[q t+1=j|q t=i] 

 Assume a random walk that starts from state q0 and goes 

for m steps. What is the probability that S= q0,…,qm is 

the sequence of states visited? 


In HMM: P(S|M) = probability of 

generating a sequence of symbols 

P[S|M] = Σ π∈Qn+1P[π|M]P[S| π,M]  

(sum over all possible paths p=q0,…qn of product: probability of 

the path  in the model times the probability of generating given 

sequence assuming given path in the model.) 

P[π |M] =a(q0,q1) a(q1 , q2) a(q2 , q3) … a(qn-1 , qn) 

P[S| π,M] = e(q0,σ1) e(q1,σ2) e(q2 , σ3) … e(qn , σn) 

Computing P[S|M] from the definition (enumerating all sequences p) 

would take exponential time. We will do it by dynamic programming. 


π – sequence of states 

σ – generated sequence of head/tails   



 P[S| π,M] = ½ * ½* ½ * ¾ * ¾*….* ½ 

 P[π|M] = ½ * 9/10 * 9/10 * 1/10 * …. 

 P[S|M] = ½ * ½ * 9/10 * ½ * 9/10 *…. 

 S= σ0,…,σn 

 π= q0,…,qn 

P(σi|qi) 

P(qi-1|qi)


Typical context for using HMM in 

computational biology 

•  Select a basic method to model a sequence 

family – what are the states, what are the 

emitted symbols. 

•  Using a training set – a set of known 

sequences in the family – estimate the 

parameters of the model: emission and 

transition probabilities.  


Algorithmic problems 

•  Given a sequence, and several HMMs we 

would like to decide which of HMM models 

in most likely to generate it  

•  Given a HMM model M and a sequence S 

we would like to be able find the most 

likely path in M that generates S (example – 

predicting CpG island). 


Finding the most likely path 

•  Most likely path for a sequence S is a path p in M 

that maximizes the probability of generating the 

sequence S along this path: P(S|π). 

•  Why it is often interesting to know most likely 

path: 

–  Frequently HMM are designed in such a way that one 

can assign biological relevance to a state: e.g. position in 

protein sequence, beginning of coding region, beginning/

end of an intron.  


Recall: P(S|M) = probability of 

generating a sequence S by HMM M 

M = (Q, Σ, a,e) 

  S = σ0…σn 

 P[S|M] = Σ π∈Qn+1P[π |M]P[S| π,M]  

(sum over all possible paths π=q0,…qn of product: probability of 

the path  in the model times the probability of generating given 

sequence assuming given path in the model.) 

P[π |M] =a(q0,q1) a(q1 , q2) a(q2 , q3) … a(qn-1 , qn) 

P[S| π,M] = e(q0,σ1) e(q1,σ2) e(q2 , σ3) … e(qn , σn) 

Computing P[S|M] from the definition (enumerating all sequences p) 

would take exponential time. We will do it by dynamic programming. 


Finding most likely path for sequence S 

in model M :Viterbi algorithm 

Let vj(i) = the probability of 

generating the prefix Si  using 

the most likely path subject to 

the restriction that the path ends 

at state j at step i 

Let S = σ0…σn and let  

σi+1 = s = symbol emitted in time i+1 

v k(i+1)= e(k,s) max j vj(i)a(j,k) 



Idea: Use dynamic programming to compute for 

every state k and every position i in the sequence 

the value: 

 


Dynamic programming table 

Cost = O(n2 t) 

v0(i) 

v1(i) 

v2(i) 

vk(i) 

vn(i) 

Position in the sequence 

States,j 

0 

0 

1 

1 

k 

. 

i 

i+1 

e(2, σi+1 ) max j vj(i)a(j,2) 

n 

t 

e(k, σi+1 ) max j vj(i)a(j,k) 


The algorithm for most likely path 

generating sequence S = σ0…σt 

•  Initialization: v 0(0)=1; v k(0)=0; for k &gt;0; 

•  Recursion:  

for i= 1 to t 

    for all states k 

          v k(i)= e(k,σi) max j vj(i-1)a(j,k) 

  trace-back for path determination: 

         trace(k,i) = record the source of the arrow that that gives 

 

 

 

 

  max jvj(i-1)a(j,k) 

    P*(t) =trace(stop,t);  P(t) is predecessor of state t 

•  Trace-back: 

     for i = t to 1 do P*(i-1) = trace(P*(i),i) 


So far: 

•  Now we can compute quickly the most 

likely path (and the probability of this path). 

•  But a given sequence can be generated in 

many different ways (using other paths) 

except that with a smaller probability 

•  If we are interested in computing the 

probability of generating a sequence my a 

HMM must account for all possibilities  


Computing probability of the 

sequence in HMM: forward algorithm 

fk(i) - probability of generating the subsequence 1,…i using a 

path that ends at state k (forward variable) 

f k(i+1)=e(k,σi)Σ j fj(i)a(j,k) 

fn+1(stop)- probability of generating the sequence 


Another important measurement: probability of 

the fact that that a symbol at position i in the 

sequence was generated in state k 

 



T=0.75 

H=0.25 



T=0.5 

H=0.5 



T=0.5 

H=0.5 

0.1 

0.1 

0.9 

0.1 

0.9 

0.9 

Example, state 2 is designed to be a “decision point” 

State 1 (Fair) 

State 2 (Transition) 

State 3 (Biased) 


Probability that observation σi came 

from state k 

S=σ1,… σi…σt  - sequence 

Compute : P(σi = k|S) the posterior probability 

Method: 

P(S|σi = k) = 

P(σ1,… σi-1|σi =k) 

x  

P(σi+1,…σt |σi =k) 



   i-1         i         i+1 


The backward algorithm 

P(σ1,…σi =k) is computed as “forward variable” in 

in the previous algorithm 

P(σi+1,…σt |σi =k) run the algorithm “backward” 

that is assume that stop state is the start state. 

This is almost exactly the same as the forward with 

one caveat: a slight modification is required not to 

count the emission probability at state k twice 

(going from both directions) 

 


Designing HMM 

•  Model structure design (deciding on number 

of states, connections between the states) – 

no theory is developed. 

•  Parameters estimation – given topology of 

the model assign transition and emission 

probabilities  

–  Gather statistics and compute the model in an 

explicit way (what we have done so far).  

–  Have a computer algorithm that given training 

set as the input constricts best fitting model 

well developed methods.  


Parameter Estimation from a training set 

•  Problem: Given a training set S1, ..Sn and a 

topology of HMM find emission and transition 

probabilities M that maximize the likelihood 

that S1, ..Sn are generated by the model. 

•  Assumption: S1, ..Sn are independent(!): 

             P(S1, ..Sn |M) = Πi P(Si |M)  

•  As usual, substituting likelihood with score: 

Score (S1, ..Sn |M) = log P(S1, ..Sn |M)  

                   = Σi log P(Si |M)  

 


Two variants of parameter estimation 

method 

1.  For each sequence in the training set we 

know the path in HMM used to generate 

this sequence. 

2.  Paths is unknown.  


Estimation when the path is known 

•  Examples:  features of DNA sequence intrins/

exons etc – we may know the path from 

experimental data 

•  Estimation method: 

Akl , Ekb # of times given transitions/ emission is 

chosen (we know them since we know the paths!) 

Set: 

 a(k,l) =(Akl)/(Σl’ Akl’) ; e(k,b) = (Ekb)/(Σb’ Ekb’)  


Adding pseudocounters (Bayesian 

approach)  

•  Potential problem – insufficient data; what if 

some state is never used in training? 

•  Extend the statistics by adding “pseudocounters” 

rkl, r’kb : 

A kl  # of times given transitions + rkl; 

Ekb  # of times given emission is chosen+ r’kb 

•  Choosing pseudocounters: 

–   no prior knowledge – small total values Σ rkl Σ r’kl 

–   large total values – reflection of prior knowledge. 


Parameter estimation when paths are 

unknown: Baum-Welsh method 

1. 

Start with some initial parameters (say uniform 

probability distribution) 

2. 

For a given training set compute expected number of 

times, Akl, Ekl , each transition/emission is used. 

3. 

Estimate parameters of the model from Akl, Ekl (use 

these values in the same way as the corresponding values for 

the algorithm when the paths are known) 

4. 

Repeat steps 2 and 3 until a convergence criterion is 

reached. 

 1. How to compute Akl, Ekl ? 

 2. Does the parameter estimation converges ? 

 3. What it converges to? 

QUESTIONS: 


Calculating Akl, Ekl in Baum Welch (training 

 

Dynamic programming algorithm using method 

similar to the algorithm for computing 

probability most likely state that generated ith 

symbol. 


convergence 

•  B-W  training is an example example of EM 

(Expectation maximization) method 

•  Each iteration is an improvement (for expectation of 

generating a given training set) (unless maximum is 

reached). (In next model sequences are generated with higher probability 

than on the model form previous iteration) 

•  Converges to local maximum (but not necessarily in a 

finite number of steps) 

•  Sample termination criterions : 

–  The improvement is small 

–  Maximum number of iterations is reached 

•  Problem with the method: once transition probability is 

set to zero it will remain to be so in subsequent 

iterations. Slow. 


Viterbi learning  

•  Replace the calculation involving all possible 

paths with calculation using most likely path. 

•  Faster algorithm – no backward computation 

is needed. 

•  Viterbi path is interpretable in terms of the 

sequence features. 

•   Meta-MEME (Motif-based Hidden Markov 

Models of Protein,  San Diego) trains using 

the Viterbi algorithm  


Profile HMM 

•  Goal: to model sequence similarities 

•  Idea – extending  the idea of sequence profile 

•  Simplified goal: Assume that the sequences 

are aligned without gaps (e.g.. Blocks).  

•  Method: Have each state corresponding to 

one position of the alignments.Each state 

emits amino-acids with probability equal to 

the frequency of an amino-acid at this 

position. 


Building Profile HMM 

Model 1: No gaps-same sequence length 

 aligned sequences : 

ACATTC 

ACCTTC 

ACATTC 

AGAATA 

 

Position/ 

letter 

1 

2 

3 

4 

5 

6 

A 

1.0 

0.0 

.75 

0.25 

0.0 

0.25 

C 

0.0 

0.75 

0.25 

0.0 

0.0 

0.0 

T 

0.0 

0.0 

0.0 

0.75 

1.0 

0.0 

G 

0.0 

.25 

0.0 

0.0 

0.0 

.75 

Profile 

HMM: 

A   1.0 

C   0.0 

T   0.0 

G   0.0 

A   0.0 

C   .75 

T   0.0 

G   .25 

A   .75 

C   .25 

T   0.0 

G   0.0 

A   .25 

C   0.0 

T   .75 

G   0.0 

A   0.0 

C   0.0 

T   1.0 

G   0.0 

A   .25 

C   0.0 

T   0.0 

G   .75 

6 sates, linear topology, transition probability form i to i+1 equal 1 

Emission probability defined by profile 

1 

1 

1 

1 

1 


Building Profile HMM 

Model 2: Insertions of length 1 

 aligned sequences : 

ACATT - C 

ACCTT - C 

ACATT - C 

AGAATGA 

 

HMM: 

A   1.0 

C   0.0 

T   0.0 

G   0.0 

A   0.0 

C   .75 

T   0.0 

G   .25 

A   .75 

C   .25 

T   0.0 

G   0.0 

A   .25 

C   0.0 

T   .75 

G   0.0 

A   0.0 

C   0.0 

T   1.0 

G   0.0 

A   .25 

C   0.0 

T   0.0 

G   .75 

“Insert” state added 

1.0 

1.0 

1.0 

1.0 

.75 

0.25 

1.0 

A   0.0 

C   0.0 

T   0.0 

G   1.0 


Building Profile HMM  

Model 3: Insertions of  variable length gap 

 aligned sequences : 

ACATT - -  - -  C 

ACCAT - - - -   C 

ACAAT - - - -  C 

AGAATGCGCA 

 

HMM: 

A   1.0 

C   0.0 

T   0.0 

G   0.0 

A   0.0 

C   0.75 

T   0.0 

G   0.25 

A   .75 

C   .25 

T   0.0 

G   0.0 

A   .25 

C   0.0 

T   .75 

G   0.0 

A   0.0 

C   0.0 

T   1.0 

G   0.0 

A   .25 

C   0.0 

T   0.0 

G   .75 

 e – gap extension probability: 3/12 since there are 

12 possibilities 3 of them extensions 

1.0 

1.0 

1.0 

1.0 

.75 

0.25 

1.0 – e  

A   0 

C   .5 

T   0 

G   .5 

e 

Problems: The insert model is to weak to capture GCGC pattern in 

the insert and the extension probability does not depend on gap 

length. 


Building Profile HMM  

Model 4: Constant length deletion 

 aligned sequences : 

ACATT - -  - -  C 

ACCAT - - - -   C 

ACAAT - - - -  C 

AGAATGCGCA 

AC- -  TGCGC C 

HMM: 

A   1.0 

C   0.0 

T   0.0 

G   0.0 

A   0.0 

C   0.8 

T   0.0 

G   .2 

A   .75 

C   .25 

T   0.0 

G   0.0 

A   .75 

C   0.0 

T   .25 

G   0.0 

A   0.0 

C   0.0 

T   1.0 

G   0.0 

A   .2 

C   .8 

T   0.0 

G   0.0 

1 

0.8 

1.0 

1.0 

.6 

0.4 

1.0 – e  

A   0 

C   .5 

T   0 

G   .5 

e 

Delete state – no emission 

.2 

1.0 


Building Profile HMM  

Model 5: Variable length deletion 

HMM: 

A   1.0 

C   0.0 

T   0.0 

G   0.0 

A   0.0 

C   0.8 

T   0.0 

G   .2 

A   .75 

C   .25 

T   0.0 

G   0.0 

A   .25 

C   0.0 

T   .75 

G   0.0 

A   0.0 

C   0.0 

T   1.0 

G   0.0 

A   .2 

C   .8 

T   0.0 

G   0.0 

1 

.8 

1 

1.0 

.6 

0.4 

1.0 – e  

A   0 

C   .5 

T   0 

G   .5 

e 

1.0 

Deletion of length 1 or 2 

.2 

b 

c 

1-c 


Building Profile HMM:Complete Model 



Matching states 

Insertion states 

Deletion states 

Parameters: 

number of matching states = average sequence length in the family 

emission and transition probabilities – estimated form training set 


Example 

Subcluster 

of block 

BL00094F 

YKQAGNSVVV 

YKQLGNSVTV 

YRQFGNSVAV 

YKQAGNSITV 

YKQAGNSITV   

YKQTGNSITV   

YRQFGNSVSV   

WRQFGNSVPV   

YRQFGNSVVV   

YKQFGNSVVI   

YKQFGNSVAV   

YRQMGNSVVV   

YRQFGNSVCV   

YKQFGNSVAV   

 

 Recall example: 

Y 14/15 

W 1/15 

K 9/15 

R 6/15 

K 10/15 

R 5/15 

Q 14/16 

E 1/15 

1.0 

1.0 

1.0 

1.0 

With pseudcounters set to 1 

(14 + 1) / (14 + 20) = 15/34 

Better with smaller pseudcounters set to .1: 

(14 + .1) / (14 + 2.0) = 15/16 


Comments on training set 

•  Size of training set must be large relatively to the 

number of states to avoid possibility over fitting 

Usually we partition randomly the data into training 

set and test set 

•  It may be necessary to weight training set. 

Subcluster of block BL00094F 

YKQAGNSVVV 

YKQLGNSVTV 

YRQFGNSVAV 

YKQAGNSITV 

YKQAGNSITV   

YKQTGNSITV   

YRQFGNSVSV   

WRQFGNSVPV   

YRQFGNSVVV   

YKQFGNSVVI   

YKQFGNSVAV   

YRQMGNSVVV   

YRQFGNSVCV   

YKQFGNSVAV 

RKQIGMAVPP 

Representative of 

another sub cluster 

If the two sub-clusters 

are of different sizes one 

may consider to assign 

weights to each training 

sequence reverse 

proportional to the size 

of the cluster. 


Sample applications of HMM 

•  Representing protein families (Profile HMM) 

•  Multiple sequence alignment 

•  Database searches 

•  Fold recognition 

•  Gene finding 

•  Genomic region discrimination (CpG islands) 


Sequence alignment and HMM 

1.  Construct profile HMM for your family.  

2.  For each sequence find most likely path. 

3.  The matching/insert/delete states define the 

alignment. 



In point 1 often a “seed” alignment is used 


Pfam database 

The Pfam database is a large collection of protein (domain) families, 

each represented by multiple sequence alignments and hidden Markov 

models (HMMs). 

 

http://pfam.sanger.ac.uk/ 

 

Proteins are generally composed of one or more functional regions, 

commonly termed domains. 









DNA polymerase, B family 


Pfam database 

There are two components to Pfam: Pfam-A and Pfam-B. Pfam-A 

entries are high quality, manually curated families. Although of lower 

quality, Pfam-B families can be useful for identifying functionally 

conserved regions when no Pfam-A entries are found.  

 

Pfam also generates higher-level groupings of related families, known 

as clans. A clan is a collection of Pfam-A entries which are related by 

similarity of sequence, structure or profile-HMM.  







