
Tufts University

Electrical and Computer Engineering

EE194 – Network Information Theory

Prof. Mai Vu

Lecture 1: Entropy and mutual information

1

Introduction

Imagine two people Alice and Bob living in Toronto and Boston respectively. Alice (Toronto) goes

jogging whenever it is not snowing heavily. Bob (Boston) doesn’t ever go jogging.

Notice that Alice’s actions give information about the weather in Toronto. Bob’s actions give

no information. This is because Alice’s actions are random and correlated with the weather in

Toronto, whereas Bob’s actions are deterministic.

How can we quantify the notion of information?

2

Entropy

Deﬁnition The entropy of a discrete random variable X with pmf pX(x) is

H(X) = −

�

x

p(x) log p(x) = −E[ log(p(x)) ]

(1)

The entropy measures the expected uncertainty in X. We also say that H(X) is approximately

equal to how much information we learn on average from one instance of the random variable X.

Note that the base of the algorithm is not important since changing the base only changes the

value of the entropy by a multiplicative constant.

Hb(X) = − �

x p(x) logb p(x) = logb(a)[�

x p(x) loga p(x)] = logb(a)Ha(X). Customarily, we use

the base 2 for the calculation of entropy.

2.1

Example

Suppose you have a random variable X such that:

X =

� 0

with prob p

1

with prob 1 − p,

(2)

then the entropy of X is given by

H(X) = −p log p − (1 − p) log(1 − p) = H(p)

(3)

Note that the entropy does not depend on the values that the random variable takes (0 and 1

in this case), but only depends on the probability distribution p(x).

1


Tufts University

Electrical and Computer Engineering

EE194 – Network Information Theory

Prof. Mai Vu

2.2

Two variables

Consider now two random variables X, Y jointly distributed according to the p.m.f p(x, y). We now

deﬁne the following two quantities.

Deﬁnition The joint entropy is given by

H(X, Y ) = −

�

x,y

p(x, y) log p(x, y).

(4)

The joint entropy measures how much uncertainty there is in the two random variables X and Y

taken together.

Deﬁnition The conditional entropy of X given Y is

H(X|Y ) = −

�

x,y

p(x, y) log p(x|y) = −E[ log(p(x|y)) ]

(5)

The conditional entropy is a measure of how much uncertainty remains about the random variable

X when we know the value of Y .

2.3

Properties

The entropic quantities deﬁned above have the following properties:

• Non negativity: H(X) ≥ 0, entropy is always non-negative. H(X) = 0 iﬀ X is deterministic.

• Chain rule: We can decompose the joint entropy as follows:

H(X1, X2, . . . , Xn) =

n

�

i=1

H(Xi|Xi−1),

(6)

where we use the notation Xi−1 = {X1, X2, . . . , Xi−1}.

For two variables, the chain rule becomes:

H(X, Y )

=

H(X|Y ) + H(Y )

(7)

=

H(Y |X) + H(X).

(8)

Note that in general H(X|Y ) ̸= H(Y |X).

• Monotonicity:

Conditioning always reduces entropy:

H(X|Y ) ≤ H(X).

(9)

In other words “information never hurts”.

2


Tufts University

Electrical and Computer Engineering

EE194 – Network Information Theory

Prof. Mai Vu

• Maximum entropy:

Let X be set from which the random variable X takes its values

(sometimes called the alphabet), then

H(X) ≤ log |X|.

(10)

The above bound is achieved when X is uniformly distributed.

• Non increasing under functions:

Let X be a random variable and let g(X) be some

deterministic function of X. We have that:

H(X) ≥ H(g(X)),

(11)

with equality iﬀ g is invertible.

Proof: We will the two diﬀerent expansions of the chain rule for two variables.

H(X, g(X))

=

H(X, g(X))

(12)

H(X) + H(g(X)|X)

�

��

�

=0

=

H(g(X)) + H(X|g(X)),

(13)

so we have

H(X) − H(g(X) = H(X|g(X)) ≥ 0.

(14)

with equality if and only if we can deterministically guess X given g(X), which is only the

case if g is invertible. □

3

Continuous random variables

Similarly to the discrete case we can deﬁne entropic quantities for continuous random variables.

Deﬁnition The diﬀerential entropy of a continuous random variable X with p.d.f f(x) is

h(X) = −

�

f(x) log f(x)dx = −E[ log(f(x)) ]

(15)

Deﬁnition Consider a pair of continuous random variable (X, Y ) distributed according to the joint

p.d.f f(x, y). The joint entropy is given by

h(X, Y ) = −

� �

f(x, y) log f(x, y)dxdy,

(16)

while the conditional entropy is

h(X|Y ) = −

� �

f(x, y) log f(x|y)dxdy.

(17)

3


Tufts University

Electrical and Computer Engineering

EE194 – Network Information Theory

Prof. Mai Vu

3.1

Properties

Some of the properties of the discrete random variables carry over to the continuous case, but some

do not. Let us go through the list again.

• Non negativity doesn’t hold: h(X) can be negative.

Example:

Consider the R.V. X uniformly distributed on the interval [a, b]. The entropy is

given by

h(X) = −

� b

a

1

b − a log

1

b − adx = log(b − a),

(18)

which can be a negative quantity if b − a is less than 1.

• Chain rule holds for continuous variables:

h(X, Y )

=

h(X|Y ) + h(Y )

(19)

=

h(Y |X) + h(X).

(20)

• Monotonicity:

h(X|Y ) ≤ h(X)

(21)

The proof follows from the non-negativity of mutual information (later).

• Maximum entropy:

We do not have a bound for general p.d.f functions f(x), but we do

have a formula for power-limited functions. Consider a R.V. X ∼ f(x), such that

E[x2] =

�

x2f(x)dx ≤ P,

(22)

then

max h(X) = 1

2 log(2πeP),

(23)

and the maximum is achieved by X ∼ N(0, P).

To verify this claim one can use standard Lagrange multiplier techniques from calculus to

solve the problem max h(f) = −

�

f log fdx, subject to E[x2] =

�

x2fdx ≤ P.

• Non increasing under functions:

Doesn’t necessarily hold since we can’t guarantee

h(X|g(X)) ≥ 0.

4

Mutual information

Deﬁnition The mutual information between two discreet random variables X, Y jointly distributed

according to p(x, y) is given by

I(X; Y )

=

�

x,y

p(x, y) log p(x, y)

p(x)p(y)

(24)

=

H(X) − H(X|Y )

=

H(Y ) − H(Y |X)

=

H(X) + H(Y ) − H(X, Y ).

(25)

4


Tufts University

Electrical and Computer Engineering

EE194 – Network Information Theory

Prof. Mai Vu

We can also deﬁne the analogous quantity for continuous variables.

Deﬁnition The mutual information between two continuous random variables X, Y with joint p.d.f

f(x, y) is given by

I(X; Y )

=

� �

f(x, y) log f(x, y)

f(x)f(y)dxdy.

(26)

For two variables it is possible to represent the diﬀerent entropic quantities with an analogy

to set theory. In Figure 4 we see the diﬀerent quantities, and how the mutual information is the

uncertainty that is common to both X and Y .

H(X)

I(X : Y )

H(X|Y )

H(Y |X)

H(Y )

Figure 1: Graphical representation of the conditional entropy and the mutual information.

4.1

Non-negativity of mutual information

In this section we will show that

I(X; Y ) ≥ 0,

(27)

and this is true for both the discrete and continuous cases.

Before we get to the proof, we have to introduce some preliminary concepts like Jensen’s in-

equality and the relative entropy.

Jensen’s inequality

tells us something about the expected value of a random variable after

applying a convex function to it.

We say a function is convex on the interval [a, b] if, ∀x1, x2 ∈ [a, b] we have:

f(θx1 + (1 − θ)x2) ≤ θf(x1) + (1 − θ)f(x2).

(28)

Another way stating the above is to say that the function always lies below the imaginary line

joining the points (x1, f(x1)) and (x2, f(x2)). For a twice-diﬀerentiable function f(x), convexity is

equivalent to the condition f ′′(x) ≥ 0, ∀x ∈ [a, b].

5


Tufts University

Electrical and Computer Engineering

EE194 – Network Information Theory

Prof. Mai Vu

Lemma Jensen’s inequality states that for any convex function f(x), we have

E[f(x)] ≥ f(E[x]).

(29)

The proof can be found in [Cover &amp; Thomas].

Note that an analogue of Jensen’s inequality exists for concave functions where the inequality

simply changes sign.

Relative entropy

A very natural way to measure the distance between two probability distribu-

tions is the relative entropy, also sometimes called the Kullback-Leibler divergence.

Deﬁnition The relative entropy between two probability distributions p(x) and q(x) is given by

D(p(x)||q(x)) =

�

x

p(x) log p(x)

q(x).

(30)

The reason why we are interested in the relative entropy in this section is because it is related

to the mutual information in the following way

I(X; Y ) = D(p(x, y)||p(x)p(y)).

(31)

Thus, if we can show that the relative entropy is a non-negative quantity, we will have shown that

the mutual information is also non-negative.

Proof of non-negativity of relative entropy:

Let p(x) and q(x) be two arbitrary probability distri-

butions. We calculate the relative entropy as follows:

D(p(x)||q(x))

=

�

x

p(x) log p(x)

q(x)

=

−

�

x

p(x) log q(x)

p(x)

=

−E

�

log q(x)

p(x)

�

≥

− log

�

E

�q(x)

p(x)

��

(by Jensen’s inequality for concave function log(·))

=

− log

��

x

p(x)q(x)

p(x)

�

=

− log

��

x

q(x)

�

=

0.

□

6


Tufts University

Electrical and Computer Engineering

EE194 – Network Information Theory

Prof. Mai Vu

4.2

Conditional mutual information

Deﬁnition Let X, Y, Z be jointly distributed according to some p.m.f. p(x, y, z). The conditional

mutual information between X, Y given Z is

I(X; Y |Z)

=

−

�

x,y,z

p(x, y, z) log

p(x, y|z)

p(x|z)p(y|z)

(32)

=

H(X|Z) − H(X|Y Z)

=

H(XZ) + H(Y Z) − H(XY Z) − H(Z).

The conditional mutual information is a measure of how much uncertainty is shared by X and

Y , but not by Z.

4.3

Properties

• Chain rule:

We have the following chain rule

I(X ; Y1Y2 . . . Yn) =

n

�

i=1

I(X; Yi|Y i−1),

(33)

where we have used again the shorthand notation Y i−1 = {Y1, Y2, . . . , Yi−1}.

• No monotonicity:

Conditioning can either increase or decrease the mutual information

between two variables, so

I(X; Y |Z) ≱ I(X; Y ),

and

I(X; Y |Z) ≰ I(X; Y ).

(34)

To illustrate the last point, consider the following two examples where conditioning has diﬀerent

eﬀects. In both cases we will make use of the following equation

I(X; Y Z)

=

I(X; Y Z)

I(X; Y ) + I(X; Z|Y )

=

I(X; Z) + I(X; Y |Z).

(35)

Increasing example:

If we have some X, Y, Z such that I(X; Z) = 0 (which means X and Z

are independent variables), then equation (35) becomes:

I(X; Y ) + I(X; Z|Y ) = I(X; Y |Z),

(36)

so I(X; Y |Z) − I(X; Y ) = I(X; Z|Y ) ≥ 0, which implies

I(X; Y |Z) ≥ I(X; Y ).

(37)

Decreasing example:

On the other hand if we have a situation in which I(X; Z|Y ) = 0,

equation (35) becomes:

I(X; Y ) = I(X; Z) + I(X; Y |Z),

(38)

which in implies that I(X; Y |Z) ≤ I(X; Y ).

So we see that conditioning of the mutual information can both increase or decrease it depending

on the situation.

7


Tufts University

Electrical and Computer Engineering

EE194 – Network Information Theory

Prof. Mai Vu

5

Data processing inequality

For three variables X, Y, Z one situation which is of particular interest is when they form a Markov

chain:

X → Y

→ Z.

This relation is implies that the probability distribution p(x, z|y) =

p(x|y)p(z|y) which in turn implies that I(X; Z|Y ) = 0 like in the example above.

This situation often occurs when we have some input X that gets transformed by a channel to

give an output Y and then we want to apply some processing to obtain a signal Z as illustrated

below.

X → Channel → Y → Processing → Z

In this case we have the data processing inequality:

I(X; Z) ≤ I(X; Y ).

(39)

In other words, processing cannot increase the information contained in a signal.

8

