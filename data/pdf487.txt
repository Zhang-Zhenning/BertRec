
CS262 Lecture Notes: Hidden Markov Models

Sarah S

January 21 2016

1

Summary

Last lecture introduced hidden Markov models, and began to discuss some of

the algorithms that can be used with HMMs to learn about sequences. In this

lecture, we dive more deeply into the capabilities of HMMs, focusing mostly on

their use in evaluation. We discuss higher order HMMs, and end with a brief

discussion of HMM learning, to be expanded on in the next lecture.

2

CpG Islands

The main example used to illustrate the concepts of HMMs is the idea of “CpG

islands.” The notation ‘CpG’ indicates two adjacent nucleotides, a C and a G

(these are next to each other, not paired across the strand — the p stands for

the phosphate inbetween the cytosine and guanine).

CpG islands are inter-

esting because the distribution of adjacent Cs and Gs is diﬀerent from other

nucleotide pairs — they occur much less often than you’d expect in the overall

genome (“CG supression”).

However, there are places in the genome where they occur much more often.

These are CpG islands, and they’re interesting because they tend to be methy-

lated (an extra methyl is added to a cytosine base).

Methylation closes up

the structure of the strand, making it too close together for the transcription

enzymes to get to — so the section can’t be transcribed. When this happens

near a promoter, and entire gene can be permanently silenced. Over 60% of

promoters occur in CpG islands.

1




[1]

Research has linked methylation of CpG groups near the promoters of tumor

supression genes to human cancer [2]. So there is a lot of value in being able to

ﬁnd and identify CpG islands in the genome. And we can do that with hidden

Markov models.

3

A Review of Decoding

Decoding is one of the three main uses of HMMs. You know the model and

the sequence. You are maximizing for the likeliest path to produce a known

sequence.

The other two are:

Evaluation: you know the model and the sequence, and are looking for the par-

ticular probability of some substring being produced by the model.

Learning: you know the sequence, but the model is unspeciﬁed, so you need to

ﬁgure out the parameters that maximize the probability of getting that sequence.

Formally:



2


Consider the question of decoding in the context of CpG islands. Decoding

can tell you which areas are islands, and which are not, by determining what

path is most likely to have led to the sequence at hand. (For detailed formulas

and math, see the notes from last lecture)

4

Evaluation

This determines the likelihood a sequence is generated by a particular model.

Or, given the sequence and the model, what is the most likely state at some

position? If you remember the casino example (a casino is switching in a loaded

die that rolls sixes half the time), evaluation asks “is a particular region loaded

or fair?” You pick a particular area, then compare against all possible states to

ﬁnd out what the greatest probability is. (N.B. remember to account for the

probability cost of switching between states at the borders of the selected area.)

Speciﬁcally, Evaluation can compute:

P(x) – Probability of x given the model

P(xi. . . xj) – Probability of a substring of x given the model

P(πi = k|x) – “Posterior” probability that the ith state is k, given x

5

Probability Review

Pr(Y ) = �

x∈X Pr(X, Y )

Joint probability: Pr(X, Y ) = Pr(X|Y ) ∗ Pr(Y )

Bayes Rule: P(X|Y ) = (P(Y |X)P(X))/P(Y )

6

The Forward Algorithm

So how do you calculate P(x) given the model? With a “forward probability.”

The forward probability is the probability that some position xi is what it is,

given everything that came before. So, sum over all possible ways of generating

x: P(x) = �

π P(x, π) =π P(x|π)P(π)

Seems reasonable, but this creates an exponential number of paths! Luckily,

we don’t have to deal with all of them. Really all we care about is generating

the ﬁrst i characters of x, and ending up in state k. So we can simplify to:

fk(i) = P(x1. . . xi, πi = k).

This leads to fk(i) = ek(xi) �

l fl(i–1)alk

Derivation:

fk(i) = P(x1...xi, πi = k)

use probability rules to obtain:

3


= �

π1...πi−1 P(x1...xi−1, π1....πi−1, πi = k)ek(xi)

ek(xi) is the emission probability, i.e. the probability that state k will emit

the value xi

= �

l

�

π1...πi−2 P(x1...xi−1, π1....πi−2, πi−1 = l)alkek(xi)

alk is the transition probability, that between states πi−1 and πi we will

transition from state l to state k

Simplify:

= �

l P(x1...xi−1, πi−1 = l)alkek(xi)

And now using the original deﬁnition:

= ek(xi) �

l fl(i − 1)alk

Now that we know how to get a forward probability, how do we ﬁgure out

the maximum probability? Dynamic programming!

Initialization: f0(0) = 1 and fk(0) = 0, for all k ¿ 0

Iteration: fk(i) = ek(xi) �

l fl(i–1)alk

Termination: P(x) = �

k fk(N)

7

Backwards Algorithm

Now, what if we want some P(πi = k|x), the probability distribution that the

ith position is k, given x? We need a helper to ﬁgure out the probability at our

point of interest. We already have the forward probability — now we need the

backwards probability.

Fun fact! P(x) can be deﬁned with the backwards probability as well as the

forward —- they are identical in their structure of computational steps and

recursion.

7.1

Derivation of backwards probability

Deﬁne the backward probability: Starting from ith state = k, generate rest of x

bk(i) = P(xi + 1. . . xN|πi = k)

Expand with probability rules:

= � +πi+1. . . πNP(xi+1, xi+2, ...xN, πi+1...πN|πi = k)

Extract the state of πi+1:

= �

l

� πi...πNP(xi+1. . . xN, πi+1 = l, πx+2, . . . , πN|πi = k)

e is the emission probability; a is the transition probability:

= �

l el(xi+1)akl

� πi...πNP(xi+2, . . . , xN, πi+2, . . . , πN|πi+1 = l)

4


Final result:

= �

l el(xi+1)aklbl(i + 1)

And again, dynamic programming saves the day, as we ﬁnd the best proba-

bility by computing bk(i) for all k, i

Initialization: bk(N) = 1, for all k

Iteration: bk(i) = �

l el(xi+1)aklbl(i + 1)

Termination: P(x) = �

l a0lel(x1)bl(1)

7.2

Computational Complexity for forward and backward

Not great. O(K2N) time, and O(KN) space.

There’s also a danger of underﬂows, since all this multiplying decimals leads

to really small numbers. Remember that when using Viterbi, you can use the

sum of logs to avoid underﬂow? Well, when computing Forward or Backward

probabilities, you can rescale at each few positions by multiplying by a constant.

That keeps the numbers big enough for the computer to handle.

8

Posterior Decoding:

Posterior probability asks: what is the most likely state at position i of sequence

x? Or, what is the probability that the ith state is k, given x?

Since we’ve shown how to compute fk(i) and bk(i), we can calculate P(πi =

k|x) = (fk(i)bk(i))/P(x)

Let’s introduce a bit of new notation:

πˆi = argmaxkP(πi = k|x)

Posterior decoding produces a curve of likelihood of state for each position:

5




But some of these paths may be invalid, made up of states that cannot oc-

cur in that sequence. The probability of such a path will be zero. How does

this happen? You might end up with the most likely state at xi not having a

transition to get there from xi−1!

Step back for a moment: what are the diﬀerences between Viterbi and pos-

terior decoding? In Viterbi you are ﬁnding the entire path; in posterior you are

ﬁnding one state.

Another way to see it: the probability of entering a state is the probability

of all the paths that go through that state added up.

6


9

Comparison of what we’ve seen



Note that it only requires small tweaks to adapt one algorithm to the other.

10

Variants of HMMs

10.1

Higher order HMMs

The models we’ve been considering have a ”memory” of one position.

But

sometimes we might be interested in information about more than one time

point.

In math:

P(πi+1 = l|πi = k)akl

versus

P(πi+1 = l|πi = k, πi−1 = j)ajkl

Take a look at these example models for ﬂipping a coin, one is a single state

model, the other a double state model.

7






In a double state model, some of the transitions can’t happen — but that

depends on how you choose to deﬁne transitions. For example, if we think of

coin ﬂips, heads as H and tails as T, HT to HT isn’t possible if the second letter

of the ﬁrst double state is the ﬁrst letter of the second state, because T is not

H. But HT to TH makes sense, because the T overlaps.

What happens if we try Viterbi on a double state model?

The dimensions of the matrix increase: a k state, n-length parse leads to k2

states, and a k2n computation. This shows why you should try and avoid too

much past dependence: if you have to model every possible state in the world,

it takes too much time and space!

10.2

Modeling the Duration of States

Let’s go back to our CpG example. At this point, we can ask WHERE are the

CpG islands, can we also ask HOW LONG are the CpG islands?

That answer can tell us about accumulation of mutations over generations,

or susceptibility to mutation in an individual.

To do that, we need the probability you will stay in or switch out of a state.

The canonical example is, of course, ﬂipping a coin. The question: How

many times do I have to ﬂip coin before seeing a tails? (i.e. how long do I stay

in a heads-state before transitioning to a tails-state?)

P(num of heads before ﬁrst tail)

= P(ktimes)

= pk−1 ∗ (1 − p)

(little p is probability of ﬂipping heads)

8


Expectation[k] = 1/(1-p)

Note that if p is large, 1-p is small, so E[k] is large.

This results in a geometric distribution.

But there’s an interesting problem here. Take a look at these graphs:



These graphs show the exon lengths in genes. Only the upper left graph

(which actually shows introns) has a nice geometric distribution. In the rest,

there is a peak, and we can’t just ignore the smaller values to the left. We need

a model that can account for those exons so they don’t get lost. A geometric

sequence will decrease right from the start, so it doesn’t match what we see in

real life.

So what can we do?

One option is to chain several states together.



In this case, we end up with lx = C+ geometric with mean 1/(1-p).

This means the distribution is still very inﬂexible, though it will catch a diﬀer-

ent set of possible graphs.

9


Another option is to use a negative binomial distribution. It turns out that

this is a very good model for gene distribution.



We can see the result here, in a graph of prokaryotic gene length. Take a look

at EasyGene, a prokaryotic gene-ﬁnder to try it yourself. (http://www.cbs.dtu.dk/services/EasyGene/)



[3]

Our last potential solution is duration modeling.

Duration modeling seems like a very elegant solution. It is very ﬂexible, and

a simpler model than some of our other options.

10


Rather than add a bunch more states, simply choose a duration d according

to a probability distribution, then generate letters d times while staying in the

same state. The letters generated are controlled by the emission probabilities.

Once the d letters have been generated, choose the next state to transition to,

according to the transition probabilities.



However, the complexity gets much worse.



So choose your method carefully, with an eye to what distributions most

accurately model the data.

11

A brief intro to Learning

Often we don’t know transition probabilities for a model, and we have to ﬁgure

them out ourselves.

11


Questions that can arise in that situation include:

What is the distribution of a fair vs loaded die?

What does a CpG island look like?

To answer these, we’ll recursively update paramaters to maximize P, using

expectation maximization.

The biggest problem is overﬁtting to small data. Imagine if in your data

some output doesn’t show up – it will have a probability of zero in the cal-

culated transition probabilities, if we’re simply counting how often we see an

output in our data. But that might not be true!

Psuedocounts can compensate for overﬁtting. To guess a pseudo count, you

can use your prior beliefs or knowledge about a situation, or simply use very

small probabilities to stand in for ones you don’t know anything about.

For a full treatment of this topic, see the next lecture.

———

[1] http://helicase.pbworks.com/w/page/17605615/DNA%20Methylation

[2] See this 1983 Nature article for interesting early work in this area: ”Hy-

pomethylation distinguishes genes of some human cancers from their normal

counterparts” – http://www.ncbi.nlm.nih.gov/pubmed/6185846

[3] Larsen TS, Krogh A

12

