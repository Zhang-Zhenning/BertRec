
arXiv:1204.5488v5  [stat.ME]  9 Nov 2015

Estimation of a Two-component Mixture Model with

Applications to Multiple Testing

Rohit Kumar Patra and Bodhisattva Sen

Columbia University, USA

Abstract

We consider a two-component mixture model with one known component. We develop

methods for estimating the mixing proportion and the unknown distribution nonparamet-

rically, given i.i.d. data from the mixture model, using ideas from shape restricted function

estimation. We establish the consistency of our estimators. We ﬁnd the rate of convergence

and asymptotic limit of the estimator for the mixing proportion. Completely automated

distribution-free honest ﬁnite sample lower conﬁdence bounds are developed for the mixing

proportion. Connection to the problem of multiple testing is discussed. The identiﬁability

of the model, and the estimation of the density of the unknown distribution are also ad-

dressed. We compare the proposed estimators, which are easily implementable, with some

of the existing procedures through simulation studies and analyse two data sets, one arising

from an application in astronomy and the other from a microarray experiment.

Keywords: Cram´er-von Mises statistic, cross-validation, functional delta method, identiﬁability,

local false discovery rate, lower conﬁdence bound, microarray experiment, projection operator,

shape restricted function estimation.

1

Introduction

Consider a mixture model with two components, i.e.,

F(x) = αFs(x) + (1 − α)Fb(x),

(1)

where the cumulative distribution function (CDF) Fb is known, but the mixing proportion

α ∈ [0, 1] and the CDF Fs (̸= Fb) are unknown.

Given a random sample from F, we wish

to (nonparametrically) estimate Fs and the parameter α.

This model appears in many contexts. In multiple testing problems (microarray analysis,

neuroimaging) the p-values, obtained from the numerous (independent) hypotheses tests, are

uniformly distributed on [0,1], under H0, while their distribution associated with H1 is unknown;

see e.g., Efron (2010) and Robin et al. (2007). Translated to the setting of (1), Fb is the uni-

form distribution and the goal is to estimate the proportion of false null hypotheses α and the

distribution of the p-values under the alternative. In addition, a reliable estimator of α is im-

portant when we want to assess or control multiple error rates, such as the false discovery rate

of Benjamini and Hochberg (1995).

1


In contamination problems, the distribution Fb, for which reasonable assumptions can be

made, may be contaminated by an arbitrary distribution Fs, yielding a sample drawn from F

as in (1); see e.g., McLachlan and Peel (2000). For example, in astronomy, such situations arise

quite often: when observing some variable(s) of interest (e.g., metallicity, radial velocity) of stars

in a distant galaxy, foreground stars from the Milky Way, in the ﬁeld of view, contaminate the

sample; the galaxy (“signal”) stars can be diﬃcult to distinguish from the foreground stars as we

can only observe the stereographic projections and not the three dimensional position of the stars

(see Walker et al. (2009)). Known physical models for the foreground stars help us constrain Fb,

and the focus is on estimating the distribution of the variable for the signal stars, i.e., Fs. We

discuss such an application in more detail in Section 9.2. Such problems also arise in High Energy

physics where often the signature of new physics is evidence of a signiﬁcant-looking peak at some

position on top of a rather smooth background distribution; see e.g., Lyons (2008).

Most of the previous work on this problem assume some constraint on the form of the un-

known distribution Fs, e.g., it is commonly assumed that the distributions belong to certain para-

metric models, which lead to techniques based on maximum likelihood (see e.g., Cohen (1967)

and Lindsay (1983)), minimum chi-square (see e.g., Day (1969)), method of moments (see e.g.,

Lindsay and Basak (1993)), and moment generating functions (see e.g., Quandt and Ramsey (1978)).

Bordes et al. (2006) assume that both the components belong to an unknown symmetric location-

shift family. Jin (2008) and Cai and Jin (2010) use empirical characteristic functions to estimate

Fs under a semiparametric normal mixture model. In multiple testing, this problem has been

addressed by various authors and diﬀerent estimators and conﬁdence bounds for α have been

proposed in the literature under certain assumptions on Fs and its density, see e.g., Storey (2002),

Genovese and Wasserman (2004), Meinshausen and Rice (2006), Meinshausen and B¨uhlmann (2005),

Celisse and Robin (2010) and Langaas et al. (2005). For the sake of brevity, we do not discuss

the above references here but come back to this application in Section 7.

In this paper we provide a methodology to estimate α and Fs (nonparametrically), with-

out assuming any constraint on the form of Fs. The main contributions of our paper can be

summarised in the following.

• We investigate the identiﬁability of (1) in complete generality.

• When F is a continuous CDF, we develop an honest ﬁnite sample lower conﬁdence bound

for the mixing proportion α.

We believe that this is the ﬁrst attempt to construct a

distribution-free lower conﬁdence bound for α that is also tuning parameter-free.

• Two diﬀerent estimators of α are proposed and studied. We derive the rate of convergence

and asymptotic limit for one of the proposed estimators.

• A nonparametric estimator of Fs using ideas from shape restricted function estimation is

proposed and its consistency is proved. Further, if Fs has a non-increasing density fs, we

can also consistently estimate fs.

The paper is organised as follows. In Section 2 we address the identiﬁability of the model

given in (1). In Section 3 we propose an estimator of α and investigate its theoretical properties,

including its consistency, rate of convergence and asymptotic limit. In Section 4 we develop a

completely automated distribution-free honest ﬁnite sample lower conﬁdence bound for α. As the

2


performance of the estimator proposed in Section 3 depends on the choice of a tuning parameter,

in Section 5 we study a tuning parameter-free heuristic estimator of α. We discuss the estimation

of Fs and its density fs in Section 6. Connection to the multiple testing problem is developed in

Section 7. In Section 8 we compare the ﬁnite sample performance of our procedures, including a

plug-in and cross-validated choice of the tuning parameter for the estimator proposed in Section 3,

with other methods available in the literature through simulation studies, and provide a clear

recommendation to the practitioner. Two real data examples, one arising in astronomy and the

other from a microarray experiment, are analysed in Section 9. Appendix D gives the proofs of

the results in the paper.

2

The model and identiﬁability

2.1

When α is known

Suppose that we observe an i.i.d. sample X1, X2, . . . , Xn from F as in (1). If α ∈ (0, 1] were

known, a naive estimator of Fs would be

ˆF α

s,n = Fn − (1 − α)Fb



α

,

(2)

where Fn is the empirical CDF of the observed sample, i.e., Fn(x) = �n

i=1 1{Xi ≤ x}/n. Al-

though this estimator is consistent, it does not satisfy the basic requirements of a CDF: ˆF α

s,n

need not be non-decreasing or lie between 0 and 1. This naive estimator can be improved by

imposing the known shape constraint of monotonicity. This can be accomplished by minimising

�

{W(x) − ˆF α

s,n(x)}2 dFn(x) ≡ 1



n

n

�

i=1

{W(Xi) − ˆF α

s,n(Xi)}2

(3)

over all CDFs W. Let ˇF α

s,n be a CDF that minimises (3). The above optimisation problem is

the same as minimising ∥θ − V∥2 over θ = (θ1, . . . , θn) ∈ Θinc where

Θinc = {θ ∈ Rn : 0 ≤ θ1 ≤ θ2 ≤ . . . ≤ θn ≤ 1},

V = (V1, V2, . . . , Vn), Vi := ˆF α

s,n(X(i)), i = 1, 2, . . . , n, X(i) being the i-th order statistic of

the sample, and ∥ · ∥ denotes the usual Euclidean norm in Rn. The estimator ˆθ is uniquely

deﬁned by the projection theorem (see e.g., Proposition 2.2.1 on page 88 of Bertsekas (2003));

it is the Euclidean projection of V on the closed convex set Θinc ⊂ Rn. ˆθ is related to ˇF α

s,n via

ˇF α

s,n(X(i)) = ˆθi, and can be easily computed using the pool-adjacent-violators algorithm (PAVA);

see Section 1.2 of Robertson et al. (1988). Thus, ˇF α

s,n is uniquely deﬁned at the data points Xi,

for all i = 1, . . . , n, and can be deﬁned on the entire real line by extending it to a piece-wise

constant right continuous function with possible jumps only at the data points. The following

result, derived easily from Chapter 1 of Robertson et al. (1988), characterises ˇF α

s,n.

Lemma 1. Let ˜F α

s,n be the isotonic regression (see e.g., page 4 of Robertson et al. (1988)) of the

set of points { ˆF α

s,n(X(i))}n

i=1. Then ˜F α

s,n is characterised as the right-hand slope of the greatest

convex minorant of the set of points {i/n, �i

j=0 ˆF α

s,n(X(j))}n

i=0. The restriction of ˜F α

s,n to [0, 1],

3


i.e., ˇF α

s,n = min{max{ ˜F α

s,n, 0}, 1}, minimises (3) over all CDFs.

Isotonic regression and the PAVA are very well studied in the statistical literature with many

text-book length treatments; see e.g., Robertson et al. (1988) and Barlow et al. (1972). If skill-

fully implemented, PAVA has a computational complexity of O(n) (see Grotzinger and Witzgall (1984)).

2.2

Identiﬁability of Fs

When α is unknown, the problem is considerably harder; in fact, it is non-identiﬁable. If (1)

holds for some Fb and α then the mixture model can be re-written as

F = (α + γ)

�

α



α + γ Fs +

γ



α + γ Fb

�

+ (1 − α − γ)Fb,

for 0 ≤ γ ≤ 1 − α, and the term (αFs + γFb)/(α + γ) can be thought of as the nonparametric

component. A trivial solution occurs when we take α + γ = 1, in which case (3) is minimised

when W = Fn. Hence, α is not uniquely deﬁned. To handle the identiﬁability issue, we redeﬁne

the mixing proportion as

α0 := inf {γ ∈ (0, 1] : [F − (1 − γ)Fb]/γ is a CDF} .

(4)

Intuitively, this deﬁnition makes sure that the “signal” distribution Fs does not include any

contribution from the known “background” Fb.

In this paper we consider the estimation of α0 as deﬁned in (4). Identiﬁability of mixture

models has been discussed in many papers, but generally with parametric assumptions on the

model. Genovese and Wasserman (2004) discuss identiﬁability when Fb is the uniform distribu-

tion and F has a density. Hunter et al. (2007) and Bordes et al. (2006) discuss identiﬁability for

location shift mixtures of symmetric distributions. Most authors try to ﬁnd conditions for the

identiﬁability of their model, while we go a step further and quantify the non-identiﬁability by

calculating α0 and investigating the diﬀerence between α and α0. In fact, most of our results are

valid even when (1) is non-identiﬁable.

Suppose that we start with a ﬁxed Fs, Fb and α satisfying (1).

As seen from the above

discussion we can only hope to estimate α0, which, from its deﬁnition in (4), is smaller than α,

i.e., α0 ≤ α. A natural question that arises now is: under what condition(s) can we guarantee

that the problem is identiﬁable, i.e., α0 = α? The following lemma gives the connection between

α and α0.

Lemma 2. Let F be as in (1) and α0 as deﬁned in (4). Then

α0 = α − sup {0 ≤ ǫ ≤ 1 : αFs − ǫFb is a sub-CDF} ,

(5)

where sub-CDF is a non-decreasing right-continuous function taking values between 0 and 1. In

particular, α0 &lt; α if and only if there exists ǫ ∈ (0, 1) such that αFs − ǫFb is a sub-CDF.

Furthermore, α0 = 0 if and only if F = Fb.

In the following we separately identify α0 for any distribution, be it continuous or discrete

or a mixture of the two, with a series of lemmas proved in Appendix A. By an application of

4


the Lebesgue decomposition theorem in conjunction with the Jordan decomposition theorem (see

page 142, Chapter V, Section 3a∗ of Feller (1971)), we have that any CDF G can be uniquely

represented as a weighted sum of a piecewise constant CDF G(d), an absolutely continuous CDF

G(a), and a continuous but singular CDF G(s), i.e., G = η1G(a) + η2G(d) + η3G(s), where ηi ≥ 0,

for i = 1, 2, 3, and η1 + η2 + η3 = 1. However, from a practical point of view, we can assume

η3 = 0, since singular functions almost never occur in practice; see e.g., Parzen (1960). Hence,

we may assume

G = ηG(a) + (1 − η)G(d),

(6)

where (1 − η) is the sum total of all the point masses of G. Let d(G) denote the set of all jump

discontinuities of G, i.e., d(G) = {x ∈ R : G(x) − G(x−) &gt; 0}. Let us deﬁne JG : d(G) → [0, 1]

to be a function deﬁned only on the jump points of G such that JG(x) = G(x) − G(x−) for

all x ∈ d(G). The following result addresses the identiﬁability issue when both Fs and Fb are

discrete CDFs.

Lemma 3. Let Fs and Fb be discrete CDFs.

If d(Fb) ̸⊂ d(Fs), then α0 = α, i.e., (1) is

identiﬁable. If d(Fb) ⊂ d(Fs), then α0 = α

�

1 − infx∈d(Fb) JFs(x)/JFb(x)

�

. Thus, α0 = α if and

only if infx∈d(Fb) JFs(x)/JFb(x) = 0.

Next, let us assume that both Fs and Fb are absolutely continuous CDFs.

Lemma 4. Suppose that Fs and Fb are absolutely continuous, i.e., they have densities fs and

fb, respectively. Then

α0 = α

�

1 − ess inf fs



fb

�

,

where, for any function g, ess inf g = sup{a ∈ R : m({x : g(x) &lt; a}) = 0}, m being the Lebesgue

measure. As a consequence, α0 &lt; α if and only if there exists c &gt; 0 such that fs ≥ cfb, almost

everywhere w.r.t. m.

The above lemma states that if there does not exist any c &gt; 0 for which fs(x) ≥ cfb(x), for

almost every x, then α0 = α and we can estimate the mixing proportion correctly. Note that, in

particular, if the support of Fs is strictly contained in that of Fb, then the problem is identiﬁable

and we can estimate α.

In Appendix A we apply the above two lemmas to two discrete (Poisson and binomial)

distributions and two absolutely continuous (exponential and normal) distributions to obtain the

exact relationship between α and α0. In the following lemma, proved in greater generality in

Appendix A, we give conditions under which a general CDF F, that can be represented as in

(6), is identiﬁable.

Lemma 5. Suppose that F = κF (a) + (1 − κ)F (d), where F (a) is an absolutely continuous CDF

and F (d) is a piecewise constant CDF, for some κ ∈ (0, 1). Then (1) is identiﬁable, if either F (a)

or F (d) are identiﬁable.

5


3

Estimation

3.1

Estimation of the mixing proportion α0

In this section we consider the estimation of α0 as deﬁned in (5). For the rest of the paper, unless

otherwise noted, we assume

X1, X2, . . . , Xn is an i.i.d. sample from F as in (1).

Recall the deﬁnitions of ˆF γ

s,n and ˇF γ

s,n, for γ ∈ (0, 1]; see (2) and (3). When γ = 1, we have

ˆF γ

s,n = Fn = ˇF γ

s,n as ˆF γ

s,n (for γ = 1) is a CDF. Whereas, when γ is much smaller than α0 the

regularisation of ˆF γ

s,n modiﬁes it, and thus ˆF γ

s,n and ˇF γ

s,n are quite diﬀerent. We would like to

compare the naive and isotonised estimators ˆF γ

s,n and ˇF γ

s,n, respectively, and choose the smallest

γ for which their distance is still small. This leads to the following estimator of α0:

ˆαcn

0 = inf

�

γ ∈ (0, 1] : γdn( ˆF γ

s,n, ˇF γ

s,n) ≤ cn



√



n

�

,

(7)

where cn is a sequence of constants and dn stands for the L2(Fn) distance, i.e., if g, h : R → R

are two functions, then d2

n(g, h) =

�

{g(x) − h(x)}2 dFn(x). It is easy to see that

dn(Fn, γ ˇF γ

s,n + (1 − γ)Fb) = γdn( ˆF γ

s,n, ˇF γ

s,n).

(8)

For simplicity of notation, using (8), we deﬁne γdn( ˆF γ

s,n, ˇF γ

s,n) for γ = 0 as

lim

γ→0+ γdn( ˆF γ

s,n, ˇF γ

s,n) = dn(Fn, Fb).

(9)

This convention is followed in the rest of the paper.

The choice of cn is important, and in the following sections we address this issue in detail.

We derive conditions on cn that lead to consistent estimators of α0. We will also show that

particular (distribution-free) choices of cn will lead to honest lower conﬁdence bounds for α0.

Next, we prove a result which implies that, in the multiple testing problem, estimators of α0

do not depend on whether we use p-values or z-values to perform our analysis. Let Ψ : R → R be a

known continuous non-decreasing function. We deﬁne Ψ−1(y) := inf{t ∈ R : y ≤ Ψ(t)}, and Yi :=

Ψ−1(Xi). It is easy to see that Y1, Y2, . . . , Yn is an i.i.d. sample from G := αFs ◦Ψ+(1−α)Fb ◦Ψ.

Suppose now that we work with Y1, Y2, . . . , Yn, instead of X1, X2, . . . , Xn, and want to estimate

α. We can deﬁne αY

0 as in (4) but with {G, Fb ◦ Ψ} instead of {F, Fb}. The following result

shows that α0 and its estimators proposed in this paper are invariant under such monotonic

transformations.

Theorem 1. Let Gn be the empirical CDF of Y1, Y2, . . . , Yn.

Also, let ˆGs,n and ˇGγ

s,n be as

deﬁned in (2) and (3), respectively, but with {Gn, Fb ◦ Ψ} instead of {Fn, Fb}. Then α0 = αY

0

and γdn( ˆF γ

s,n, ˇF γ

s,n) = γdn( ˆGγ

s,n, ˇGγ

s,n) for all γ ∈ (0, 1].

3.2

Consistency of ˆαcn

0

We start with two elementary results on the behaviour of our criterion function γdn( ˇF γ

s,n, ˆF γ

s,n).

6


Lemma 6. For 1 ≥ γ ≥ α0, γdn( ˇF γ

s,n, ˆF γ

s,n) ≤ dn(F, Fn). Thus,

γdn( ˆF γ

s,n, ˇF γ

s,n)

a.s.

→

�

0,

γ − α0 ≥ 0,

&gt; 0,

γ − α0 &lt; 0.

(10)

Lemma 7. The set An := {γ ∈ [0, 1] : √



nγdn( ˆF γ

s,n, ˇF γ

s,n) ≤ cn} is convex. Thus, An = [ˆαcn

0 , 1].

The following result shows that for a broad range of choices of cn, our estimation procedure

is consistent.

Theorem 2. If cn = o(√



n) and cn → ∞, then ˆαcn

0

P→ α0.

A proper choice of cn is important and crucial for the performance of ˆαcn

0 . We suggest doing

cross-validation to ﬁnd the optimal tuning parameter cn. In Section 8.2.1 we detail this approach

and illustrate its good ﬁnite sample performance through simulation examples; see Tables 2-5,

Section 8.2.4, and Appendix B. However, cross-validation can be computationally expensive.

Another useful choice for cn is to take cn = 0.1 log log n. After extensive simulations, we observe

that cn = 0.1 log log n has good ﬁnite sample performance for estimating α0; see Section 8 and

Appendix B for more details.

3.3

Rate of convergence and asymptotic limit

We ﬁrst discuss the case α0 = 0. In this situation, under minimal assumptions, we show that as

the sample size grows, ˆαcn

0

exactly equals α0 with probability converging to 1.

Lemma 8. When α0 = 0, if cn → ∞ as n → ∞, then P(ˆαcn

0 = 0) → 1.

For the rest of this section we assume that α0 &gt; 0. The following theorem gives the rate of

convergence of ˆαcn

0 .

Theorem 3. Let rn := √



n/cn. If cn → ∞ and cn = o(n1/4) as n → ∞, then rn(ˆαcn

0 − α0) =

OP (1).

The proof of the above result is involved and we give the details in Appendix D.9.

Remark 1. Genovese and Wasserman (2004) show that the estimators of α0 proposed by Hengartner and Stark (1995)

and Swanepoel (1999) have convergence rates of (n/ log n)1/3 and n2/5/(log n)δ, for δ &gt; 0, respec-

tively. Morover, both results require smoothness assumptions on F – Hengartner and Stark (1995)

require F to be concave with a density that is Lipschitz of order 1, while Swanepoel (1999) requires

even stronger smoothness conditions on the density. Nguyen and Matias (2013) prove that when

the density of F α0

s

vanishes at a set of points of measure zero and satisﬁes certain regularity

assumptions, then any √



n-consistent estimator of α0 will not have ﬁnite variance in the limit (if

such an estimator exists).

We can take rn = √



n/cn arbitrarily close to √



n by choosing cn that increases to inﬁnity very

slowly. If we take cn = log log n, we get an estimator that has a rate of convergence √



n/ log log n.

In fact, as the next result shows, rn(ˆαcn

0 − α0) converges to a degenerate limit. In Section 8.2,

we analyse the eﬀect of cn on the ﬁnite sample performance of ˆαcn

0

for estimating α0 through

simulations and advocate a proper choice of the tuning parameter cn.

7


Theorem 4. When α0 &gt; 0, if rn → ∞, cn = o(n1/4) and cn → ∞, as n → ∞, then

rn(ˆαcn

0 − α0)

P→ c,

where c &lt; 0 is a constant that depends on α0, F and Fb.

4

Lower conﬁdence bound for α0

The asymptotic limit of the estimator ˆαcn

0 discussed in Section 3 depends on unknown parameters

(e.g., α0, F) in a complicated fashion and is of little practical use. Our goal in this sub-section is

to construct a ﬁnite sample (honest) lower conﬁdence bound ˆαL with the property

P(α0 ≥ ˆαL) ≥ 1 − β,

(11)

for a speciﬁed conﬁdence level (1−β) (0 &lt; β &lt; 1), that is valid for any n and is tuning parameter

free. Such a lower bound would allow one to assert, with a speciﬁed level of conﬁdence, that the

proportion of “signal” is at least ˆαL.

It can also be used to test the hypothesis that there is no “signal” at level β by rejecting

when ˆαL &gt; 0. The problem of no “signal’ is known as the homogeneity problem in the statistical

literature. It is easy to show that α0 = 0 if and only if F = Fb. Thus, the hypothesis of no

“signal” or homogeneity can be addressed by testing whether α0 = 0 or not. There has been a

considerable amount of work on the homogeneity problem, but most of the papers make para-

metric model assumptions. Lindsay (1995) is an authoritative monograph on the homogeneity

problem but the components are assumed to be from a known exponential family. Walther (2001)

and Walther (2002) discuss the homogeneity problem under the assumption that the densities

are log-concave. Donoho and Jin (2004) and Cai and Jin (2010) discuss the problem of detecting

sparse heterogeneous mixtures under parametric settings using the ‘higher criticism’ statistic; see

Appendix C for more details.

It will be seen that our approach will lead to an exact lower conﬁdence bound when α0 = 0, i.e.,

P(ˆαL = 0) = 1−β. The methods of Genovese and Wasserman (2004) and Meinshausen and Rice (2006)

usually yield conservative lower bounds.

Theorem 5. Let Hn be the CDF of √



ndn(Fn, F).

Let ˆαL be deﬁned as in (7) with cn =

H−1

n (1 − β). Then (11) holds. Furthermore if α0 = 0, then P(ˆαL = 0) = 1 − β, i.e., it is an

exact lower bound.

The proof of the above theorem can be found in Appendix D.13. Note that Hn is distribution-

free (i.e., it does not depend on Fs and Fb) when F is a continuous CDF and can be readily

approximated by Monte Carlo simulations using a sample of uniforms. For moderately large n

(e.g., n ≥ 500) the distribution Hn can be very well approximated by that of the Cram´er-von

Mises statistic, deﬁned as

√



nd(Fn, F) :=

�



�

n{Fn(x) − F(x)}2dF(x).

Letting Gn be the CDF of √



nd(Fn, F), we have the following result.

8


Theorem 6. supx∈R |Hn(x) − Gn(x)| → 0 as n → ∞.

Hence in practice, for moderately large n, we can take cn to be the (1 − β)-quantile of Gn or

its asymptotic limit, which are readily available (e.g., see Anderson and Darling (1952)). When

F is a continuous CDF, the asymptotic 95% quantile of Gn is 0.6792, and is used in our data

analysis. Note that

P(α0 ≥ ˆαL) = P(√



nα0dn( ˆF α0

s,n, ˇF α0

s,n) ≥ H−1

n (1 − β)).

The following theorem gives the explicit asymptotic limit of P(α0 ≥ ˆαL) but it is not useful for

practical purposes as it involves the unknown F α0

s

and F.

Theorem 7. Assume that α0 &gt; 0. Then √



nα0dn( ˆF α0

s,n, ˇF α0

s,n)

d→ U, where U is a random variable

whose distribution depends only on α0, F, and Fb.

The proof of the above theorem and the explicit from of U can be found in Appendix D.

The proof of Theorem 6 and a detailed discussion on the performance of the lower conﬁ-

dence bound for detecting heterogeneity in the moderately sparse signal regime considered in

Donoho and Jin (2004) can be found in Appendix C.

5

A heuristic estimator of α0

In simulations, we observe that the ﬁnite sample performance of (7) is aﬀected by the choice of

cn (for an extensive simulation study on this see Section 8.2). This motivates us to propose a

method to estimate α0 that is completely automated and has good ﬁnite sample performance.

We start with a lemma that describes the shape of our criterion function, and will motivate our

procedure.

Lemma 9. γdn( ˆF γ

s,n, ˇF γ

s,n) is a non-increasing convex function of γ in (0, 1).

Writing

ˆF γ

s,n = Fn − F



γ

+

�α0



γ F α0

s

+

�

1 − α0



γ

�

Fb

�

,

we see that for γ ≥ α0, the second term in the right hand side is a CDF. Thus, for γ ≥ α0,

ˆF γ

s,n is very close to a CDF as Fn − F = OP (n−1/2), and hence ˇF γ

s,n should also be close to

ˆF γ

s,n. Whereas, for γ &lt; α0, ˆF γ

s,n is not close to a CDF, and thus the distance γdn( ˆF γ

s,n, ˇF γ

s,n)

is appreciably large. Therefore, at α0, we have a “regime” change: γdn( ˆF γ

s,n, ˇF γ

s,n) should have

a slowly decreasing segment to the right of α0 and a steeply non-increasing segment to the left

of α0. Fig. 1 shows two typical such plots of the function γdn( ˆF γ

s,n, ˇF γ

s,n), where the left panel

corresponds to a mixture of N(2, 1) with N(0, 1) (setting I) and in the right panel we have a

mixture of Beta(1,10) and Uniform(0, 1) (setting II). We will use these two settings to illustrate

our methodology in the rest of this section and also in Section 8.1.

Using the above heuristics, we can see that the “elbow” of the function should provide a good

estimate of α0; it is the point that has the maximum curvature, i.e., the point where the second

derivative is maximal. We denote this estimator by ˜α0. Notice that both the estimators ˜α0 and

ˆαcn

0

are derived from γdn( ˆF γ

s,n, ˇF γ

s,n), as a function of γ, albeit they look at two diﬀerent aspects

of the function.

9


γ

0

0.05

0.1

0.15

0.2

0.25

0

0.01

0.02

0.03

0.04

0.05

γ

0

0.05

0.1

0.15

0.2

0

0.01

0.02

0.03

0.04

Figure 1: Plots of γdn( ˆF γ

s,n, ˇF γ

s,n) (in solid blue) overlaid with its (scaled) second derivative (in

dashed red) for α0 = 0.1 and n = 5000. Left panel: setting I; right panel: setting II.

In the above plots we have used numerical methods to approximate the second derivative of

γdn( ˆF γ

s,n, ˇF γ

s,n) (using the method of double diﬀerencing). We advocate plotting the function

γdn( ˆF γ

s,n, ˇF γ

s,n) as γ varies between 0 and 1. In most cases, plots similar to Fig. 1 would immedi-

ately convey to the practitioner the most appropriate choice of ˜α0. In some cases though, there

can be multiple peaks in the second derivative, in which case some discretion on the part of the

practitioner might be required. It must be noted that the idea of ﬁnding the point where the

second derivative is large to detect an “elbow” or “knee” of a function is not uncommon; see e.g.,

Salvador and Chan (2004). However, in Section 8.2.4 and Appendix B, we show some simulation

examples where ˜α0 fails to consistently estimate the “elbow” of γdn( ˆF γ

s,n, ˇF γ

s,n).

6

Estimation of the distribution function and its density

6.1

Estimation of Fs

Let us assume for the rest of this section that (1) is identiﬁable, i.e., α = α0, and α0 &gt; 0. Thus

F α0

s

= Fs. Once we have a consistent estimator ˇαn (which may or may not be ˆαcn

0

as discussed

in the previous sections) of α0, a natural nonparametric estimator of Fs is ˇF ˇαn

s,n, deﬁned as the

minimiser of (3). In the following theorem we show that, indeed, ˇF ˇαn

s,n is uniformly consistent for

estimating Fs. We also derive the rate of convergence of ˇF ˇαn

s,n.

Theorem 8. Suppose that ˇαn

P→ α0. Then, as n → ∞, supx∈R | ˇF ˇαn

s,n(x) − Fs(x)|

P→ 0. Further-

more, if qn(ˇαn − α0) = OP (1), where qn = o(√



n), then supx∈R qn| ˇF ˇαn

s,n(x) − Fs(x)| = OP (1).

Additionally, for ˆαcn

0

as deﬁned in (7), we have

sup

x∈R

|rn( ˆF ˆαcn

0

s,n − Fs)(x) − Q(x)|

P→ 0

and

rnd( ˇF ˆαcn

0

s,n , Fs)

P→ c

for a function Q : R → R and a constant c &gt; 0 depending only on α0, F, and Fb.

An immediate consequence of Theorem 8 is that dn( ˇF ˇαn

s,n, ˆF ˇαn

s,n)

P→ 0 as n → ∞. Left panel of

Fig. 2 shows our estimator ˇF ˇαn

s,n along with the true Fs for the same data set used in the right

panel of Fig. 1.

10


0

0.05

0.1

0.15

0.2

0.25

0.3

CDF

0

0.2

0.4

0.6

0.8

1

0

0.1

0.2

0.3

0.4

Density

0

2

4

6

8

10

Figure 2: Left panel: Plots of ˇF ˜α0

s,n (in dashed red), F †

s,n (in solid blue) and Fs (in dotted black)

for setting II; right panel: plots of f †

s,n (in dashed red) and fs (in solid blue) for setting II.

6.2

Estimating the density of Fs

Suppose now that Fs has a density fs. Obtaining nonparametric estimators of fs can be diﬃcult

as it requires smoothing and usually involves the choice of tuning parameter(s) (e.g., smoothing

bandwidths), and especially so in our set-up.

In this sub-section we describe a tuning parameter free approach to estimating fs, under

the additional assumption that fs is non-increasing. The assumption that fs is non-increasing,

i.e., Fs is concave on its support, is natural in many situations (see Section 7 for an applica-

tion in the multiple testing problem) and has been investigated by several authors, including

Grenander (1956), Langaas et al. (2005) and Genovese and Wasserman (2004). Without loss of

generality, we assume that fs is non-increasing on [0, ∞).

For a bounded function g : [0, ∞) → R, let us represent the least concave majorant (LCM)

of g by LCM[g].

Thus, LCM[g] is the smallest concave function that lies above g.

Deﬁne

F †

s,n := LCM[ ˇF ˇαn

s,n]. Note that F †

s,n is a valid CDF. We can now estimate fs by f †

s,n, where f †

s,n

is the piece-wise constant function obtained by taking the left derivative of F †

s,n. In the following

result we show that both F †

s,n and f †

s,n are consistent estimators of their population versions.

Theorem 9. Assume that Fs(0) = 0 and that Fs is concave on [0, ∞). If ˇαn

P→ α0, then, as

n → ∞,

sup

x∈R

|F †

s,n(x) − Fs(x)|

P→ 0.

(12)

Further, if for any x &gt; 0, fs(x) is continuous at x, then, f †

s,n(x)

P→ fs(x).

Computing F †

s,n and f †

s,n are straightforward, an application of the PAVA gives both the

estimators; see e.g., Chapter 1 of Robertson et al. (1988). In Fig. 2 the left panel shows the

LCM F †

s,n whereas the right panel shows its derivative f †

s,n along with the true density fs for the

same data set used in the right panel of Fig. 1.

7

Multiple testing problem

The problem of estimating the proportion of false null hypotheses α0 is of interest in situations

where a large number of hypothesis tests are performed. Recently, various such situations have

arisen in applications.

One major motivation is in estimating the proportion of genes that

11


are diﬀerentially expressed in deoxyribonucleic acid (DNA) microarray experiments. However,

estimating the proportion of true null hypotheses is also of interest, for example, in functional

magnetic resonance imaging (see Turkheimer et al. (2001)) and source detection in astrophysics

(see Miller et al. (2001)).

Suppose that we wish to test n null hypotheses H01, H02, . . . , H0n on the basis of a data set

X. Let Hi denote the (unobservable) binary variable that is 0 if H0i is true, and 1 otherwise,

i = 1, . . . , n. We want a decision rule D that will produce a decision of “null” or “non-null”

for each of the n cases. In their seminal work, Benjamini and Hochberg (1995) argued that an

important quantity to control is the false discovery rate (FDR) and proposed a procedure with

the property FDR ≤ β(1 − α0), where β is the user-deﬁned level of the FDR procedure. When

α0 is signiﬁcantly bigger than 0 an estimate of α0 can be used to yield a procedure with FDR

approximately equal to β and thus will result in an increased power. This is essentially the

idea of the adapted control of FDR (see Benjamini and Hochberg (2000)). See Storey (2002),

Black (2004), Langaas et al. (2005), Benjamini et al. (2006), and Donoho and Jin (2004) for a

discussion on the importance of eﬃcient estimation of α0 and some proposed estimators.

Our method can be directly used to yield an estimator of α0 that does not require the spec-

iﬁcation of any tuning parameter, as discussed in Section 5. We can also obtain a completely

nonparametric estimator of Fs, the distribution of the p-values arising from the alternative hy-

potheses. Suppose that Fb has a density fb and Fs has a density fs. To keep the following

discussion more general, we allow fb to be any known density, although in most multiple testing

applications we will take fb to be Uniform(0, 1). The local false discovery rate (LFDR) is deﬁned

as the function l : (0, 1) → [0, ∞), where

l(x) = P(Hi = 0|Xi = x) = (1 − α0)fb(x)



f(x)

,

and f(x) = α0fs(x) + (1 − α0)fb(x) is the density of the observed p-values. The estimation of

the LFDR l is important because it gives the probability that a particular null hypothesis is true

given the observed p-value for the test. The LFDR method can help us get easily interpretable

thresholding methods for reporting the “interesting” cases (e.g., l(x) ≤ 0.20). Obtaining good

estimates of l can be tricky as it involves the estimation of an unknown density, usually requiring

smoothing techniques; see Section 5 of Efron (2010) for a discussion on estimation and inter-

pretation of l. From the discussion in Section 6.1, under the additional assumption that fs is

non-increasing, we have a natural tuning parameter free estimator ˆl of the LFDR:

ˆl(x) =

(1 − ˇαn)fb(x)



ˇαnf †

s,n(x) + (1 − ˇαn)fb(x)

,

for x ∈ (0, 1).

The assumption that fs is non-increasing, i.e., Fs is concave, is quite natural – when the alter-

native hypothesis is true the p-value is generally small – and has been investigated by several

authors, including Genovese and Wasserman (2004) and Langaas et al. (2005).

12


Table 1: Coverage probabilities of nominal 95% lower conﬁdence bounds for the three methods

when n = 1000 and n = 5000.



n = 1000

n = 5000





Setting I

Setting II

Setting I

Setting II









α

ˆαL

ˆαGW

L

ˆαMR

L

ˆαL

ˆαGW

L

ˆαMR

L

ˆαL

ˆαGW

L

ˆαMR

L

ˆαL

ˆαGW

L

ˆαMR

L



0

0.95

0.98

0.93

0.95

0.98

0.93

0.95

0.97

0.93

0.95

0.97

0.93

0.01

0.97

0.98

0.99

0.97

0.97

0.99

0.98

0.98

0.99

0.98

0.98

0.99

0.03

0.98

0.98

0.99

0.98

0.98

0.99

0.98

0.98

0.99

0.98

0.98

0.99

0.05

0.98

0.98

0.99

0.98

0.98

0.99

0.99

0.99

0.99

0.98

0.98

0.99

0.10

0.99

0.99

1.00

0.99

0.98

0.99

0.99

0.99

1.00

0.99

0.98

0.99



8

Simulation

To investigate the ﬁnite sample performance of the estimators developed in this paper, we carry

out several simulation experiments. We also compare the performance of these estimators with

existing methods. The R language (R Development Core Team (2008)) codes used to implement

our procedures are available at http://stat.columbia.edu/∼rohit/research.html.

8.1

Lower bounds for α0

Although there has been some work on estimation of α0 in the multiple testing setting, Meinshausen and Rice (2006)

and Genovese and Wasserman (2004) are the only papers we found that discuss methodology for

constructing lower conﬁdence bounds for α0. These procedures are connected and the methods in

Meinshausen and Rice (2006) are extensions of those proposed in Genovese and Wasserman (2004).

The lower bounds proposed in both the papers approximately satisfy (11) and have the form

supt∈(0,1)(Fn(t)−t−ηn,βδ(t))/(1−t), where ηn,β is a bounding sequence for the bounding function

δ(t) at level β; see Meinshausen and Rice (2006). Genovese and Wasserman (2004) use a con-

stant bounding function, δ(t) = 1, with ηn,β =

�



log(2/β)/2n, whereas Meinshausen and Rice (2006)

suggest a class of bounding functions but observe that the standard deviation-proportional bound-

ing function δ(t) =

�



t(1 − t) has optimal properties among a large class of possible bounding

functions. We use this bounding function and a bounding sequence suggested by the authors.

We denote the lower bound proposed in Meinshausen and Rice (2006) by ˆαMR

L

, the bound in

Genovese and Wasserman (2004) by ˆαGW

L

, and the lower bound discussed in Section 4 by ˆαL. To

be able to use the methods of Meinshausen and Rice (2006) and Genovese and Wasserman (2004)

in setting I, introduced in Section 5, we transform the data such that Fb is Uniform(0, 1) ; see

Section 3.1 for the details.

We take α ∈ {0, 0.01, 0.03, 0.05, 0.10} and compare the performance of the three lower bounds

in the two diﬀerent simulation settings discussed in Section 5.

For each setting we take the

sample size n to be 1000 and 5000. We present the estimated coverage probabilities, obtained by

averaging over 5000 independent replications, of the lower bounds for both settings in Table 1. We

can immediately see from the table that the bounds are usually quite conservative. However, it is

worth pointing out that when α0 = 0, our method has exact coverage, as discussed in Section 4.

Also, the fact that our procedure is simple, easy to implement, and completely automated, makes

it very attractive.

13


8.2

Estimation of α0

In this sub-section, we illustrate and compare the performance of diﬀerent estimators of α0

under two sampling scenarios. In scenario A, we proceed as in Langaas et al. (2005). Let Xj =

(X1j, X2j, . . . , Xnj), for j = 1, . . . , J, and assume that each Xj ∼ N(µn×1, Σn×n) and that

X1, X2, . . . , XJ are independent.

We test H0i : µi = 0 versus H1i : µi ̸= 0 for each i =

1, 2, . . . , n. We set µi to zero for the true null hypotheses, whereas for the false null hypotheses,

we draw µi from a symmetric bi-triangular density with parameters a = log2(1.2) = 0.263

and b = log2(4) = 2; see page 568 of Langaas et al. (2005) for the details. Let xij denote a

realisation of Xij and α be the proportion of false null hypotheses. Let ¯xi = �J

j=1 xij/J and

s2

i = �J

j=1(xij − ¯xi)2/(J − 1). To test H0i versus H1i, we calculate a two-sided p-value based on

a one-sample t-test, with pi = 2P(TJ−1 ≥ |¯xi/

�



s2

i /J|), where TJ−1 is a t-distributed random

variable with J − 1 degrees of freedom.

In scenario B, we generate n+L independent random variables w1, w2, . . . , wn+L from N(0, 1)

and set zi =

1



√



L+1

�i+L

j=i wj for i = 1, 2, . . ., n. The dependence structure of the zi’s is determined

by L. For example, L = 0 corresponds to the case where the zi’s are i.i.d. standard normal. Let

Xi = zi + mi, for i = 1, 2, . . ., n, where mi = 0 under the null, and under the alternative,

|mi| is randomly generated from Uniform(m∗, m∗ + 1) and sgn(mi), the sign of mi, is randomly

generated from {−1, 1} with equal probabilities. Here m∗ is a suitable constant that describes

the simulation setting. Let 1−α be the proportion of true null hypotheses. Scenario B is inspired

by the numerical studies in Cai and Jin (2010) and Jin (2008).

Figure 3:

Plots of the means of diﬀerent estimators of α0, computed over 500 independent

replications, as the sample size increases from 3000 to 2 × 105; left panel: scenario A with

Σ = In×n; right panel: scenario B with L = 0 and m∗ = 1. The horizontal line (in dotted blue)

indicates the value of α0.

We use ˆαS,B

0

to denote the estimator proposed by Storey (2002) when bootstrapping is used

to choose the required tuning parameter, and denote by ˆαS,λ

0

the estimator when the value of

the tuning parameter is ﬁxed at λ. Langaas et al. (2005) proposed an estimator that is tuning

parameter free but crucially uses the known shape constraint of a convex and non-increasing fs;

we denote it by ˆαL

0 . We evaluate ˆαL

0 using the convest function in the R library limma. We also

use the estimator proposed in Meinshausen and Rice (2006) for two bounding functions: δ(t) =

14


Table 2:

Means×10 and RMSEs×100 (in parentheses) of estimators discussed in Section 8.2 for

scenario A with Σ = In×n, J = 10, n = 5000, and kn = log log n.



10α0

ˆα.1kn

0

ˆαCV

0

˜α0

ˆαGW

0

ˆαMR

0

ˆαS,0.5

0

ˆαJ

0

ˆαCJ

0

ˆαL

0

ˆαE

0



0.10

0.13

0.15

0.13

0.00

0.01

0.09

0.14

0.05

0.16

0.36

(1.00)

(1.79)

(0.83)

(1.00)

(0.88)

(1.41)

(1.50)

(5.32)

(1.20)

(3.70)

0.30

0.30

0.35

0.27

0.02

0.12

0.29

0.29

0.15

0.35

0.36

(1.02)

(1.87)

(1.01)

(2.80)

(1.84)

(1.41)

(1.83)

(5.46)

(1.26)

(3.96)

0.50

0.48

0.51

0.46

0.18

0.26

0.47

0.49

0.26

0.55

0.35

(1.09)

(1.9)

(1.12)

(3.29)

(2.46)

(1.49)

(1.91)

(5.73)

(1.34)

(3.80)

1.00

0.93

0.97

0.93

0.62

0.65

0.95

0.96

0.51

1.02

0.33

(1.35)

(1.86)

(1.32)

(3.88)

(3.57)

(1.51)

(1.94)

(7.16)

(1.36)

(3.73)



�



t(1 − t) and δ(t) = 1. For its implementation, we must choose a sequence {βn} going to zero as

n → ∞. Meinshausen and Rice (2006) did not specify any particular choice of {βn} but required

the sequence satisfy some conditions. We choose βn = 0.05/√



n and denote the estimators by

ˆαMR

0

when δ(t) =

�



t(1 − t) and by ˆαGW

0

when δ(t) = 1 (see Genovese and Wasserman (2004)).

We also compare our results with ˆαE

0 , the estimator proposed in Efron (2007) using the central

matching method, computed using the locfdr function in the R library locfdr. Jin (2008) and

Cai and Jin (2010) propose estimators when the model is a mixture of Gaussian distributions;

we denote the estimator proposed in Section 2.2 of Jin (2008) by ˆαJ

0 and in Section 3.1 of

Cai and Jin (2010) by ˆαCJ

0 . Some of the competing methods require Fb to be of a speciﬁc form

(e.g., standard normal) in which case we transform the observed data suitably.

The estimator ˆαcn

0

depends on the choice of cn and in the following we investigate a proper

choice of cn. We take α0 = 0.1 and evaluate the performance of ˆατ×log log n

0

for diﬀerent values

of τ, as n increases, for scenarios A and B. The choice cn = τ × log log n, for diﬀerent values

of τ, is suggested after extensive simulations. We also include ˜α0, ˆαGW

0

, ˆαMR

0

, and ˆαJ

0 in the

comparison. For scenario A, we ﬁx the sample size n at 5000 and Σ = In×n. For scenario B, we

ﬁx n = 5 × 104, L = 0, and m∗ = 1. In Fig. 3, we illustrate the eﬀect of cn on estimation of α0

as n varies from 3000 to 105. Recall that ˜α0 denotes the estimator proposed in Section 5. For

both scenarios, the sample mean of the estimators of α0 proposed in this paper converge to the

true α0, as the sample size grows. The methods developed in this paper perform favorably in

comparison to ˆαGW

0

, ˆαMR

0

, and ˆαJ

0 . Since, the choice of cn dictates the ﬁnite sample performance

of ˆαcn

0 , we propose cross-validation to ﬁnd an appropriate value of the tuning parameter.

8.2.1

Cross-validation

In this sub-section, we use c instead of cn to simplify the notation. In the following we brieﬂy

describe our cross-validation procedure. For a K-fold cross validation, we randomly partition the

data into K sets, say D1, . . . , DK. Let Fk

n be the empirical CDF of the data in Dk. Let ˆαc

0,−k be

the estimator deﬁned in (7) using all data except those in Dk and tuning parameter c. Further,

let ˇF

ˆαc

0,−k,−k

s,n

be the estimator of Fs as deﬁned in Lemma 1 using ˆαc

0,−k and all data except those

15


Table 3:

Means×10 and RMSEs×100 (in parentheses) of estimators discussed in Section 8.2 for

scenario B with L = 0, m∗ = 1, n = 5 × 104, and kn = log log n.



10α0

ˆα.1kn

0

ˆαCV

0

˜α0

ˆαGW

0

ˆαMR

0

ˆαS,B

0

ˆαJ

0

ˆαCJ

0

ˆαL

0

ˆαE

0



0.07

0.03

0.04

0.08

0.00

0.00

0.04

0.11

0.19

0.03

0.06

(0.44)

(0.67)

(0.28)

(0.66)

(0.66)

(0.65)

(0.96)

(2.96)

(0.38)

(0.77)

0.20

0.14

0.18

0.16

0.00

0.01

0.08

0.28

0.55

0.07

0.05

(0.73)

(0.79)

(0.62)

(1.98)

(1.89)

(2.25)

(1.33)

(4.41)

(1.26)

(1.28)

0.33

0.25

0.31

0.28

0.02

0.04

0.12

0.48

0.92

0.12

0.05

(0.89)

(0.85)

(0.95)

(3.15)

(2.91)

(3.83)

(1.77)

(6.48)

(2.14)

(1.90)

0.66

0.55

0.62

0.58

0.12

0.14

0.23

0.95

1.83

0.23

0.05

(1.21)

(1.00)

(1.48)

(5.38)

(5.25)

(7.73)

(3.04)

(11.98)

(4.34)

(3.84)



in Dk. Deﬁne the cross-validated estimator of c as

ccv := arg min

c∈R

K

�

k=1

�

(Fk

n − ˆF k)2dFk

n,

(13)

where ˆF k := ˆαc

0,−k ˇF

ˆαc

0,−k,−k

s

+ (1 − ˆαc

0,−k)Fb. In all simulations in this paper, we use K = 10 and

denote this estimator by ˆαCV

0

; see Section 7.10 of Hastie et al. (2009) for a more detailed study

of cross-validation and a justiﬁcation for K = 10. Fig. 4 illustrates the superior performance of

ˆαCV

0

across diﬀerent simulation settings; also see Sections 8.2.2 and 8.2.4, and Appendix B

8.2.2

Performance under independence

In this sub-section, we take α ∈ {0.01, 0.03, 0.05, 0.10} and compare the performance of the

diﬀerent estimators under the independence setting of scenarios A and B. In Tables 2 and 3, we

give the mean and root mean squared error (RMSE) of the estimators over 5000 independent

replications. For scenario A, we ﬁx the sample size n at 5000 and Σ = In×n. For scenario B,

we ﬁx n = 5 × 104, L = 0, and m∗ = 1. By an application of Lemma 4, it is easy to see that

in scenario A, the model is identiﬁable (i.e., α0 = α), while in scenario B, α0 = α × 0.67. For

scenario A, the sample means of ˆαCV

0

, ˜α0, ˆαJ

0 , ˆαL

0 , and ˆα0.1kn

0

for kn = log log n are comparable.

However, the RMSEs of ˜α0 and ˆα0.1kn

0

are lower than those of ˆαCV

0

, ˆαJ

0 , and ˆαL

0 . For scenario B,

the sample means of ˜α0, ˆαCV

0

, and ˆα0.1kn

0

are comparable. In scenario B, the performances of ˆαJ

0

and ˆαCJ

0

are not comparable to the estimators proposed in this paper, as ˆαJ

0 and ˆαCJ

0

estimate

α, while ˜α0, ˆαCV

0

, and ˆαcn

0 estimate α0. Note that ˆαL

0 fails to estimate α0 because the underlying

assumption inherent in their estimation procedure, that fs be non-increasing, does not hold. In

scenario A, ˆαS,0.5

0

has the best performance among the diﬀerent values of λ, while in scenario B,

ˆαS,λ

0

has poor performance for all values of λ ∈ [0, 1]. Furthermore, ˆαGW

0

, ˆαMR

0

, ˆαCJ

0 , ˆαS,B

0

and

ˆαE

0 perform poorly in both scenarios for all values of α0.

8.2.3

Performance under dependence

The simulation settings of this sub-section are designed to investigate the eﬀect of dependence on

the performance of the estimators. For scenario A, we use the setting of Langaas et al. (2005).

We take Σ to be a block diagonal matrix with block size 100.

Within blocks, the diagonal

16


Table 4:

Means×10 and RMSEs×100 (in parentheses) of estimators discussed in Section 8.2 for

scenario A with Σ as described in Section 8.2.3, J = 10, n = 5000, and kn = log log n.



10α0

ˆα.1kn

0

ˆαCV

0

˜α0

ˆαGW

0

ˆαMR

0

ˆαS,0.5

0

ˆαJ

0

ˆαCJ

0

ˆαL

0

ˆαE

0



0.10

0.46

0.42

0.33

0.07

0.06

0.28

0.22

0.07

0.32

0.37

(5.15)

(4.23)

(3.84)

(1.72)

(1.27)

(4.11)

(3.03)

(10.61)

(4.37)

(3.91)

0.30

0.52

0.53

0.41

0.14

0.17

0.65

0.34

0.15

0.49

0.39

(3.80)

(3.64)

(3.59)

(2.72)

(1.90)

(6.58)

(3.25)

(10.35)

(4.30)

(4.31)

0.50

0.66

0.76

0.54

0.26

0.31

0.54

0.49

0.25

0.66

0.37

(3.52)

(5.43)

(3.85)

(3.56)

(2.50)

(2.61)

(3.60)

(10.45)

(4.31)

(4.03)

1.00

1.06

1.13

0.97

0.68

0.69

1.15

0.97

0.53

1.11

0.36

(3.09)

(3.92)

(4.00)

(4.15)

(3.54)

(6.01)

(3.61)

(10.55)

(4.13)

(3.99)



Table 5:

Means×10 and RMSEs×100 (in parentheses) of estimators discussed in Section 8.2 for

scenario B with L = 30, m∗ = 1, n = 5 × 104, and kn = log log n.



10α0

ˆα.1kn

0

ˆαCV

0

˜α0

ˆαGW

0

ˆαMR

0

ˆαS,B

0

ˆαJ

0

ˆαCJ

0

ˆαL

0

ˆαE

0



0.07

0.29

0.38

0.17

0.04

0.05

0.26

0.20

0.21

0.13

0.22

(2.92)

(3.70)

(1.62)

(1.02)

(1.36)

(3.71)

(2.80)

(9.87)

(1.75)

(2.22)

0.20

0.30

0.42

0.18

0.04

0.04

0.16

0.33

0.55

0.13

0.19

(1.84)

(2.88)

(1.25)

(1.75)

(1.71)

(2.24)

(3.25)

(10.35)

(1.42)

(2.27)

0.33

0.38

0.52

0.20

0.06

0.06

0.17

0.50

0.93

0.16

0.18

(1.54)

(2.74)

(1.89)

(2.83)

(2.73)

(3.51)

(3.71)

(11.52)

(2.03)

(2.59)

0.67

0.63

0.77

0.31

0.14

0.15

0.24

0.95

1.82

0.25

0.16

(1.53)

(2.25)

(4.32)

(5.26)

(5.13)

(7.60)

(4.54)

(15.13)

(4.23)

(4.08)



elements (i.e., variances) are set to 1 and the oﬀ-diagonal elements (within-block correlations)

are set to ρ = 0.5. Outside of the blocks, all entries are set to 0. Tables 4 and 5 show that in

both scenarios, none of the methods perform well for small values of α0. However, in scenario

A, the performances of ˆα0.1kn

0

, ˜α0, and αJ

0 are comparable, for larger values of α0. In scenario

B, ˆα0.1kn

0

performs well for α0 = 0.033 and 0.067. Observe that, as in the independence setting,

ˆαGW

0

, ˆαMR

0

, ˆαS,B

0

, ˆαCJ

0 , and ˆαE

0 perform poorly in both scenarios for all values of α0.

8.2.4

Comparing the performance of ˆαcn

0 , ˆαCV

0

, and ˜α0

Although the heuristic estimator ˜α0 performs quite well in most of the simulation settings con-

sidered, there exists scenarios where ˜α0 can fail to consistently estimate α0. To illustrate this we

consider four diﬀerent CDFs Fs and ﬁx Fb to be the uniform distribution on (0, 1) (see the top

left plot of Fig. 4) and compare the performance of ˆαCV

0

, ˜α0, ˆα0.1kn

0

with the best performing

competing estimators (in each setting).

We see that ˜α0 may fail to estimate the “elbow” of γdn( ˆF γ

s,n, ˇF γ

s,n), as a function of γ, when

Fs has a multi-modal density (see the middle row of Fig. 4).

Observe that ˆαCV

0

and ˆα0.1kn

0

perform favorably compared to all competing estimators and in the two scenarios where ˜α0 fails

to consistently estimate α0, all our competing estimators also fail.

The ﬁrst two toy examples have been carefully constructed to demonstrate situations where

the point of maximum curvature (˜α0) is diﬀerent from the “elbow” of the function; see the top

right plot of Fig. 4 (also see Appendix B for further such examples).

17


0

0.2

0.4

0.6

0.8

1

Densties

0

2

4

6

Dist 1

Dist 2

Dist 3

Dist 4

γ

0

0.05

0.1

0.15

0

0.005

0.01

0.015

0.02

Sample size

×105

0.5

1

1.5

2

Estimated α

0

0.05

0.1

0.1kn

˜α0

ˆαCV

0

ˆαMR

0

ˆα

S,0.2

0

Sample size

×105

0.5

1

1.5

2

Estimated α

0

0.05

0.1

0.15

0.2

0.1kn

˜α0

ˆαCV

0

ˆαGW

0

ˆα

S,B

0

Sample size

×105

0.5

1

1.5

2

Estimated α

0.04

0.05

0.06

0.1kn

˜α0

ˆαCV

0

ˆαJ

0

Sample size

×105

0.5

1

1.5

2

Estimated α

0.08

0.1

0.12

0.1kn

˜α0

ˆαCV

0

ˆαGW

0

ˆα

S,B

0

Figure 4: Top row left panel: density functions for diﬀerent choices of Fs; top row right panel:

plot of γdn( ˆF γ

s,n, ˇF γ

s,n) (in blue), the scaled second derivative (in red), ˆαCV

0

(in black), and ˆα0.1kn

0

(in brown) for 5 independent samples of size 5000 corresponding to “Dist 1”; the blue star denotes

α0. The bottom two rows show the means of diﬀerent competing estimators of α0, computed

over 500 independent samples for Dist 1-4 (left-right, top-bottom) as sample size increases from

3000 to 2 × 105; in each ﬁgure the dotted black line denotes the true α0.

8.2.5

Our recommendation

In this paper we study two estimators for α0. For ˆαcn

0 , a proper choice of cn is important for

good ﬁnite sample performance. We suggest using cross-validation to ﬁnd the optimal tuning

parameter cn. However, cross-validation can be computationally expensive. An attractive alter-

native in this situation is to use ˜α0, which is easy to implement and has very good ﬁnite sample

performance in most scenarios, especially with large sample sizes. We feel that a visual analysis

of the plot of γdn( ˆF γ

s,n, ˇF γ

s,n) can be useful in checking the validity of ˜α0 as an estimator of the

“elbow”, and thus for α0.

18


0

0.01

0.02

0.03

0.04

0.05

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

p-value

LFDR

0

0.2

0.4

0.6

0.8

1

0

2

4

6

8

10

12

14

16

18

20

Density

0

0.1

0.2

0.3

0.4

0.5

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

CDF

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

50

100

150

200

250

p values

Frequency

(a)

(b)

(d)

(e)

(b)

(d)

(a)

(c)

Figure 5:

Plots for the prostate data: (a) Histogram of the p-values. The horizontal line (in

solid black) indicates the Uniform(0, 1) distribution. (b) Plot of γdn( ˆF γ

s,n, ˇF γ

s,n) (in solid blue)

overlaid with its (scaled) second derivative (in dashed red). The vertical line (in dotted black)

indicates the point of maximum curvature ˜α0 = 0.088. (c) ˇF ˜α0

s,n (in dotted red) and F †

s,n (in solid

blue); (d) f †

s,n; (e) estimated LFDR ˆl for p-values less than 0.05.

9

Real data analysis

9.1

Prostate data

Genetic expression levels for n = 6033 genes were obtained for m = 102 men, m1 = 50 normal

control subjects and m2 = 52 prostate cancer patients. Without going into the biology involved,

the principal goal of the study was to discover a small number of “interesting” genes, that is, genes

whose expression levels diﬀer between the cancer and control patients. Such genes, once identiﬁed,

might be further investigated for a causal link to prostate cancer development. The prostate

data is a 6033 × 102 matrix X having entries xij = expression level for gene i on patient j,

i = 1, 2, . . ., n, and j = 1, 2, . . ., m, with j = 1, 2, . . ., 50, for the normal controls, and j =

51, 52, . . ., 102, for the cancer patients. Let ¯xi(1) and ¯xi(2) be the averages of xij for the normal

controls and for the cancer patients, respectively, for gene i. The two-sample t-statistic for testing

signiﬁcance of gene i is ti = {¯xi(1) − ¯xi(2)}/si, where si is an estimate of the standard error of

¯xi(1) − ¯xi(2), i.e., s2

i = (1/50 + 1/52)[�50

j=1{xij − ¯xi(1)}2 + �102

j=51{xij − ¯xi(2)}2]/100.

We work with the p-values obtained from the 6033 two-sided t-tests instead of the “t-values”

as then the distribution under the alternative will have a non-increasing density which we can

estimate using the method developed in Section 6.1. Note that in our analysis we ignore the

dependence of the p-values, which is only a moderately risky assumption for the prostate data;

see Chapters 2 and 8 of Efron (2010) for further analysis and justiﬁcation.

Fig. 5 show the

plots of various quantities of interest, found using the methodology developed in Section 6.1 and

Section 7, for the prostate data example. The 95% lower conﬁdence bound ˆαL for this data is

found to be 0.05. In Table 6, we display estimates of α0 based on the methods considered in this

19


Table 6:

Estimates of α0 for the two data sets.



Data set

ˆα0.1kn

0

ˆαCV

0

˜α0

ˆαGW

0

ˆαMR

0

ˆαS,B

0

ˆαJ

0

ˆαCJ

0

ˆαL

0

ˆαE

0



Prostate

0.08

0.10

0.09

0.04

0.01

0.19

0.10

0.02

0.11

0.02

Carina

0.36

0.35

0.36

0.31

0.30

0.45

0.61

1.00

0.38

NA



paper for the prostate data and the Carina data (described below).

9.2

Carina data – an application in astronomy

In this sub-section we analyse the radial velocity (RV) distribution of stars in Carina, a dwarf

spheroidal (dSph) galaxy.

The dSph galaxies are low luminosity galaxies that are compan-

ions of the Milky Way. The data have been obtained by Magellan and MMT telescopes (see

Walker et al. (2007)) and consist of radial (line of sight) velocity measurements of n = 1266

stars from Carina, contaminated with Milky Way stars in the ﬁeld of view. We would like to

understand the distribution of the RV of stars in Carina. For the contaminating stars from the

Milky Way in the ﬁeld of view we assume a non-Gaussian velocity distribution Fb that is known

from the Besancon Milky Way model (Robin et al. (2003)), calculated along the line of sight to

Carina.

0

0.2

0.4

0.6

0.8

0

0.05

0.1

0.15

0.2

Radial Velocity (RV)

-100

0

100

200

300

0

0.005

0.01

0.015

0.02

0.025

Line of sight velocity

190

200

210

220

230

240

250

CDF

0

0.2

0.4

0.6

0.8

1

Figure 6: Plots for RV data in Carina dSph; left panel: γdn( ˆF γ

s,n, ˇF γ

s,n) (in solid blue) overlaid

with its (scaled) second derivative (in dashed red); middle panel: density of the RV distribution

of the contaminating stars overlaid with the (scaled) kernel density estimator of the observed

sample; right panel: ˇF ˜α0

s,n (in dashed red) overlaid with its closest Gaussian distribution (in solid

blue).

The 95% lower conﬁdence bound for α0 is found to be 0.323. The right panel of Fig. 6 shows

the estimate of Fs and the closest (in terms of minimising the L2( ˇF ˜α0

s,n) distance) ﬁtting Gaussian

distribution. Astronomers usually assume the distribution of the RVs for these dSph galaxies

to be Gaussian. Indeed we see that the estimated Fs is close to a normal distribution (with

mean 222.9 and standard deviation 7.51), although a formal test of this hypothesis is beyond the

scope of the present paper. The estimate due to Cai and Jin (2010), ˆαCJ

0 , is greater than one,

while Efron’s method (see Efron (2007)), implemented using the “locfdr” package in R, fails to

estimate α0.

20


10

Concluding remarks

In this paper we develop procedures for estimating the mixing proportion and the unknown

distribution in a two component mixture model using ideas from shape restricted function esti-

mation. We discuss the identiﬁability of the model and introduce an identiﬁable parameter α0,

under minimal assumptions on the model. We propose an honest ﬁnite sample lower conﬁdence

bound of α0 that is distribution-free. Two point estimators of α0, ˆαcn

0

and ˜α0, are studied. We

prove that ˆαcn

0

is a consistent estimator of α0 and show that the rate of convergence of ˆαcn

0

can

be arbitrarily close to √



n, for proper choices of cn. These proposed estimators crucially rely on

γdn( ˆF γ

s,n, ˇF γ

s,n), as a function of γ, whose plot provides useful insights about the nature of the

problem and performance of the estimators.

We observe that the estimators of α0 proposed in this paper have superior ﬁnite sample

performance than most competing methods. In contrast to most previous work on this topic the

results discussed in this paper hold true even when (1) is not identiﬁable. Under the assumption

that (1) is identiﬁable, we can ﬁnd an estimator of Fs which is uniformly consistent. Furthermore,

if Fs is known to have a non-increasing density fs we can ﬁnd a consistent estimator of fs. All

these estimators are tuning parameter free and easily implementable.

We conclude this section by outlining some possible future research directions. Construction

of two-sided conﬁdence intervals for α0 remains a hard problem as the asymptotic distribution

of ˆαcn

0

depends on the unknown F. We are currently developing estimators of α0 when we do

not exactly know Fb but only have an estimator of Fb (e.g., we observe a second i.i.d. sample

from Fb).

Investigating consistent alternative ways of detecting the “elbow” of the function

γdn( ˆF γ

s,n, ˇF γ

s,n), as an estimator of ˜α0, is an interesting future research direction. As we have

observed in the astronomy application, formal goodness-of-ﬁt tests for Fs are important – they

can guide the practitioner to use appropriate parametric models for further analysis – but are

presently unknown. The p-values in the prostate data example, considered in Section 9.1, can

have slight dependence. Therefore, investigating the performance and properties of the methods

introduced in this paper under appropriate dependence assumptions on X1, . . . , Xn is another

important direction for future research.

Acknowledgements

We thank the Joint Editor, the Associate Editor, and ﬁve anonymous referees for their careful

reading and constructive comments that lead to an improved version of the paper.

A

Identiﬁability of Fs

In this section we continue the discussion on the identiﬁability of Fs. First, we give some remarks

to illustrate Lemmas 3 and 4.

Remark 2. We consider mixtures of Poisson and binomial distributions to illustrate Lemma 3.

If Fs is Poisson(λs) and Fb is Poisson(λb), then

inf

x∈d(Fb)

JFs(x)



JFb(x)

=

inf

k∈N∪{0}

λk

s exp(−λs)



λk

b exp(−λb) = exp(λb − λs)

inf

k∈N∪{0}

�λs



λb

�k

.

21


By an application of Lemma 3, we have if λs &lt; λb then α0 = α; otherwise α0 = α(1 − exp(λb −

λs)).

In the case of a binomial mixture, i.e., Fs = Bin(n, ps) and Fb = Bin(n, pb),

α0 =







α

�

1 − ( 1−ps



1−pb )n�

,

ps ≥ pb,

α

�

1 − ( ps



pb )n�

,

ps &lt; pb.

Remark 3. If Fs is N(µs, σ2

s) and Fb (̸= Fs) is N(µb, σ2

b) then it can be easily shown that the

problem is identiﬁable if and only if σs ≤ σb. When σs &gt; σb, the model is not identiﬁable, an

application of Lemma 4 gives α0 = α

�

1 − (σb/σs) exp

�

− σsσb(µb − µs)2/2

��

. Thus, α0 increases

to α as |µs − µb| tends to inﬁnity. It should be noted that the problem is actually identiﬁable if

we restrict ourselves to the parametric family of a two-component Gaussian mixture model.

Remark 4. Now consider a mixture of exponential random variables, i.e., Fs is E(as, σs) and

Fb (̸= Fs) is E(ab, σb), where E(a, σ) is the distribution that has the density (1/σ) exp(−(x −

a)/σ)1(a,∞)(x). In this case, the problem is identiﬁable if as &gt; ab, as this implies the support of

Fs is a proper subset of the support of Fb. But when as ≤ ab, the problem is identiﬁable if and

only if σs ≤ σb.

Remark 5. It is also worth pointing out that even in cases where the problem is not identiﬁable

the diﬀerence between the true mixing proportion α and the estimand α0 may be very small.

Consider the hypothesis test H0 : θ = 0 versus H1 : θ ̸= 0 for the model N(θ, 1) with test statistic

¯X. The density of the p-values under θ is

fθ(p) = 1



2e−mθ2/2[e−√



mθ2Φ−1(1−p/2) + e

√



mθ2Φ−1(1−p/2)],

where m is the sample size. Here fθ(1) = e−mθ2/2 &gt; 0, so the model is not identiﬁable. As

Fb is uniform, it can be easily veriﬁed that α0 = α − α infp fθ(p). However, as the value of

fθ decreases exponentially with m, in many practical situations, where m is not too small, the

diﬀerence between α and α0 will be negligible.

In the following lemma, we try to ﬁnd the relationship between α and α0 when F is a general

CDF.

Lemma 10. Suppose that

F = κF (a) + (1 − κ)F (d),

(14)

where F (a) is an absolutely continuous CDF and F (d) is a piecewise constant CDF, for some

κ ∈ (0, 1). Then

α0 = α − min

�

ακs − α(a)

0 κ



κb

, α(1 − κs) − α(d)

0 (1 − κ)



(1 − κb)

�

,

where α(a)

0

and α(d)

0

are deﬁned as in (4), but with {F (a), F (a)

b

} and {F (d), F (d)

b

}, respectively

(instead of {F, Fb}). Similarly, κs and κb are deﬁned as in (14), but for Fs and Fb, respectively.

22


Proof. From the deﬁnition of κs and κb, we have Fs = κsF (a)

s

+ (1 − κs)F (d)

s

, and Fb = κbF (a)

b

+

(1 − κb)F (d)

b

. Thus from (1), we get

F = ακsF (a)

s

+ (1 − α)κbF (a)

b

+ α(1 − κs)F (d)

s

+ (1 − α)(1 − κs)F (d)

b

.

Now using the deﬁnition of κ, we see that κ = ακs +(1−α)κb, 1−κ = α(1−κs)+(1−α)(1−κb).

If we write

F (a) = α(a)F (a)

s

+ (1 − α(a))F (a)

b

,

it can easily seen that α(a) = ακs



κ ; and similarly, α(d) = α(1−κs)



1−κ

. Then, we can ﬁnd α(d)

0

and α(a)

0

as in Lemmas 3 and 4, respectively. Note that

sup {0 ≤ ǫ ≤ 1 : αFs − ǫFb is a sub-CDF}

=

sup

�

0 ≤ ǫ ≤ 1 : α(κsF (a)

s

+ (1 − κs)F (d)

s

) − ǫ(κbF (a)

b

+ (1 − κb)F (d)

b

) is a sub-CDF

�

=

sup

�

0 ≤ ǫ ≤ 1 : both ακsF (a)

s

− ǫκbF (a)

b

, α(1 − κs)F (d)

s

− ǫ(1 − κb)F (d)

b

are sub-CDFs

�

=

min

�

sup

�

0 ≤ ǫ ≤ 1 : ακsF (a)

s

− ǫκbF (a)

b

is a sub-CDF

�

,

sup

�

0 ≤ ǫ ≤ 1 : α(1 − κs)F (d)

s

− ǫ(1 − κb)F (d)

b

is a sub-CDF

��

=

min

�

ακs



κb

ess inf f (a)

s



f (a)

b

, α(1 − κs)



(1 − κb)

inf

x∈d(F (d)

b

)

JF (d)

s (x)



JF (d)

b

(x)

�

=

min

�

(ακs − α(a)

0 κ)



κb

, (α(1 − κs) − α(d)

0 (1 − κ))



(1 − κb)

�

,

where JG and d(JG) are deﬁned before Lemma 3 and we use the notion that 0



0 = 1. Hence, by

(5) the result follows.









Lemma 5 is now a corollary of this result.

B

Performance comparison of ˆαcn

0 , ˆαCV

0

, and ˜α0

In Figs. 7 and 8 we present further simulation experiments to investigate the ﬁnite sample perfor-

mance of ˆαcn

0 , ˆαCV

0

, and ˜α0 across diﬀerent simulation scenarios. In each setting we also include

the performance of the best performing competing estimators discussed in Section 8.2.

C

Detection of sparse heterogeneous mixtures

In this section we draw a connection between the lower conﬁdence bound developed in Section 4

and the Higher Criticism method of Donoho and Jin (2004) for detection of sparse heterogeneous

mixtures.

The detection of heterogeneity in sparse models arises in many applications, e.g.,

detection of a disease outbreak (see Kulldorﬀ et al. (2005)) or early detection of bioweapons use

(see Donoho and Jin (2004)). Generally, in large scale multiple testing problems, when the non-

null eﬀect is sparse it is important to detect the existence of non-null eﬀects (see Cai et al. (2007)).

23


(a)

(b)

(c)

(d)

(e)

(f)

Figure 7: Plots comparing the performance of ˆαcn

0 , ˆαCV

0

, and ˜α0; (a) density functions for four

diﬀerent choices of Fs; (b) plot of the average of �K

k=1

�

(Fk

n − ˆF k)2dFk

n (see (13) of the main

paper), as a function of c, computed over 500 independent samples of size 50000 corresponding

to Dist 1-4; (c)-(f) gives the means of diﬀerent competing estimators of α0, computed over 500

independent samples for Dist 1-4 respectively (in each ﬁgure the horizontal dotted black line

denotes the true α0).

24


(b)

(c)

(d)

(e)

(f)

( ��

Figure 8: Plots comparing the performance of ˆαcn

0 , ˆαCV

0

, and ˜α0; (a) density functions for four

diﬀerent choices of Fs; (b) plot of the average of �K

k=1

�

(Fk

n − ˆF k)2dFk

n (see (13) of the main

paper), as a function of c, computed over 500 independent samples of size 50000 corresponding

to Dist 1-4; (c)-(f) gives the means of diﬀerent competing estimators of α0, computed over 500

independent samples for Dist 1-4 respectively (in each ﬁgure the horizontal dotted black line

denotes the true α0).

25


Donoho and Jin (2004) consider n i.i.d. data from one of the two possible situations:

H0 : Xi ∼ Fb,

1 ≤ i ≤ n,

H(n)

1

: Xi ∼ F n := αnFn,s + (1 − αn)Fb,

1 ≤ i ≤ n,

where αn ∼ n−λ and Fn,s is such that d(Fn,s, Fb) is bounded away from 0. In Donoho and Jin (2004)

the main focus is on testing H0, i.e., αn = 0. We can test this hypothesis by rejecting H0 when

ˆαL &gt; 0. The following lemma shows that indeed this yields a valid testing procedure for λ &lt; 1/2.

Theorem 10. If αn ∼ n−λ, for λ &lt; 1/2, then PH0(Reject H0) = β and PH(n)

1

(ˆαL &gt; 0) → 1 as

n → ∞.

Proof. Note that {ˆαL &gt; 0} is equivalent to {cn ≤ √



ndn(Fn, Fb)} which shows that

cn

≤

√



ndn(Fn, (1 − αn)Fb + αnFn,s) + √



ndn(αnFb, αnFn,s)

=

√



ndn(Fn, F n) + αn

√



ndn(Fn,s, Fb),

where cn is chosen as in Theorem 5. It is easy to see that √



ndn(Fn, F n) is OP (1) and αn

√



ndn(Fn,s, Fb) →

∞, for λ &lt; 1/2, which shows that PH(n)

1

(ˆαL &gt; 0) → 1. It can be easily seen that PH0(ˆαL &gt; 0) =

PH0(Reject H0) = β.









D

Proofs of theorems and lemmas in the main paper

D.1

Proof of Lemma 2

From the deﬁnition of α0, we have

α0

=

inf {0 ≤ γ ≤ α : [F − (1 − γ)Fb]/γ is a valid CDF}

=

inf {0 ≤ γ ≤ α : [αFs + (1 − α)Fb − (1 − γ)Fb]/γ is a valid CDF}

=

inf {0 ≤ γ ≤ α : [αFs − (α − γ)Fb]/γ is a valid CDF}

=

α − sup {0 ≤ ǫ ≤ α : αFs − ǫFb is a sub-CDF}

=

α − sup {0 ≤ ǫ ≤ 1 : αFs − ǫFb is a sub-CDF} ,

where the ﬁnal equality follows from the fact that if ǫ &gt; α, then αFs −ǫFb will not be a sub-CDF.

To show that α0 = 0 if and only if F = Fb let us deﬁne δ = α − ǫ. Note that α0 = 0, if and

only if

sup {0 ≤ ǫ ≤ 1 : αFs − ǫFb is a sub-CDF} = α

⇔

inf {0 ≤ δ ≤ 1 : α(Fs − Fb) + δFb is a sub-CDF} = 0.

However, it is easy to see that the last equality is true if and only if Fs − Fb ≡ 0.

26


D.2

Proof of Lemma 3

When d(Fb) ̸⊂ d(Fs), there exists a x ∈ d(Fb)−d(Fs), i.e., there exists a x which satisﬁes Fb(x)−

Fb(x−) &gt; 0 and Fs(x) − Fs(x−) = 0. Then for all ǫ &gt; 0, Fs(x−) − ǫFb(x−) &gt; Fs(x) − ǫFb(x).

This shows that Fs −ǫFb cannot be a sub-CDF, and hence by Lemma 2 the model is identiﬁable.

Now let us assume that d(Fb) ⊂ d(Fs).

{0 ≤ ǫ ≤ 1 : αFs − ǫFb is a sub-CDF}

=

{0 ≤ ǫ ≤ 1 : αJFs(x) − ǫJFb(x) ≥ 0, ∀x ∈ d(JFb)}

=

�

0 ≤ ǫ ≤ 1 : JFs(x)



JFb(x) ≥ ǫ



α, ∀x ∈ d(JFb)

�

=

�

0 ≤ ǫ ≤ 1 :

inf

x∈d(Fb)

JFs(x)



JFb(x) ≥ ǫ



α

�

.

Therefore, using (5), we get the desired result.

D.3

Proof of Lemma 4

From (5), we have

α0

=

α − sup {0 ≤ ǫ ≤ 1 : αFs − ǫFb is a sub-CDF}

=

α − sup {0 ≤ ǫ ≤ 1 : αfs(x) − ǫfb(x) ≥ 0 almost every x}

=

α − sup

�

0 ≤ ǫ ≤ 1 : αfs



fb

(x) ≥ ǫ almost every x

�

=

α

�

1 − ess inf fs



fb

�

.

D.4

Proof of Theorem 1

Without loss of generality, we can assume that Fb is the uniform distribution on (0, 1) and, for

clarity, in the following we write U instead of Fb. Let us deﬁne

A

:=

�

γ ∈ (0, 1] : F − (1 − γ)U



γ

is a valid CDF

�

,

AY

:=

�

γ ∈ (0, 1] : G − (1 − γ)U ◦ Ψ



γ

is a valid CDF

�

.

Since α0 = inf A, and αY

0 = inf AY for the ﬁrst part of the theorem it is enough to show that

A = AY . Let us ﬁrst show that AY ⊂ A. Suppose η ∈ AY . We ﬁrst show that (F − (1 − η)U)/η

is a non-decreasing function. For all t1 ≤ t2, we have that

G(t1) − (1 − η)U(Ψ(t1))



η

≤ G(t2) − (1 − η)U(Ψ(t2))



η

.

Let y1 ≤ y2. Then,

G(Ψ−1(y1)) − (1 − η)U(Ψ(Ψ−1(y1)))



η

≤ G(Ψ−1(y2)) − (1 − η)U(Ψ(Ψ−1(y2)))



η

,

27


since y1 ≤ y2 ⇒ Ψ−1(y1) ≤ Ψ−1(y2).

However, as Ψ is continuous, Ψ(Ψ−1(y)) = y and

G(Ψ−1(y)) = αFs(y) + (1 − α)U(y) = F(y). Hence, we have

F(y1) − (1 − η)U(y1)



η

≤ F(y2) − (1 − η)U(y2)



η

.

As F and U are CDFs, it is easy to see that limx→−∞ (F(x) − (1 − η)U(x))/η = 0,

limx→∞ (F(x) − (1 − η)U(x))/η = 1 and (F − (1 − η)U)/η is a right continuous function. Hence,

for η ∈ AY , (F − (1 − η)U)/η is a CDF and thus, η ∈ A. We can similarly prove A ⊂ AY .

Therefore, A = AY and α0 = αY

0 .

Note that

γdn( ˆF γ

s,n, ˇF γ

s,n) = min

W∈F

1



n

n

�

i=1

{W(Xi) − ˆF γ

s,n(Xi)}2,

where F is the class of all CDFs. For the second part of theorem it is enough to show that

min

W∈F

1



n

n

�

i=1

{W(Xi) − ˆF γ

s,n(Xi)}2 = min

B∈F

1



n

n

�

i=1

{B(Yi) − ˆGγ

s,n(Yi)}2.

First note that

Gn(y)

=

1



n

n

�

i=1

1{Ψ−1(Xi) ≤ y}

=

1



n

n

�

i=1

1{Xi ≤ Ψ(y)}

=

Fn(Ψ(y)).

Thus, from the deﬁnition of ˆGγ

s,n, we have

ˆGγ

s,n(Yi)

=

Fn(Ψ(Yi)) − (1 − γ)U(Ψ(Yi))



γ

=

Fn(Xi) − (1 − γ)U(Xi)



γ

= ˆF γ

s,n(Xi).

Therefore,

1



n

n

�

i=1

{B(Yi) − ˆGγ

s,n(Yi)}2

=

1



n

n

�

i=1

{B(Yi) − ˆF γ

s,n(Xi)}2

=

1



n

n

�

i=1

{B(Ψ−1(Xi)) − ˆF γ

s,n(Xi)}2

=

1



n

n

�

i=1

{W(Xi) − ˆF γ

s,n(Xi)}2,

where W(x) := B(Ψ−1(x)). W is a valid CDF as Ψ−1 is non-decreasing.

28


D.5

Proof of Lemma 6

Letting F γ

s = (F − (1 − γ)Fb)/γ, observe that

γdn( ˆF γ

s,n, F γ

s ) = dn(F, Fn).

Also note that F γ

s is a valid CDF for γ ≥ α0. As ˇF γ

s,n is deﬁned as the function that minimises

the L2(Fn) distance of ˆF γ

s,n over all CDFs,

γdn( ˇF γ

s,n, ˆF γ

s,n) ≤ γdn( ˆF γ

s,n, F γ

s ) = dn(F, Fn).

To prove the second part of the lemma, notice that for γ ≥ α0 the result follows from above

and the fact that dn(F, Fn)

a.s.

→ 0 as n → ∞.

For γ &lt; α0, F γ

s is not a valid CDF, by the deﬁnition of α0. Note that as n → ∞, ˆF γ

s,n

a.s.

→ F γ

s

point-wise. So, for large enough n, ˆF γ

s,n is not a valid CDF, whereas ˇF γ

s,n is always a CDF. Thus,

dn( ˆF γ

s,n, ˇF γ

s,n) converges to something positive.

D.6

Proof of Lemma 7

Assume that γ1 ≤ γ2 and γ1, γ2 ∈ An. If γ3 = ηγ1 + (1 − η)γ2, for 0 ≤ η ≤ 1, it is easy to observe

from (2) that

η(γ1 ˆF γ1

s,n) + (1 − η)(γ2 ˆF γ2

s,n) = γ3 ˆF γ3

s,n.

Note that [η(γ1 ˇF γ1

s,n) + (1 − η)(γ2 ˇF γ2

s,n)]/γ3 is a valid CDF, and thus from the deﬁnition of ˇF γ3

s,n,

we have

dn( ˆF γ3

s,n, ˇF γ3

s,n)

≤

dn

�

ˆF γ3

s,n, [η(γ1 ˇF γ1

s,n) + (1 − η)(γ2 ˇF γ2

s,n)]/γ3

�

=

dn

�η(γ1 ˆF γ1

s,n) + (1 − η)(γ2 ˆF γ2

s,n)



γ3

, η(γ1 ˇF γ1

s,n) + (1 − η)(γ2 ˇF γ2

s,n)



γ3

�

≤

ηγ1



γ3

dn( ˆF γ1

s,n, ˇF γ1

s,n) + (1 − η)γ2



γ3

dn( ˆF γ2

s,n, ˇF γ2

s,n)

(15)

where the last step follows from the triangle inequality. But as γ1, γ2 ∈ An, the above inequality

yields

dn( ˆF γ3

s,n, ˇF γ3

s,n) ≤ ηγ1



γ3

cn



√



nγ1

+ (1 − η)γ2



γ3

cn



√



nγ2

=

cn



√



nγ3

.

Thus γ3 ∈ An.

D.7

Proof of Theorem 2

We need to show that P(|ˆαcn

0 − α0| &gt; ǫ) → 0 for any ǫ &gt; 0. Let us ﬁrst show that

P(ˆαcn

0 − α0 &lt; −ǫ) → 0.

The statement is obviously true if α0 ≤ ǫ. So let us assume that α0 &gt; ǫ. Suppose ˆαcn

0 − α0 &lt; −ǫ,

i.e., ˆαcn

0 &lt; α0 − ǫ. Then by the deﬁnition of ˆαcn

0

and the convexity of An, we have (α0 − ǫ) ∈ An

29


(as An is a convex set in [0, 1] with 1 ∈ An and ˆαcn

0 ∈ An), and thus

dn( ˆF α0−ǫ

s,n

, ˇF α0−ǫ

s,n

) ≤

cn



√



n(α0 − ǫ).

(16)

But by (10) the left-hand side of (16) goes to a non-zero constant in probability.

Hence, if

cn/√



n → 0,

P(ˆαcn

0 − α0 &lt; −ǫ) ≤ P

�

dn( ˆF α0−ǫ

s,n

, ˇF α0−ǫ

s,n

) ≤

cn



√



n(α0 − ǫ)

�

→ 0.

This completes the proof of the ﬁrst part of the claim.

Now suppose that ˆαcn

0 − α0 &gt; ǫ. Then,

ˆαcn

0 − α0 &gt; ǫ

⇒

√



ndn( ˆF α0+ǫ

s,n

, ˇF α0+ǫ

s,n

) ≥

cn



α0 + ǫ

⇒

√



ndn(Fn, F) ≥ cn.

The ﬁrst implication follows from the deﬁnition of ˆαcn

0 , while the second implication is true by

Lemma 6. The right-hand side of the last inequality is (asymptotically similar to) the Cram´er–

von Mises statistic for which the asymptotic distribution is well-known and thus if cn → ∞ the

result follows.

D.8

Proof of Lemma 8

As α0 = 0,

P(ˆαcn

0 = 0) = 1 − P(ˆαcn

0 &gt; 0) = 1 − P(√



ndn(Fn, F) &gt; cn) → 1,

(17)

since √



ndn(Fn, F) = OP (1) by Theorem 6.

D.9

Proof of Theorem 3

As the proof of this result is slightly involved we break it into a number of lemmas (whose proofs

are provided later in this sub-section) and give the main arguments below.

We need to show that given any ǫ &gt; 0, we can ﬁnd an M &gt; 0 and n0 ∈ N (depending on ǫ)

for which supn&gt;n0 P(rn|ˆαcn

0 − α0| &gt; M) ≤ ǫ.

Lemma 11. If cn → ∞, then for any M &gt; 0, supn&gt;n0 P (rn(ˆαcn

0 − α0) &gt; M) &lt; ǫ, for large

enough n0 ∈ N.

Finding an rn such that P (rn(ˆαcn

0 − α0) &lt; −M) &lt; ǫ for large enough n is more complicated.

We start with some notation.

Let F be the class of all CDFs and H be the Hilbert space

L2(F) := {f : R → R|

�

f 2dF &lt; ∞}. For a closed convex subset K of H and h ∈ H, we deﬁne

the projection of h onto K as

Π(h|K) := arg min

f∈K

d(f, h),

(18)

where d stands for the L2(F) distance, i.e., if g, h ∈ H, then d2(g, h) =

�

(g − h)2dF. We deﬁne

30


the tangent cone of F at f0 ∈ F, as

TF(f0) := {λ(f − f0) : λ ≥ 0, f ∈ F}.

(19)

For any H ∈ F and γ &gt; 0, let us deﬁne

ˆHγ := H − (1 − γ)Fb



γ

,

ˇHγ

n := arg min

G∈F

γdn( ˆHγ, G),

and

¯Hγ

n := arg min

G∈F

γd( ˆHγ, G).

For H = Fn and γ = α0 we deﬁne the three quantities above and call them ˆF α0

s,n, ˇF α0

s,n, and ¯F α0

s,n

respectively. Note that

P (rn(ˆαcn

0 − α0) &lt; −M) = P(√



nγn dn( ˆF γn

s,n, ˇF γn

s,n) &lt; cn),

(20)

where γn = α0 − M/rn. To study the limiting behavior of dn( ˆF γn

s,n, ˇF γn

s,n) we break it as the

sum of dn( ˆF γn

s,n, ˇF γn

s,n) − d( ˆF γn

s,n, ¯F γn

s,n) and d( ˆF γn

s,n, ¯F γn

s,n).

The following two lemmas (proved in

Sections D.9.2 and D.9.3 respectivley) give the asymptotic behavior of the two terms. The proof

of Lemma 13 uses the functional delta method (cf. Theorem 20.8 of Van der Vaart (1998)) for

the projection operator; see Theorem 1 of Fils-Villetard et al. (2008).

Lemma 12. If √



n/r2

n → 0, then Un := √



nγndn( ˆF γn

s,n, ˇF γn

s,n) − √



nγnd( ˆF γn

s,n, ¯F γn

s,n)

P→ 0.

Lemma 13. If cn → ∞, then

√



nγn



cnM d( ˆF γn

s,n, ¯F γn

s,n)

P→

��

V 2dF

�1/2

&gt; 0

where

V := (F α0

s

− Fb) − Π (F α0

s

− Fb| TF(F α0

s )) ̸= 0

and

F α0

s

:= F − (1 − α0)Fb



α0

.

(21)

Using (20), and the notation introduced in the above two lemmas we see that

P (rn(ˆαcn

0 − α0) &lt; −M)

=

P

� 1



cn

Un +

√



nγn



cn

d( ˆF γn

s,n, ¯F γn

s,n) &lt; 1

�

.

(22)

However, Un

P→ 0 (by Lemma 12) and

√



nγn



cnM d( ˆF γn

s,n, ¯F γn

s,n)

P→ � V 2dF (by Lemma 13). The result

now follows from (22), by taking a large enough M.

D.9.1

Proof of Lemma 11

Note that

P(rn(ˆαcn

0 − α0) &gt; M) ≤ P (ˆαcn

0 &gt; α0)

=

P

�√



nα0dn( ˆF α0

s,n, ˇF α0

s,n) &gt; cn

�

≤

P

�√



nα0dn( ˆF α0

s,n, F α0

s ) &gt; cn

�

=

P

�√



ndn(Fn, F) &gt; cn

�

→ 0,

31


as cn → ∞, since √



ndn(Fn, F) = OP (1). Therefore, the result holds for suﬃciently large n.

D.9.2

Proof of Lemma 12

It is enough to show that

Wn := nγ2

nd2

n( ˆF γn

s,n, ˇF γn

s,n) − nγ2

nd2( ˆF γn

s,n, ¯F γn

s,n)

P→

0,

(23)

since U 2

n ≤ |Wn|. Note that

ˇF γn

s,n

=

arg min

G∈F

dn(Fn, γnG + (1 − γn)Fb),

¯F γn

s,n

=

arg min

G∈F

d(Fn, γnG + (1 − γn)Fb).

For each positive integer n and c &gt; 0, we introduce the following classes of functions:

Gc(n) =

�√



n(G − (1 − γn)Fb − γn ˇGγn

n )2 : G ∈ F, ∥G − F∥ &lt;

c



√



n

�

,

Hc(n) =

�√



n(H − (1 − γn)Fb − γn ¯Hγn

n )2 : H ∈ F, ∥H − F∥ &lt;

c



√



n

�

.

Let us also deﬁne

Dn := sup

t∈R

√



n|Fn(t) − F(t)| = ∥Fn − F∥.

From the deﬁnition of the minimisers ˇF γn

s,n and ¯F γn

s,n, we see that

γ2

n |d2

n( ˆF γn

s,n, ˇF γn

s,n) − d2( ˆF γn

s,n, ¯F γn

s,n)| ≤ max

�

|(d2

n − d2)(Fn, γn ˇF γn

s,n + (1 − γn)Fb)| ,

|(d2

n − d2)(Fn, γn ¯F γn

s,n + (1 − γn)Fb)|

�

.

(24)

Observe that

nγ2

n [(d2

n − d2)(Fn, γn ˇF γn

s,n + (1 − γn)Fb)] = √



n(Pn − P)[gn] = νn(gn),

where gn := √



n{Fn − γn ˇF γn

s,n − (1 − γn)Fb}2, Pn denotes the empirical measure of the data, and

νn := √



n(Pn − P) denotes the usual empirical process. Similarly,

nγ2

n [(d2

n − d2)(Fn, γn ¯F γn

s,n + (1 − γn)Fb)] = √



n(Pn − P)[hn] = νn(hn),

where hn := √



n{Fn − γn ¯F γn

s,n − (1 − γn)Fb}2. Thus, combining (23), (24) and the above two

displays, we get, for any δ &gt; 0,

P(|Wn| &gt; δ) ≤ P (|νn(gn)| &gt; δ) + P (|νn(hn)| &gt; δ) .

(25)

32


The ﬁrst term in the right hand side of (25) can be bounded above as

P(|νn(gn)| &gt; δ)

=

P(|νn(gn)| &gt; δ, gn ∈ Gc(n)) + P(|νn(gn)| &gt; δ, gn /∈ Gc(n))

≤

P(|νn(gn)| &gt; δ, gn ∈ Gc(n)) + P(gn /∈ Gc(n))

≤

P

�

sup

g∈Gc(n)

|νn(g)| &gt; δ

�

+ P(gn /∈ Gc(n))

≤

1



δ E

�

sup

g∈Gc(n)

|νn(g)|

�

+ P(gn /∈ Gc(n))

≤

J[ ]

P[G2

c,n]



δ

+ P(gn /∈ Gc(n)),

(26)

where Gc,n := 6c2/√



n + 16√



n M2



r2n ∥F α0

s

− Fb∥2 is an envelope for Gc(n) and J[ ] is a constant.

Note that to derive the last inequality, we have used the maximal inequality in Corollary (4.3) of

Pollard (1989); the class Gc(n) is “manageable” in the sense of Pollard (1989) (as a consequence

of equation (2.5) of Van de Geer (2000)).

To see that Gc,n is an envelope for Gc(n), observe that for any G ∈ F,

G − (1 − γn)Fb

=

G − F + M



rn

(F α0

s

− Fb) + γnF α0

s .

Hence,

F α0

s

−

M



rnγn

∥F α0

s

− Fb∥ − ∥G − F∥



γn

≤ G − (1 − γn)Fb



γn

≤ F α0

s

+

M



rnγn

∥F α0

s

− Fb∥ + ∥G − F∥



γn

.

As the two bounds are monotone, from the properties of isotonic estimators (see e.g., Theorem

1.3.4 of Robertson et al. (1988)), we can always ﬁnd a version of ˇGγn

s

such that

F α0

s

−

M



rnγn

∥F α0

s

− Fb∥ − ∥G − F∥



γn

≤ ˇGγn

s

≤ F α0

s

+

M



rnγn

∥F α0

s

− Fb∥ + ∥G − F∥



γn

.

Therefore,

−2M



rn

∥F α0

s

−Fb∥−∥G−F∥ ≤ γn ˇGγn

s −γnF α0

s

− M



rn

(F α0

s

−Fb) ≤ 2M



rn

∥F α0

s

−Fb∥+∥G−F∥. (27)

33


Thus, for √



n(G − (1 − γn)Fb − γn ˇGγn

s )2 ∈ Gc(n),

(G − (1 − γn)Fb − γn ˇGγn

s )2

=

�

(G − F) +

�

γn ˇGγn

s

− γnF α0

s

− M



rn

(Fb − F α0

s )

��2

≤

2(G − F)2 + 2

�

γn ˇGγn

s

− γnF α0

s

− M



rn

(Fb − F α0

s )

�2

≤

2∥G − F∥2 + 2

�

2M



rn

∥F α0

s

− Fb∥ + ∥G − F∥

�2

≤

6∥G − F∥2 + 16M 2



r2n

∥F α0

s

− Fb∥2

≤

6c2 + 16M 2



r2n

∥F α0

s

− Fb∥2 = Gc,n



√



n ,

where the second inequality follows from (27).

From the deﬁnition of gn and D2

n, we have

|gn(t)| ≤

6



√



nD2

n + 16√



n M2



r2n ∥F α0

s

− Fb∥2, for all t ∈ R. As Dn = OP (1), for any given ǫ &gt; 0, there

exists c &gt; 0 (depending on ǫ) such that

P(gn /∈ Gc(n))

=

P

�

∥Fn − F∥ ≥

c



√



n

�

= P(Dn ≥ c) ≤ ǫ,

(28)

for all suﬃciently large n.

Therefore, for any given δ &gt; 0 and ǫ &gt; 0, we can make both J{6 c2



√



n + 16√



n M2



r2n ∥F α0

s

− Fb∥2}2

and P(gn /∈ Gc(n)) less than ǫ for large enough n and c(&gt; 0), using the fact that √



n/r2

n → 0 and

(28). Thus, P(|νn(gn)| &gt; δ) ≤ 2ǫ by (26).

A similar analysis can be done for the second term of (25). The result now follows.

D.9.3

Proof of Lemma 13

Note that

√



nγn



cn

( ˆF γn

s,n − ¯F γn

s,n) =

√



nγn



cn

( ˆF γn

s,n − F α0

s ) −

√



nγn



cn

( ¯F γn

s,n − F α0

s ).

However, a simpliﬁcation yields

√



nγn



cn

( ˆF γn

s,n − F α0

s ) = 1



cn

√



n(Fn − F) +

√



nM



cnrnα0

(F − Fb).

Since √



n(Fn − F)/cn is oP (1), √



n = cnrn, and F − Fb = α0(F α0

s

− Fb), we have

√



nγn



cnM ( ˆF γn

s,n − F α0

s )

P→ F α0

s

− Fb

in H.

(29)

By applying the functional delta method (see Theorem 20.8 of Van der Vaart (1998)) for the

projection operator (see Theorem 1 of Fils-Villetard et al. (2008)) to (29), we have

√



nγn



cnM ( ¯F γn

s,n − F α0

s )

P→ Π (F α0

s

− Fb| TF(F α0

s ))

in H.

(30)

34


By combining (29) and (30), we have

√



nγn



cnM ( ˆF γn

s,n − ¯F γn

s,n)

P→ (F α0

s

− Fb) − Π (F α0

s

− Fb| TF(F α0

s ))

in H.

(31)

The result now follows by applying the continuous mapping theorem to (31). We prove V ̸= 0 by

contradiction. Suppose that V = 0, i.e., (F α0

s

− Fb) ∈ TF(F α0

s ). Therefore, for some distribution

function G and η &gt; 0, we have V = (η + 1)F α0

s

− Fb − ηG, by the deﬁnition of TF(F α0

s ). By the

discussion leading to (5), it can be easily seen that ηG is a sub-CDF, while (η + 1)F α0

s

− Fb is

not (as that would contradict (5)). Therefore, V ̸= 0 and thus

�

V 2dF &gt; 0.

D.10

Proof of Theorem 4

The constant c deﬁned in the statement of the theorem can be explicitly expressed as

c = −

��

V 2dF

�− 1



2

,

where

V = (Fs − Fb) − Π(Fs − Fb|TF(Fs)),

and Π and TF(·) are deﬁned in (18) and (19), respectively.

Let x &gt; 0. Obviously,

P(rn(ˆαcn

0 − α0) ≤ x) = 1 − P(rn(ˆαcn

0 − α0) &gt; x).

By Lemma 11, we have that P(rn(ˆαcn

0 −α0) &gt; x) → 0 if cn → ∞. Now let x ≤ 0. In this case the

left hand side of the above display equals P(√



nγndn( ˆF γn

s,n, ˇF γn

s,n) ≤ cn), where γn = α0 + x/rn.

A simpliﬁcation yields

√



n



cn

γn( ˆF γn

s,n − F α0

s )

P→ −x(F α0

s

− Fb), in H,

(32)

since√



n(Fn − F)/cn is oP (1); see the proof of Lemma 13 (Section D.9.3) for the details. By ap-

plying the functional delta method (cf. Theorem 20.8 of Van der Vaart (1998)) for the projection

operator (see Theorem 1 of Fils-Villetard et al. (2008)) to (32), we have

√



n



cn

γn( ¯F γn

s,n − F α0

s )

d→ Π (−x(F α0

s

− Fb)| TF(F α0

s ))

in H.

(33)

Adding (32) and (33), we get

√



n



cn

γn( ˆF γn

s,n − ¯F γn

s,n) → −x(F α0

s

− Fb) − Π (−x(F α0

s

− Fb)| TF(F α0

s ))

in H.

By the continuous mapping theorem, we get √



n/cnγnd( ˆF γn

s,n, ¯F γn

s,n)

P→ |x|

��

V 2dF

�1/2 . Hence,

35


by Lemma 12,

P(rn(ˆαcn

0 − α0) ≤ x) →























1,

if x &gt; 0,

1,

if x ≤ 0 and |x| ≤

��

V 2dF

�−1/2

,

0,

otherwise.

D.11

Proof of Theorem 5

Letting cn = H−1

n (1 − β), we have

P(α0 ≥ ˆαL)

=

P

�√



nα0 dn( ˆF α0

s,n, ˇF α0

s,n) ≤ cn

�

≥

P

�√



nα0 dn( ˆF α0

s,n, F α0

s ) ≤ cn

�

= Hn(cn) = 1 − β,

where we have used the fact that α0dn( ˆF α0

s,n, F α0

s ) = dn(Fn, F). Note that, when α0 = 0, F = Fb,

and using (9) we get

P(α0 ≥ ˆαL) = P

�√



n dn(Fn, Fb) ≤ cn

�

= P

�√



n dn(Fn, F) ≤ cn

�

= 1 − β.

D.12

Proof of Theorem 6

It is enough to show that supx |Hn(x) − G(x)| → 0, where G is the limiting distribution of the

Cram´er-von Mises statistic, a continuous distribution. As supx |Gn(x) − G(x)| → 0, it is enough

to show that

√



ndn(Fn, F) − √



nd(Fn, F)

P→ 0.

(34)

We now prove (34). Observe that

n(d2

n − d2)(Fn, F) = √



n(Pn − P)[ˆgn] = νn(ˆgn),

(35)

where ˆgn = √



n(Fn − F)2, Pn denotes the empirical measure of the data, and νn := √



n(Pn − P)

denotes the usual empirical process. We will show that νn(ˆgn)

P→ 0, which will prove (35).

For each positive integer n, we introduce the following class of functions

Gc(n) =

�√



n(H − F)2 : H ∈ F and sup

t∈R

|H(t) − F(t)| &lt;

c



√



n

�

.

Let us also deﬁne

Dn := sup

t∈R

√



n|Fn(t) − F(t)|.

From the deﬁnition of ˆgn and D2

n, we have ˆgn(t) ≤

1



√



nD2

n, for all t ∈ R. As Dn = OP (1), for any

given ǫ &gt; 0, there exists c &gt; 0 (depending on ǫ) such that

P(ˆgn /∈ Gc(n)) = P(√



n sup

t |ˆgn(t)| ≥ c2) = P(D2

n ≥ c2) ≤ ǫ,

(36)

36


for all suﬃciently large n. Therefore, for any δ &gt; 0, using the same sequence of steps as in (26),

P(|νn(ˆgn)| &gt; δ)

≤

J[ ]

E[G2

c(n)]



δ

+ P(ˆgn /∈ Gc(n)),

(37)

where Gc(n) :=

c2



√



n is an envelope for Gc(n) and J[ ] is a constant. Note that to derive the

last inequality we have used the maximal inequality in Corollary (4.3) of Pollard (1989); the

class Gc(n) is “manageable” in the sense of Pollard (1989) (as a consequence of equation (2.5) of

Van de Geer (2000)).

Therefore, for any given δ &gt; 0 and ǫ &gt; 0, for large enough n and c &gt; 0 we can make both

J[ ]c4/(δn) and P(ˆgn /∈ Gc(n)) less than ǫ, using (36) and (37), and thus, P(|νn(ˆgn)| &gt; δ) ≤ 2ǫ.

The result now follows.

D.13

Proof of Theorem 7

The random variable U deﬁned in the statement of the theorem can be explicitly expressed as

U :=

� �

{GF − Π(GF |TF(F α0

s )}2 dF

�1/2

,

where GF is the F-Brownian bridge.

By the same line of arguments as in the proof of Lemma 12 (see Section D.9.2), it can be

easily seen that √



nα0 dn( ˆF α0

s,n, ˇF α0

s,n)− √



nα0 d( ˆF α0

s,n, ¯F α0

s,n)

P→ 0. Moreover, by Donsker’s theorem,

√



nα0( ˆF α0

s,n − F α0

s )

d→ GF .

By applying the functional delta method for the projection operator, in conjunction with the

continuous mapping theorem to the previous display, we have

√



nα0( ¯F α0

s,n − F α0

s )

d→ Π(GF |TF(F α0

s ))

in

H,

where Π, TF(·), and F α0

s

are deﬁned in (18), (19), and (21), respectively. Hence, by an application

of the continuous mapping theorem, we have √



nα0d( ˆF α0

s,n, ¯F α0

s,n)

d→ U. The result now follows.

D.14

Proof of Lemma 9

Let 0 &lt; γ1 &lt; γ2 &lt; 1. Then,

γ2dn( ˆF γ2

s,n, ˇF γ2

s,n)

≤

γ2dn( ˆF γ2

s,n, (γ1/γ2) ˇF γ1

s,n + (1 − γ1/γ2)Fb)

=

dn(γ1 ˆF γ1

s,n + (γ2 − γ1)Fb, γ1 ˇF γ1

s,n + (γ2 − γ1)Fb)

≤

γ1dn( ˆF γ1

s,n, ˇF γ1

s,n),

which shows that γdn( ˆF γ

s,n, ˇF γ

s,n) is a non-increasing function. To show that γdn( ˆF γ

s,n, ˇF γ

s,n) is

convex, let 0 &lt; γ1 &lt; γ2 &lt; 1 and γ3 = ηγ1 + (1 − η)γ2, for 0 ≤ η ≤ 1. Then, by (15) we have the

desired result.

37


D.15

Proof of Theorem 8

The constant c and the function Q deﬁned in the statement of the theorem can be explicitly

expressed as

c = d(Q, Π (Q|TF(Fs))),

and

Q := (Fs − Fb)

�

α2

0

�

V 2dF

�−1/2

,

where

rn = √



n/cn,

V = (Fs − Fb) − Π(Fs − Fb|TF(Fs)),

and Π and TF(·) are deﬁned in (18) and (19), respectively.

Recall the notation of Section D.9. Note that from (2),

ˆF ˇαn

s,n(x) = α0



ˇαn

Fs(x) + ˇαn − α0



ˇαn

Fb(x) + (Fn − F)(x)



ˇαn

,

for all x ∈ R. Thus we can bound ˆF ˇαn

s,n(x) as follows:

α0



ˇαn

Fs(x) − |ˇαn − α0|



ˇαn

− D′

n



ˇαn

≤ ˆF ˇαn

s,n(x) ≤ α0



ˇαn

Fs(x) + |ˇαn − α0|



ˇαn

+ D′

n



ˇαn

,

where D′

n = supx∈R |Fn(x) − F(x)|. As both the upper and lower bounds are monotone, we can

always ﬁnd a version of ˇF ˇαn

s,n such that

α0



ˇαn

Fs − |ˇαn − α0|



ˇαn

− D′

n



ˇαn

≤ ˇF ˇαn

s,n ≤ α0



ˇαn

Fs + |ˇαn − α0|



ˇαn

+ D′

n



ˇαn

.

Therefore,

| ˇF ˇαn

s,n − Fs|

≤

|α0 − ˇαn|



ˇαn

Fs + |ˇαn − α0|



ˇαn

+ D′

n



ˇαn

≤

2|α0 − ˇαn|



ˇαn

+ D′

n



ˇαn

P→ 0,

as n → ∞, using the fact ˇαn

P→ α0 ∈ (0, 1). Furthermore, if qn(ˇαn − α0) = OP (1), where

qn/√



n → 0, it is easy to see that qn| ˇF ˇαn

s,n − Fs| = OP (1), as qnD′

n = oP (1). Note that

rn ˆαcn

0 ( ˆF ˆαcn

0

s,n − Fs) = rn(Fn − F) + rn(α0 − ˆαcn

0 ) (Fs − Fb)

Thus

sup

x∈R

|rn( ˆF

ˆαcn

0

s,n − Fs)(x) − Q(x)|

P→ 0.

Hence by an application of functional delta method for the projection operator, in conjunction

with the continuous mapping theorem, we have

rnd( ˇF ˆαcn

0

s,n , Fs)

P→ d(Q, Π(Q|TF(Fs))).

38


D.16

Proof of Theorem 9

Let ǫn := supx∈R | ˇF ˇαn

s,n(x)− Fs(x)|. Then the function Fs + ǫn is concave on [0, ∞) and majorises

ˇF ˇαn

s,n. Hence, for all x ∈ [0, ∞), ˇF ˇαn

s,n(x) ≤ F †

s,n(x) ≤ Fs(x) + ǫn, as F †

s,n is the LCM of ˇF ˇαn

s,n.

Thus,

−ǫn ≤ ˇF ˇαn

s,n(x) − Fs(x) ≤ F †

s,n(x) − Fs(x) ≤ ǫn,

and therefore,

sup

x∈R

|F †

s,n(x) − Fs(x)| ≤ ǫn.

By Theorem 8, as ǫn

P→ 0, we must also have (12).

The second part of the result follows immediately from the lemma is page 330 of Robertson et al. (1988),

and is similar to the result in Theorem 7.2.2 of that book.

References

Anderson, T. W. and D. A. Darling (1952).

Asymptotic theory of certain “goodness of ﬁt”

criteria based on stochastic processes. Ann. Math. Statistics 23, 193–212.

Barlow, R. E., D. J. Bartholomew, J. M. Bremner, and H. D. Brunk (1972). Statistical inference

under order restrictions. The theory and application of isotonic regression. John Wiley &amp; Sons,

London-New York-Sydney. Wiley Series in Probability and Mathematical Statistics.

Benjamini, Y. and Y. Hochberg (1995). Controlling the false discovery rate: a practical and

powerful approach to multiple testing. J. Roy. Statist. Soc. Ser. B 57(1), 289–300.

Benjamini, Y. and Y. Hochberg (2000).

On the adaptive control of the false discovery rate

in multiple testing with independent statistics. J. Educational and Behavioral Statistics 25,

60–83.

Benjamini, Y., A. Krieger, and D. Yekutieli (2006). Adaptive linear step-up procedures that

control the false discovery rate. Biometrika 93(3), 491–507.

Bertsekas, D. P. (2003). Convex analysis and optimization. Athena Scientiﬁc, Belmont, MA.

With Angelia Nedi´c and Asuman E. Ozdaglar.

Black, M. A. (2004). A note on the adaptive control of false discovery rates. J. R. Stat. Soc.

Ser. B Stat. Methodol. 66(2), 297–304.

Bordes, L., S. Mottelet, and P. Vandekerkhove (2006). Semiparametric estimation of a two-

component mixture model. Ann. Statist. 34(3), 1204–1232.

Cai, T., J. Jin, and M. G. Low (2007). Estimation and conﬁdence sets for sparse normal mixtures.

Ann. Statist. 35(6), 2421–2449.

Cai, T. T. and J. Jin (2010). Optimal rates of convergence for estimating the null density and

proportion of nonnull eﬀects in large-scale multiple testing. Ann. Statist. 38(1), 100–145.

39


Celisse, A. and S. Robin (2010). A cross-validation based estimation of the proportion of true

null hypotheses. J. Statist. Planng. Inf. 140(11), 3132–3147.

Cohen, A. C. (1967). Estimation in mixtures of two normal distributions. Technometrics 9,

15–28.

Day, N. E. (1969).

Estimating the components of a mixture of normal distributions.

Biometrika 56, 463–474.

Donoho, D. and J. Jin (2004). Higher criticism for detecting sparse heterogeneous mixtures. Ann.

Statist. 32(3), 962–994.

Efron, B. (2007). Size, power and false discovery rates. Ann. Statist. 35(4), 1351–1377.

Efron, B. (2010). Large-scale inference, Volume 1 of Institute of Mathematical Statistics Mono-

graphs. Cambridge: Cambridge University Press. Empirical Bayes methods for estimation,

testing, and prediction.

Feller, W. (1971). An introduction to probability theory and its applications. Vol. II. Second

edition. John Wiley &amp; Sons, Inc., New York-London-Sydney.

Fils-Villetard, A., A. Guillou, and J. Segers (2008). Projection estimators of Pickands dependence

functions. Canad. J. Statist. 36(3), 369–382.

Genovese, C. and L. Wasserman (2004). A stochastic process approach to false discovery control.

Ann. Statist. 32(3), 1035–1061.

Grenander, U. (1956). On the theory of mortality measurement. I. Skand. Aktuarietidskr. 39,

70–96.

Grotzinger, S. J. and C. Witzgall (1984). Projections onto order simplexes. Appl. Math. Op-

tim. 12(3), 247–270.

Hastie, T., R. Tibshirani, J. Friedman, T. Hastie, J. Friedman, and R. Tibshirani (2009). The

elements of statistical learning, Volume 2. Springer.

Hengartner, N. W. and P. B. Stark (1995). Finite-sample conﬁdence envelopes for shape-restricted

densities. Ann. Statist. 23(2), 525–550.

Hunter, D. R., S. Wang, and T. P. Hettmansperger (2007). Inference for mixtures of symmetric

distributions. Ann. Statist. 35(1), 224–251.

Jin, J. (2008). Proportion of non-zero normal means: universal oracle equivalences and uniformly

consistent estimators. J. R. Stat. Soc. Ser. B Stat. Methodol. 70(3), 461–493.

Kulldorﬀ, M., J. Heﬀernan, R. Hartman, R. Assuncao, and F. Mostashari (2005). A space-time

permutation scan statistic for disease outbreak detection. PLoS Med. 2(3), e59.

Langaas, M., B. H. Lindqvist, and E. Ferkingstad (2005). Estimating the proportion of true

null hypotheses, with application to DNA microarray data.

J. R. Stat. Soc. Ser. B Stat.

Methodol. 67(4), 555–572.

40


Lindsay, B. G. (1983). The geometry of mixture likelihoods: a general theory. Ann. Statist. 11(1),

86–94.

Lindsay, B. G. (1995). Mixture models: Theory, geometry and applications. NSF-CBMS Regional

Conference Series in Probability and Statistics 5, 1–163.

Lindsay, B. G. and P. Basak (1993). Multivariate normal mixtures: a fast consistent method of

moments. J. Amer. Statist. Assoc. 88(422), 468–476.

Lyons, L. (2008). Open statistical issues in particle physics. Ann. Appl. Stat. 2(3), 887–915.

McLachlan, G. and D. Peel (2000).

Finite mixture models.

Wiley Series in Probability and

Statistics: Applied Probability and Statistics. Wiley-Interscience, New York.

Meinshausen, N. and P. B¨uhlmann (2005). Lower bounds for the number of false null hypotheses

for multiple testing of associations under general dependence structures. Biometrika 92(4),

893–907.

Meinshausen, N. and J. Rice (2006). Estimating the proportion of false null hypotheses among

a large number of independently tested hypotheses. Ann. Statist. 34(1), 373–393.

Miller, C. J., C. Genovese, R. C. Nichol, L. Wasserman, A. Connolly, D. Reichart, A. Hopkins,

and A. Schneider, J.and Moore (2001). Controlling the false-discovery rate in astrophysical

data analysis. Astron. J. 122(6), 3492–3505.

Nguyen, V. H. and C. Matias (2013).

On eﬃcient estimators of the proportion of true null

hypotheses in a multiple testing setup. arXiv:1205.4097.

Parzen, E. (1960). Modern probability theory and its applications. John Wiley &amp; Sons, Incorpo-

rated.

Pollard, D. (1989).

Asymptotics via empirical processes.

Statist. Sci. 4(4), 341–366.

With

comments and a rejoinder by the author.

Quandt, R. E. and J. B. Ramsey (1978).

Estimating mixtures of normal distributions and

switching regressions.

J. Amer. Statist. Assoc. 73(364), 730–752.

With comments and a

rejoinder by the authors.

R Development Core Team (2008). R: A Language and Environment for Statistical Computing.

Vienna, Austria: R Foundation for Statistical Computing. ISBN 3-900051-07-0.

Robertson, T., F. T. Wright, and R. L. Dykstra (1988). Order restricted statistical inference. Wi-

ley Series in Probability and Mathematical Statistics: Probability and Mathematical Statistics.

Chichester: John Wiley &amp; Sons Ltd.

Robin, A. C., C. Reyl, S. Derrire, and S. Picaud (2003). A synthetic view on structure and

evolution of the milky way. Astronomy and Astrophysics 409(1), 523–540.

Robin, S., A. Bar-Hen, J.-J. Daudin, and L. Pierre (2007). A semi-parametric approach for

mixture models: application to local false discovery rate estimation. Comput. Statist. Data

Anal. 51(12), 5483–5493.

41


Salvador, S. and P. Chan (2004). Determining the number of clusters/segments in hierarchical

clustering/segmentation algorithms. Proc. 16th IEEE Intl. Conf. on Tools with AI 25, 576–584.

Storey, J. D. (2002). A direct approach to false discovery rates. J. R. Stat. Soc. Ser. B Stat.

Methodol. 64(3), 479–498.

Swanepoel, J. W. H. (1999). The limiting behavior of a modiﬁed maximal symmetric 2s-spacing

with applications. Ann. Statist. 27(1), 24–35.

Turkheimer, F., C. Smith, and K. Schmidt (2001).

Estimation of the number of “true null

hypotheses in multivariate analysis of neuroimaging data. NeuroImage 13(5), 920–930.

Van de Geer, S. A. (2000). Applications of empirical process theory, Volume 6 of Cambridge

Series in Statistical and Probabilistic Mathematics. Cambridge: Cambridge University Press.

Van der Vaart, A. W. (1998). Asymptotic statistics, Volume 3 of Cambridge Series in Statistical

and Probabilistic Mathematics. Cambridge: Cambridge University Press.

Walker, M., M. Mateo, E. Olszewski, B. Sen, and M. Woodroofe (2009). Clean kinematic samples

in dwarf spheroidals: An algorithm for evaluating membership and estimating distribution

parameters when contamination is present. The Astronomical Journal 137, 3109.

Walker, M. G., M. Mateo, E. W. Olszewski, O. Y. Gnedin, X. Wang, B. Sen, and M. Woodroofe

(2007). Velocity dispersion proﬁles of seven dwarf spheroidal galaxies. Astrophysical J. 667(1),

L53–L56.

Walther, G. (2001). Multiscale maximum likelihood analysis of a semiparametric model, with

applications. Ann. Statist. 29(5), 1297–1319.

Walther, G. (2002). Detecting the presence of mixing with multiscale maximum likelihood. J.

Amer. Statist. Assoc. 97(458), 508–513.

42


(c)

(e)







