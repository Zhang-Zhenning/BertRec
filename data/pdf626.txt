


Nov 3, 2019

·

14 min read

BERT Explained: A Complete Guide with Theory and

Tutorial

BERT

Why was BERT needed?

What is the core idea behind it?

How does it work?

When can we use it and how to fine-tune it?

How can we use it? Using BERT for Text Classification — Tutorial

Part I

1. Why was BERT needed?



U


pre-training

fine-tuned

2. What is the core idea behind it?

bidirectionally trained 

Masked LM

same time. 

But why is this non-directional approach so powerful?

context-free

context-based

unidirectional

bidirectional


3. How does it work?

Token embeddings

Segment embeddings

Positional embeddings

1. Masked LM (MLM)


2. Next Sentence Prediction (NSP)

Architecture

BERT-Base

BERT-Large

Fun fact

4. When can we use it and how to fine-tune it?


a question answering application

start

vector

end vector.

Part II

5. How can we use it? Using BERT for Text Classification — Tutorial

Yelp Reviews Polarity dataset

1. Installation

git clone https://github.com/google-research/bert.git

BERT-Base, Uncased

BERT-Large, Uncased

BERT-Base, Cased

BERT-Large, Cased


2. Preparing the data

tsv 

Column 0:

Column 1:

Column 2:

Column 3:

train.tsv

dev.tsv

test.tsv

train.tsv

dev.tsv

test.tsv


Here 

3. Training Model using Pre-trained BERT model

“data”

“bert_output”

pre-trained BERT model

“./”

“no free lunch!“

bert_output

test_results.tsv


4. Making predictions on new data

test.tsv

bert_output

model.ckpt

run_classifier.py

test_results.tsv

5. Taking it a step further

 fine tuning

creating a single new layer trained to adapt BERT

PyTorch

Tensorflow

Final Thoughts

Like to Learn AI/ML concepts in an intuitive way?

ML blog


References and Further Readings

1



Follow



Sr. Applied Scientist/AI Engineer @ Microsoft | Continuous Learning | Living to the Fullest | ML Blog: https://towardsml.com/

Machine Learning

NLP

Bert

AI

Tutorial






