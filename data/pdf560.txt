




Written by Cem Dilmegani

Large language models (LLMs) took the internet by storm at the end of 2022 as ChatGPT from OpenAI reached 1 million users just 5 days

after its launch. ChatGPT’s capabilities and wide applications can be accredited to the 175 billion parameters the GPT-3 language model has. 

Although it is easy to use end-product language models like ChatGPT, developing a large language model takes significant computer science

knowledge, time, and resources. We created this article to inform business leaders on:

1. Definition of large language models

2. Examples of large language models

3. Architecture of large language models

4. The training process of large language models,

So that they can leverage artificial intelligence and machine learning effectively.

What is a large language model?

A large language model is a type of machine learning model that is trained on a large corpus of text data to generate outputs for various

natural language processing (NLP) tasks, such as text generation, question answering, and machine translation. 

Large language models are typically based on deep learning neural networks such as the Transformer architecture and are trained on massive

amounts of text data, often involving billions of words. Larger models, such as Google’s BERT model, are trained with a large dataset from

various data sources which allows them to generate output for many tasks.

If you are new to large language models, check our “Large Language Models: Complete Guide in 2023” article.

Top large language models by parameter size

We compiled the 7 largest large language models by parameter size in the table below.1

Model

Developer

Parameter Size

WuDao 2.0 Beijing Academy of Artificial Intelligence 1.75 trillion

MT-NLG

Nvidia and Microsoft

530 billion

Bloom

Hugging Face and BigScience

176 billion

GPT-3

OpenAI

175 billion

LaMDA

Google

137 billion

ESMFold

Meta AI

15 billion

Gato

DeepMind

1.18 billion

Check our article on large language model examples for more models with in-depth information.

Architecture of large language models

The architecture of large language models, such as OpenAI’s GPT-3, is based on a type of deep learning called the Transformer architecture. It

consists of the following main components (see Figure 1):

Figure 1: Transformer architecture 

NLP , DEEP LEARNING



Large Language Model Training in 2023

UPDATED ON FEBRUARY 16, 2023

PUBLISHED ON FEBRUARY 3, 2023

5 MINUTE READ




Source:2

1. Input embedding

 The input sequence is first transformed into a dense vector representation, known as an embedding, which captures the relationships between

words in the input.

2. Multi-head self-attention

The core component of the transformer block architecture is the multi-head self-attention mechanism, which allows the model to attend to

different parts of the input sequence to capture its relationships and dependencies.

3. Feed-forward network

 After the self-attention mechanism, the output is fed into a feed-forward neural network, which performs a non-linear transformation to generate

a new representation.

4. Normalization and residual connections

To stabilize the training process, the output from each layer is normalized, and a residual connection is added to allow the input to be passed

directly to the output, allowing the model to learn which parts of the input are most important.

These components are repeated several times to form a deep neural network, which can process long sequences of text and generate high-

quality outputs for various language tasks, such as text generation, question answering, and translation.

Developers continue to develop large language models by implementing new techniques to:

Simplify the model (decrease the model size or memory required to train),

Improve performance,

Lower price,

Decrease model training time.

Training large language models

There are four steps to training large language models:

1. Data collection and preprocessing 

The first step is to gather the training data set, which is the resource that the LLM will be trained on. The data can come from various sources

such as books, websites, articles, and open datasets. 

Popular public sources to find datasets are:

Kaggle

Google Dataset Search

Hugging Face


Data.gov

Wikipedia database

The data then needs to be cleaned and prepared for training. This may involve converting the dataset to lowercase, removing stop words, and

tokenizing the text into sequences of tokens that make up the text. 

2. Model selection and configuration

Large models such as Google’s BERT and OpenAI’s GPT-3 both use transformer deep learning architecture, which is the common choice for

sophisticated NLP applications in recent years. Some key elements of the model such as:

Number of layers in transformer blocks

Number of attention heads

Loss function

Hyperparameters

need to be specified when configuring a transformer neural network. The configuration can depend on the desired use case and the training

data. The configuration of the model directly influences the training time of the model. 

3. Model training

The model is trained on the pre-processed text data using supervised learning. During training, the model is presented with a sequence of

words and is trained to predict the next word in the sequence. The model adjusts its weights based on the difference between its prediction and

the actual next word. This process is repeated millions of times until the model reaches a satisfactory level of performance.

Since the models and data are large in size, it requires immense computation power to train models. To decrease training time, a technique

called model parallelism is used. Model parallelism enables different parts of a large model to be spread across multiple GPUs, allowing the

model to be trained in a distributed manner with AI chips. 

By dividing the model into smaller parts, each part can be trained in parallel, resulting in a faster training process compared to training the

entire model on a single GPU or processor. This results in faster convergence and better overall performance, making it possible to train even

larger language models than before. Common types of model parallelism include:

Data parallelism

Sequence parallelism

Pipeline parallelism

Tensor parallelism 

Training a large language model from the ground up requires significant investment, a more economical alternative is to fine-tune an existing

language model to tailor it to your specific use case. A single training run for GPT-3 is estimated to cost around $5 million.

4. Evaluation and fine-tuning

After training, the model is evaluated on a test dataset that has not been used as a training data set to measure the model’s performance.

Based on the evaluation results, the model may require some fine-tuning by adjusting its hyperparameters, changing the architecture, or training

on additional data to improve its performance. 

Training LLMs for specific use cases

Training of an LLM consists of two parts: pre-training and task-specific training. Pre-training is part of training that enables the model to learn

the general rules and dependencies within a language, which takes a significant amount of data, computational power, and time to complete.

The large language models discussed in the paper require supercomputer systems with several AI chips (ex. NVIDIA DGX A100 starts at

$199,999). Once maintenance and power costs are added, pre-training of a large language model is an investment in the magnitude of millions.

To make large language models more accessible for enterprises, LLM developers are offering services for enterprises looking to leverage

language models. NVIDIA’s NeMO is an example of these services, which offer pre-trained LLMs for fine-tuning and specific task training to suit

specific use cases. The specific task training adds an additional layer to the model which requires much less data, power, and time to train;

making large models accessible for enterprise use. The new task-specific layer is trained with few-shot learning, which aims for accurate outputs

with less training data.

Since the model is already pre-trained and familiar with the language, few-shot learning is a viable method to teach domain-specific words and

phrases to the model.

The video below introduces NVIDIA’s NeMO LLM service.




Cem Dilmegani

Share on Twitter

If you have further questions on large language models, do not hesitate to contact us:



Find the Right Vendors

This article was drafted by former AIMultiple industry analyst Berke Can Agagündüz.

1. Wodecki, Ben (July 22, 2022). “7 language models you need to know“. AI Business. Retrieved February 3, 2023.

2. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2017). Attention is All you Need.

Neural Information Processing Systems, 30, 5998–6008. https://arxiv.org/pdf/1706.03762v5

Share on LinkedIn

Cem has been the principal analyst at AIMultiple since 2017. AIMultiple informs hundreds of thousands of businesses (as per similarWeb) including 55% of

Fortune 500 every month.

Cem's work has been cited by leading global publications including Business Insider, Forbes, Washington Post, global firms like Deloitte, HPE  and NGOs like

World Economic Forum and supranational organizations like European Commission. You can see more reputable companies and resources that referenced

AIMultiple.

Throughout his career, Cem served as a tech consultant, tech buyer and tech entrepreneur. He advised enterprises on their technology decisions at McKinsey &amp;

Company and Altman Solon for more than a decade. He also published a McKinsey report on digitalization.

He led technology strategy and procurement of a telco while reporting to the CEO. He has also led commercial growth of deep tech company Hypatos that

reached a 7 digit annual recurring revenue and a 9 digit valuation from 0 within 2 years. Cem's work in Hypatos was covered by leading technology publications

like TechCrunch like Business Insider. 

Cem regularly speaks at international technology conferences. He graduated from Bogazici University as a computer engineer and holds an MBA from

Columbia Business School.





NLP , Generative AI

IN-DEPTH GUIDE TO KNOWLEDGE GRAPH:

BENEFITS, USE CASES &amp; EXAMPLES



Generative AI , NLP

LLM FINE TUNING GUIDE FOR

ENTERPRISES IN 2023



NLP , Generative AI

THE FUTURE OF LARGE LANGUAGE

MODELS

RELATED RESEARCH

Leave a Reply

YOUR EMAIL ADDRESS WILL NOT BE PUBLISHED. REQUIRED FIELDS ARE MARKED * 

Comment *

Name Surname

 E-mail Address

 


Solutions

RPA

Data Annotation

Process Mining

Recommendation Engine

Voice Bots

All

For Tech Users

Shortlist Solutions

Get Advice

Vendors

Claim Your Product

Learn Best Practices

Investors

Identify Hidden Gems

Tech Firms By Country

Tech Firms By City

AIMultiple

Mission

About

Career

Contact

LinkedIn

Twitter

POST COMMENT

0 Comments



Subscribe to the latest news &amp;



updates from our experts 

Enter your e-mail



 SUBSCRIBE



 



 By checking this box, you confirm that you have read and agreed to our terms and conditions.

Businesses face the most complex technology landscape. To solve a single problem, firms can leverage hundreds of solution categories with hundreds of vendors in each category. We bring

transparency and data-driven decision making to emerging tech procurement of enterprises. Use our vendor lists or research articles to identify how technologies like AI / machine learning /

data science, IoT, process mining, RPA, synthetic data can transform your business.

Data-driven, Transparent, Practical New Tech Industry Analysis

Terms of Use - Privacy Policy

© Copyright 2023 AIMultiple

