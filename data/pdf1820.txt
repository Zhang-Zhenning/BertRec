
Generative model

Toggle the table of contents



 9

languages

Article

Talk

Tools From Wikipedia, the free encyclopedia

This article is about generative models in the context of statistical classification. For generative models of Markov decision processes, see Markov decision process § Simulator models. For Generative Modelling

Language (GML) in computer graphics and generative computer programming, see Generative Modelling Language. For Generative Artificial Intelligence (Generative A.I.) models/systems, see Generative artificial

intelligence.

In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling.

Terminology is inconsistent,[a] but three major types can be distinguished, following Jebara (2004):

1. A generative model is a statistical model of the joint probability distribution 

 on given observable variable X and target variable Y;[1]

2. A discriminative model is a model of the conditional probability 

 of the target Y, given an observation x; and

3. Classifiers computed without using a probability model are also referred to loosely as "discriminative".

The distinction between these last two classes is not consistently made;[2] Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng &amp; Jordan (2002) only

distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes.[3] Analogously,

a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a

model.

Standard examples of each, all of which are linear classifiers, are:

generative classifiers:

naive Bayes classifier and

linear discriminant analysis

discriminative model:

logistic regression

In application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one

can estimate the probability of a label given an observation, 

 (discriminative model), and base classification on that; or one can estimate the joint distribution 

 (generative model), from that compute the

conditional probability 

, and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different

approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches.

Definition [edit]

An alternative division defines these symmetrically as:

a generative model is a model of the conditional probability of the observable X, given a target y, symbolically, 

[4]

a discriminative model is a model of the conditional probability of the target Y, given an observation x, symbolically, 

[5]

Regardless of precise definition, the terminology is constitutional because a generative model can be used to "generate" random instances (outcomes), either of an observation and target 

, or of an observation x given

a target value y,[4] while a discriminative model or discriminative classifier (without a model) can be used to "discriminate" the value of the target variable Y, given an observation x.[5] The difference between "discriminate"

(distinguish) and "classify" is subtle, and these are not consistently distinguished. (The term "discriminative classifier" becomes a pleonasm when "discrimination" is equivalent to "classification".)

The term "generative model" is also used to describe models that generate instances of output variables in a way that has no clear relationship to probability distributions over potential samples of input variables.

Generative adversarial networks are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs. Such models are not classifiers.

Relationships between models [edit]

In application to classification, the observable X is frequently a continuous variable, the target Y is generally a discrete variable consisting of a finite set of labels, and the conditional probability 

 can also be

interpreted as a (non-deterministic) target function 

, considering X as inputs and Y as outputs.

Given a finite set of labels, the two definitions of "generative model" are closely related. A model of the conditional distribution 

 is a model of the distribution of each label, and a model of the joint distribution is

equivalent to a model of the distribution of label values 

, together with the distribution of observations given a label, 

; symbolically, 

 Thus, while a model of the joint probability distribution is more

informative than a model of the distribution of label (but without their relative frequencies), it is a relatively small step, hence these are not always distinguished.

Given a model of the joint distribution, 

, the distribution of the individual variables can be computed as the marginal distributions 

 and 

 (considering X as continuous, hence integrating

over it, and Y as discrete, hence summing over it), and either conditional distribution can be computed from the definition of conditional probability: 

 and 

.

Given a model of one conditional probability, and estimated probability distributions for the variables X and Y, denoted 

 and 

, one can estimate the opposite conditional probability using Bayes' rule:

For example, given a generative model for 

, one can estimate:

and given a discriminative model for 

, one can estimate:

Note that Bayes' rule (computing one conditional probability in terms of the other) and the definition of conditional probability (computing conditional probability in terms of the joint distribution) are frequently conflated as

well.

Contrast with discriminative classifiers [edit]

A generative algorithm models how the data was generated in order to categorize a signal. It asks the question: based on my generation assumptions, which category is most likely to generate this signal? A discriminative

algorithm does not care about how the data was generated, it simply categorizes a given signal. So, discriminative algorithms try to learn 

 directly from the data and then try to classify data. On the other hand,

generative algorithms try to learn 

 which can be transformed into 

 later to classify the data. One of the advantages of generative algorithms is that you can use 

 to generate new data similar to existing data. On

the other hand, it has been proved that some discriminative algorithms give better performance than some generative algorithms in classification tasks.[6]

Despite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express complex relationships between the observed and target variables. But in general,

they don't necessarily perform better than generative models at classification and regression tasks. The two classes are seen as complementary or as different views of the same procedure.[7]

Deep generative models [edit]

With the rise of deep learning, a new family of methods, called deep generative models (DGMs),[8][9][10] is formed through the combination of generative models and deep neural networks. An increase in the scale of the

neural networks is typically accompanied by an increase in the scale of the training data, both of which are required for good performance.[11]

Popular DGMs include variational autoencoders (VAEs), generative adversarial networks (GANs), and auto-regressive models. Recently, there has been a trend to build very large deep generative models.[8] For example,

GPT-3, and its precursor GPT-2,[12] are auto-regressive neural language models that contain billions of parameters, BigGAN[13] and VQ-VAE[14] which are used for image generation that can have hundreds of millions of

parameters, and Jukebox is a very large generative model for musical audio that contains billions of parameters.[15]

Types [edit]

Generative models [edit]

Types of generative models are:

Gaussian mixture model (and other types of mixture model)

Hidden Markov model

Probabilistic context-free grammar








v · t · e

Bayesian network (e.g. Naive bayes, Autoregressive model)

Averaged one-dependence estimators

Latent Dirichlet allocation

Boltzmann machine (e.g. Restricted Boltzmann machine, Deep belief network)

Variational autoencoder

Generative adversarial network

Flow-based generative model

Energy based model

Diffusion model

If the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to maximize the data likelihood is a common method. However, since most statistical models are only

approximations to the true distribution, if the model's application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are

necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a discriminative model (see below), although application-specific details will

ultimately dictate which approach is most suitable in any particular case.

Discriminative models [edit]

k-nearest neighbors algorithm

Logistic regression

Support Vector Machines

Decision Tree Learning

Random Forest

Maximum-entropy Markov models

Conditional random fields

Examples [edit]

Simple example [edit]

Suppose the input data is 

, the set of labels for  is 

, and there are the following 4 data points: 

For the above data, estimating the joint probability distribution 

 from the empirical measure will be the following:

while 

 will be following:

Text generation [edit]

Shannon (1948) gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with "representing and speedily is an good"; which is not proper English but which will

increasingly approximate it as the table is moved from word pairs to word triplets etc.

See also [edit]



Mathematics portal

Discriminative model

Graphical model

Notes [edit]

a. ^ Three leading sources, Ng &amp; Jordan 2002, Jebara 2004, and Mitchell 2015, give different divisions and definitions.

References [edit]

1. ^ Ng &amp; Jordan (2002): "Generative classifiers learn a model of the joint probability, 

, of the inputs x and the label y, and make their predictions by using Bayes rules to calculate 

, and then picking the most likely label y.

2. ^ Jebara 2004, 2.4 Discriminative Learning: "This distinction between conditional learning and discriminative learning is not currently a well established convention in the field."

3. ^ Ng &amp; Jordan 2002: "Discriminative classifiers model the posterior 

 directly, or learn a direct map from inputs x to the class labels."

4. ^ a b Mitchell 2015: "We can use Bayes rule as the basis for designing learning algorithms (function approximators), as follows: Given that we wish to learn some target function 

, or equivalently, 

, we use the training data to

learn estimates of 

 and 

. New X examples can then be classified using these estimated probability distributions, plus Bayes rule. This type of classifier is called a generative classifier, because we can view the distribution 

as describing how to generate random instances X conditioned on the target attribute Y.

5. ^ a b Mitchell 2015: "Logistic Regression is a function approximation algorithm that uses training data to directly estimate 

, in contrast to Naive Bayes. In this sense, Logistic Regression is often referred to as a discriminative

classifier because we can view the distribution 

 as directly discriminating the value of the target value Y for any given instance X

6. ^ Ng &amp; Jordan 2002

7. ^ Bishop, C. M.; Lasserre, J. (24 September 2007), "Generative or Discriminative? getting the best of both worlds", in Bernardo, J. M. (ed.), Bayesian statistics 8: proceedings of the eighth Valencia International Meeting, June 2-6,

2006 , Oxford University Press, pp. 3–23, ISBN 978-0-19-921465-5

8. ^ a b "Scaling up—researchers advance large-scale deep generative models" . Microsoft. April 9, 2020.

9. ^ "Generative Models" . OpenAI. June 16, 2016.

10. ^ Tomczak, Jakub (2022). Deep Generative Modeling . Cham: Springer. p. 197. doi:10.1007/978-3-030-93158-2 . ISBN 978-3-030-93157-5. S2CID 246946335 .

11. ^ Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario (2020). "Scaling Laws for Neural Language Models". arXiv:2001.08361

[stat.ML ].

12. ^ "Better Language Models and Their Implications" . OpenAI. February 14, 2019.

13. ^ Brock, Andrew; Donahue, Jeff; Simonyan, Karen (2018). "Large Scale GAN Training for High Fidelity Natural Image Synthesis". arXiv:1809.11096  [cs.LG ].

14. ^ Razavi, Ali; van den Oord, Aaron; Vinyals, Oriol (2019). "Generating Diverse High-Fidelity Images with VQ-VAE-2". arXiv:1906.00446  [cs.LG ].

External links [edit]

Shannon, C. E. (1948). 



"A Mathematical Theory of Communication"

 (PDF). Bell System Technical Journal. 27 (July, October): 379–423, 623–656. doi:10.1002/j.1538-7305.1948.tb01338.x . hdl:10338.dmlcz/101429 .

Mitchell, Tom M. (2015). 



"3. Generative and Discriminative Classifiers: Naive Bayes and Logistic Regression"

 (PDF). Machine Learning.

Ng, Andrew Y.; Jordan, Michael I. (2002). 



"On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes"

 (PDF). Advances in Neural Information Processing Systems.

Jebara, Tony (2004). Machine Learning: Discriminative and Generative . The Springer International Series in Engineering and Computer Science. Kluwer Academic (Springer). ISBN 978-1-4020-7647-3.

Jebara, Tony (2002). Discriminative, generative, and imitative learning (PhD). Massachusetts Institute of Technology. hdl:1721.1/8323 ., (



mirror

, mirror ), published as book (above)

Code accompanying the book (Tomczak, Jakub (2022). Deep Generative Modeling . Cham: Springer. p. 197. doi:10.1007/978-3-030-93158-2 . ISBN 978-3-030-93157-5. S2CID 246946335 .): "Introductory examples" . GitHub.

Retrieved October 21, 2022.

Statistics

Outline · Index


Privacy policy About Wikipedia Disclaimers

Contact Wikipedia Mobile view Developers

Statistics

Cookie statement





This page was last edited on 6 April 2023, at 20:13 (UTC).

Text is available under the Creative Commons Attribution-ShareAlike License 3.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a

non-profit organization.

Descriptive statistics

Continuous data

Center

Mean (Arithmetic · Arithmetic-Geometric · Cubic · Generalized/power · Geometric · Harmonic · Heronian · Heinz · Lehmer) · Median · Mode

Dispersion

Average absolute deviation · Coefficient of variation · Interquartile range · Percentile · Range · Standard deviation · Variance

Shape

Central limit theorem · Moments (Kurtosis · L-moments · Skewness)

Count data

Index of dispersion

Summary tables

Contingency table · Frequency distribution · Grouped data

Dependence

Partial correlation · Pearson product-moment correlation · Rank correlation (Kendall's τ · Spearman's ρ) · Scatter plot

Graphics

Bar chart · Biplot · Box plot · Control chart · Correlogram · Fan chart · Forest plot · Histogram · Pie chart · Q–Q plot · Radar chart · Run chart · Scatter plot · Stem-and-leaf display · Violin plot

Data collection

Study design

Effect size · Missing data · Optimal design · Population · Replication · Sample size determination · Statistic · Statistical power

Survey methodology

Sampling (Cluster · Stratified) · Opinion poll · Questionnaire · Standard error

Controlled experiments

Blocking · Factorial experiment · Interaction · Random assignment · Randomized controlled trial · Randomized experiment · Scientific control

Adaptive designs

Adaptive clinical trial · Stochastic approximation · Up-and-down designs

Observational studies

Cohort study · Cross-sectional study · Natural experiment · Quasi-experiment

Statistical inference

Statistical theory

Population · Statistic · Probability distribution · Sampling distribution (Order statistic) · Empirical distribution (Density estimation) · Statistical model (Model specification · Lp space) · Parameter (location · scale · shape) ·

Parametric family (Likelihood (monotone) · Location–scale family · Exponential family) · Completeness · Sufficiency · Statistical functional (Bootstrap · U · V) · Optimal decision (loss function) · Efficiency ·

Statistical distance (divergence) · Asymptotics · Robustness

Frequentist inference

Point estimation

Estimating equations (Maximum likelihood · Method of moments · M-estimator · Minimum distance) · Unbiased estimators (Mean-unbiased minimum-variance (Rao–Blackwellization ·

Lehmann–Scheffé theorem) · Median unbiased) · Plug-in

Interval estimation

Confidence interval · Pivot · Likelihood interval · Prediction interval · Tolerance interval · Resampling (Bootstrap · Jackknife)

Testing hypotheses

1- &amp; 2-tails · Power (Uniformly most powerful test) · Permutation test (Randomization test) · Multiple comparisons

Parametric tests

Likelihood-ratio · Score/Lagrange multiplier · Wald

Specific tests

Z-test (normal) · Student's t-test · F-test

Goodness of fit

Chi-squared · G-test · Kolmogorov–Smirnov · Anderson–Darling · Lilliefors · Jarque–Bera · Normality (Shapiro–Wilk) · Likelihood-ratio test · Model selection (Cross validation · AIC · BIC)

Rank statistics

Sign (Sample median) · Signed rank (Wilcoxon) (Hodges–Lehmann estimator) · Rank sum (Mann–Whitney) · Nonparametric anova (1-way (Kruskal–Wallis) · 2-way (Friedman) ·

Ordered alternative (Jonckheere–Terpstra)) · Van der Waerden test

Bayesian inference

Bayesian probability (prior · posterior) · Credible interval · Bayes factor · Bayesian estimator (Maximum posterior estimator)

Correlation · Regression analysis

Correlation

Pearson product-moment · Partial correlation · Confounding variable · Coefficient of determination

Regression analysis

Errors and residuals · Regression validation · Mixed effects models · Simultaneous equations models · Multivariate adaptive regression splines (MARS)

Linear regression

Simple linear regression · Ordinary least squares · General linear model · Bayesian regression

Non-standard predictors

Nonlinear regression · Nonparametric · Semiparametric · Isotonic · Robust · Heteroscedasticity · Homoscedasticity

Generalized linear model

Exponential families · Logistic (Bernoulli) / Binomial / Poisson regressions

Partition of variance

Analysis of variance (ANOVA, anova) · Analysis of covariance · Multivariate ANOVA · Degrees of freedom

Categorical / Multivariate / Time-series / Survival analysis

Categorical

Cohen's kappa · Contingency table · Graphical model · Log-linear model · McNemar's test · Cochran–Mantel–Haenszel statistics

Multivariate

Regression · Manova · Principal components · Canonical correlation · Discriminant analysis · Cluster analysis · Classification · Structural equation model (Factor analysis) · Multivariate distributions

(Elliptical distributions (Normal))

Time-series

General

Decomposition · Trend · Stationarity · Seasonal adjustment · Exponential smoothing · Cointegration · Structural break · Granger causality

Specific tests

Dickey–Fuller · Johansen · Q-statistic (Ljung–Box) · Durbin–Watson · Breusch–Godfrey

Time domain

Autocorrelation (ACF) (partial (PACF)) · Cross-correlation (XCF) · ARMA model · ARIMA model (Box–Jenkins) · Autoregressive conditional heteroskedasticity (ARCH) · Vector autoregression (VAR)

Frequency domain

Spectral density estimation · Fourier analysis · Least-squares spectral analysis · Wavelet · Whittle likelihood

Survival

Survival function

Kaplan–Meier estimator (product limit) · Proportional hazards models · Accelerated failure time (AFT) model · First hitting time

Hazard function

Nelson–Aalen estimator

Test

Log-rank test

Applications

Biostatistics

Bioinformatics · Clinical trials / studies · Epidemiology · Medical statistics

Engineering statistics

Chemometrics · Methods engineering · Probabilistic design · Process / quality control · Reliability · System identification

Social statistics

Actuarial science · Census · Crime statistics · Demography · Econometrics · Jurimetrics · National accounts · Official statistics · Population statistics · Psychometrics

Spatial statistics

Cartography · Environmental statistics · Geographic information system · Geostatistics · Kriging



Category · 



 Mathematics portal · 



Commons · 



 WikiProject

Categories: Machine learning

Statistical models

Probabilistic models



