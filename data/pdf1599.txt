
Published in

MTI Technology



May 24, 2020

·

14 min read

N-gram language models

Part 2: Higher n-gram models

Background

unigram language model

higher n-gram models

Higher n-gram language models

Training the model






n-1 words

(n-1)-

grams

For simplicity, all words are lower-cased in the language model, and punctuations are ignored. The presence of the

[END] tokens are explained in part 1.

Dealing with words near the start of sentence

sentence-starting symbols [S]

S_train: number of sentences in training text

Dealing with unknown n-grams

Laplace smoothing

Laplace smoothing for unigram model: each unigram is added a pseudo-count of k. N: total number of words in

training text. V: number of unique unigrams in training text.

Evaluating the model


average log likelihood

Coding the n-gram models

Training the model

NgramCounter

UnigramCounter

nltk.util.ngrams

1.


List of bigrams from a tokenized sentence

counts

start_counts

There are 3 occurrences of ‘he was a’ in the training text, including 2 at the beginning of a sentence


NgramModel

NgramCounter

train

counts

token_count

probs

2.


start_counts

NgramCounter

sentence_count

start_probs

start_probs


Evaluating the model

word level

V: number of unique unigrams in training text. N: total number of tokens in training text. Equal signs: identical

starting probabilities, which are stored only once in the 

 dict of the model

Building the probability matrix

uniform_prob

ngram_end = token_position + 1

ngram_start = token_position + 1 — ngram_length

token_position

ngram_length

ngram_end

ngram_start

train_probs

1.

2.


probs

evaluate

NgramModel


Model interpolation


Result

N-gram length:

N-gram interpolation weight:

Evaluation text:

Result

Left panel: train

Middle panel: dev1


Amory Lorch

A more complex model is not necessarily better, especially when the

training data is small.

Right panel: dev2

1. Difference in n-gram distributions:

2. Unknown n-grams:

Number above each data point indicates the n-gram length of the model


Effect of model interpolation

gives a small probability to the unknown n-grams

reduces model over-fit

Final remarks

sparsity problem

References

1.

2.

Naturallanguageprocessing

Data Science


1

A place where MTI-ers can publish ideas about new technologies, agile concepts and their working experiences



Read more from MTI Technology





Statistics

Ngrams

Machine Learning

