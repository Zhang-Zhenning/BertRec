




























































































Log in

Sign up



nbro

37.6k

12

92

166



Exploring

173

3

15



nbro

37.6k

12

92

166



Isbister

156

4



jasonh

3

3



What is the difference between a language model and a word embedding?

Asked 2 years, 1 month ago

Modified 1 year, 11 months ago

Viewed 5k times

4

 

 

I am self-studying applications of deep learning on the NLP and machine translation.

I am confused about the concepts of "Language Model", "Word Embedding", "BLEU Score".

It appears to me that a language model is a way to predict the next word given its previous word. Word2vec is the similarity between two tokens. BLEU score is a way to measure the effectiveness of the language model.

Is my understanding correct? If not, can someone please point me to the right articles, paper, or any other online resources?

Share

Improve this question

edited Mar 10, 2021 at 0:45

asked Mar 9, 2021 at 21:43

2 Answers

Sorted by:

4

 

Simplified: Word Embeddings does not consider context, Language Models does.

For e.g Word2Vec, GloVe, or fastText, there exists one fixed vector per word.

Think of the following two sentences:

and

If you averaged their word embeddings, they would have the same vector, but, in reality, their meaning (semantic) is very different.

Then the concept of contextualized word embeddings arose with language models that do consider the context, and give different embeddings depending on the context.

Both word embeddings (e.g Word2Vec) and language models (e.g BERT) are ways of representing text, where language models capture more information and are considered state-of-the-art for representing natural

language in a vectorized format.

BLEU score is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Which is not directly related to the difference between traditional word embeddings

and contextualized word embeddings (aka language models).

Share

Improve this answer

edited Mar 10, 2021 at 15:45

answered Mar 10, 2021 at 10:45

2

 

 

A language model aims to estimate the probability of one or more words given the surrounding words. Given a sentence composed of w1,...,wi−1,_,wi+1,..,wn, you can find which is the i-th missing word using a

language model. In this way, you can estimate which is the most probable word using for example the conditional probability P(wi =w|w1,…,wn). An example of a simple language model is an n-gram where instead of

conditioning on all previous words, you look only to the previous n words.

Word embeddings are a distributed representation of a word. Instead of using an index or a one-hot encoding to represent a word, a dense vector is used. If two words have similar embeddings then these words share

some properties. These properties are driven by the way embeddings are constructed, for example in word2vec two words with similar embeddings are two words that often appear in the same context, which is not to

say they have the same meaning. Sometimes words with opposite meanings can have similar embeddings just because they are placed within the same sentences/contexts.

The BLEU score is a way to quantify the translation quality of an automatic translation. The score aims to look at how different model translation is to human translation.

Share

Improve this answer

edited Jun 3, 2021 at 23:16

answered Mar 10, 2021 at 22:45

Ask Question

natural-language-processing comparison word-embedding language-model

bleu

Follow





Highest score (default)

The fish ate the cat.

The cat ate the fish.

Follow



Follow


ARTIFICIAL INTELLIGENCE

Tour

Help

Chat

Contact

Feedback

COMPANY

Stack Overflow

Teams

Advertising

Collectives

Talent

About

Press

Legal

Privacy Policy

Terms of Service

Cookie Settings

Cookie Policy

STACK EXCHANGE NETWORK

Technology

Culture &amp; recreation

Life &amp; arts

Science

Professional

Business

API

Data

Blog

Facebook

Twitter

LinkedIn

Instagram

Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2023.4.21.43403

SMattia

21

3

You must log in to answer this question.

Not the answer you're looking for? Browse other questions tagged natural-language-processing comparison word-embedding language-model

bleu .

Related

1

How is the word embedding represented in the paper "Recurrent neural network based language model"?

33

What is the difference between latent and embedding spaces?

2

Do we have cross-language vector space for word embedding?

1

What are the main differences between a language model and a machine translation model?

1

Should I need to use BERT embeddings while tokenizing using BERT tokenizer?

3

What is input (and shape) to K/V/Q of self-attention of EACH Decoder block of Language-translation model Transformer's tokens during Inference?

Hot Network Questions



What were the most popular text editors for MS-DOS in the 1980s?



Can I general this code to draw a regular polyhedron?



Tikz: Numbering vertices of regular a-sided Polygon



Has the cause of a rocket failure ever been mis-identified, such that another launch failed due to the same problem?



Which was the first Sci-Fi story to predict obnoxious "robo calls"?

more hot questions

 Question feed



Featured on Meta



New blog post from our CEO Prashanth: Community is the future of AI



Improving the copy in the close modal and post notices - 2023 edition

Your privacy

By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.

 

Accept all cookies

Necessary cookies only

Customize settings

