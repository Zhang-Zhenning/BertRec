
The Kullback-Leibler divergence

·

5 min read

·

Feb 3, 2022

Listen

Share

Photo by Clay Banks

Definition

Kullback-Leibler divergence

relative entropy



Photo by Clay Banks


not symmetrical

Some motivations behind the definition

Fig. 1. Two continuous distribution densities and their respective logarithmic transformations

Connection with cross entropy

Quick example


Fig. 2. Two discrete distributions

Fig. 3. Overlapping distributions

Fig. 4. Logarithm of distributions with difference D

Evaluate KL divergence with Python

entropy


Originally posted on m0nads.

Support this blog.

Useful links

Kl Divergence

Probability Distributions

Python

Scipy




Follow



104 Followers

This blog is not monetized. Images may appear low quality (not my fault). Visit https://m0nads.wordpress.com



PyTorch backward function

Machine Learning




·



Multi-Head Attention

·






Hinton’s Forward-Forward algorithm

·






Diffusion models

·

See all from m0nads






Why I Keep Failing Candidates During Google Interviews…

·

·






U-Nets with attention

·

·






Diffusion Model Clearly Explained!

·

·






What are Precision and Recall terminologies?

·

·






Understanding and Coding the Attention Mechanism — The Magic Behind

Transformers

·

·






The Intuitions for the Discrete Distributions: Bernoulli, Binomial, Beta, Dirichlet

Distributions

·

·

See more recommendations



