
GANs and Divergence Minimization

Colin Raffel

December 21st, 2018

colinraffel.com/blog

This post discusses a perspective on GANs which is not new but I think is often overlooked. I'll

use this perspective to motivate an evaluation procedure for GANs which I think is underutilized

and understudied. For setup, I'll first give a quick review of maximum likelihood estimation and

the forward KL divergence; if you're familiar with these concepts you can skip to section 3.

1 The Forward KL divergence and Maximum Likelihood

In generative modeling, our goal is to produce a model 

 of some “true” underlying probability

distribution 

. For the moment, let's consider modeling the 2D Gaussian distribution shown

below. This is a toy example; in practice we want to model extremely complex distributions in

high dimensions, such as the distribution of natural images.



We don't actually have access to the true distribution; instead, we have access to samples drawn

as 

. Shown below are some samples from this Gaussian distribution. We want to be able to

choose the parameters of our model 

 using these samples alone.



(x)

qθ

p(x)

x ∼ p

(x)

qθ


Let's fit a Gaussian distribution to these samples. This will produce our model 

, which ideally

will match the true distribution 

. To do so, we need to adjust the parameters  (in this case, the

mean and covariance) of our Gaussian model so that they minimize some measure of the

difference between 

 and the samples from 

. In practice, we'll use gradient descent over  for

this minimization. Let's start by using the KL divergence as a measure of the difference. Our goal

will be to minimize the KL divergence (using gradient descent) between 

 and 

 to find the

best set of parameters :

where the separation of terms in eqn.  comes from the linearity of expectation. The first term, 

, is just the negative entropy of the true distribution 

. Changing  won't change this

quantity, so we can ignore it for the purposes of finding . This is nice because we also can't

compute it in practice — it requires evaluating 

, and we do not have access to the true

distribution. This gives us

Eqn.  states that we want to find the value of  which assigns samples from 

 the highest

possible log probability under 

. This is exactly the equation for maximum likelihood

estimation, which we have shown is equivalent to minimizing 

. Let's see what happens

when we optimize the parameters  of our Gaussian 

 to fit the samples from 

 via maximum

likelihood:



Looks like a good fit!

2 Model Misspecification

The above example was somewhat unrealistic in the sense that both our true distribution 

 and

our model 

 were Gaussian distributions. To make things a bit harder, let's consider the case

where our true distribution is a mixture of Gaussians:

(x)

qθ

p(x)

θ

(x)

qθ

p(x)

θ

p(x)

(x)

qθ

θ∗

θ∗ =

=

=

arg

KL(p(x)|| (x))

min

θ

qθ

arg

[logp(x) − log

(x)]

min

θ Ex∼p

qθ

arg

[logp(x)] −

[log

(x)]

min

θ Ex∼p

Ex∼p

qθ

(1)

(2)

(3)

3

[logp(x)]

Ex∼p

p(x)

θ

θ∗

p(x)

θ∗ =

=

=

arg

−

[log

(x)]

min

θ

[logp(x)]

Ex∼p

Ex∼p

qθ

arg

−

[log

(x)]

min

θ

Ex∼p

qθ

arg

[log

(x)]

max

θ Ex∼p

qθ

(4)

(5)

(6)

6

θ

p(x)

(x)

qθ

KL(p(x)|| (x))

qθ

θ

(x)

qθ

p(x)

p(x)

(x)

qθ




Here's what happens when we fit a 2D Gaussian distribution to samples from this mixture of

Gaussians using maximum likelihood:



We can see that 

 “spreads out” to try to cover the entirety of 

. Why does this happen? Let's

look at the maximum likelihood equation again:

What happens if we draw a sample from 

 and it has low probability under 

? As 

approaches zero for some 

, 

 goes to negative infinity. Since we are trying to maximize 

,

this means it's really really bad if we draw a sample from 

 and 

 assigns a low probability to

it. In contrast, if some  has low probability under 

 but high probability under 

, this will not

affect maximum likelihood loss much. The result is that the estimated model tries to cover the

entire support of the true distribution, and in doing so ends up assigning probability mass to

regions of space (between the two mixture components) which have low probability under 

. In

looser terms, this means that samples from 

 might be “unrealistic”.

3 The Reverse KL Divergence

To get around this issue, let's try something simple: Instead of minimizing the KL divergence

between 

 and 

, let's try minimizing the KL divergence between 

 and 

. This is called the

“reverse” KL divergence:

(x)

qθ

p(x)

= arg

[log

(x)]

θ∗

max

θ Ex∼p

qθ

(7)

p(x)

(x)

qθ

(x)

qθ

x ∼ p log

(x)

qθ

(x)

qθ

p(x)

(x)

qθ

x

p(x)

(x)

qθ

p(x)

(x)

qθ

p(x)

(x)

qθ

(x)

qθ

p(x)

θ∗

arg

KL( ||p)

min

θ

(8)


The two terms in equation  each have an intuitive description: The first term 

 is

simply the entropy of 

. So, we want our model to have high entropy, or to put it intuitively, its

probability mass should be as spread out as possible. The second term 

 is the log

probability of samples from 

 under the true distribution 

. In other words, any sample from 

 has to be reasonably “realistic” according to our true distribution. Note that without the first

term, our model could “cheat” by simply assigning all of its probability mass to a single sample

which has high probability under 

. This solution is essentially memorization of a single point,

and the entropy term discourages this behavior. Let's see what happens when we fit a 2D

Gaussian to the mixture of Gaussians using the reverse KL divergence:



Our model basically picks a single mode and models it well. This solution is reasonably high-

entropy, and any sample from the estimated distribution has a reasonably high probability

under 

, because the support of  is basically a subset of the support of 

. The drawback here

is that we are basically missing an entire mixture component of the true distribution.

When might this be a desirable solution? As an example, let's consider image superresolution,

where we want to recover a high-resolution image (right) from a low-resolution version (left):



This figure was made by my colleague David Berthelot. In this task, there are multiple possible

“good” solutions. In this case, it may be much more important that our model produces a single

high-quality output than that it correctly models the distribution over all possible outputs. Of

course, reverse KL provides no control over which output is chosen, just that the distribution

learned by the model has high probability under the true distribution. In contrast, maximum

likelihood can result in a “worse” solution in practice because it might produce low-quality or

incorrect outputs by virtue of trying to model every possible outcome despite model

misspecification or insufficient capacity. Note that one way to deal with this is to train a model

with more capacity; a recent example of this approach is Glow [kingma2018], a maximum

likelihood-based model which achieves impressive results with over 100 million parameters.

θ∗ =

=

=

=

arg

KL( ||p)

min

θ

qθ

arg

[log

(x) − logp(x)]

min

θ Ex∼qθ

qθ

arg

[log

(x)] −

[logp(x)]

min

θ Ex∼qθ

qθ

Ex∼qθ

arg

−

[log

(x)] +

[logp(x)]

max

θ

Ex∼qθ

qθ

Ex∼qθ

(8)

(9)

(10)

(11)

11

−

[log

(x)]

Ex∼qθ

qθ

(x)

qθ

[logp(x)]

Ex∼qθ

(x)

qθ

p(x)

(x)

qθ

p(x)

p(x)

qθ

p(x)


4 Generative Adversarial Networks

In using the reverse KL divergence above, I've glossed over an important detail: We can't

actually compute the second term 

 because it requires evaluating the true probability 

 of a sample 

. In practice, we don't have access to the true distribution, we only have access

to samples from it. So, we can't actually use reverse KL divergence to optimize the parameters of

our model. In Section 3, I “cheated” since I knew what the true model was in our toy problem.

So far, we have been fitting the parameters of 

 by minimizing a divergence between 

 and 

— the forward KL divergence in Section 1 and the reverse KL divergence in Section 3.

Generative Adversarial Networks (GANs) [goodfellow2014] fit the parameters of 

 via the

following objective:

The first bit of this equation is unchanged: We are still choosing  via a minimization over . What

has changed is the quantity we're minimizing. Instead of minimizing over some analytically

defined divergence, we're minimizing the quantity 

 which can be loosely

considered a “learned divergence”. Let's unpack this a bit: 

 is a neural network typically called

the “discriminator” or “critic” and is parametrized by . It takes in samples from 

 or 

 and

outputs a scalar value. 

 is a loss function which 

 is trained to maximize. The original GAN

paper used the following loss function:

where 

 is required to output a value between 0 and 1.

Interestingly, if 

 can represent any function, choosing  via Equation  using the loss function

in Equation  is equivalent to minimizing the Jensen-Shannon divergence between 

 and 

.

More generally, it is possible to construct loss functions 

 and critic architectures which result

(in some limit) in minimization of some analytical divergence. This can allow for minimization of

divergences which are otherwise intractable or impossible to minimize directly. For example,

[nowozin2016] showed that the following loss function corresponds to minimization of the

reverse KL divergence:

Let's go ahead and do this in the example above of fitting a 2D Gaussian to a mixture of

Gaussians:



Sure enough, the solution found by minimizing the GAN objective with the loss function in

Equation  looks roughly the same as the one found by minimizing the reverse KL divergence,

but did not require “cheating” by evaluating 

.

[logp(x)]

Ex∼qθ

p(x)

x ∼ qθ

(x)

qθ

(x)

qθ

p(x)

(x)

qθ

= arg

V(

(x),

( ))

θ∗

min

θ max

ϕ Ex∼p, ∼

x^ qθ

fϕ

fϕ x^

(12)

θ∗

θ

V(

(x),

( ))

maxϕEx∼p, ∼

x^ qθ

fϕ

fϕ x^

(x)

fϕ

ϕ

p(x)

(x)

qθ

V(⋅,⋅)

(x)

fϕ

V(

(x),

( )) = log

(x) + log[1 −

( )]

fϕ

fϕ x^

fϕ

fϕ x^

(13)

(x)

fϕ

(x)

fϕ

θ∗

12

13

p(x)

(x)

qθ

V(⋅,⋅)

V(

(x),

( )) = −exp(

(x)) + 1 +

( )

fϕ

fϕ x^

fϕ

fϕ x^

(14)

14

p(x)


To re-emphasize the importance of this, the GAN framework opens up the possibility of

minimizing divergences which we can't compute or minimize otherwise. This allows learning

generative models using objectives other than maximum likelihood, which has been the

dominant paradigm for roughly a century. Maximum likelihood's ubiquity is not without good

reason — it is tractable (unlike, say, the reverse KL divergence) and has nice theoretical

properties, like its efficiency and consistency. Nevertheless, the GAN framework opens the

possibility of using alternative objectives which, for example and loosely speaking, prioritize

“realism” over covering the entire support of 

.

As a final note on this perspective, the statements above about how GANs minimize some

underlying analytical divergence can lead people thinking of them as “just minimizing the

Jensen-Shannon (or whichever other) divergence”. However, the proofs of these statements rely

on assumptions that don't hold up in practice. For example, we don't expect 

 to have the

ability to represent any function for any reasonable neural network architecture. Further, we

perform the maximization over  via gradient ascent, which for neural networks is not

guaranteed to converge to any kind of optimal solution. As a result, stating that GANs are simply

minimizing some analytical divergence is misleading. To me, this is actually another thing that

makes GANs interesting, because it allows us to imbue prior knowledge about our problem in

our “learned divergence”. For example, if we use a convolutional neural network for 

, this

suggests some amount of translation invariance in the objective minimized by 

, which might

be a useful structural prior for modeling the distribution of natural images.

5 Evaluation

One appealing characteristic of maximum likelihood estimation is that it facilitates a natural

measure of “generalization”: Assuming that we hold out a set of samples from 

 which were not

used to train 

 (call this set 

), we can compute the likelihood assigned by our model to these

samples:

If our model assigns a similar likelihood to these samples as it did to those it was trained on, this

suggests that it has not “overfit”. Note that Equation  simply computes the divergence used to

train the model (ignoring the data entropy term, which is independent of the model) over 

.

Typically, the GAN framework is not thought to allow this kind of evaluation. As a result, various

ad-hoc and task-specific evaluation functions have been proposed (such as the Inception Score

and the Frechet Inception Distance for modeling natural images). However, following the

reasoning above actually provides a natural analog to the evaluation procedure used for

maximum likelihood: After training our model, we train an “independent critic” (used only for

evaluation) from scratch on our held-out set of samples from 

 and samples from 

 with  held

fixed:

Both Equation  and Equation  compute the divergence used for training our model over the

samples in 

. Of course, Equation  requires training a neural network from scratch, but it

nevertheless loosely represents the divergence we used to find the parameters .

While not widely used, this evaluation procedure has seen some study, for example in

[danihelka2017] and [im2018]. In recent work [gulrajani2018], we argue that this evaluation

procedure facilitates some notion of generalization and include some experiments to gain better

insight into its behavior. I plan to discuss this work in a future blog post.

p(x)

(x)

fϕ

ϕ

(x)

fϕ

(x)

qθ

p(x)

(x)

qθ

xtest

[log

(x)] ≈

log

(x)

Ex∼p

qθ

1

|

|

xtest ∑

x∈xtest

qθ

(15)

15

xtest

p(x)

(x)

qθ

θ

V(

(x),

( )) ≈

V(

(x),

( ))

max

ϕ Ex∼p, ∼

x^ qθ

fϕ

fϕ x^

max

ϕ

1

|

|

xtest ∑

x∈xtest

E ∼

x^ qθ

fϕ

fϕ x^

(16)

15

16

xtest

16

θ


6 Pointers

The perspective given in this blog post is not new. [theis2015] and [huszar2015] both discuss

the different behavior of maximum likelihood, reverse KL, and GAN-based training in terms of

support coverage. Huszár also has a few follow-up blog posts on the subject [huszar2016a],

[huszar2016b]. [poole2016] further develops the use of the GAN framework for minimizing

arbitrary f-divergences. [fedus2017] demonstrates how GANs are not always minimizing some

analytical divergence in practice. [huang2017] provides some perspective on the idea that the

design of the critic architecture allows us to imbue task-specific priors in our objective. Finally,

[arora2017] and [liu2017] provide some theory about the “adversarial divergences” learned and

optimized in the GAN framework.

Acknowledgements

Thanks to Ben Poole, Durk Kingma, Avital Oliver, and Anselm Levskaya for their feedback on

this blog post.

References

[goodfellow2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-

Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial

Networks. arXiv:1406.2661, 2014.

[nowozin2016] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training

Generative Neural Samplers using Variational Divergence Minimization.

arXiv:1606.00709, 2016.

[danihelka2017] Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra, and

Peter Dayan. Comparison of Maximum Likelihood and GAN-based training of Real

NVPs. arXiv:1705.05263, 2017.

[im2018] Daniel Jiwoong Im, He Ma, Graham Taylor, and Kristin Branson. Quantitatively

Evaluating GANs With Divergences Proposed for Training. arXiv:1803.01045, 2018.

[gulrajani2018] Ishaan Gulrajani, Colin Raffel, and Luke Metz. Towards GAN Benchmarks

Which Require Generalization. To appear at ICLR 2019.

[theis2015] Lucas Theis, Aäron van den Oord, and Matthias Bethge. A note on the evaluation

of generative models. arXiv:1511.01844, 2015.

[huszar2015] Ferenc Huszár. How (not) to Train your Generative Model: Scheduled Sampling,

Likelihood, Adversary? arXiv:1511.05101, 2015.

[huszar2016a] Ferenc Huszár. An Alternative Update Rule for Generative Adversarial

Networks. https://www.inference.vc/an-alternative-update-rule-for-generative-

adversarial-networks/, 2015.

[huszar2016b] Ferenc Huszár. Understanding Minibatch Discrimination in GANs.

https://www.inference.vc/understanding-minibatch-discrimination-in-gans/, 2015.

[poole2016] Ben Poole, Alexander A. Alemi, Jascha Sohl-Dickstein, and Anelia Angelova.

Improved generator objectives for GANs. arXiv:1612.02780, 2016.

[fedus2017] William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir

Mohamed, and Ian Goodfellow. Many Paths to Equilibrium: GANs Do Not Need to

Decrease a Divergence At Every Step. arXiv:1710.08446, 2017.

[huang2017] Gabriel Huang, Hugo Berard, Ahmed Touati, Gauthier Gidel, Pascal Vincent, and

Simon Lacoste-Julien. Parametric Adversarial Divergences are Good Task Losses for

Generative Modeling. arXiv:1708.02511, 2017.

[arora2017] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization

and Equilibrium in Generative Adversarial Nets (GANs). arXiv:1703.00573, 2017.

[liu2017] Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and


Convergence Properties of Generative Adversarial Learning. arXiv:1705.08991,

2017.

formatted by Markdeep 1.03  �

