


Published in

Towards Data Science



Dec 21, 2021

·

5 min read

·

Save

Exploring BERT variants (Part 1): ALBERT,

RoBERTa, ELECTRA

A gentle deep-dive into the BERT family…

Photo by nadi borodina on Unsplash












Attention, Transformer and BERT: A Simulating NLP Journey

BERT Base Model 

MLM and Next Sentence Prediction (NSP)

MLM

NSP

Image by Author

Given the length of the article, I chose to divide this article into two parts. . In this part, we

will dig into the first three variants i.e. ALBERT, RoBERTa and ELECTRA. The second

part will cover the remaining three. Here is the link to the second part:

Exploring BERT variants (Part 2): SpanBERT, DistilBERT, TinyBERT






ALBERT

Cross-layer parameter sharing

Factorized embedding layer parameterization

Recommendation System — Matrix Factorization

RoBERTa

Dynamic Masking

Remove NSP Task

More data Points


Large Batch size

ELECTRA

Replaced Token Detection: 

No NSP pre-training

Better Training: 

Bert

Roberta

Electra

Distilbert

Albert


1



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





