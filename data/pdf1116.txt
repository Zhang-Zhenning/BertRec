










Parameter Estimation

We have learned many different distributions for random variables and all of those distributions had

parameters: the numbers that you provide as input when you define a random variable. So far when we

were working with random variables, we either were explicitly told the values of the parameters, or, we

could divine the values by understanding the process that was generating the random variables.

What if we don't know the values of the parameters and we can't estimate them from our own expert

knowledge? What if instead of knowing the random variables, we have a lot of examples of data

generated with the same underlying distribution? In this chapter we are going to learn formal ways of

estimating parameters from data.

These ideas are critical for artificial intelligence. Almost all modern machine learning algorithms work like

this: (1) specify a probabilistic model that has parameters. (2) Learn the value of those parameters from

data.

Parameters

Before we dive into parameter estimation, first let's revisit the concept of parameters. Given a model, the

parameters are the numbers that yield the actual distribution. In the case of a Bernoulli random variable,

the single parameter was the value $p$. In the case of a Uniform random variable, the parameters are the

$a$ and $b$ values that define the min and max value. Here is a list of random variables and the

corresponding parameters. From now on, we are going to use the notation $\theta$ to be a vector of all

the parameters:

Distribution

Parameters

Bernoulli($p$)

$\theta = p$

Poisson($\lambda$)

$\theta = \lambda$

Uniform($a, b$)

$\theta = [a, b]$

Normal($\mu, \sigma^2$)

$\theta = [\mu, \sigma^2]$

In the real world often you don't know the "true" parameters, but you get to observe data. Next up, we

will explore how we can use data to estimate the model parameters.

It turns out there isn't just one way to estimate the value of parameters. There are two main schools of

thought: Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP). Both of these

schools of thought assume that your data are independent and identically distributed (IID) samples: $X_1,

X_2, \dots X_n$.

