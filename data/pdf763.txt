


Published in

Towards Data Science



Nov 24, 2019

·

7 min read

Save

NLP 101: Word2Vec — Skip-gram and CBOW

A crash course in word embedding.

Photo by Sincerely Media on Unsplash

What does word embedding mean?

Why do we need word embedding?








semantics

What are good quality word embedding and how to generate them?

## Current situation

## Ideal situation

Continuous Bag of Words Model (CBOW) and Skip-gram

Source: Exploiting Similarities among Languages for Machine Translation paper.

CBOW

predict the word in the middle

Skip-gram


predict the context

Skip-gram Model

The word highlighted in yellow is the source word and the words highlighted in green are its neighboring words.

juice

Architecture for skip-gram model. Source: McCormickml tutorial

CBOW


Skip-gram

CBOW

Negative Sampling

GloVe 

References

Other Articles by Me That I think You would Enjoy :D

Machine Learning

NLP

Artificial Intelligence


6



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





Word2vec

AI

