
Published in

Analytics Vidhya



Aug 25, 2019

·

5 min read

Expectation-Maximization Algorithm Step-by-Step

Gaussian Mixture Model, Bayesian Inference, Hard vs. Soft Clustering

Source: sepdek

Problem Setup

n training data points 

Gaussian Mixture Model (GMM)

hard

clustering

 soft clustering

K components 






μ 

 �²

Note: I did not put Identity Matrix but that should not affect how we understand this

mixture weight P

The “occurrence” of component j follows a multinomial distribution with K probability parameters

Maximum Likelihood Estimation

We often take log of this likelihood, thus log-likelihood, so that product becomes sum of log terms.

Initialization

E-Step (Expectation)

the posterior probability

Prior

likelihood 

M-Step (Maximization)


 

This is the likelihood function that we specified earlier

 

Iteration

Additional Notes

Machine Learning

Algorithms

Gaussian Mixture Model


1

Analytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem

https://www.analyticsvidhya.com



Read more from Analytics Vidhya





Bayesian Statistics

Bayesian Inference

