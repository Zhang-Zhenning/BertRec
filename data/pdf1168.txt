


Published in

Towards Data Science



Nov 14, 2020

·

6 min read

Save

Entropy, Cross Entropy, and KL Divergence

Let’s understand entropy in layman terms

Photo by Paul Wong on Unsplash

Entropy

If you are a random reader…

entropy








So, how to calculate entropy?

What if you always read my whole article?

Now, what about my followers?

≈

≈

≈

What if there are more than two possible events?

∑

Entropy vs the number of events

Cross Entropy


KL Divergence

Cat vs dog classification

When is KL Divergence 0?


3



Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





Data Science

Machine Learning

Artificial Intelligence

Math

Statistics

