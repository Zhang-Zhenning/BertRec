


Skip-Gram: NLP context words prediction

algorithm

·

Published in

Towards Data Science

5 min read

·

Mar 16, 2019

Listen

Share

word embeddings








skip-gram example

sat

cat, mat

-1 and

3

sat

0

the

Architecture

The Skip-gram model architecture (Source: https://arxiv.org/pdf/1301.3781.pdf Mikolov el al.)

Variables we’ll be using

N


N = context window

Working steps

one hot encoding

Probability function

softmax probability

j

c

c

j

c

Loss function

Loss function

c

L

Advantages


Disadvantages

N

c

6



Follow



577 Followers

·

Writer for 

Towards Data Science

Currently working as a Backend Developer. Exploring how to make machines smarter than me.

Machine Learning

NLP

Word2vec

Skip Gram

Unsupervised Learning








Various Optimization Algorithms For Training Neural Network

·






Zero-ETL, ChatGPT, And The Future of Data Engineering

·






The Portfolio that Got Me a Data Scientist Job

·

·






To read emails and download attachments in Python

·

See all from Sanket Doshi

·

See all from Towards Data Science






Generating Word Embeddings from Text Data using Skip-Gram Algorithm and Deep

Learning in Python

·

·






The Portfolio that Got Me a Data Scientist Job

·

·






You’re Using ChatGPT Wrong! Here’s How to Be Ahead of 99% of ChatGPT Users

·

·






Can ChatGPT Write Better SQL than a Data Analyst?

·

·







Wanna Break into Data Science in 2023? Think Twice!

·

·






How to Train a Word2Vec Model from Scratch with Gensim

·

·




See more recommendations

