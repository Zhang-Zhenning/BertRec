


Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models











Mixture Models

Econ 690

Purdue University

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models



In virtually all of the previous lectures, our models have made

use of normality assumptions.



From a computational point of view, the reason for this

assumption is clear: combined with normal priors for

regression parameters, this yields convenient posterior (or

conditional) posteriors for regression parameters, whence

standard simulation methods can be applied.



However, such assumptions may not be supported at all by

the data, and diagnostic checks could reveal evidence against

normality.



So, what should we do in these cases?



Are there any more ﬂexible alternatives which retain

computational tractability?

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models



To this end, we ﬁrst describe scale mixtures of normals

models.



The most popular of these involve generalizing our models to

allow for Student-t errors, so that our model can

accommodate fat tails in the data.



Other distributions can also be obtained as a scale mixture of

normals, including (among others): double exponential errors

and logistic errors.



Such models, though more ﬂexible than the textbook

Gaussian model, are symmetric and can not accommodate

features such as skew and multimodality.



We then discuss ﬁnite Gaussian mixtures as an alternative for

these cases, and also talk (a little) about models with

skew-Normal errors.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models



Scale Mixtures of Normals Models



We ﬁrst review how the Student-t density can be regarded as a

scale mixture of the Gaussian density.

Suppose you specify:

y|β, λ, σ2∼N(xβ, λσ2)

and choose the following prior for λ (treating ν as given):

λ|ν ∼ IG

�ν

2, 2

ν

�

⇒ p(λ) =

�

Γ

�ν

2

� �2

ν

�ν/2�−1

λ−[(ν/2)+1] exp

�

− ν

2λ

�

so that the prior for λ is independent of β and σ2.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

For this model, we seek to:

(a) Derive the density

p(y|β, σ2) =

� ∞

0

p(y|β, λ, σ2)p(λ) dλ.

(b) Given the result in (a), comment on how the addition of λ to

the error variance can be a useful computational device for an

applied Bayesian researcher.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

It follows that

p(y|β, σ2)

=

� ∞

0

��√

2πσ

�−1

λ−(1/2) exp

�

− 1

2λ

�y − xβ

σ

�2��

×

��

Γ

�ν

2

� � 2

ν

�ν/2�−1

λ−[(ν/2)+1] exp

�

− ν

2λ

��

dλ

=

�√

2πσ

�−1

�

Γ

�ν

2

� � 2

ν

�ν/2�−1

×

� ∞

0

λ−[(ν+3)/2] exp

�

− 1

λ

�

1

2

�y − xβ

σ

�2

+ ν

2

��

dλ.

The integral above is the kernel of an

IG



ν + 1

2

,

�

1

2

��y − xβ

σ

�2

+ ν

��−1



density.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

Thus,

p(y|β, σ2)

=

�√

2πσ

�−1

�

Γ

�ν

2

� �2

ν

�ν/2�−1

×

Γ

�ν + 1

2

� �

1

2

��y − xβ

σ

�2

+ ν

��−[(ν+1)/2]

.

Rearranging and canceling terms, we obtain

p(y|β, σ2) =

Γ

� ν+1

2

�

√νπσΓ

� ν

2

�

�

1 + 1

ν

�y − xβ

σ

�2�−[(ν+1)/2]

,

which is in the form of a Student-t density, i.e.,

y|β, σ2 ∼ t(xβ, σ, ν).

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models



In a sense, we can think about this procedure as something

like data augmentation. The parameter λ is not necessarily an

object of interest (though it could be), but is, instead, a

useful device for allowing for Student-t errors.



Speciﬁcally, conditioned on λ, all of our usual Gibbs sampling

results will apply.



Similarly, given all of the other parameters of the model,

sampling from λ’s posterior conditional is also

straight-forward.



In other words, this result is useful to the applied Bayesian

researcher as it, in conjunction with the Gibbs sampler, allows

the estimation of models with Student-t errors, thus relaxing

the Normality assumption.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

To see this connection more formally, regard (a) as one

observation’s contribution to the likelihood function and note

(adding i subscripts to denote the individual observations)

p(β, σ2, {λi}|y) ∝

� n

�

i=1

φ(yi; xiβ, λiσ2)p(λi)

�

p(β, σ2),

which implies that



Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

Thus working with the seemingly more complicated joint posterior

which contains the inverted-Gamma mixing variables λi yields the

same inference for β and σ2 that would be obtained by directly

working with a regression model with Student-t errors.

We will show how this is done in the context of a linear regression

model below.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

Consider the regression model:



We seek to show how the Gibbs sampler can be used to ﬁt this

model.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

To implement the Gibbs sampler we need to obtain the complete

posterior conditionals for the parameters β, σ2 and {λi}.

The joint posterior distribution is given as

p(β, {λi}, σ2|y) ∝

� n

�

i=1

φ(yi; xiβ, λiσ2)p(λi)

�

p(β)p(σ2).

Given this joint posterior, we need to obtain the posterior

conditionals: p(β|λ, σ2, y), p(λ|β, σ2, y) and p(σ2|β, λ, y).

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

The following complete conditional posterior distributions are

obtained:

β|{λi}, σ2|y

∼

N

� �

X ′Λ−1X/σ2 + V −1

β

�−1 �

X ′Λ−1y + V −1

β µβ

�

,

�

X ′Λ−1X/σ2 + V −1

β

�−1 �

,

where Λ ≡ diag{λi} and thus Λ−1 = diag{λ−1

i

}, and X and y are

stacked appropriately.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

As for the posterior conditional for the variance parameter σ2,

σ2|β, {λi}, y ∼ IG

�

n

2 + a,

�

b−1 + 1

2(y − Xβ)′Λ−1(y − Xβ)

�−1�

.

Finally, we can apply our previous result to derive the posterior

conditional for each λi:



Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models



A Gibbs sampler involves cycling through these conditionals.



Note that diﬀerent choices of ν in the hierarchical prior for λi

yield models with diﬀerent tail properties.



Finally, note that this procedure can be extended to allow for

double exponential errors under an exponential prior for λ and

logistic errors provided the mixing variables λ have the

asymptotic distribution of the Kolmogorov distance statistic

[Andrews and Mallows (JRSS B, 1974)].

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models



Finite Gaussian Mixtures



Consider a two-component Normal mixture model



Note that, to generate values y from this model, one can ﬁrst draw

from a two-point distribution with probabilities P and 1 − P.

Given a draw from this two-point distribution, one can then draw

from the associated component of the mixture [either N(µ1, σ2

1) or

N(µ2, σ2

2)] to obtain a draw y.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

Using the above intuition, we will augment the mixture model with

a set of component indicator variables, say {τi}n

i=1, where τi is

either zero or one, and τi = 1 implies that the ith observation is

drawn from the ﬁrst component of the mixture. (When τi = 0, the

implication is that the ith observation is drawn from the second

component).

We will also assign a hierarchical prior to τi so that the probability

associated with the ﬁrst component is P, and then place a Beta

prior on P. Using this augmented structure, we will describe how a

Gibbs sampler can be employed to ﬁt the mixture model.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

Before describing the augmented representation, let θ denote all

the model’s parameters and θ−x denote all parameters other than

x.

The model can be written as

p(y|θ, {τi})

=

n

�

i=1

�

φ(yi; µ1, σ2

1)

�τi �

φ(yi µ2, σ2

2)

�1−τi

τi

iid∼

B(1, P),

i = 1, 2, · · · n

P

∼

β(p1, p2),

µi

ind

∼

N(µi, vi),

i = 1, 2

σ2

i

ind

∼

IG(ai, bi),

i = 1, 2.

In the above B(1, P) denotes a Binomial density on one trial with

“success” probability P, or equivalently, a Bernoulli density with

success probability P. Similarly, β(a, b) denotes the Beta density

with parameters a and b.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

Note that when marginalizing the conditional likelihood

p(y|θ, {τi}) over τi, we are left with the two-component mixture

model described at the outset of this section. To see this, note

that the assumed conditional independence across observations,

together with the fact that τi is binary, implies

p(y|θ)

=

n

�

i=1

1

�

j=0

p(yi|θ, τi = j)Pr(τi = j|θ)

=

n

�

i=1

�

Pφ(yi; µ1, σ2

1) + (1 − P)φ(yi; µ2, σ2

2)

�

.

Thus, the component indicators serve the practical purpose of

facilitating computation, but their presence does not aﬀect the

joint posterior distribution of our parameters of interest.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

The following complete posterior conditionals are obtained:

µ1|θ−µ1, y ∼ N(Dµ1dµ1, Dµ1)

where

Dµ1 =

�

n1/σ2

1 + v−1

1

�−1 , dµ1 =

�

i

τiyi/σ2

1 + v−1

1 µ1,

n1 ≡ �

i τi denotes the number of observations “in” the ﬁrst

component of the mixture, and n2 will be deﬁned as

n2 ≡ �

i(1 − τi) = n − n1. The complete conditional for µ2 follows

similarly.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

As for the conditional posterior distribution for the variance

parameters,

σ2

2|θ−σ2

2, {τi}, y ∼ IG



n2/2 + a2,

�

b−1

2

+ .5

n

�

i=1

(1 − τi)(yi − µ2)2

�−1

 ,

and the complete conditional for σ2

1 follows similarly.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

Finally, for the component indicator variables, and component

probability P,



and



With these conditionals in hand, a Gibbs sampler can be

implemented, noting, of course, that similar conditionals need to

be obtained for µ2 and σ2

1.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

To illustrate the ﬂexibility of the 2-component mixture model, we

perform some generated data experiments. First, we generate:



2,000 observations from a lognormal distribution with

parameters µ = ln 10 and σ2 = .04.



5,000 observations from a Chi-square distribution with 10

degrees of freedom.



(d) 3,000 observations from a two-component mixture model

with P = .4, µ1 = 0, µ2 = 2, σ2

1 = 1 and σ2

2 = .5.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

−2

−1

0

1

2

3

4

5

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

Mixture

0

5

10

15

20

0

0.05

0.1

0.15

0.2

0.25

Lognormal

0

10

20

30

0

0.02

0.04

0.06

0.08

0.1

0.12

Chi−square

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models



A Regression Model with More than 2 Components



Consider the general set-up for a regression model using G Normal

mixture components:



In this model we allow each mixture component to possess its own

variance parameter, σg, and set of regression parameters, βg.

This level of generality is not required - if desired, we could restrict

some of these parameters to be constant across the mixture

components.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

For the purposes of computation, consider augmenting this model

with a set of component label vectors, {zi}n

i=1 where

zi = [z1i z2i · · · zGi],

and zgi = 1 implies that the ith individual is “drawn from” the gth

component of the mixture.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

To complete the augmentation step, we add a Multinomial prior

(multivariate generalization of a Binomial) for the component label

vector zi that depends on a vector of component probabilities π,

and then specify a Dirichlet prior (multivariate generalization of

the beta) for π.

The following priors are also employed:

βg

ind

∼

N(β0g, Vβg ),

g = 1, 2, · · · , G

σ2

g

ind

∼

IG(ag, bg),

g = 1, 2, · · · , G

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

If we condition on the values of the component indicator variables,

the conditional likelihood function can be expressed as

L(θ) =

n

�

i=1

�

φ(yi; xiβ1, σ2

1)

�z1i �

φ(yi; xiβ2, σ2

2)

�z2i · · ·

�

φ(yi; xiβG, σ2

G)

�zGi .

As stated, we add the following priors for the component indicators

and component probabilities:



Note that, taking the conditional likelihood and integrating out the

component indicators gives an unconditional likelihood equivalent

to our original model.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

The augmented posterior density p({βg, σ2

g, πg}G

g=1, {zi}n

i=1|y) is

proportional to the product of the augmented likelihood, the

Multinomial and Beta priors, and the given priors for the regression

and variance parameters.

It follows that the following complete posterior conditionals can be

obtained:

βg|θ−βg , y ind

∼ N(Dβg dβg , Dβg ),

g = 1, 2, · · · G

where

Dβg =

�

(

�

i

zgix′

i xi)/σ2

g + V −1

βg

�−1

, dβg = (

�

i

zgix′

i yi)/σ2

g+V −1

βg β0g.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

As for the variance parameters within each component,

σ2

g|θ−σ2g , y

ind

∼ IG

�

ng/2 + ag,

�

b−1

g

+ (1/2)

�

i

zgi(yi − xiβg)2

�−1�

g = 1, 2, · · · G,

where ng ≡ �N

i=1 zgi denotes the number of observations in the

gth component of the mixture.

Finally,

zi|θ−zi , y

ind

∼

M

�

1,

�

π1φ(yi; xiβ1, σ2

1)

�G

g=1 πgφ(yi; xiβg, σ2g)

π2φ(yi; xiβ2, σ2

2)

�G

g=1 πgφ(yi; xiβg, σ2g)

· · ·

πGφ(yi; xiβG, σ2

G)

�G

g=1 πgφ(yi; xiβg, σ2g)

�′�

,

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

and



Fitting this model requires algorithms for drawing from the

multinomial (a multivariate generalization of the binomial) and

Dirichlet (a multivariate generalization of the beta) densities.

This is reasonably straight-forward: a Dirichlet draw can be

obtained from a series of Beta draws, and likewise, a multinomial

draw can be obtained from a series of binomial draws (I have code

for doing this if you require it).

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models



Skew-Normal Models



Suppose that your error terms seem to be characterized by skew,

but not multimodality, and you seek a more parsimonious

alternative than the ﬁnite mixture approach. To this end, you

consider a model of the form:



Thus, z has a half-Normal distribution.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

We seek to answer the following questions related to this model:

(a) For y, a scalar generated from the above speciﬁcation, derive

the mixture density p(y|x, β, δ, σ2). Comment on the role of δ in

this conditional distribution.

(b) Let β∗ = [β′ δ]′. Employing priors of the form σ2 ∼ IG(a, b)

and β∗ ∼ N(0, Vβ), derive a posterior simulator for ﬁtting this

regression model.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

(a) For ease of exposition, let us drop the conditioning on β, σ2

and δ in our notation and leave this implicit. We note



The density above is know n as a skew-Normal distribution and is

sometimes written as y ∼ SN(xβ, σ2 + δ2, δ/σ).

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

The parameter δ acts as a skewness parameter, and speciﬁcally,







When δ = 0, the density is symmetric and we obtain

y ∼ N(xβ, σ2).

On the following page, we provide plots of the skew-Normal

density across diﬀerent values of δ when σ2 = 1 and xβ = 0.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

−10

0

10

20

 

0.1 

0.25

δ = −3

−10

0

10

20

0.15

0.35

δ = −1

−10

0

10

20

0.2 

0.4 

δ = 0

−10

0

10

20

0.08

0.16

δ = 5

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

(b) For our posterior simulator, we make use of data augmentation

and include z = [z1 z2 · · · zn]′ in the posterior distribution.

Before presenting these posterior conditionals, we ﬁrst observe:

p(z, β∗, σ2|y)

∝

p(β∗)p(σ2)p(y, z|β∗, σ2)

∝

p(β∗)p(σ2)

n

�

i=1

φ(yi; xiβ + ziδ, σ2) exp

�

−1

2z2

i

�

I(zi &gt; 0).

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

It follows that

zi|θ−zi, y ∝ exp

�

− 1

2σ2 (yi − xiβ − ziδ)2

�

exp

�

−1

2z2

i

�

I(zi &gt; 0).

Completing the square on zi, and noting that zi is truncated at

zero, we obtain

zi|θ−zi, y ind

∼ TN(0,∞)

�δ(yi − xiβ)

σ2 + δ2

,

σ2

σ2 + δ2

�

, i = 1, 2, · · · , n.

.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models

Let

X =





x1

x2

...

xn





and W = [X z]. With this notation, the posterior conditional for

β∗ is of the form

β∗|θ−β∗, y ∼ N(Dβdβ, Dβ),

where

Dβ = (W ′W /σ2 + V −1

β )−1,

dβ = W ′y/σ2.

Finally,

σ2|θ−σ2, y ∼ IG

�n

2 + a,

�

b−1 + (1/2)(y − W β∗)′(y − W β∗)

�−1�

.

Justin L. Tobias

Mixture Models




Motivation

Scale Mixutres of Normals

Finite Gaussian Mixtures

Skew-Normal Models



Skew-Normal and Nonparametric Wage Estimates



0

10

20

30

40

50

0

0.01

0.02

0.03

0.04

0.05

0.06

0.07

Hourly Wage

Density

Skew−Normal 

Nonparametric 

Justin L. Tobias

Mixture Models

