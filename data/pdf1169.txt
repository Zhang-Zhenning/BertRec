


Published in

Towards Data Science



Jun 17, 2020

·

8 min read

·

Save

Entropy, Cross-Entropy, and KL-Divergence

Explained!

Let us try to understand the most widely used loss function — Cross-Entropy.

Cross-Entropy 

Entropy

Cross-Entropy

KL-Divergence

What is Entropy?








Entropy


probability distribution p

Cross-Entropy

Entropy = -(0.35 * log(0.35) + 0.35 * log(0.35) + 0.1 * log(0.1) + 0.1 * log(0.1) + 0.04 * log(0.04)

+ 0.04 * log(0.04) + 0.01 * log(0.01) + 0.01 * log(0.01))

Entropy = 2.23 bits

 binary log.

35%*2 + 35%*2 + 10%*3 + 10%* 3+ 4%*4 + 4%*4 + 1%*5 + 1%*5 = 2.42 bits


1%*2 + 1%*2 + 4%*3 + 4%* 3+ 10%*4 + 10%*4 + 35%*5 + 35%*5 = 4.58 bits

q

p

p 

q 

 base 2

Relative Entropy

Kullback-Leibler

Divergence (KL Divergence)

K-L Divergence = CrossEntropy-Entropy = 4.58–2.23 = 2.35 bits.

Application


Cross-Entropy loss

Cross-Entropy is -1*log(0.3) = — log(0.3) = 1.203

Conclusion

References

Artificial Intelligence

Machine Learning

Deep Learning

Data Science

Technology




Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.



Read more from Towards Data Science





