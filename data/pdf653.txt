


Aug 7, 2021

·

3 min read

Document Ranking using Bert

transformer-like models such as BERT, ROBERTA, GPT, etc. 

Probability Ranking Principle

In the candidate generation stage (also called initial retrieval or first-stage retrieval), candidate texts are retrieved

from the corpus, typically with bag-of-words queries against inverted indexes. These candidates are then reranked

with a transformer model such as monobert

MonoBERT

Document Ranking by Birch




Anserini (built on FAISS). 

. Document Ranking by BERT–MaxP and Variants

3. Contextual embedding for Document Ranking

The architecture of CEDR, which comprises two main sources of relevance signals: the [CLS] representation and

the similarity matrix computed from the contextual embeddings of the query and the candidate text. This illustration

contains a number of intentional simplifications in order to clearly convey the model’s high-level design.


1



Follow



Staff ML Scientist and PHD Researcher at IIT Delhi.





Information Retrieval

Bert

Document Retrieval

Transformers

NLP

