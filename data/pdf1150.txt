
Maximum Likelihood Estimates

Class 10, 18.05

Jeremy Orloﬀ and Jonathan Bloom

1

Learning Goals

1. Be able to deﬁne the likelihood function for a parametric model given data.

2. Be able to compute the maximum likelihood estimate of unknown parameter(s).

2

Introduction

Suppose we know we have data consisting of values x1, . . . , xn drawn from an exponential

distribution. The question remains: which exponential distribution?!

We have casually referred to the exponential distribution or the binomial distribution or the

normal distribution. In fact the exponential distribution exp(λ) is not a single distribution

but rather a one-parameter family of distributions. Each value of λ deﬁnes a diﬀerent dis-

tribution in the family, with pdf fλ(x) = λe−λx on [0, ∞). Similarly, a binomial distribution

bin(n, p) is determined by the two parameters n and p, and a normal distribution N(µ, σ2)

is determined by the two parameters µ and σ2 (or equivalently, µ and σ). Parameterized

families of distributions are often called parametric distributions or parametric models.

We are often faced with the situation of having random data which we know (or believe)

is drawn from a parametric model, whose parameters we do not know. For example, in

an election between two candidates, polling data constitutes draws from a Bernoulli(p)

distribution with unknown parameter p.

In this case we would like to use the data to

estimate the value of the parameter p, as the latter predicts the result of the election.

Similarly, assuming gestational length follows a normal distribution, we would like to use

the data of the gestational lengths from a random sample of pregnancies to draw inferences

about the values of the parameters µ and σ2.

Our focus so far has been on computing the probability of data arising from a parametric

model with known parameters. Statistical inference ﬂips this on its head: we will estimate

the probability of parameters given a parametric model and observed data drawn from it.

In the coming weeks we will see how parameter values are naturally viewed as hypotheses,

so we are in fact estimating the probability of various hypotheses given the data.

3

Maximum Likelihood Estimates

There are many methods for estimating unknown parameters from data.

We will ﬁrst

consider the maximum likelihood estimate (MLE), which answers the question:

For which parameter value does the observed data have the biggest probability?

The MLE is an example of a point estimate because it gives a single value for the unknown

parameter (later our estimates will involve intervals and probabilities). Two advantages of

1


18.05 class 10, Maximum Likelihood Estimates , Spring 2014

2

the MLE are that it is often easy to compute and that it agrees with our intuition in simple

examples. We will explain the MLE through a series of examples.

Example 1. A coin is ﬂipped 100 times. Given that there were 55 heads, ﬁnd the maximum

likelihood estimate for the probability p of heads on a single toss.

Before actually solving the problem, let’s establish some notation and terms.

We can think of counting the number of heads in 100 tosses as an experiment. For a given

value of p, the probability of getting 55 heads in this experiment is the binomial probability

P(55 heads) =

�100�

p55(1

p

55

− )45.

The probability of getting 55 heads depends on the value of p, so let’s include p in by using

the notation of conditional probability:

P(55 heads | p) =

�100�

p55(1 − p)45.

55

You should read P(55 heads | p) as:

‘the probability of 55 heads given p,’

or more precisely as

‘the probability of 55 heads given that the probability of heads on a single toss is p.’

Here are some standard terms we will use as we do statistics.

• Experiment: Flip the coin 100 times and count the number of heads.

• Data: The data is the result of the experiment. In this case it is ‘55 heads’.

• Parameter(s) of interest: We are interested in the value of the unknown parameter p.

• Likelihood, or likelihood function: this is P(data | p). Note it is a function of both the

data and the parameter p. In this case the likelihood is

P(55 heads | p) =

�100

55

�

p55(1 − p)45.

Notes: 1. The likelihood P(data | p) changes as the parameter of interest p changes.

2. Look carefully at the deﬁnition. One typical source of confusion is to mistake the likeli-

hood P(data | p) for P(p | data). We know from our earlier work with Bayes’ theorem that

P(data | p) and P(p | data) are usually very diﬀerent.

Deﬁnition: Given data the maximum likelihood estimate (MLE) for the parameter p is

the value of p that maximizes the likelihood P(data | p). That is, the MLE is the value of

p for which the data is most likely.

answer: For the problem at hand, we saw above that the likelihood

100

P(55 heads | p) =

�

55

�

p55(1 − p)45.


18.05 class 10, Maximum Likelihood Estimates , Spring 2014

3

We’ll use the notation pˆ for the MLE. We use calculus to ﬁnd it by taking the derivative of

the likelihood function and setting it to 0.

d

100

P(data p) =

(55p54(1

p)45

45p55(1

p)44) = 0.

dp

|

�

55

�

−

−

−

Solving this for p we get

55p54(1 − p)45 = 45p55(1 − p)44

55(1 − p) = 45p

55 = 100p

the MLE is pˆ = .55

Note: 1. The MLE for p turned out to be exactly the fraction of heads we saw in our data.

2. The MLE is computed from the data. That is, it is a statistic.

3. Oﬃcially you should check that the critical point is indeed a maximum. You can do this

with the second derivative test.

3.1

Log likelihood

If is often easier to work with the natural log of the likelihood function. For short this is

simply called the log likelihood. Since ln(x) is an increasing function, the maxima of the

likelihood and log likelihood coincide.

Example 2. Redo the previous example using log likelihood.

answer: We had the likelihood P(55 heads | p) =

�100�

p55(1 − p)45.

Therefore the log

55

likelihood is

ln(P(55 heads | p) = ln

��100��

+ 55 ln(p) + 45 ln(1 − p).

55

Maximizing likelihood is the same as maximizing log likelihood. We check that calculus

gives us the same answer as before:

d

dp(log likelihood) = d

100

ln

dp

�

��

55

��

+ 55 ln(p) + 45 ln(1 − p)

55

�

= p −

45

= 0

1 − p

⇒ 55(1 − p) = 45p

⇒ pˆ = .55

3.2

Maximum likelihood for continuous distributions

For continuous distributions, we use the probability density function to deﬁne the likelihood.

We show this in a few examples. In the next section we explain how this is analogous to

what we did in the discrete case.


18.05 class 10, Maximum Likelihood Estimates , Spring 2014

4

Example 3. Light bulbs

Suppose that the lifetime of Badger brand light bulbs is modeled by an exponential distri-

bution with (unknown) parameter λ. We test 5 bulbs and ﬁnd they have lifetimes of 2, 3,

1, 3, and 4 years, respectively. What is the MLE for λ?

answer: We need to be careful with our notation. With ﬁve diﬀerent values it is best to

use subscripts. Let X be the lifetime of the ith

j

bulb and let xi be the value Xi takes. Then

each X

λxi

i has pdf fXi(xi) = λe−

. We assume the lifetimes of the bulbs are independent,

so the joint pdf is the product of the individual densities:

f(x , x , x , x , x | λ) = (λe−λx1)(λe−λx2)(λe−λx3)(λe−λx4)(λe−λx5) = λ5e−λ(x1+x2+x3+x4+x5)

1

2

3

4

5

.

Note that we write this as a conditional density, since it depends on λ. Viewing the data

as ﬁxed and λ as variable, this density is the likelihood function. Our data had values

x1 = 2, x2 = 3, x3 = 1, x4 = 3, x5 = 4.

So the likelihood and log likelihood functions with this data are

f(2, 3, 1, 3, 4 | λ) = λ5e−13λ,

ln(f(2, 3, 1, 3, 4 | λ) = 5 ln(λ) − 13λ

Finally we use calculus to ﬁnd the MLE:

d

dλ(log likelihood) = 5

λ − 13 = 0 ⇒

ˆλ = 5

13 .

Note: 1.

In this example we used an uppercase letter for a random variable and the

corresponding lowercase letter for the value it takes. This will be our usual practice.

ˆ

2. The MLE for λ turned out to be the reciprocal of the sample mean x¯, so X ∼ exp(λ)

satisﬁes E(X) = x¯.

The following example illustrates how we can use the method of maximum likelihood to

estimate multiple parameters at once.

Example 4. Normal distributions

Suppose the data x1, x2, . . . , xn is drawn from a N(µ, σ2) distribution, where µ and σ are

unknown. Find the maximum likelihood estimate for the pair (µ, σ2).

answer: Let’s be precise and phrase this in terms of random variables and densities. Let

uppercase X

2

1, . . . , Xn be i.i.d. N(µ, σ ) random variables, and let lowercase xi be the value

Xi takes. The density for each Xi is

1

fXi(xi) = √

2π σe− (xi−µ)2

2

2σ

.

Since the Xi are independent their joint pdf is the product of the individual pdf’s:

1

f(x1, . . . , xn | µ, σ) =

�

√

2π σ

�n

e− �n

i=1

(xi−µ)2

2

2σ

.

For the ﬁxed data x1, . . . , xn, the likelihood and log likelihood are

1

f(x1, . . . , xn|µ, σ) =

�

√

2π σ

�n

e− �n

i=1

(xi−µ)2

2

2σ

,

ln(f(x1, . . . , xn|µ, σ)) = −n ln(

√

�

n (x

2π)−n ln(σ)−

i − µ)2

i=1

.

2σ2


18.05 class 10, Maximum Likelihood Estimates , Spring 2014

5

Since ln(f(x1, . . . , xn|µ, σ)) is a function of the two variables µ, σ we use partial derivatives

to ﬁnd the MLE. The easy value to ﬁnd is µˆ:

∂f(x1, . . . , xn|µ, σ)

∂µ

=

n

�

i=1

(xi − µ)

σ2

= 0 ⇒

n

�

i=1

xi = nµ ⇒ ˆµ =

�n

i=1 xi

n

= x.

To ﬁnd σˆ we diﬀerentiate and solve for σ:

∂f(x1, . . . , xn|µ, σ)

∂σ

= −n

�

n (xi

+

− µ)2

σ

i=1

σ3

= 0 ⇒ ˆσ2 =

�n

i=1(xi − µ)2

n

.

We already know ˆµ = x, so we use that as the value for µ in the formula for σˆ. We get the

maximum likelihood estimates

µˆ

= x

= the mean of the data

ˆσ2

=

n

�

i=1

1

n(xi − ˆµ)2 =

n

�

i=1

1

n(xi − x)2

= the variance of the data.

Example 5. Uniform distributions

Suppose our data x1, . . . xn are independently drawn from a uniform distribution U(a, b).

Find the MLE estimate for a and b.

answer: This example is diﬀerent from the previous ones in that we won’t use calculus to

ﬁnd the MLE. The density for U(a, b) is

1

b−a on [a, b]. Therefore our likelihood function is

f(x1, . . . , xn | a, b) =

��

1

al

b

�n

if all x

in

−

i are in the

terv

[a, b]

a

0

otherwise.

This is maximized by making b − a as small as possible. The only restriction is that the

interval [a, b] must include all the data. Thus the MLE for the pair (a, b) is

ˆ

aˆ = min(x1, . . . , xn)

b = max(x1, . . . , xn).

Example 6. Capture/recapture method

The capture/recapture method is a way to estimate the size of a population in the wild.

The method assumes that each animal in the population is equally likely to be captured by

a trap.

Suppose 10 animals are captured, tagged and released. A few months later, 20 animals are

captured, examined, and released. 4 of these 20 are found to be tagged. Estimate the size

of the wild population using the MLE for the probability that a wild animal is tagged.

answer: Our unknown parameter n is the number of animals in the wild. Our data is that

4 out of 20 recaptured animals were tagged (and that there are 10 tagged animals). The

likelihood function is

n

P(data | n animals) =

� −10

16

��10

4

�

of

� n

20

(The numerator is the number

ways to choose 16 animals

�

from among the n−10 untagged

ones times the number of was to choose 4 out of the 10 tagged animals. The denominator


18.05 class 10, Maximum Likelihood Estimates , Spring 2014

6

is the number of ways to choose 20 animals from the entire population of n.) We can use

R to compute that the likelihood function is maximized when n = 50. This should make

some sense. It says our best estimate is that the fraction of all animals that are tagged is

10/50 which equals the fraction of recaptured animals which are tagged.

Example 7.

Hardy-Weinberg. Suppose that a particular gene occurs as one of two

alleles (A and a), where allele A has frequency θ in the population. That is, a random copy

of the gene is A with probability θ and a with probability 1 − θ. Since a diploid genotype

consists of two genes, the probability of each genotype is given by:

genotype

AA

Aa

aa

probability

θ2

2θ(1 − θ)

(1 − θ)2

Suppose we test a random sample of people and ﬁnd that k1 are AA, k2 are Aa, and k3 are

aa. Find the MLE of θ.

answer: The likelihood function is given by

+ k

|

�k1

2 + k3

k2 + k3

k3

P(k1, k

2k1

k2

2k3

2, k3 θ) =

1

��

k2

��

θ

(2θ(1

θ))

(1

θ)

.

k

k3

�

−

−

So the log likelihood is given by

constant + 2k1 ln(θ) + k2 ln(θ) + k2 ln(1 − θ) + 2k3 ln(1 − θ)

We set the derivative equal to zero:

2k1 + k2

θ

− k2 + 2k3 = 0

1 − θ

Solving for θ, we ﬁnd the MLE is

2k

ˆ

1 + k2

θ =

,

2k1 + 2k2 + 2k3

which is simply the fraction of A alleles among all the genes in the sampled population.

4

Why we use the density to ﬁnd the MLE for continuous

distributions

The idea for the maximum likelihood estimate is to ﬁnd the value of the parameter(s) for

which the data has the highest probability. In this section we ’ll see that we’re doing this

is really what we are doing with the densities. We will do this by considering a smaller

version of the light bulb example.

Example 8.

Suppose we have two light bulbs whose lifetimes follow an exponential(λ)

distribution.

Suppose also that we independently measure their lifetimes and get data

x1 = 2 years and x2 = 3 years. Find the value of λ that maximizes the probability of this

data.

answer: The main paradox to deal with is that for a continuous distribution the probability

of a single value, say x1 = 2, is zero. We resolve this paradox by remembering that a single


18.05 class 10, Maximum Likelihood Estimates , Spring 2014

7

measurement really means a range of values, e.g. in this example we might check the light

bulb once a day. So the data x1 = 2 years really means x1 is somewhere in a range of 1 day

around 2 years.

If the range is small we call it dx1. The probability that X1 is in the range is approximated

by fX1(x1|λ) dx1. This is illustrated in the ﬁgure below. The data value x2 is treated in

exactly the same way.

x

density fX1(x1|λ)

λ

x1

dx1

probability ≈ fX1(x1|λ) dx1

x

density fX2(x2|λ)

λ

x2

dx2

probability ≈ fX2(x2|λ) dx2

The usual relationship between density and probability for small ranges.

Since the data is collected independently the joint probability is the product of the individual

probabilities. Stated carefully

P(X1 in range, X2 in range|λ) ≈ fX1(x1|λ) dx1 · fX2(x2|λ) dx2

Finally, using the values x1 = 2 and x2 = 3 and the formula for an exponential pdf we have

P(X in range, X in range|λ) ≈ λe−2λ dx · λe−3λ

2

1

2

1

dx2 = λ e−5λ dx1 dx2.

Now that we have a genuine probability we can look for the value of λ that maximizes it.

Looking at the formula above we see that the factor dx1 dx2 will play no role in ﬁnding the

maximum. So for the MLE we drop it and simply call the density the likelihood:

likelihood = f(x1, x2|λ) = λ2e−5λ.

ˆ

The value of λ that maximizes this is found just like in the example above. It is λ = 2/5.

5

Appendix: Properties of the MLE

For the interested reader, we note several nice features of the MLE. These are quite technical

and will not be on any exams.

The MLE behaves well under transformations. That is, if pˆ is the MLE for p and g is a

one-to-one function, then g(pˆ) is the MLE for g(p). For example, if σˆ is the MLE for the

standard deviation σ then (σˆ)2 is the MLE for the variance σ2.

Furthermore, the MLE is asymptotically unbiased and has asymptotically minimal variance.

To explain these notions, note that the MLE is itself a random variable since the data is

random and the MLE is computed from the data. Let x1, x2, . . . be an inﬁnite sequence of

samples from a distribution with parameter p. Let pˆn be the MLE for p based on the data

x1, . . . , xn.

Asymptotically unbiased means that as the amount of data grows, the mean of the MLE

converges to p. In symbols: E(pˆn) → p as n → ∞. Of course, we would like the MLE to be


18.05 class 10, Maximum Likelihood Estimates , Spring 2014

8

close to p with high probability, not just on average, so the smaller the variance of the MLE

the better. Asymptotically minimal variance means that as the amount of data grows, the

MLE has the minimal variance among all unbiased estimators of p. In symbols: for any

unbiased estimator p˜n and ϵ &gt; 0 we have that Var(p˜n) + ϵ &gt; Var(pˆn) as n → ∞.


MIT OpenCourseWare

https://ocw.mit.edu

18.05 Introduction to Probability and Statistics

Spring 2014

For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms.

