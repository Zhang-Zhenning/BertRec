
IR&amp;DM ’13/’14

III.4 Statistical Language Models

1.  

Basics of Statistical Language Models 

2.  

Query-Likelihood Approaches 

3.  

Smoothing Methods 

4.  

Divergence Approaches 

5.  

Extensions 

 

 

 

 

 

 

 

 

Based on MRS Chapter 12 and [Zhai 2008]

!78


IR&amp;DM ’13/’14

1. Basics of Statistical Language Models

• Statistical language models (LMs) are generative models of 

word sequences (or, bags of words, sets of words, etc.) 

!

!

!

!

• Application examples: 

• Speech recognition, e.g., to select among multiple phonetically similar 

sentences (“get up at 8 o’clock” vs. “get a potato clock”) 

• Statistical machine translation, e.g., to select among multiple candidate 

translations (“logical closing” vs. “logical reasoning”) 

• Information retrieval, e.g., to rank documents in response to a query

!79

0.1

0.9

dog : 

0.5 

cat : 

0.4 

hog : 

0.1

P(h hog i)

=

0.1 ⇥ 0.1

P(h cat, dog i)

=

0.4 ⇥ 0.9 ⇥ 0.5 ⇥ 0.1

P(h dog, dog, hog i)

=

0.5 ⇥ 0.9 ⇥ 0.5 ⇥ 0.9 ⇥ 0.1 ⇥ 0.1


IR&amp;DM ’13/’14

Types of Language Models

• Unigram LM based on only single words (unigrams), considers 

no context, and assumes independent generation of words 

!

!

• Bigram LM conditions on the preceding term 

!

!

• n-Gram LM conditions on the preceding (n-1) terms

!80

P(h t1, . . . , tm i) = P(t1)

m

Y

i=2

P(ti|ti�1)

P(h t1, . . . , tm i) =

m

Y

i=1

P(ti)

P(h t1, . . . , tm i) = P(t1) P(t2|t1) . . .

m

Y

i=n

P(ti|ti�n+1 . . . ti�1)


IR&amp;DM ’13/’14

Parameter Estimation

• Parameters (e.g., P(ti), P(ti | ti-1)) of language model θ are 

estimated based on a sample of documents, which are  

assumed to have been generated by θ 

• Example: Unigram language models θSports and θPolitics  

estimated from documents about sports and politics

!81

θSports 

soccer 

: 

0.20 

goal 

: 

0.15 

tennis 

: 

0.10 

player 

: 

0.05 

 

 

:

θPolitics 

party  

: 

0.20 

debate 

: 

0.20 

scandal 

: 

0.15 

election 

: 

0.05 

 

 

:

Sample

Sample

generates

generates


IR&amp;DM ’13/’14

Probabilistic IR vs. Statistical Language Models

!82

P[R|d, q]

/ P[R|d, q]

P[ ¯R|d, q]

/

P [q,d|R]

P [q,d| ¯

R]

=

P [q|d,R]

P [q|d, ¯

R]

P [R|d]

P [ ¯

R|d]

/

P[q|d, R]

Probabilistic IR 

ranks according to 

relevance odds

Statistical LMs 

rank according to 

query likelihood

“User finds document d 

relevant to query q”


IR&amp;DM ’13/’14

2. Query-Likelihood Approaches

!

!

!

!

!

• P(q|d) is the likelihood that the query was generated by  

the language model θd estimated from document d 

• Intuition: 

• User formulates query q by selecting words from a prototype document 

• Which document is “closest” to that prototype document

!83

d1

d2

Sample

Sample

θd1 

apple : 

0.20 

pie 

: 

0.15 

 

:

θd2 

cake : 

0.20 

apple : 

0.15 

 

:

q

P(q|d1)

P(q|d2)


IR&amp;DM ’13/’14

Multi-Bernoulli LM

• Query q is seen as a set of terms and generated from document d 

by tossing a coin for every word from the vocabulary V 

 

 

 

 

 

• [Ponte and Croft ’98] pioneered the use of LMs in IR

!84

P(q|d)

=

Q

t2q

P(t|d) ⇥

Q

t2V \q

(1 � P(t|d))

⇡

Q

t2q

P(t|d)

(assuming |q| &lt;&lt; |V |)


IR&amp;DM ’13/’14

Multinomial LM

• Query q is seen as a bag of terms and generated from document d 

by drawing terms from the bag of terms corresponding to d 

 

 

 

 

 

 

 

 

• Multinomial LM is more expressive than Multi-Bernoulli LM 

and therefore usually preferred

!85

P(q|d)

=

✓

|q|

tf(t1, q) . . . tf(t|q|, q)

◆ Q

ti2q

P(ti|d) tf(ti,q)

/

Q

ti2q

P(ti|d) tf(ti,q)

⇡

Q

ti2q

P(ti|d)

(assuming 8ti 2 q : tf(ti, q) = 1)


IR&amp;DM ’13/’14

Multinomial LM (cont’d)

• Maximum-likelihood estimate for parameters P(ti|d) 

 

 

 

is prone to overfitting and leads to 

• bias in favor of short documents / against long documents 

• conjunctive query semantics, i.e., query can not be generated from 

language models of documents that miss one of the query terms 

 

 

!86

P(ti|d) = tf(ti, d)

|d|


IR&amp;DM ’13/’14

3. Smoothing

• Smoothing methods avoid overfitting to the sample (often: one 

document) and are essential for LMs to work in practice 

• Laplace smoothing (cf. Chapter III.3) 

• Absolute discounting 

• Jelinek-Mercer smoothing 

• Dirichlet smoothing 

• Good-Turing smoothing 

• Katz’s back-off model 

• … 

• Choice of smoothing method and parameter setting still mostly 

“black art” (or empirical, i.e., based on training data)

!87


IR&amp;DM ’13/’14

Jelinek-Mercer Smoothing

• Uses a linear combination (mixture) of document language 

model θd and document-collection language model θD 

 

 

 

with document D as concatenation of entire document collection 

• Parameter λ can be tuned by cross-validation with held-out data 

• divide set of relevant (q, d) pairs into n partitions 

• build LM on the pairs from n-1 partitions 

• choose λ to maximize precision (or recall or F1) on held-out partition  

• iterate with different choice of nth partition and average 

• Parameter λ can be made document- or term-dependent

!88

P(t|d) = � tf(t, d)

|d|

+ (1 � �) tf(t, D)

|D|


IR&amp;DM ’13/’14

Jelinek-Mercer Smoothing vs. TF*IDF

!

!

!

!

!

!

!

• (Jelinek-Mercer) smoothing has effect similar to IDF weighting 

• Jelinek-Mercer smoothing leads to a TF*IDF-style model

!89

P(q|d)

=

Q

t2q

P(t|d)

=

Q

t2q

⇣

� tf(t,d)

|d|

+ (1 � �) tf(t,D)

|D|

⌘

/

P

t2q

log

⇣

� tf(t,d)

|d|

+ (1 � �) tf(t,D)

|D|

⌘

/

P

t2q

log

⇣

1 +

�

1��

tf(t,d)

|d|

|D|

tf(t,D)

⌘

~ tf

~ idf


IR&amp;DM ’13/’14

Dirichlet-Prior Smoothing

• Uses Bayesian estimation with a conjugate Dirichlet prior 

instead of the Maximum-Likelihood Estimation 

!

!

• Intuition: Document d is extended by α terms generated  

by the document-collection language model 

• Parameter α usually set as multiple of average document length

!90

P(t|d) =

tf(t, d) + ↵ tf(t,D)

|D|

|d| + ↵


IR&amp;DM ’13/’14

Dirichlet Smoothing vs. Jelinek-Mercer Smoothing

!

!

!

!

!

• Jelinek-Mercer smoothing with document-dependent λ 

becomes a special case of Dirichlet smoothing

!91

P(t|d)

=

� tf(t,d)

|d|

+ (1 � �) tf(t,D)

|D|

=

|d|

|d|+↵

tf(t,d)

|d|

+

↵

|d|+↵

tf(t,D)

|D|

(set � =

|d|

|d|+↵)

=

tf(t,d)+↵ tf(t,D)

|D|

|d|+↵


IR&amp;DM ’13/’14

4. Divergence Approaches

!

!

!

!

!

!

• Query-likelihood approaches see query as a sample from a LM 

• Query expansion, relevance feedback, etc. are difficult to express 

as query-likelihood approaches, since they would require 

tinkering with the sample (i.e., the query) and more fine-grained 

control than adding/removing terms

!92

d1

d2

θd1 

apple : 

0.20 

pie 

: 

0.15 

 

:

θd2 

cake : 

0.20 

apple : 

0.15 

 

:

q

θq 

apple : 

0.20 

muffin : 

0.15 

 

:

D(✓q||✓d1)

D(✓q||✓d2)


IR&amp;DM ’13/’14

Kullback-Leibler Divergence

• Kullback-Leibler divergence (aka. information gain or relative 

entropy) is an information-theoretic non-symmetric measure of 

distance between probability distributions 

!

!

• Example:

!93

D(✓q||✓d)

=

P

t2V

P(t|✓q) log P (t|✓q)

P (t|✓d)

θq 

apple : 

0.50 

muffin : 

0.50

θd 

apple : 

0.25 

muffin : 

0.25 

recipe : 

0.10 

water : 

0.10 

sugar : 

0.30

D(✓qk✓d)

=

P(apple|✓q) log P (apple|✓q)

P (apple|✓d) + P(mu�n|✓q) log P (mu�n|✓q)

P (mu�n|✓d)

=

0.50 log 0.50

0.25 + 0.50 log 0.50

0.25

=

1.00


IR&amp;DM ’13/’14

Relevance Feedback LM

• [Zhai and Lafferty ’01] re-estimate query language model as 

 

 

with F as the set of documents with positive feedback from user 

• MLE of θF obtained by maximizing log-likelihood function 

 

 

 

with tf(t, F) as the total term frequency of t in documents from F 

and θD as the document-collection language model

!94

P(t|✓0

q) = (1 � ↵) P(t|✓q) + ↵ P(t|✓F )

log P(F|✓F ) =

X

t2V

tf(t, F) log ((1 � �) P(t|✓F ) + � P(t|✓D))


IR&amp;DM ’13/’14

5. Extensions

• Statistical language models have been one of the highly active 

areas in IR research during the past decade and continue to be 

• Extensions: 

• Term-specific and document-specific smoothing 

(JM-style smoothing with term-specific λt or document-specific λd) 

• (Semantic) Translation LMs 

(e.g., to consider synonyms or support cross-lingual IR) 

• Time-based LMs 

(e.g., with time-dependent document prior to favor recent documents) 

• LMs for (semi-)structured XML and RDF data 

(e.g., for entity search or question answering) 

• …

!95


IR&amp;DM ’13/’14

Translation LM for Cross-Lingual IR

• Cross-Lingual IR: 

• Users issue queries in their native language (e.g., German) 

(e.g., spionage usa bundesregierung ) 

• System returns documents in another known language (e.g., English) 

(e.g., reactions of the German government to U.S. eavesdropping on … ) 

 

 

• Translation probabilities P(t|w) obtained from a dictionary or 

estimated based on a parallel cross-lingual corpus 

• [Federico and Bertoldi ’01] as more advanced approach based 

on a Hidden-Markov Model that also considers term contexts

!96

P(q|d) =

Y

t2q

X

w

P(t|w) P(w|d)


IR&amp;DM ’13/’14

Time-Based LMs

• Intuition: For news-related queries (e.g., german election) 

documents published more recently are often preferable 

• [Li and Croft ’03] rank documents according to 

 

 

 

 

with document publication timestamp t and time-dependent 

exponentially decaying document prior P(dt) 

• [Peetz and de Rijke ’13] consider other document priors 

motivated by cognitive psychology research on human memory

!97

P(q|d) P(d t) =

 Y

t2q

P(t|d t)

! ⇣

� e�� (now�t)⌘


IR&amp;DM ’13/’14

LM for Entity Search

• Objective: Retrieve entities (e.g., people, locations, organizations) 

relevant to query q as opposed to only documents [Ni et al. ‘07] 

 

 

 

 

• Language model θe for entity e can be estimated from contexts 

in which the entity is mentioned in the document collection, 

possibly taking into account extraction accuracy

!98

Query q: 

dutch soccer 

player munich

Candidate Entities: 

1. Arjen Robben 

2. Rafael van der Vaart 

3. Louis van Gaal 

4. Daniel van Buyten 

5. Toni Kroos



rayand@flickr









…munich’s flying dutchman… 

…one of bayern’s most valuable players… 

…winning soccer’s most prestigious champions league… 

…with the dutch national team…


IR&amp;DM ’13/’14

Summary of III.4

• Statistical language models  

widely used in natural language applications other than IR  

• Query-likelihood approaches  

see the query as a sample from the document LM 

• Divergence approaches  

are more expressive comparing query LM against document LM 

• Smoothing methods  

are absolutely essential to make LMs work in practice 

• Various extensions  

for advanced tasks such as cross-lingual IR or entity search

!99


IR&amp;DM ’13/’14

Additional Literature for III.4

• D. Hiemstra: Using Language Models for Information Retrieval, Ph.D. Thesis, 

University of Twente, 2001 

• M. Federico and N. Bertoldi: Statistical Cross-Language Information Retrieval using 

N-Best Query Translations, SIGIR 2001 

• Z. Nie, Y. Ma, S. Shi, J.-R. Wen and W.-Y. Ma: Web Object Retrieval, 

WWW 2007 

• H. M. Peetz and M. de Rijke: Cognitive Temporal Document Priors,  

ECIR 2013 

• J. M. Ponte and B. Croft: A Language Modeling Approach to Information Retrieval, 

SIGIR 1998 

• C. Zhai and J. Lafferty: Model-based Feedback in the Language Modeling Approach 

for Information Retrieval, CIKM 2001 

• C. Zhai: Statistical Language Models for Information Retrieval A Critical Review, 

Foundations and Trends in Information Retrieval 2(3):137-213, 2008

!100


IR&amp;DM ’13/’14

III.5 Latent Topic Models

1.  

Latent Semantic Indexing 

2.  

Probabilistic Latent Semantic Indexing 

3.  

Latent Dirichlet Allocation 

 

 

 

 

 

 

 

 

 

 

 

Based on MRS Chapter 18 and [Blei ‘12]

!101


IR&amp;DM ’13/’14

Latent Topic Models

• Retrieval models seen so far (e.g., TF*IDF, LMs) do not handle 

synonymy (e.g., car and automobile), polysemy (e.g., java), etc. 

• Word co-occurrence can help us, e.g.: 

• car and automobile both occur together with garage, exhaust, fuel,… 

• java occurs together with class and method but also with grind and coffee 

• Latent topic models assume that documents are composed from 

a small number k of latent (i.e., hidden, unknown) topics 

• Latent Semantic Indexing (LSI) [Deerwester et al. ‘90] 

• Probabilistic Latent Semantic Indexing (pLSI) [Hofmann ‘99] 

• Latent Dirichlet Allocation (LDA) [Blei et al. ‘03]

!102


IR&amp;DM ’13/’14

• Idea: Apply SVD to m-by-n term-document matrix A 

!

!

!

!

!

• Uk, VkT, Σk contain the first k singular vectors and values 

• Uk maps terms to topics 

• Vk maps documents to topics

1. Latent Semantic Indexing (LSI)

!103

term

A

document

U

Σ

VT

⇡

⇥

⇥

term

topic

topic

topic

topic

document

Uk

Σk

VkT


IR&amp;DM ’13/’14

Operations in Latent Topic Space

• We can map a query q from m-dimensional term space  

into the k-dimensional topic space by q → UkT q = q’ 

• Ranking of documents can then be determined by comparing q’ 

against the columns of VkT using dot product or cosine similarity 

• We can fold in a new document from m-dimensional term space 

by mapping it to k-dimensional topic space as d → UkT d = d’ 

and appending it as a new column to VkT (with quality 

deteriorating over time)

!104


IR&amp;DM ’13/’14

LSI (Example)

!105

m = 6 (terms) 

t1 

: 

bak(e,ing) 

t2 

: 

recipe(s) 

t3 

: 

bread 

t4 

: 

cake 

t5 

: 

pastr(y,ies) 

t6 

: 

pie

n = 5 (documents) 

d1 

: 

how to bake bread without recipes 

d2 

: 

the classic art of viennese pastry 

d3 

: 

numerical recipes: the art of scientific computing 

d4 

: 

breads, pastries, pies and cakes: quantity baking recipes 

d5 

: 

pastry: a book of best french recipes

IR&amp;DM, WS'11/12

19 January 2012

p

( )

IRDM  WS 2007

4-74

Example 2 for Latent Semantic Indexing

m=6 terms

t1: bak(e,ing)

t2: recipe(s)

t3: bread

t4: cake

t5: pastr(y,ies)

t6: pie

n=5 documents

d1: How to bake bread without recipes

d2: The classic art of Viennese Pastry

d3: Numerical recipes: the art of

scientific computing

d4: Breads, pastries, pies and cakes:

quantity baking recipes

d5: Pastry: a book of best French recipes

��

�

�

�

�

�

�

�

�

��

�

�

�

�

�

�

�

�

�

0000

.0

4082

.0

0000

.0

0000

.0

0000

.0

7071

.0

4082

.0

0000

.0

0000

.1

0000

.0

0000

.0

4082

.0

0000

.0

0000

.0

0000

.0

0000

.0

4082

.0

0000

.0

0000

.0

5774

.0

7071

.0

4082

.0

0000

.1

0000

.0

5774

.0

0000

.0

4082

.0

0000

.0

0000

.0

5774

.0

A


IR&amp;DM ’13/’14

LSI (Example)

!106

IR&amp;DM WS'11/12

IX 2&amp;3-

19 January 2012

LSI example (2)

3

IRDM  WS 2007

4-75

Example 2 for Latent Semantic Indexing (2)

�

A

��

�

�

�

�

�

��

�

�

�

�

�

�

4195

.0

0000

.0

0000

.0

0000

.0

0000

.0

8403

.0

0000

.0

0000

.0

0000

.0

0000

.0

1158

.1

0000

.0

0000

.0

0000

.0

0000

.0

6950

.1

��

�

�

�

�

�

��

�

�

�

�

�

�

�

�

�

�

�

�

�

�

0577

.0

6571

.0

1945

.0

2760

.0

6715

.0

3712

.0

5711

.0

6247

.0

0998

.0

3688

.0

2815

.0

0346

.0

3568

.0

7549

.0

4717

.0

5288

.0

4909

.0

4412

.0

3067

.0

4366

.0

��

�

�

�

�

�

�

�

�

��

�

�

�

�

�

�

�

�

�

�

�

�

�

�

�

�

�

6394

.0

2774

.0

0127

.0

1182

.0

1158

.0

0838

.0

8423

.0

5198

.0

6394

.0

2774

.0

0127

.0

1182

.0

2847

.0

5308

.0

2567

.0

2670

.0

0816

.0

5249

.0

3981

.0

7479

.0

2847

.0

5308

.0

2567

.0

2670

.0

U

�

VT

U

Σ

VT


IR&amp;DM ’13/’14

LSI (Example)

!107

IR&amp;DM, WS'11/12

IX.2

19 January 2012

LSI example (3)

Example 2 for Latent Semantic Indexing (3)

�

3

A

��

�

�

�

�

�

�

�

�

��

�

�

�

�

�

�

�

�

�

�

�

�

�

�

�

0155

.0

2320

.0

0522

.0

0740

.0

1801

.0

7043

.0

4402

.0

0094

.0

9866

.0

0326

.0

0155

.0

2320

.0

0522

.0

0740

.0

1801

.0

0069

.0

4867

.0

0232

.0

0330

.0

4971

.0

7091

.0

3858

.0

9933

.0

0094

.0

6003

.0

0069

.0

4867

.0

0232

.0

0330

.0

4971

.0

T

V

U

3

3

3

�

�

�

�= U3Σ3V3T


IR&amp;DM ’13/’14

LSI (Example)

• Query: baking bread 

• q = (1 0 1 0 0 0)T 

• q’ = U3Tq = (0.5340 -0.5134 1.0616)T 

• Dot-product similarity in topic space 

• sim(q, d1) ≈ 0.86 / sim(q, d2) ≈ -0.12 / sim(q, d3) ≈ -0.24 

• Adding d6 = “algorithmic recipes for the computation of pie” 

• d = (0 0.07071 0 0 0 0.07071)T 

• d’ =  U3Td = (0.5 -0.28 -0.15)T 

• d’ becomes a new column of VkT

!108


IR&amp;DM ’13/’14

Issues with LSI

• Parameter tuning 

• How to select proper number of latent topics k? 

• Memory consumption 

• Term-by-document matrix A is usually sparse 

• SVD factors U and V are almost never sparse 

• Computational cost 

• SVD still expensive to compute when m and n at the order of millions 

• Retrieval effectiveness 

• LSI achieved only mediocre performance on TREC datasets 

with good gains for some queries but losses for others 

!109


IR&amp;DM ’13/’14

2. Probabilistic Latent Semantic Indexing (pLSI)

• Idea: Model documents as (probabilistic) mixtures of topics 

• Each topic generates terms with topic-specific probabilities 

• Assume conditional independence of word w and document d 

given topic t: 

!

!

!

• Generative model

!110

P[w, d, t] = P[w, d|t] P[t] = P[w|t] P[d|t] P[t]

P[w|d] =

X

t

P[w|t] P[t|d]

P[w, d] =

X

t

P[w|t]P[d|t]P[t]


IR&amp;DM ’13/’14

pLSI Generative Model

!111

d1

dn

S

P

F

goal

fight

ball

law

jury

oath

cod

meat

rice

…

P[w|d] =

X

t

P[w|t] P[t|d]


IR&amp;DM ’13/’14

Computing pLSI

• Parameters P[t|d] and P[w|t] can be determined using  

the iterative method Expectation Maximization (EM) 

• Query q is folded in by estimating the topic distribution P[t|q] 

that provides the best explanation of the query terms 

• Ranking of documents can then be determined by comparing the 

topic distributions P[t|q] and P[t|d], e.g., using KL divergence

!112


IR&amp;DM ’13/’14

!

!

!

!

!

!

• Differences to SVD: 

• probabilities P[w|t], P[d|t], and P[t] are non-negative and normalized 

• loss function is Kullback-Leibler divergence instead of squared loss

pLSI vs. LSI

!113

term

A

document

U

Σ

VT

⇡

⇥

⇥

term

topic

topic

topic

topic

document

Uk

Σk

VkT

P[w, d] =

X

t

P[w|t] P[d|t] P[t]


IR&amp;DM ’13/’14

pLSI (Example)

• Topics (10 of 128) extracted from 12K Science Magazine articles 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Source: Thomas Hofmann, Tutorial at ADFOCS 2004

!114

IRDM  WS 2009

4-97

Experimental Results: Example



Source: Thomas Hofmann, Tutorial at ADFOCS 2004

P[w|t]

P[w|t]


IR&amp;DM ’13/’14

3. Latent Dirichlet Allocation (LDA)

• Multiple-cause mixture model (MCMM) 

• Documents contain multiple topics 

• Topics are expressed by specific word distributions 

• LDA provides a generative model for this

!115

IRDM  WS 2009

4-101

Latent Dirichlet Allocation (LDA)

State-of-the-art multiple-cause mixture model (MCMM)



LDA is a generative model for this situation:

Dirichlet topic mixtures


IR&amp;DM ’13/’14

• For each of the D documents d 

• Choose document length N (# word occurrences) ~ Poisson(λ) 

• Choose topic-probability distribution parameters β ~ Dirichlet(α) 

• For each of the N word occurrences in d (at position n) 

• Choose one of k topics tn ~Multinomial(β, k) 

• Choose one of M words wn from per-topic distribution ~ Multinomial(θ, M)

LDA Generative Model

!116

Dirichlet(α)

Multinomial(β, k)

topic t

word w

N

D

Multinomial(θ, M)

Latent (hidden) RV

Observable RV (data)

per document

per word occurrence


IR&amp;DM ’13/’14

Comparison to Other Generative Models

!117

Dirichlet(α)

Multinomial(β, k)

topic t

word w

N

D

Multinomial(θ, M)

document d

topic t

word w

topic t

word w

word w

LDA

pLSI

Single-Cause  

Mixture of Unigrams

Simple 

Unigram Model


IR&amp;DM ’13/’14

Computing LDA

• Dirichlet(α) probability density function 

 

 

 

 

 

 

with αi ≥ 0, βi ≥ 0 and Σ βi = 1 

• Probability of document d given α and θ 

 

 

 

• Log-likelihood function (for corpus of D documents) 

is analytically intractable 

!118

f(�|↵) =

�(

kP

i=1

↵i)

kQ

i=1

�(↵i)

k

Y

i=1

�↵i�1

i

P[d|↵, ✓] =

Z

f(�|↵)

 N

Y

n=1

k

X

tn=1

�tn✓tn,wn

!

d�


IR&amp;DM ’13/’14

Computing LDA (cont’d)

• Parameters α and θ can be estimated using Expectation 

Maximization (EM) with lower-bound distributions 

 

 

 

 

 

• E-Step: 

Determine optimal parameters γ* and φ* of  

 

 

 

lower-bound distributions given α (i-1) and θ (i-1) 

• M-Step: 

Given fixed lower-bound distributions determine   

 

 

 

parameters α (i) and θ (i) that maximize log-likelihood 

• Full details: [Blei et al ’03]

!119

Dirichlet(α)

Multinomial(β, k)

topic t

word w

N

D

Multinomial(θ, M)

D

topic t

N

Dirichlet(γ)

Multinomial(θ, k)

Multinomial(φ, k)


IR&amp;DM ’13/’14

LDA (Example)

• Topics from 5K scientific articles and 16K newswire articles 

 

 

 

 

 

 

 

 

 

 

 

 

 

Source: [Blei et al. ’03]

!120











































































































































































































































































































































































































































































































































































































































































































































































































The William Randolph Hearst Foundation will give $1.25 million to Lincoln Center, Metropoli-

tan Opera Co., New York Philharmonic and Juilliard School.

“Our board felt that we had a

real opportunity to make a mark on the future of the performing arts with these grants an act

every bit as important as our traditional areas of support in health, medical research, education

and the social services,” Hearst Foundation President Randolph A. Hearst said Monday in

announcing the grants.

Lincoln Center’s share will be $200,000 for its new building, which

will house young artists and provide new public facilities. The Metropolitan Opera Co. and

New York Philharmonic will receive $400,000 each. The Juilliard School, where music and

the performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter

of the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000

donation, too.

Figure 8: An example article from the AP corpus. Each color codes a different factor from which

the word is putatively generated.


IR&amp;DM ’13/’14

Summary of III.5

• Latent topic models 

consider word co-occurrence and implicitly handle synonymy etc. 

• Latent Semantic Indexing (LSI) 

applies SVD to term-document matrix A 

• Probabilistic Latent Semantic Indexing (pLSI) 

uses a non-negative probabilistic decomposition of A 

• Latent Dirichlet Allocation (LDA) 

uses a probabilistic generative model

!121


IR&amp;DM ’13/’14

Additional Literature for III.5

• D. M. Blei, A. Y. Ng, M. I. Jordan: Latent Dirichlet Allocation, 

Journal of Machine Learning Research 3:993-1022, 2003 

• D. M. Blei: Probabilistic Topic Models,  

CACM 55(4):77-84, 2012 

• S. Deerwester, S. Dumais, G. W. Furnas, T. K. Landauer, R. Hashman:  

Indexing by Latent Semantic Analysis, 1990 

• T. Hofmann: Probabilistic Latent Semantic Indexing, 

SIGIR 1999

!122

