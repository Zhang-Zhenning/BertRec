
EM ALGORITHM

EM ALGORITHM

• EM algorithm is a general iterative method of 

maximum likelihood estimation for incomplete data

• Used to tackle a wide variety of problems, some of 

which would not usually be viewed as an incomplete 

data problem


• Natural situations

– Missing data problems

– Grouped data problems

– Truncated and censored data problems

• Not so obvious situations

– Variance component estimation

– Latent variable situations and random effects models

– Mixture models


• Areas of applications

– Image analysis

– Epidemiology and Medicine

– Engineering

– Genetics and Biology


• Seminal Paper

Dempster, A.P., Laird, N.M. and 

Rubin,D.B. (1977). Maximum likelihood 

from incomplete data via the EM algorithm 

(with discussion). JRSS B 39: 1-38


EM algorithm closely related to the following ad hoc process 

of handling missing data

1. Fill in the missing values by their estimated values

2. Estimate the parameters for this completed dataset

3. Use the estimated parameters to re-estimate the missing 

values

4. Re-estimate the parameters from this updated completed 

dataset

Alternate between steps 3 and 4 until convergence of the 

parameter estimates


• The EM algorithm formalises this approach

The essential idea behind the EM algorithm is to calculate the 

maximum likelihood estimates for the incomplete data 

problem by using the complete data likelihood instead of the 

observed likelihood because the observed likelihood might be 

complicated or numerically infeasible to maximise.

To do this, we augment the observed data with manufactured 

data so as to create a complete likelihood that is 

computationally more tractable. We then replace, at each 

iteration, the incomplete data, which are in the sufficient 

statistics for the parameters in the complete data likelihood, by 

their conditional expectation given the observed data and the 

current parameter estimates (Expectation step: E-step) 


The new parameter estimates are obtained from these 

replaced sufficient statistics as though they had come 

from the complete sample (Maximisation step: M-step)

Alternating E- and M-steps, the sequence of estimates 

often converges to the mle’s under very general 

conditions


EXAMPLES

EXAMPLES

1. Genetic Linkage Model

2. Censored (survival) data

3. Mixture of two univariate normals


• Genetic Linkage Model

197 animals distributed into four categories

Y is postulated to have arisen from a multinomial d/n 

with cell probabilities

⎟

⎠

⎞

⎜

⎝

⎛

−

−

+

4

),

1(

4

1

),

1(

4

1

,

4

2

1

θ

θ

θ

θ

1

2

3

4

(

,

,

,

)

(125,18,20,34)

y

y y

y

y

=

=


(

)

data'

 

real

' 

as

 

25

 treat 

:

step

-

M

25

2

5.0

5.0

125

;

|

:

 

estimate,

 

initial

an 

obtain 

 

to

n 

expectatio

 l

conditiona

 

its

by 

 

 

replace

 :

step

-

E

5.0

estimate,

 

initial

 

 the

as

 

Take

)

0

(

2

)

0

(

2

)

0

(

2

)

0

(

2

2

)

0

(

=

=

+

×

=

Ε

=

=

x

Y

X

x

x

x

θ

θ


(

)

14

.

29

2

608

.0

608

.0

125

;

|

:

 

of

 

estimate

 

improved

Obtain 

608

.0

)

20

18

34

25

(

)

34

25

(

:

 

of

 

estimate

 

Update

)

1

(

2

)

1

(

2

2

(1)

=

+

×

=

Ε

=

=

+

+

+

+

=

θ

θ

θ

Y

X

x

x


Alternate E and M-steps

step

θ(m)

0

0.5

1

0.6082

2

0.6243

3

0.6265

4

0.6268

5

0.6268

6

0.6268


• Survival time data: right censored exponential(θ) data

3 uncensored observations: t1 = 0.5, t2 = 1.5  and  t3 = 4

2 right-censored observations: t4 = 1*  and t5 = 3*

Recall lack of memory property

θ

1

)

|

(

+

=

&gt;

Ε

t

t

T

T


5.0

)

4

5.1

5.0

(

3

 

 

:

rate

the

of

estimate

initial

an 

obtain 

to

data

censored

Ignore

(0)

=

+

+

=

θ

5

2

3

1

3

)

3

|

(

 

of

 

 

estimate

 

initial

an 

Obtain 

3

2

1

1

1

)1

|

(

 

of

 

 

estimate

 

initial

an 

Obtain 

expectatio

 l

conditiona

by their 

 

data

 

censored

 

replace

 :

step

-

E

)

0

(

5

5

)

0

(

5

5

)

0

(

5

)

0

(

4

4

)

0

(

4

4

)

0

(

4

=

+

=

+

=

&gt;

Ε

=

=

+

=

+

=

&gt;

Ε

=

θ

θ

T

T

t

t

t

T

T

t

t

t


8.5

1

3

)

3

|

(

 

of

 

 

estimate

 

improved

Obtain 

8.3

1

1

)1

|

(

 

of

 

 

estimate

 

improved

Obtain 

3571

.0

)

5

3

4

5.1

5.0

(

5

:

rate

 

 the

of

 

estimate

 

Update

data'

 

real

' 

as

 5

 

and

 3

 treat 

:

step

-

M

)

1

(

5

5

)

1

(

5

5

)

1

(

5

)

1

(

4

4

)

1

(

4

4

)

1

(

4

(1)

)

0

(

5

)

0

(

4

=

+

=

&gt;

Ε

=

=

+

=

&gt;

Ε

=

=

+

+

+

+

=

=

=

θ

θ

θ

T

T

t

t

t

T

T

t

t

t

t

t


step

θ(m)

0

0.5

1

0.3571

2

0.3205

3

0.3079

4

0.3031

5

0.3012

6

0.3005

7

0.3002

8

0.3001


• Mixture of two univariate normals (Old Faithful’s eruptions)

step

pi

mu[1]

mu[2]

sigma2[1]

sigma2[2]

0

0.35

2

4.3

0.1

0.2

1

0.34920810

2.02049523

4.27511436

0.05694792

0.18870948

2

0.34889072

2.01974514

4.27441727

0.05637577

0.18961652

3

0.34869541

2.01928712

4.27398638

0.05602933

0.19018038

4

0.34857750

2.01901127

4.27372586

0.05582123

0.19052192

5

0.34850700

2.01884660

4.27357000

0.05569720

0.19072650

6

0.34846512

2.01874885

4.27347731

0.05562365

0.19084823

7

0.34844032

2.01869101

4.27342243

0.05558015

0.19092034

8

0.34842567

2.01865686

4.27339000

0.05555447

0.19096296

9

0.34841703

2.01863671

4.27337087

0.05553933

0.19098811

10

0.34841194

2.01862484

4.27335959

0.05553041

0.19100294

11

0.34840893

2.01861784

4.27335294

0.05552515

0.19101167

12

0.34840717

2.01861372

4.27334903

0.05552205

0.19101682

13

0.34840613

2.01861129

4.27334672

0.05552023

0.19101985

14

0.34840551

2.01860986

4.27334537

0.05551916

0.19102164

15

0.34840515

2.01860902

4.27334457

0.05551852

0.19102269

16

0.34840494

2.01860853

4.27334410

0.05551815

0.19102331

17

0.34840481

2.01860823

4.27334382

0.05551793

0.19102367

18

0.34840470

2.01860810

4.27334370

0.05551780

0.19102390

19

0.34840470

2.01860796

4.27334356

0.05551773

0.19102401

20

0.34840467

2.01860790

4.27334350

0.05551768

0.19102409


faithful$eruptions

1

2

3

4

5

6

0.0

0.2

0.4

0.6

0.8

Normal mixture

Kernel density


EM ALGORITHM FOR THE REGULAR 

EM ALGORITHM FOR THE REGULAR 

EXPONENTIAL FAMILY

EXPONENTIAL FAMILY

)

(

))

(

exp(

)

(

)

;

(

 

 

 

from

 

(wlog)

 d

distribute

 

be

  )

,

(

Let 

θ

θ

θ

a

X

t

X

b

X

g

Z

Y

X

T

C

T

T

T

=

=

[

]

say

 

,

;

|)

(

 

 

 

 

of

 

computing

 

 the

requires

 

step

-

E

)

(

)

(

m

m

t

Y

X

t

=

Ε

θ

[

]

)

(

|)

(

 

 

 

 

solving

 

requires

 

step

-

M

m

t

X

t

=

Ε

θ


EM ALGORITHM FOR THE FINITE MIXTURE 

EM ALGORITHM FOR THE FINITE MIXTURE 

PROBLEM

PROBLEM

Let XT = (YT, ZT) be the complete data vector. Y is the 

observed data vector and Z the unobserved data vector

The observed likelihood is 

∏∑

=

=

=

n

i

k

j

j

i

j

j

y

g

y

L

1

1

)

;

(

 

)

|

(

ψ

π

θ

which is difficult to maximise


compo

pth 

 

{

1

1

 

and

 )

,

,

(

 

 where

),

,

,

(

 

Define

∈

Ι

=

=

=

iy

ip

ik

i

T

i

T

n

T

T

z

z

z

z

z

z

Z

…

…

ij

ij

z

j

i

j

n

i

k

j

z

j

C

y

g

x

L

)

;

(

)

|

(

1

1

ψ

π

θ

∏∏

=

=

=

Thus,

and

{

}

∑

=

+

=

=

n

i

i

T

i

C

C

u

v

z

x

L

x

l

1

)

(

)

(

)

|

(

log

)

|

(

ψ

π

θ

θ


where

)

;

(

log

,

),

;

(

(log

)

(

)

log

,

,

(log

)

(

 

1

1

1

k

i

k

i

T

i

k

T

y

g

y

g

u

v

ψ

ψ

ψ

π

π

π

…

…

=

=

In the E-step, we compute

∑

∑

=

=

+

=

n

i

i

T

m

i

n

i

T

m

i

m

u

w

v

w

Q

1

)

(

1

)

(

)

(

(

)

(

)

(

)

(

)

,

(

ψ

θ

π

θ

θ

θ

where

[

]

)

(

;

|

)

(

m

i

i

i

y

z

w

θ

θ

Ε

=


∑

=

=

k

j

m

j

i

j

m

j

m

j

i

j

m

j

m

ij

y

g

y

g

w

1

)

(

)

(

)

(

)

(

)

(

)

;

(

)

;

(

)

(

ψ

π

ψ

π

θ

and

In the M-step, we simply maximise Q(θ, θ(m))


PROPERTIES OF THE EM ALGORITHM

PROPERTIES OF THE EM ALGORITHM

• Stability/Monotonicity

• Under suitable regularity conditions, if  θ(m) ’s converge 

then they converge to a stationary point of l(θ

 

| y)

• EM algorithm converges at a linear rate, with the rate 

depending on the proportion of information about θ

 

in the 

observed density 


STANDARD ERRORS OF PARAMETERS

STANDARD ERRORS OF PARAMETERS

Louis (1982) showed that

θ

θ

θ

θ

θ

θ

θ

θ

θ

θ

θ

θ

θ

ˆ

ˆ

;

|

)

|

(

log

cov

)

;ˆ

(

ˆ

;

|

)

|

(

log

)

|

(

log

)

;ˆ

(

)

;ˆ

(

=

⎭⎬⎫

⎩⎨⎧

∂

∂

−

=

⎦

⎤

⎢⎣

⎡

∂

∂

∂

∂

Ε

−

=

Ι

y

x

L

y

I

y

x

L

x

L

y

I

y

C

C

T

C

C

C

Invert to get approximate covariance matrix for the parameter 

estimates


Returning to Example 1 (Genetic Linkage),

2

3

2

2

4

2

2

3

2

4

2

)

1(

)

(

)

|

(

log

)

1(

)

|

(

log

θ

θ

θ

θ

θ

θ

θ

θ

θ

−

+

+

+

=

∂

∂

∂

−

−

+

−

+

=

∂

∂

y

y

y

x

x

L

y

y

y

x

x

L

T

C

C

Therefore,

5.

435

)ˆ

1(

ˆ

2

ˆ

ˆ

125

)ˆ

1(

)

(

ˆ

]ˆ

;

|

[

)

;ˆ

(

2

3

2

2

4

2

3

2

2

4

2

=

−

+

+

+

+

=

−

+

+

+

Ε

=

θ

θ

θ

θ

θ

θ

θ

θ

y

y

y

y

y

y

y

x

y

I C


and

5

2

ˆ

2

2

ˆ

ˆ

ˆ

125

ˆ

)ˆ

;

|

var(

ˆ

;

|

)

|

(

log

cov

2

2

2

ˆ

=

⎟

⎠

⎞

⎜

⎝

⎛

+

⎟⎟

⎠

⎞

⎜⎜

⎝

⎛

+

=

=

⎭⎬⎫

⎩⎨⎧

∂

∂

=

θ

θ

θ

θ

θ

θ

θ

θ

θ

θ

θ

y

x

y

x

LC

Thus,

7.

377

8.

57

5.

435

)

;ˆ

(

=

−

=

Ι

y

θ

and the standard error of θˆ is equal to 

.

05

.0

7.

377

1

=

