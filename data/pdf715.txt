




Search models, datasets, users...







Transformers documentation

BERT

BERT

Join the Hugging Face community

and get access to the augmented documentation experience

Sign Up

Sign Up

to get started





All model pages

All model pages bert

bert



�� Hugging Face

�� Hugging Face Spaces

Spaces

The BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob

Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. Itʼs a bidirectional transformer pretrained using a combination of

masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and

Wikipedia.

The abstract from the paper is the following:

We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from

Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations

from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can

be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question

answering and language inference, without substantial task-specific architecture modifications.

BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language

processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6%

absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1

to 83.1 (5.1 point absolute improvement).

Tips:

BERT is a model with absolute position embeddings so itʼs usually advised to pad the inputs on the right rather than the left.

BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is efficient at

predicting masked tokens and at NLU in general, but is not optimal for text generation.

Corrupts the inputs by using random masking, more precisely, during pretraining, a given percentage of tokens (usually 15%) is

masked by:

a special mask token with probability 0.8

a random token different from the one masked with probability 0.1

the same token with probability 0.1

The model must predict the original sentence, but has a second objective: inputs are two sentences A and B (with a separation

token in between). With probability 50%, the sentences are consecutive in the corpus, in the remaining 50% they are not related.

The model has to predict if the sentences are consecutive or not.

This model was contributed by thomwolf. The original code can be found here.

A list of official Hugging Face and community (indicated by �) resources to help you get started with BERT. If youʼre interested

in submitting a resource to be included here, please feel free to open a Pull Request and weʼll review it! The resource should ideally

demonstrate something new instead of duplicating an existing resource.

Text Classification

A blog post on BERT Text Classification in a different language.

A notebook for Finetuning BERT (and friends) for multi-label text classification.

A notebook on how to Finetune BERT for multi-label classification using PyTorch. �

A notebook on how to warm-start an EncoderDecoder model with BERT for summarization.

BertForSequenceClassification is supported by this example script and notebook.

TFBertForSequenceClassification is supported by this example script and notebook.

FlaxBertForSequenceClassification is supported by this example script and notebook.

Text classification task guide

Token Classification

A blog post on how to use Hugging Face Transformers with Keras: Fine-tune a non-English BERT for Named Entity Recognition.

A notebook for Finetuning BERT for named-entity recognition using only the first wordpiece of each word in the word label during

tokenization. To propagate the label of the word to all wordpieces, see this version of the notebook instead.

BertForTokenClassification is supported by this example script and notebook.

TFBertForTokenClassification is supported by this example script and notebook.

FlaxBertForTokenClassification is supported by this example script.

Token classification chapter of the � Hugging Face Course.

BERT

Overview

Resources


Token classification task guide

Fill-Mask

BertForMaskedLM is supported by this example script and notebook.

TFBertForMaskedLM is supported by this example script and notebook.

FlaxBertForMaskedLM is supported by this example script and notebook.

Masked language modeling chapter of the � Hugging Face Course.

Masked language modeling task guide

Question Answering

BertForQuestionAnswering is supported by this example script and notebook.

TFBertForQuestionAnswering is supported by this example script and notebook.

FlaxBertForQuestionAnswering is supported by this example script.

Question answering chapter of the � Hugging Face Course.

Question answering task guide

Multiple choice

Multiple choice

BertForMultipleChoice is supported by this example script and notebook.

TFBertForMultipleChoice is supported by this example script and notebook.

Multiple choice task guide

�� Inference

Inference

A blog post on how to Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia.

A blog post on how to Accelerate BERT inference with DeepSpeed-Inference on GPUs.

�� Pretraining

Pretraining

A blog post on Pre-Training BERT with Hugging Face Transformers and Habana Gaudi.

� Deploy

Deploy

A blog post on how to Convert Transformers to ONNX with Hugging Face Optimum.

A blog post on how to Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on AWS.

A blog post on Autoscaling BERT with Hugging Face Transformers, Amazon SageMaker and Terraform module.

A blog post on Serverless BERT with HuggingFace, AWS Lambda, and Docker.

A blog post on Hugging Face Transformers BERT fine-tuning using Amazon SageMaker and Training Compiler.

A blog post on Task-specific knowledge distillation for BERT using Transformers &amp; Amazon SageMaker.



class transformers.BertConfig

BertConfig

&lt; &gt;

( vocab_size

 , hidden_size

 , num_hidden_layers

 , num_attention_heads

 , intermediate_size

 ,

hidden_act

 , hidden_dropout_prob

 , attention_probs_dropout_prob

 , max_position_embeddings

 ,

type_vocab_size

 , initializer_range

 , layer_norm_eps

 , pad_token_id

 , position_embedding_type

 , use_cache

 , classifier_dropout

 , **kwargs )

BertConfig



































Parameters

Parameters

vocab_size

vocab_size (int, optional, defaults to 30522) — Vocabulary size of the BERT model. Defines the number of different tokens that

can be represented by the inputs_ids passed when calling BertModel or TFBertModel.

hidden_size

hidden_size (int, optional, defaults to 768) — Dimensionality of the encoder layers and the pooler layer.

num_hidden_layers

num_hidden_layers (int, optional, defaults to 12) — Number of hidden layers in the Transformer encoder.

num_attention_heads

num_attention_heads (int, optional, defaults to 12) — Number of attention heads for each attention layer in the Transformer

encoder.

intermediate_size

intermediate_size (int, optional, defaults to 3072) — Dimensionality of the “intermediate” (often named feed-forward) layer in

the Transformer encoder.

hidden_act

hidden_act (str or Callable, optional, defaults to "gelu") — The non-linear activation function (function or string) in the

encoder and pooler. If string, "gelu", "relu", "silu" and "gelu_new" are supported.

hidden_dropout_prob

hidden_dropout_prob (float, optional, defaults to 0.1) — The dropout probability for all fully connected layers in the

embeddings, encoder, and pooler.

attention_probs_dropout_prob

attention_probs_dropout_prob (float, optional, defaults to 0.1) — The dropout ratio for the attention probabilities.

max_position_embeddings

max_position_embeddings (int, optional, defaults to 512) — The maximum sequence length that this model might ever be

used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).

type_vocab_size

type_vocab_size (int, optional, defaults to 2) — The vocabulary size of the token_type_ids passed when calling BertModel or

TFBertModel.


This is the configuration class to store the configuration of a BertModel or a TFBertModel. It is used to instantiate a BERT

model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults

will yield a similar configuration to that of the BERT bert-base-uncased architecture.

Configuration objects inherit from PretrainedConfig and can be used to control the model outputs. Read the documentation

from PretrainedConfig for more information.



class transformers.BertTokenizer

BertTokenizer

&lt; &gt;

( vocab_file , do_lower_case

 , do_basic_tokenize

 , never_split

 , unk_token

 , sep_token

 , pad_token

 , cls_token

 , mask_token

 , tokenize_chinese_chars

 , strip_accents

 , **kwargs )

initializer_range

initializer_range (float, optional, defaults to 0.02) — The standard deviation of the truncated_normal_initializer for initializing

all weight matrices.

layer_norm_eps

layer_norm_eps (float, optional, defaults to 1e-12) — The epsilon used by the layer normalization layers.

position_embedding_type

position_embedding_type (str, optional, defaults to "absolute") — Type of position embedding. Choose one of "absolute",

"relative_key", "relative_key_query". For positional embeddings use "absolute". For more information on

"relative_key", please refer to Self-Attention with Relative Position Representations (Shaw et al.). For more information on

"relative_key_query", please refer to Method 4 in Improve Transformer Models with Better Relative Position Embeddings

(Huang et al.).

is_decoder

is_decoder (bool, optional, defaults to False) — Whether the model is used as a decoder or not. If False, the model is used as

an encoder.

use_cache

use_cache (bool, optional, defaults to True) — Whether or not the model should return the last key/values attentions (not used

by all models). Only relevant if config.is_decoder=True.

classifier_dropout

classifier_dropout (float, optional) — The dropout ratio for the classification head.

Examples:

&gt;&gt;&gt; from transformers import BertConfig, BertModel

&gt;&gt;&gt; # Initializing a BERT bert-base-uncased style configuration

&gt;&gt;&gt; configuration = BertConfig()

&gt;&gt;&gt; # Initializing a model (with random weights) from the bert-base-uncased style configuration

&gt;&gt;&gt; model = BertModel(configuration)

&gt;&gt;&gt; # Accessing the model configuration

&gt;&gt;&gt; configuration = model.config

BertTokenizer

























Parameters

Parameters

vocab_file

vocab_file (str) — File containing the vocabulary.

do_lower_case

do_lower_case (bool, optional, defaults to True) — Whether or not to lowercase the input when tokenizing.

do_basic_tokenize

do_basic_tokenize (bool, optional, defaults to True) — Whether or not to do basic tokenization before WordPiece.

never_split

never_split (Iterable, optional) — Collection of tokens which will never be split during tokenization. Only has an effect when

do_basic_tokenize=True

unk_token

unk_token (str, optional, defaults to "[UNK]") — The unknown token. A token that is not in the vocabulary cannot be

converted to an ID and is set to be this token instead.

sep_token

sep_token (str, optional, defaults to "[SEP]") — The separator token, which is used when building a sequence from multiple

sequences, e.g. two sequences for sequence classification or for a text and a question for question answering. It is also used as

the last token of a sequence built with special tokens.

pad_token

pad_token (str, optional, defaults to "[PAD]") — The token used for padding, for example when batching sequences of

different lengths.

cls_token

cls_token (str, optional, defaults to "[CLS]") — The classifier token which is used when doing sequence classification

(classification of the whole sequence instead of per-token classification). It is the first token of the sequence when built with

special tokens.

mask_token

mask_token (str, optional, defaults to "[MASK]") — The token used for masking values. This is the token used when training


Construct a BERT tokenizer. Based on WordPiece.

This tokenizer inherits from PreTrainedTokenizer which contains most of the main methods. Users should refer to this

superclass for more information regarding those methods.

build_inputs_with_special_tokens

&lt; &gt;

( token_ids_0

 , token_ids_1

 ) → List[int]

Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and adding

special tokens. A BERT sequence has the following format:

get_special_tokens_mask

&lt; &gt;

( token_ids_0

 , token_ids_1

 , already_has_special_tokens

 ) → List[int]

Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding special tokens

using the tokenizer prepare_for_model method.

create_token_type_ids_from_sequences

&lt; &gt;

( token_ids_0

 , token_ids_1

 ) → List[int]

this model with masked language modeling. This is the token which the model will try to predict.

tokenize_chinese_chars

tokenize_chinese_chars (bool, optional, defaults to True) — Whether or not to tokenize Chinese characters.

This should likely be deactivated for Japanese (see this issue).

strip_accents

strip_accents (bool, optional) — Whether or not to strip all accents. If this option is not specified, then it will be determined by

the value for lowercase (as in the original BERT).





Parameters

Parameters

Returns

Returns

List[int]

List of input IDs with the appropriate special tokens.

token_ids_0

token_ids_0 (List[int]) — List of IDs to which the special tokens will be added.

token_ids_1

token_ids_1 (List[int], optional) — Optional second list of IDs for sequence pairs.

single sequence: [CLS] X [SEP]

pair of sequences: [CLS] A [SEP] B [SEP]







Parameters

Parameters

Returns

Returns

List[int]

A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.

token_ids_0

token_ids_0 (List[int]) — List of IDs.

token_ids_1

token_ids_1 (List[int], optional) — Optional second list of IDs for sequence pairs.

already_has_special_tokens

already_has_special_tokens (bool, optional, defaults to False) — Whether or not the token list is already formatted with

special tokens for the model.





Parameters

Parameters

token_ids_0

token_ids_0 (List[int]) — List of IDs.

token_ids_1

token_ids_1 (List[int], optional) — Optional second list of IDs for sequence pairs.


Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence

If token_ids_1 is None, this method only returns the first portion of the mask (0s).

save_vocabulary

&lt; &gt;

( save_directory

 , filename_prefix

 )



class transformers.BertTokenizerFast

BertTokenizerFast

&lt; &gt;

( vocab_file

 , tokenizer_file

 , do_lower_case

 , unk_token

 , sep_token

 , pad_token

 , cls_token

 , mask_token

 , tokenize_chinese_chars

 , strip_accents

 , **kwargs )

Construct a “fast” BERT tokenizer (backed by HuggingFaceʼs tokenizers library). Based on WordPiece.

This tokenizer inherits from PreTrainedTokenizerFast which contains most of the main methods. Users should refer to this

superclass for more information regarding those methods.

build_inputs_with_special_tokens

Returns

Returns

List[int]

List of token type IDs according to the given sequence(s).

pair mask has the following format:

0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1

| first sequence    | second sequence |





BertTokenizerFast























Parameters

Parameters

vocab_file

vocab_file (str) — File containing the vocabulary.

do_lower_case

do_lower_case (bool, optional, defaults to True) — Whether or not to lowercase the input when tokenizing.

unk_token

unk_token (str, optional, defaults to "[UNK]") — The unknown token. A token that is not in the vocabulary cannot be

converted to an ID and is set to be this token instead.

sep_token

sep_token (str, optional, defaults to "[SEP]") — The separator token, which is used when building a sequence from multiple

sequences, e.g. two sequences for sequence classification or for a text and a question for question answering. It is also used as

the last token of a sequence built with special tokens.

pad_token

pad_token (str, optional, defaults to "[PAD]") — The token used for padding, for example when batching sequences of

different lengths.

cls_token

cls_token (str, optional, defaults to "[CLS]") — The classifier token which is used when doing sequence classification

(classification of the whole sequence instead of per-token classification). It is the first token of the sequence when built with

special tokens.

mask_token

mask_token (str, optional, defaults to "[MASK]") — The token used for masking values. This is the token used when training

this model with masked language modeling. This is the token which the model will try to predict.

clean_text

clean_text (bool, optional, defaults to True) — Whether or not to clean the text before tokenization by removing any control

characters and replacing all whitespaces by the classic one.

tokenize_chinese_chars

tokenize_chinese_chars (bool, optional, defaults to True) — Whether or not to tokenize Chinese characters. This should likely

be deactivated for Japanese (see this issue).

strip_accents

strip_accents (bool, optional) — Whether or not to strip all accents. If this option is not specified, then it will be determined by

the value for lowercase (as in the original BERT).

wordpieces_prefix

wordpieces_prefix (str, optional, defaults to "##") — The prefix for subwords.


&lt; &gt;

( token_ids_0 , token_ids_1

 ) → List[int]

Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and adding

special tokens. A BERT sequence has the following format:

create_token_type_ids_from_sequences

&lt; &gt;

( token_ids_0

 , token_ids_1

 ) → List[int]

Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence

If token_ids_1 is None, this method only returns the first portion of the mask (0s).



class transformers.TFBertTokenizer

TFBertTokenizer

&lt; &gt;

( *args , **kwargs )





Parameters

Parameters

Returns

Returns

List[int]

List of input IDs with the appropriate special tokens.

token_ids_0

token_ids_0 (List[int]) — List of IDs to which the special tokens will be added.

token_ids_1

token_ids_1 (List[int], optional) — Optional second list of IDs for sequence pairs.

single sequence: [CLS] X [SEP]

pair of sequences: [CLS] A [SEP] B [SEP]





Parameters

Parameters

Returns

Returns

List[int]

List of token type IDs according to the given sequence(s).

token_ids_0

token_ids_0 (List[int]) — List of IDs.

token_ids_1

token_ids_1 (List[int], optional) — Optional second list of IDs for sequence pairs.

pair mask has the following format:

0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1

| first sequence    | second sequence |

TFBertTokenizer





Parameters

Parameters

vocab_list

vocab_list (list) — List containing the vocabulary.

do_lower_case

do_lower_case (bool, optional, defaults to True) — Whether or not to lowercase the input when tokenizing.

cls_token_id

cls_token_id (str, optional, defaults to "[CLS]") — The classifier token which is used when doing sequence classification

(classification of the whole sequence instead of per-token classification). It is the first token of the sequence when built with

special tokens.

sep_token_id

sep_token_id (str, optional, defaults to "[SEP]") — The separator token, which is used when building a sequence from

multiple sequences, e.g. two sequences for sequence classification or for a text and a question for question answering. It is

also used as the last token of a sequence built with special tokens.

pad_token_id

pad_token_id (str, optional, defaults to "[PAD]") — The token used for padding, for example when batching sequences of

different lengths.

padding

padding (str, defaults to "longest") — The type of padding to use. Can be either "longest", to pad only up to the longest

sample in the batch, or `“max_length”, to pad all inputs to the maximum length supported by the tokenizer.


This is an in-graph tokenizer for BERT. It should be initialized similarly to other tokenizers, using the from_pretrained()

method. It can also be initialized with the from_tokenizer() method, which imports settings from an existing standard

tokenizer object.

In-graph tokenizers, unlike other Hugging Face tokenizers, are actually Keras layers and are designed to be run when the model

is called, rather than during preprocessing. As a result, they have somewhat more limited options than standard tokenizer

classes. They are most useful when you want to create an end-to-end model that goes straight from tf.string inputs to

outputs.

from_pretrained

&lt; &gt;

( pretrained_model_name_or_path

 , *init_inputs , **kwargs )

Instantiate a TFBertTokenizer from a pre-trained tokenizer.

from_tokenizer

&lt; &gt;

( tokenizer

 , **kwargs )

Initialize a TFBertTokenizer from an existing Tokenizer.



class transformers.models.bert.modeling_bert.BertForPreTrainingOutput

BertForPreTrainingOutput

&lt; &gt;

( loss

 , prediction_logits

 , seq_relationship_logits

 , hidden_states

 , attentions

truncation

truncation (bool, optional, defaults to True) — Whether to truncate the sequence to the maximum length.

max_length

max_length (int, optional, defaults to 512) — The maximum length of the sequence, used for padding (if padding is

“max_length”) and/or truncation (if truncation is True).

pad_to_multiple_of

pad_to_multiple_of (int, optional, defaults to None) — If set, the sequence will be padded to a multiple of this value.

return_token_type_ids

return_token_type_ids (bool, optional, defaults to True) — Whether to return token_type_ids.

return_attention_mask

return_attention_mask (bool, optional, defaults to True) — Whether to return the attention_mask.

use_fast_bert_tokenizer

use_fast_bert_tokenizer (bool, optional, defaults to True) — If set to false will use standard TF Text BertTokenizer, making it

servable by TF Serving.







Parameters

Parameters

pretrained_model_name_or_path

pretrained_model_name_or_path (str or os.PathLike) — The name or path to the pre-trained tokenizer.

Examples:

from transformers import TFBertTokenizer

tf_tokenizer = TFBertTokenizer.from_pretrained("bert-base-uncased")





Parameters

Parameters

tokenizer

tokenizer (PreTrainedTokenizerBase) — The tokenizer to use to initialize the TFBertTokenizer.

Examples:

from transformers import AutoTokenizer, TFBertTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

tf_tokenizer = TFBertTokenizer.from_tokenizer(tokenizer)

Bert specific outputs












 )

Output type of BertForPreTraining.



class transformers.models.bert.modeling_tf_bert.TFBertForPreTrainingOutput

TFBertForPreTrainingOutput

&lt; &gt;

( loss

 , prediction_logits

 ,

seq_relationship_logits

 , hidden_states

 , attentions

 )

Output type of TFBertForPreTraining.



class transformers.models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput

FlaxBertForPreTrainingOutput

&lt; &gt;

( prediction_logits

 , seq_relationship_logits

 , hidden_states

 , attentions

 )



Parameters

Parameters

loss

loss (optional, returned when labels is provided, torch.FloatTensor of shape (1,)) — Total loss as the sum of the masked

language modeling loss and the next sequence prediction (classification) loss.

prediction_logits

prediction_logits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size)) — Prediction

scores of the language modeling head (scores for each vocabulary token before SoftMax).

seq_relationship_logits

seq_relationship_logits (torch.FloatTensor of shape (batch_size, 2)) — Prediction scores of the next sequence prediction

(classification) head (scores of True/False continuation before SoftMax).

hidden_states

hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings + one for the

output of each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.











Parameters

Parameters

prediction_logits

prediction_logits (tf.Tensor of shape (batch_size, sequence_length, config.vocab_size)) — Prediction scores of the

language modeling head (scores for each vocabulary token before SoftMax).

seq_relationship_logits

seq_relationship_logits (tf.Tensor of shape (batch_size, 2)) — Prediction scores of the next sequence prediction

(classification) head (scores of True/False continuation before SoftMax).

hidden_states

hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.









Parameters

Parameters

prediction_logits

prediction_logits (jnp.ndarray of shape (batch_size, sequence_length, config.vocab_size)) — Prediction scores of the

language modeling head (scores for each vocabulary token before SoftMax).

seq_relationship_logits

seq_relationship_logits (jnp.ndarray of shape (batch_size, 2)) — Prediction scores of the next sequence prediction

(classification) head (scores of True/False continuation before SoftMax).

hidden_states

hidden_states (tuple(jnp.ndarray), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of jnp.ndarray (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).


Output type of BertForPreTraining.

replace

&lt; &gt;

( **updates )

“Returns a new object replacing the specified fields with new values.



class transformers.BertModel

BertModel

&lt; &gt;

( config , add_pooling_layer

 )

The bare Bert Model transformer outputting raw hidden-states without any specific head on top.

This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch

documentation for all matter related to general usage and behavior.

The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of cross-attention is

added between the self-attention layers, following the architecture described in Attention is all you need by Ashish Vaswani,

Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.

To behave as an decoder the model needs to be initialized with the is_decoder argument of the configuration set to True. To be

used in a Seq2Seq model, the model needs to initialized with both is_decoder argument and add_cross_attention set to True;

an encoder_hidden_states is then expected as an input to the forward pass.

forward

&lt; &gt;

( input_ids

 , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , encoder_hidden_states

 , encoder_attention_mask

 , past_key_values

 , use_cache

 , output_attentions

 , output_hidden_states

 , return_dict

 ) → transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(jnp.ndarray), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of jnp.ndarray (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.



BertModel





Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.



























Parameters

Parameters

input_ids

input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

1 for tokens that are not masked

not masked,


Returns

Returns

transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions

transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or 

 or tuple(torch.FloatTensor)

A transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or a tuple of torch.FloatTensor (if

return_dict=False is passed or when config.return_dict=False) comprising various elements depending on the

configuration (BertConfig) and inputs.

What are attention masks?

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Segment token indices to

indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

What are position IDs?

head_mask

head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) — Optionally,

instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you want more

control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

encoder_hidden_states

encoder_hidden_states (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) —

Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if the model is

configured as a decoder.

encoder_attention_mask

encoder_attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid

performing attention on the padding token indices of the encoder input. This mask is used in the cross-attention if the

model is configured as a decoder. Mask values selected in [0, 1]:

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

past_key_values

past_key_values (tuple(tuple(torch.FloatTensor)) of length config.n_layers with each tuple having 4 tensors of

shape (batch_size, num_heads, sequence_length - 1, embed_size_per_head)) — Contains precomputed key and

value hidden states of the attention blocks. Can be used to speed up decoding.

If past_key_values are used, the user can optionally input only the last decoder_input_ids (those that donʼt have their

past key value states given to this model) of shape (batch_size, 1) instead of all decoder_input_ids of shape

(batch_size, sequence_length).

use_cache

use_cache (bool, optional) — If set to True, past_key_values key value states are returned and can be used to speed up

decoding (see past_key_values).

last_hidden_state

last_hidden_state (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)) — Sequence of hidden-

states at the output of the last layer of the model.

pooler_output

pooler_output (torch.FloatTensor of shape (batch_size, hidden_size)) — Last layer hidden-state of the first token of the

sequence (classification token) after further processing through the layers used for the auxiliary pretraining task. E.g. for

BERT-family of models, this returns the classification token after processing through a linear layer and a tanh activation

function. The linear layer weights are trained from the next sentence prediction (classification) objective during pretraining.

hidden_states

hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has

an embedding layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.

attentions

attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

cross_attentions

cross_attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True and

config.add_cross_attention=True is passed or when config.output_attentions=True) — Tuple of torch.FloatTensor

(one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).

Attentions weights of the decoderʼs cross-attention layer, after the attention softmax, used to compute the weighted

average in the cross-attention heads.

past_key_values

past_key_values (tuple(tuple(torch.FloatTensor)), optional, returned when use_cache=True is passed or when

config.use_cache=True) — Tuple of tuple(torch.FloatTensor) of length config.n_layers, with each tuple having 2

tensors of shape (batch_size, num_heads, sequence_length, embed_size_per_head)) and optionally if


The BertModel forward method, overrides the __call__ special method.



class transformers.BertForPreTraining

BertForPreTraining

&lt; &gt;

( config )

Bert Model with two heads on top as done during the pretraining: a masked language modeling head and a next sentence

prediction (classification) head.

This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch

documentation for all matter related to general usage and behavior.

forward

&lt; &gt;

( input_ids

 , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , labels

 , next_sentence_label

 , output_attentions

 , output_hidden_states

 , return_dict

 ) → transformers.models.bert.modeling_bert.BertForPreTrainingOutput or tuple(torch.FloatTensor)

config.is_encoder_decoder=True 2 additional tensors of shape (batch_size, num_heads, encoder_sequence_length,

embed_size_per_head).

Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if

config.is_encoder_decoder=True in the cross-attention blocks) that can be used (see past_key_values input) to speed up

sequential decoding.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, BertModel

&gt;&gt;&gt; import torch

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = BertModel.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

&gt;&gt;&gt; outputs = model(**inputs)

&gt;&gt;&gt; last_hidden_states = outputs.last_hidden_state

BertForPreTraining



Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.























Parameters

Parameters

input_ids

input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.


The BertForPreTraining forward method, overrides the __call__ special method.

Returns

Returns

transformers.models.bert.modeling_bert.BertForPreTrainingOutput

transformers.models.bert.modeling_bert.BertForPreTrainingOutput or 

 or tuple(torch.FloatTensor)

A transformers.models.bert.modeling_bert.BertForPreTrainingOutput or a tuple of torch.FloatTensor (if

return_dict=False is passed or when config.return_dict=False) comprising various elements depending on the

configuration (BertConfig) and inputs.

What are attention masks?

token_type_ids

token_type_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Segment token indices to

indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

What are position IDs?

head_mask

head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) — Optionally,

instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you want more

control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

labels (torch.LongTensor of shape (batch_size, sequence_length), optional): Labels for computing the masked

language modeling loss. Indices should be in [-100, 0, ..., config.vocab_size] (see input_ids docstring) Tokens

with indices set to -100 are ignored (masked), the loss is only computed for the tokens with labels in [0, ...,

config.vocab_size] next_sentence_label (torch.LongTensor of shape (batch_size,), optional): Labels for computing

the next sequence prediction (classification) loss. Input should be a sequence pair (see input_ids docstring) Indices should

be in [0, 1]:

0 indicates sequence B is a continuation of sequence A,

1 indicates sequence B is a random sequence. kwargs (Dict[str, any], optional, defaults to {}): Used to hide legacy

arguments that have been deprecated.

loss

loss (optional, returned when labels is provided, torch.FloatTensor of shape (1,)) — Total loss as the sum of the masked

language modeling loss and the next sequence prediction (classification) loss.

prediction_logits

prediction_logits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size)) — Prediction

scores of the language modeling head (scores for each vocabulary token before SoftMax).

seq_relationship_logits

seq_relationship_logits (torch.FloatTensor of shape (batch_size, 2)) — Prediction scores of the next sequence

prediction (classification) head (scores of True/False continuation before SoftMax).

hidden_states

hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings + one for the

output of each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, BertForPreTraining

&gt;&gt;&gt; import torch

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = BertForPreTraining.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

&gt;&gt;&gt; outputs = model(**inputs)

&gt;&gt;&gt; prediction_logits = outputs.prediction_logits




class transformers.BertLMHeadModel

BertLMHeadModel

&lt; &gt;

( config )

Bert Model with a language modeling head on top for CLM fine-tuning.

This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch

documentation for all matter related to general usage and behavior.

forward

&lt; &gt;

( input_ids

 , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , encoder_hidden_states

 , encoder_attention_mask

 , labels

 , past_key_values

 , use_cache

 , output_attentions

 , output_hidden_states

 , return_dict

 ) →

transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)

&gt;&gt;&gt; seq_relationship_logits = outputs.seq_relationship_logits

BertLMHeadModel



Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.





























Parameters

Parameters

input_ids

input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Segment token indices to

indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

What are position IDs?

head_mask

head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) — Optionally,

instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you want more

control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under


The BertLMHeadModel forward method, overrides the __call__ special method.

Returns

Returns

transformers.modeling_outputs.CausalLMOutputWithCrossAttentions

transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or 

 or tuple(torch.FloatTensor)

A transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or a tuple of torch.FloatTensor (if

return_dict=False is passed or when config.return_dict=False) comprising various elements depending on the

configuration (BertConfig) and inputs.

returned tensors for more detail.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

encoder_hidden_states

encoder_hidden_states (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) —

Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if the model is

configured as a decoder.

encoder_attention_mask

encoder_attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid

performing attention on the padding token indices of the encoder input. This mask is used in the cross-attention if the

model is configured as a decoder. Mask values selected in [0, 1]:

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

labels

labels (torch.LongTensor of shape (batch_size, sequence_length), optional) — Labels for computing the left-to-right

language modeling loss (next word prediction). Indices should be in [-100, 0, ..., config.vocab_size] (see input_ids

docstring) Tokens with indices set to -100 are ignored (masked), the loss is only computed for the tokens with labels n [0,

..., config.vocab_size]

past_key_values

past_key_values (tuple(tuple(torch.FloatTensor)) of length config.n_layers with each tuple having 4 tensors of

shape (batch_size, num_heads, sequence_length - 1, embed_size_per_head)) — Contains precomputed key and

value hidden states of the attention blocks. Can be used to speed up decoding.

If past_key_values are used, the user can optionally input only the last decoder_input_ids (those that donʼt have their

past key value states given to this model) of shape (batch_size, 1) instead of all decoder_input_ids of shape

(batch_size, sequence_length).

use_cache

use_cache (bool, optional) — If set to True, past_key_values key value states are returned and can be used to speed up

decoding (see past_key_values).

loss

loss (torch.FloatTensor of shape (1,), optional, returned when labels is provided) — Language modeling loss (for next-

token prediction).

logits

logits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size)) — Prediction scores of the

language modeling head (scores for each vocabulary token before SoftMax).

hidden_states

hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has

an embedding layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.

attentions

attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

cross_attentions

cross_attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Cross attentions weights after the attention softmax, used to compute the weighted average in the cross-attention heads.

past_key_values

past_key_values (tuple(tuple(torch.FloatTensor)), optional, returned when use_cache=True is passed or when

config.use_cache=True) — Tuple of torch.FloatTensor tuples of length config.n_layers, with each tuple containing the

cached key, value states of the self-attention and the cross-attention layers if model is used in encoder-decoder setting. Only

relevant if config.is_decoder = True.

Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see past_key_values input)

to speed up sequential decoding.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; import torch

&gt;&gt;&gt; from transformers import AutoTokenizer, BertLMHeadModel

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = BertLMHeadModel.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

&gt;&gt;&gt; outputs = model(**inputs, labels=inputs["input_ids"])

&gt;&gt;&gt; loss = outputs.loss

&gt;&gt;&gt; logits = outputs.logits




class transformers.BertForMaskedLM

BertForMaskedLM

&lt; &gt;

( config )

Bert Model with a language modeling head on top.

This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch

documentation for all matter related to general usage and behavior.

forward

&lt; &gt;

( input_ids

 , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , encoder_hidden_states

 , encoder_attention_mask

 , labels

 , output_attentions

 , output_hidden_states

 , return_dict

 ) → transformers.modeling_outputs.MaskedLMOutput

or tuple(torch.FloatTensor)

BertForMaskedLM



Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

























Parameters

Parameters

input_ids

input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Segment token indices to

indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

What are position IDs?

head_mask

head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) — Optionally,

instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you want more

control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.


The BertForMaskedLM forward method, overrides the __call__ special method.



class transformers.BertForNextSentencePrediction

BertForNextSentencePrediction

&lt; &gt;

( config )

Returns

Returns

transformers.modeling_outputs.MaskedLMOutput

transformers.modeling_outputs.MaskedLMOutput or 

 or tuple(torch.FloatTensor)

A transformers.modeling_outputs.MaskedLMOutput or a tuple of torch.FloatTensor (if return_dict=False is passed or

when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and inputs.

labels

labels (torch.LongTensor of shape (batch_size, sequence_length), optional) — Labels for computing the masked

language modeling loss. Indices should be in [-100, 0, ..., config.vocab_size] (see input_ids docstring) Tokens

with indices set to -100 are ignored (masked), the loss is only computed for the tokens with labels in [0, ...,

config.vocab_size]

loss

loss (torch.FloatTensor of shape (1,), optional, returned when labels is provided) — Masked language modeling (MLM)

loss.

logits

logits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size)) — Prediction scores of the

language modeling head (scores for each vocabulary token before SoftMax).

hidden_states

hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has

an embedding layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.

attentions

attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, BertForMaskedLM

&gt;&gt;&gt; import torch

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = BertForMaskedLM.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")

&gt;&gt;&gt; with torch.no_grad():

...     logits = model(**inputs).logits

&gt;&gt;&gt; # retrieve index of [MASK]

&gt;&gt;&gt; mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

&gt;&gt;&gt; predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)

&gt;&gt;&gt; tokenizer.decode(predicted_token_id)

'paris'

&gt;&gt;&gt; labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

&gt;&gt;&gt; # mask labels of non-[MASK] tokens

&gt;&gt;&gt; labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

&gt;&gt;&gt; outputs = model(**inputs, labels=labels)

&gt;&gt;&gt; round(outputs.loss.item(), 2)

0.88

BertForNextSentencePrediction



Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.


Bert Model with a next sentence prediction (classification) head on top.

This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch

documentation for all matter related to general usage and behavior.

forward

&lt; &gt;

( input_ids

 , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , labels

 , output_attentions

 , output_hidden_states

 , return_dict

 , **kwargs ) →

transformers.modeling_outputs.NextSentencePredictorOutput or tuple(torch.FloatTensor)























Parameters

Parameters

Returns

Returns

transformers.modeling_outputs.NextSentencePredictorOutput

transformers.modeling_outputs.NextSentencePredictorOutput or 

 or tuple(torch.FloatTensor)

A transformers.modeling_outputs.NextSentencePredictorOutput or a tuple of torch.FloatTensor (if return_dict=False is

passed or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and

inputs.

input_ids

input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Segment token indices to

indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

What are position IDs?

head_mask

head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) — Optionally,

instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you want more

control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

labels

labels (torch.LongTensor of shape (batch_size,), optional) — Labels for computing the next sequence prediction

(classification) loss. Input should be a sequence pair (see input_ids docstring). Indices should be in [0, 1]:

0 indicates sequence B is a continuation of sequence A,

1 indicates sequence B is a random sequence.

loss

loss (torch.FloatTensor of shape (1,), optional, returned when next_sentence_label is provided) — Next sequence

prediction (classification) loss.

logits

logits (torch.FloatTensor of shape (batch_size, 2)) — Prediction scores of the next sequence prediction (classification)

head (scores of True/False continuation before SoftMax).

hidden_states

hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has

an embedding layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.


The BertForNextSentencePrediction forward method, overrides the __call__ special method.



class transformers.BertForSequenceClassification

BertForSequenceClassification

&lt; &gt;

( config )

Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g.

for GLUE tasks.

This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch

documentation for all matter related to general usage and behavior.

forward

&lt; &gt;

( input_ids

 , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , labels

 , output_attentions

 , output_hidden_states

 , return_dict

 ) →

transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)

attentions

attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, BertForNextSentencePrediction

&gt;&gt;&gt; import torch

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = BertForNextSentencePrediction.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."

&gt;&gt;&gt; next_sentence = "The sky is blue due to the shorter wavelength of blue light."

&gt;&gt;&gt; encoding = tokenizer(prompt, next_sentence, return_tensors="pt")

&gt;&gt;&gt; outputs = model(**encoding, labels=torch.LongTensor([1]))

&gt;&gt;&gt; logits = outputs.logits

&gt;&gt;&gt; assert logits[0, 0] &lt; logits[0, 1]  # next sentence was random

BertForSequenceClassification



Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.





















Parameters

Parameters

input_ids

input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing


The BertForSequenceClassification forward method, overrides the __call__ special method.

Returns

Returns

transformers.modeling_outputs.SequenceClassifierOutput

transformers.modeling_outputs.SequenceClassifierOutput or 

 or tuple(torch.FloatTensor)

A transformers.modeling_outputs.SequenceClassifierOutput or a tuple of torch.FloatTensor (if return_dict=False is

passed or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and

inputs.

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Segment token indices to

indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

What are position IDs?

head_mask

head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) — Optionally,

instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you want more

control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

labels

labels (torch.LongTensor of shape (batch_size,), optional) — Labels for computing the sequence

classification/regression loss. Indices should be in [0, ..., config.num_labels - 1]. If config.num_labels == 1 a

regression loss is computed (Mean-Square loss), If config.num_labels &gt; 1 a classification loss is computed (Cross-

Entropy).

loss

loss (torch.FloatTensor of shape (1,), optional, returned when labels is provided) — Classification (or regression if

config.num_labels==1) loss.

logits

logits (torch.FloatTensor of shape (batch_size, config.num_labels)) — Classification (or regression if

config.num_labels==1) scores (before SoftMax).

hidden_states

hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has

an embedding layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.

attentions

attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example of single-label classification:

&gt;&gt;&gt; import torch

&gt;&gt;&gt; from transformers import AutoTokenizer, BertForSequenceClassification

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("textattack/bert-base-uncased-yelp-polarity")

&gt;&gt;&gt; model = BertForSequenceClassification.from_pretrained("textattack/bert-base-uncased-yelp-polarity")

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

&gt;&gt;&gt; with torch.no_grad():

...     logits = model(**inputs).logits

&gt;&gt;&gt; predicted_class_id = logits.argmax().item()

&gt;&gt;&gt; model.config.id2label[predicted_class_id]




class transformers.BertForMultipleChoice

BertForMultipleChoice

&lt; &gt;

( config )

Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for

RocStories/SWAG tasks.

This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch

documentation for all matter related to general usage and behavior.

forward

&lt; &gt;

( input_ids

 , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , labels

 , output_attentions

 , output_hidden_states

 , return_dict

 ) →

transformers.modeling_outputs.MultipleChoiceModelOutput or tuple(torch.FloatTensor)

'LABEL_1'

&gt;&gt;&gt; # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`

&gt;&gt;&gt; num_labels = len(model.config.id2label)

&gt;&gt;&gt; model = BertForSequenceClassification.from_pretrained("textattack/bert-base-uncased-yelp-polarity", num_labels=num_labels)

&gt;&gt;&gt; labels = torch.tensor([1])

&gt;&gt;&gt; loss = model(**inputs, labels=labels).loss

&gt;&gt;&gt; round(loss.item(), 2)

0.01

Example of multi-label classification:

&gt;&gt;&gt; import torch

&gt;&gt;&gt; from transformers import AutoTokenizer, BertForSequenceClassification

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("textattack/bert-base-uncased-yelp-polarity")

&gt;&gt;&gt; model = BertForSequenceClassification.from_pretrained("textattack/bert-base-uncased-yelp-polarity", problem_type="multi_label_c

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

&gt;&gt;&gt; with torch.no_grad():

...     logits = model(**inputs).logits

&gt;&gt;&gt; predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) &gt; 0.5]

&gt;&gt;&gt; # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`

&gt;&gt;&gt; num_labels = len(model.config.id2label)

&gt;&gt;&gt; model = BertForSequenceClassification.from_pretrained(

...  "textattack/bert-base-uncased-yelp-polarity", num_labels=num_labels, problem_type="multi_label_classification"

... )

&gt;&gt;&gt; labels = torch.sum(

...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1

... ).to(torch.float)

&gt;&gt;&gt; loss = model(**inputs, labels=labels).loss

BertForMultipleChoice



Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.





















Parameters

Parameters


The BertForMultipleChoice forward method, overrides the __call__ special method.

Returns

Returns

transformers.modeling_outputs.MultipleChoiceModelOutput

transformers.modeling_outputs.MultipleChoiceModelOutput or 

 or tuple(torch.FloatTensor)

A transformers.modeling_outputs.MultipleChoiceModelOutput or a tuple of torch.FloatTensor (if return_dict=False is

passed or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and

inputs.

input_ids

input_ids (torch.LongTensor of shape (batch_size, num_choices, sequence_length)) — Indices of input sequence

tokens in the vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (torch.FloatTensor of shape (batch_size, num_choices, sequence_length), optional) — Mask to

avoid performing attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (torch.LongTensor of shape (batch_size, num_choices, sequence_length), optional) — Segment

token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (torch.LongTensor of shape (batch_size, num_choices, sequence_length), optional) — Indices of

positions of each input sequence tokens in the position embeddings. Selected in the range [0,

config.max_position_embeddings - 1].

What are position IDs?

head_mask

head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (torch.FloatTensor of shape (batch_size, num_choices, sequence_length, hidden_size), optional)

— Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if

you want more control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding

lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

labels

labels (torch.LongTensor of shape (batch_size,), optional) — Labels for computing the multiple choice classification

loss. Indices should be in [0, ..., num_choices-1] where num_choices is the size of the second dimension of the input

tensors. (See input_ids above)

loss

loss (torch.FloatTensor of shape (1,), optional, returned when labels is provided) — Classification loss.

logits

logits (torch.FloatTensor of shape (batch_size, num_choices)) — num_choices is the second dimension of the input

tensors. (see input_ids above).

Classification scores (before SoftMax).

hidden_states

hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has

an embedding layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.

attentions

attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, BertForMultipleChoice

&gt;&gt;&gt; import torch

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = BertForMultipleChoice.from_pretrained("bert-base-uncased")




class transformers.BertForTokenClassification

BertForTokenClassification

&lt; &gt;

( config )

Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-

Recognition (NER) tasks.

This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch

documentation for all matter related to general usage and behavior.

forward

&lt; &gt;

( input_ids

 , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , labels

 , output_attentions

 , output_hidden_states

 , return_dict

 ) →

transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)

&gt;&gt;&gt; prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."

&gt;&gt;&gt; choice0 = "It is eaten with a fork and a knife."

&gt;&gt;&gt; choice1 = "It is eaten while held in the hand."

&gt;&gt;&gt; labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

&gt;&gt;&gt; encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)

&gt;&gt;&gt; outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

&gt;&gt;&gt; # the linear classifier still needs to be trained

&gt;&gt;&gt; loss = outputs.loss

&gt;&gt;&gt; logits = outputs.logits

BertForTokenClassification



Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.





















Parameters

Parameters

input_ids

input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Segment token indices to

indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

What are position IDs?

head_mask

head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,


The BertForTokenClassification forward method, overrides the __call__ special method.



class transformers.BertForQuestionAnswering

BertForQuestionAnswering

Returns

Returns

transformers.modeling_outputs.TokenClassifierOutput

transformers.modeling_outputs.TokenClassifierOutput or 

 or tuple(torch.FloatTensor)

A transformers.modeling_outputs.TokenClassifierOutput or a tuple of torch.FloatTensor (if return_dict=False is passed

or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and inputs.

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) — Optionally,

instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you want more

control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

labels

labels (torch.LongTensor of shape (batch_size, sequence_length), optional) — Labels for computing the token

classification loss. Indices should be in [0, ..., config.num_labels - 1].

loss

loss (torch.FloatTensor of shape (1,), optional, returned when labels is provided) — Classification loss.

logits

logits (torch.FloatTensor of shape (batch_size, sequence_length, config.num_labels)) — Classification scores (before

SoftMax).

hidden_states

hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has

an embedding layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.

attentions

attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, BertForTokenClassification

&gt;&gt;&gt; import torch

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-large-cased-finetuned-conll03-english")

&gt;&gt;&gt; model = BertForTokenClassification.from_pretrained("dbmdz/bert-large-cased-finetuned-conll03-english")

&gt;&gt;&gt; inputs = tokenizer(

...  "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"

... )

&gt;&gt;&gt; with torch.no_grad():

...     logits = model(**inputs).logits

&gt;&gt;&gt; predicted_token_class_ids = logits.argmax(-1)

&gt;&gt;&gt; # Note that tokens are classified rather then input words which means that

&gt;&gt;&gt; # there might be more predicted token classes than words.

&gt;&gt;&gt; # Multiple token classes might account for the same word

&gt;&gt;&gt; predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]

&gt;&gt;&gt; predicted_tokens_classes

['O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'I-LOC', 'I-LOC'] 

&gt;&gt;&gt; labels = predicted_token_class_ids

&gt;&gt;&gt; loss = model(**inputs, labels=labels).loss

&gt;&gt;&gt; round(loss.item(), 2)

0.01

BertForQuestionAnswering


&lt; &gt;

( config )

Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of

the hidden-states output to compute span start logits and span end logits).

This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch

documentation for all matter related to general usage and behavior.

forward

&lt; &gt;

( input_ids

 , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , start_positions

 , end_positions

 , output_attentions

 , output_hidden_states

 , return_dict

 ) → transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)



Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.























Parameters

Parameters

input_ids

input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Segment token indices to

indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

What are position IDs?

head_mask

head_mask (torch.FloatTensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) — Optionally,

instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you want more

control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

start_positions

start_positions (torch.LongTensor of shape (batch_size,), optional) — Labels for position (index) of the start of the

labelled span for computing the token classification loss. Positions are clamped to the length of the sequence (

sequence_length). Position outside of the sequence are not taken into account for computing the loss.

end_positions

end_positions (torch.LongTensor of shape (batch_size,), optional) — Labels for position (index) of the end of the

labelled span for computing the token classification loss. Positions are clamped to the length of the sequence (


The BertForQuestionAnswering forward method, overrides the __call__ special method.



class transformers.TFBertModel

TFBertModel

&lt; &gt;

( *args , **kwargs )

Returns

Returns

transformers.modeling_outputs.QuestionAnsweringModelOutput

transformers.modeling_outputs.QuestionAnsweringModelOutput or 

 or tuple(torch.FloatTensor)

A transformers.modeling_outputs.QuestionAnsweringModelOutput or a tuple of torch.FloatTensor (if return_dict=False

is passed or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig)

and inputs.

sequence_length). Position outside of the sequence are not taken into account for computing the loss.

loss

loss (torch.FloatTensor of shape (1,), optional, returned when labels is provided) — Total span extraction loss is the sum

of a Cross-Entropy for the start and end positions.

start_logits

start_logits (torch.FloatTensor of shape (batch_size, sequence_length)) — Span-start scores (before SoftMax).

end_logits

end_logits (torch.FloatTensor of shape (batch_size, sequence_length)) — Span-end scores (before SoftMax).

hidden_states

hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has

an embedding layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.

attentions

attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, BertForQuestionAnswering

&gt;&gt;&gt; import torch

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("deepset/bert-base-cased-squad2")

&gt;&gt;&gt; model = BertForQuestionAnswering.from_pretrained("deepset/bert-base-cased-squad2")

&gt;&gt;&gt; question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

&gt;&gt;&gt; inputs = tokenizer(question, text, return_tensors="pt")

&gt;&gt;&gt; with torch.no_grad():

...     outputs = model(**inputs)

&gt;&gt;&gt; answer_start_index = outputs.start_logits.argmax()

&gt;&gt;&gt; answer_end_index = outputs.end_logits.argmax()

&gt;&gt;&gt; predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]

&gt;&gt;&gt; tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)

'a nice puppet'

&gt;&gt;&gt; # target is "nice puppet"

&gt;&gt;&gt; target_start_index = torch.tensor([14])

&gt;&gt;&gt; target_end_index = torch.tensor([15])

&gt;&gt;&gt; outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)

&gt;&gt;&gt; loss = outputs.loss

&gt;&gt;&gt; round(loss.item(), 2)

7.41

TFBertModel





Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.


The bare Bert Model transformer outputting raw hidden-states without any specific head on top.

This model inherits from TFPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all

matter related to general usage and behavior.

call

&lt; &gt;

( input_ids

 ,

attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , encoder_hidden_states

 , encoder_attention_mask

 , past_key_values

 , use_cache

 , output_attentions

 , output_hidden_states

 , return_dict

 , training

 ) →

transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions or tuple(tf.Tensor)

TensorFlow models and layers in transformers accept two formats as input:

The reason the second format is supported is that Keras methods prefer this format when passing inputs to models and

layers. Because of this support, when using methods like model.fit() things should “just work” for you - just pass your

inputs and labels in any format that model.fit() supports! If, however, you want to use the second format outside of Keras

methods like fit() and predict(), such as when creating your own layers or models with the Keras Functional API, there

are three possibilities you can use to gather all the input Tensors in the first positional argument:

Note that when creating models and layers with subclassing then you donʼt need to worry about any of this, as you can just

pass inputs like you would to any other Python function!

having all inputs as keyword arguments (like PyTorch models), or

having all inputs as a list, tuple or dict in the first positional argument.

a single Tensor with input_ids only and nothing else: model(input_ids)

a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([input_ids,

attention_mask]) or model([input_ids, attention_mask, token_type_ids])

a dictionary with one or several input Tensors associated to the input names given in the docstring: model({"input_ids":

input_ids, "token_type_ids": token_type_ids})





























Parameters

Parameters

input_ids

input_ids (np.ndarray, tf.Tensor, List[tf.Tensor] `Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each

example must have the shape (batch_size, sequence_length)) — Indices of input sequence tokens in the vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.call

call() and PreTrainedTokenizer.encode() for

details.

What are input IDs?

attention_mask

attention_mask (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Mask to avoid

performing attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Segment token indices

to indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Indices of positions of

each input sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings -

1].

What are position IDs?

head_mask

head_mask (np.ndarray or tf.Tensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (np.ndarray or tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) —

Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you

want more control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding

lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See


The TFBertModel forward method, overrides the __call__ special method.

Returns

Returns

transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions

transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions or 

 or tuple(tf.Tensor)

A transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions or a tuple of tf.Tensor (if

return_dict=False is passed or when config.return_dict=False) comprising various elements depending on the

configuration (BertConfig) and inputs.

attentions under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the

value in the config will be used instead.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will

be used instead.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple. This argument can be used

in eager mode, in graph mode the value will always be set to True.

training

training (bool, optional, defaults to `False“) — Whether or not to use the model in training mode (some modules like

dropout modules have different behaviors between training and evaluation).

encoder_hidden_states

encoder_hidden_states (tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) — Sequence of

hidden-states at the output of the last layer of the encoder. Used in the cross-attention if the model is configured as a

decoder.

encoder_attention_mask

encoder_attention_mask (tf.Tensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on the padding token indices of the encoder input. This mask is used in the cross-attention if the model is

configured as a decoder. Mask values selected in [0, 1]:

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

past_key_values

past_key_values (Tuple[Tuple[tf.Tensor]] of length config.n_layers) — contains precomputed key and value hidden

states of the attention blocks. Can be used to speed up decoding. If past_key_values are used, the user can optionally

input only the last decoder_input_ids (those that donʼt have their past key value states given to this model) of shape

(batch_size, 1) instead of all decoder_input_ids of shape (batch_size, sequence_length).

use_cache

use_cache (bool, optional, defaults to True) — If set to True, past_key_values key value states are returned and can be

used to speed up decoding (see past_key_values). Set to False during training, True during generation

last_hidden_state

last_hidden_state (tf.Tensor of shape (batch_size, sequence_length, hidden_size)) — Sequence of hidden-states at

the output of the last layer of the model.

pooler_output

pooler_output (tf.Tensor of shape (batch_size, hidden_size)) — Last layer hidden-state of the first token of the

sequence (classification token) further processed by a Linear layer and a Tanh activation function. The Linear layer weights

are trained from the next sentence prediction (classification) objective during pretraining.

This output is usually not a good summary of the semantic content of the input, youʼre often better with averaging or

pooling the sequence of hidden-states for the whole input sequence.

past_key_values

past_key_values (List[tf.Tensor], optional, returned when use_cache=True is passed or when config.use_cache=True) —

List of tf.Tensor of length config.n_layers, with each tensor of shape (2, batch_size, num_heads, sequence_length,

embed_size_per_head)).

Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see past_key_values input)

to speed up sequential decoding.

hidden_states

hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

cross_attentions

cross_attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights of the decoderʼs cross-attention layer, after the attention softmax, used to compute the weighted

average in the cross-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, TFBertModel

&gt;&gt;&gt; import tensorflow as tf

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = TFBertModel.from_pretrained("bert-base-uncased")




class transformers.TFBertForPreTraining

TFBertForPreTraining

&lt; &gt;

( *args , **kwargs )

Bert Model with two heads on top as done during the pretraining: a masked language modeling head and a next sentence

prediction (classification) head.

This model inherits from TFPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all

matter related to general usage and behavior.

call

&lt; &gt;

( input_ids

 ,

attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , output_attentions

 , output_hidden_states

 , return_dict

 , labels

 , next_sentence_label

 , training

 ) → transformers.models.bert.modeling_tf_bert.TFBertForPreTrainingOutput or tuple(tf.Tensor)

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

&gt;&gt;&gt; outputs = model(inputs)

&gt;&gt;&gt; last_hidden_states = outputs.last_hidden_state

TFBertForPreTraining





Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

TensorFlow models and layers in transformers accept two formats as input:

The reason the second format is supported is that Keras methods prefer this format when passing inputs to models and

layers. Because of this support, when using methods like model.fit() things should “just work” for you - just pass your

inputs and labels in any format that model.fit() supports! If, however, you want to use the second format outside of Keras

methods like fit() and predict(), such as when creating your own layers or models with the Keras Functional API, there

are three possibilities you can use to gather all the input Tensors in the first positional argument:

Note that when creating models and layers with subclassing then you donʼt need to worry about any of this, as you can just

pass inputs like you would to any other Python function!

having all inputs as keyword arguments (like PyTorch models), or

having all inputs as a list, tuple or dict in the first positional argument.

a single Tensor with input_ids only and nothing else: model(input_ids)

a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([input_ids,

attention_mask]) or model([input_ids, attention_mask, token_type_ids])

a dictionary with one or several input Tensors associated to the input names given in the docstring: model({"input_ids":

input_ids, "token_type_ids": token_type_ids})

























Parameters

Parameters

input_ids

input_ids (np.ndarray, tf.Tensor, List[tf.Tensor] `Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each

example must have the shape (batch_size, sequence_length)) — Indices of input sequence tokens in the vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.call

call() and PreTrainedTokenizer.encode() for

details.

What are input IDs?

attention_mask

attention_mask (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Mask to avoid

performing attention on padding token indices. Mask values selected in [0, 1]:

1 for tokens that are not masked

not masked,


The TFBertForPreTraining forward method, overrides the __call__ special method.

Returns

Returns

transformers.models.bert.modeling_tf_bert.TFBertForPreTrainingOutput

transformers.models.bert.modeling_tf_bert.TFBertForPreTrainingOutput or 

 or tuple(tf.Tensor)

A transformers.models.bert.modeling_tf_bert.TFBertForPreTrainingOutput or a tuple of tf.Tensor (if return_dict=False is

passed or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and

inputs.

What are attention masks?

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Segment token indices

to indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Indices of positions of

each input sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings -

1].

What are position IDs?

head_mask

head_mask (np.ndarray or tf.Tensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (np.ndarray or tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) —

Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you

want more control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding

lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the

value in the config will be used instead.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will

be used instead.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple. This argument can be used

in eager mode, in graph mode the value will always be set to True.

training

training (bool, optional, defaults to `False“) — Whether or not to use the model in training mode (some modules like

dropout modules have different behaviors between training and evaluation).

labels

labels (tf.Tensor of shape (batch_size, sequence_length), optional) — Labels for computing the masked language

modeling loss. Indices should be in [-100, 0, ..., config.vocab_size] (see input_ids docstring) Tokens with indices

set to -100 are ignored (masked), the loss is only computed for the tokens with labels in [0, ..., config.vocab_size]

next_sentence_label

next_sentence_label (tf.Tensor of shape (batch_size,), optional) — Labels for computing the next sequence prediction

(classification) loss. Input should be a sequence pair (see input_ids docstring) Indices should be in [0, 1]:

0 indicates sequence B is a continuation of sequence A,

1 indicates sequence B is a random sequence.

kwargs

kwargs (Dict[str, any], optional, defaults to {}) — Used to hide legacy arguments that have been deprecated.

prediction_logits

prediction_logits (tf.Tensor of shape (batch_size, sequence_length, config.vocab_size)) — Prediction scores of the

language modeling head (scores for each vocabulary token before SoftMax).

seq_relationship_logits

seq_relationship_logits (tf.Tensor of shape (batch_size, 2)) — Prediction scores of the next sequence prediction

(classification) head (scores of True/False continuation before SoftMax).

hidden_states

hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Examples:

&gt;&gt;&gt; import tensorflow as tf

&gt;&gt;&gt; from transformers import AutoTokenizer, TFBertForPreTraining




class transformers.TFBertLMHeadModel

TFBertLMHeadModel

&lt; &gt;

( *args , **kwargs )

call

&lt; &gt;

( input_ids

 ,

attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , encoder_hidden_states

 , encoder_attention_mask

 , past_key_values

 , use_cache

 , output_attentions

 , output_hidden_states

 , return_dict

 , labels

 , training

 , **kwargs ) →

transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions or tuple(tf.Tensor)

encoder_hidden_states (tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional): Sequence of

hidden-states at the output of the last layer of the encoder. Used in the cross-attention if the model is configured as a

decoder. encoder_attention_mask (tf.Tensor of shape (batch_size, sequence_length), optional): Mask to avoid

performing attention on the padding token indices of the encoder input. This mask is used in the cross-attention if the model

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = TFBertForPreTraining.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; input_ids = tokenizer("Hello, my dog is cute", add_special_tokens=True, return_tensors="tf")

&gt;&gt;&gt; # Batch size 1

&gt;&gt;&gt; outputs = model(input_ids)

&gt;&gt;&gt; prediction_logits, seq_relationship_logits = outputs[:2]

TFBertModelLMHeadModel





































Returns

Returns

transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions

transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions or 

 or tuple(tf.Tensor)

A transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions or a tuple of tf.Tensor (if return_dict=False

is passed or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig)

and inputs.

loss

loss (tf.Tensor of shape (n,), optional, where n is the number of non-masked labels, returned when labels is provided) —

Language modeling loss (for next-token prediction).

logits

logits (tf.Tensor of shape (batch_size, sequence_length, config.vocab_size)) — Prediction scores of the language

modeling head (scores for each vocabulary token before SoftMax).

hidden_states

hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

cross_attentions

cross_attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights of the decoderʼs cross-attention layer, after the attention softmax, used to compute the weighted

average in the cross-attention heads.

past_key_values

past_key_values (List[tf.Tensor], optional, returned when use_cache=True is passed or when config.use_cache=True) —

List of tf.Tensor of length config.n_layers, with each tensor of shape (2, batch_size, num_heads, sequence_length,

embed_size_per_head)).

Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see past_key_values input)

to speed up sequential decoding.


is configured as a decoder. Mask values selected in [0, 1]:

past_key_values (Tuple[Tuple[tf.Tensor]] of length config.n_layers) contains precomputed key and value hidden states

of the attention blocks. Can be used to speed up decoding. If past_key_values are used, the user can optionally input only

the last decoder_input_ids (those that donʼt have their past key value states given to this model) of shape (batch_size, 1)

instead of all decoder_input_ids of shape (batch_size, sequence_length). use_cache (bool, optional, defaults to True): If

set to True, past_key_values key value states are returned and can be used to speed up decoding (see past_key_values). Set

to False during training, True during generation labels (tf.Tensor or np.ndarray of shape (batch_size, sequence_length),

optional): Labels for computing the cross entropy classification loss. Indices should be in [0, ..., config.vocab_size -

1].



class transformers.TFBertForMaskedLM

TFBertForMaskedLM

&lt; &gt;

( *args , **kwargs )

Bert Model with a language modeling head on top.

This model inherits from TFPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all

matter related to general usage and behavior.

call

&lt; &gt;

( input_ids

 ,

attention_mask

 , token_type_ids

 , position_ids

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, TFBertLMHeadModel

&gt;&gt;&gt; import tensorflow as tf

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = TFBertLMHeadModel.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

&gt;&gt;&gt; outputs = model(inputs)

&gt;&gt;&gt; logits = outputs.logits

TFBertForMaskedLM





Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

TensorFlow models and layers in transformers accept two formats as input:

The reason the second format is supported is that Keras methods prefer this format when passing inputs to models and

layers. Because of this support, when using methods like model.fit() things should “just work” for you - just pass your

inputs and labels in any format that model.fit() supports! If, however, you want to use the second format outside of Keras

methods like fit() and predict(), such as when creating your own layers or models with the Keras Functional API, there

are three possibilities you can use to gather all the input Tensors in the first positional argument:

Note that when creating models and layers with subclassing then you donʼt need to worry about any of this, as you can just

pass inputs like you would to any other Python function!

having all inputs as keyword arguments (like PyTorch models), or

having all inputs as a list, tuple or dict in the first positional argument.

a single Tensor with input_ids only and nothing else: model(input_ids)

a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([input_ids,

attention_mask]) or model([input_ids, attention_mask, token_type_ids])

a dictionary with one or several input Tensors associated to the input names given in the docstring: model({"input_ids":

input_ids, "token_type_ids": token_type_ids})












 , head_mask

 , inputs_embeds

 , output_attentions

 , output_hidden_states

 , return_dict

 , labels

 , training

 ) → transformers.modeling_tf_outputs.TFMaskedLMOutput or tuple(tf.Tensor)

The TFBertForMaskedLM forward method, overrides the __call__ special method.

















Parameters

Parameters

Returns

Returns

transformers.modeling_tf_outputs.TFMaskedLMOutput

transformers.modeling_tf_outputs.TFMaskedLMOutput or 

 or tuple(tf.Tensor)

A transformers.modeling_tf_outputs.TFMaskedLMOutput or a tuple of tf.Tensor (if return_dict=False is passed or when

config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and inputs.

input_ids

input_ids (np.ndarray, tf.Tensor, List[tf.Tensor] `Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each

example must have the shape (batch_size, sequence_length)) — Indices of input sequence tokens in the vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.call

call() and PreTrainedTokenizer.encode() for

details.

What are input IDs?

attention_mask

attention_mask (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Mask to avoid

performing attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Segment token indices

to indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Indices of positions of

each input sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings -

1].

What are position IDs?

head_mask

head_mask (np.ndarray or tf.Tensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (np.ndarray or tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) —

Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you

want more control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding

lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the

value in the config will be used instead.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will

be used instead.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple. This argument can be used

in eager mode, in graph mode the value will always be set to True.

training

training (bool, optional, defaults to `False“) — Whether or not to use the model in training mode (some modules like

dropout modules have different behaviors between training and evaluation).

labels

labels (tf.Tensor or np.ndarray of shape (batch_size, sequence_length), optional) — Labels for computing the masked

language modeling loss. Indices should be in [-100, 0, ..., config.vocab_size] (see input_ids docstring) Tokens

with indices set to -100 are ignored (masked), the loss is only computed for the tokens with labels in [0, ...,

config.vocab_size]

loss

loss (tf.Tensor of shape (n,), optional, where n is the number of non-masked labels, returned when labels is provided) —

Masked language modeling (MLM) loss.

logits

logits (tf.Tensor of shape (batch_size, sequence_length, config.vocab_size)) — Prediction scores of the language

modeling head (scores for each vocabulary token before SoftMax).

hidden_states

hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.




class transformers.TFBertForNextSentencePrediction

TFBertForNextSentencePrediction

&lt; &gt;

( *args , **kwargs )

Bert Model with a next sentence prediction (classification) head on top.

This model inherits from TFPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all

matter related to general usage and behavior.

call

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, TFBertForMaskedLM

&gt;&gt;&gt; import tensorflow as tf

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = TFBertForMaskedLM.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")

&gt;&gt;&gt; logits = model(**inputs).logits

&gt;&gt;&gt; # retrieve index of [MASK]

&gt;&gt;&gt; mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])

&gt;&gt;&gt; selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)

&gt;&gt;&gt; predicted_token_id = tf.math.argmax(selected_logits, axis=-1)

&gt;&gt;&gt; tokenizer.decode(predicted_token_id)

'paris'

&gt;&gt;&gt; labels = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

&gt;&gt;&gt; # mask labels of non-[MASK] tokens

&gt;&gt;&gt; labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

&gt;&gt;&gt; outputs = model(**inputs, labels=labels)

&gt;&gt;&gt; round(float(outputs.loss), 2)

0.88

TFBertForNextSentencePrediction





Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

TensorFlow models and layers in transformers accept two formats as input:

The reason the second format is supported is that Keras methods prefer this format when passing inputs to models and

layers. Because of this support, when using methods like model.fit() things should “just work” for you - just pass your

inputs and labels in any format that model.fit() supports! If, however, you want to use the second format outside of Keras

methods like fit() and predict(), such as when creating your own layers or models with the Keras Functional API, there

are three possibilities you can use to gather all the input Tensors in the first positional argument:

Note that when creating models and layers with subclassing then you donʼt need to worry about any of this, as you can just

pass inputs like you would to any other Python function!

having all inputs as keyword arguments (like PyTorch models), or

having all inputs as a list, tuple or dict in the first positional argument.

a single Tensor with input_ids only and nothing else: model(input_ids)

a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([input_ids,

attention_mask]) or model([input_ids, attention_mask, token_type_ids])

a dictionary with one or several input Tensors associated to the input names given in the docstring: model({"input_ids":

input_ids, "token_type_ids": token_type_ids})


&lt; &gt;

( input_ids

 ,

attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , output_attentions

 , output_hidden_states

 , return_dict

 , next_sentence_label

 ,

training

 ) → transformers.modeling_tf_outputs.TFNextSentencePredictorOutput or

tuple(tf.Tensor)























Parameters

Parameters

Returns

Returns

transformers.modeling_tf_outputs.TFNextSentencePredictorOutput

transformers.modeling_tf_outputs.TFNextSentencePredictorOutput or 

 or tuple(tf.Tensor)

A transformers.modeling_tf_outputs.TFNextSentencePredictorOutput or a tuple of tf.Tensor (if return_dict=False is

passed or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and

inputs.

input_ids

input_ids (np.ndarray, tf.Tensor, List[tf.Tensor] `Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each

example must have the shape (batch_size, sequence_length)) — Indices of input sequence tokens in the vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.call

call() and PreTrainedTokenizer.encode() for

details.

What are input IDs?

attention_mask

attention_mask (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Mask to avoid

performing attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Segment token indices

to indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Indices of positions of

each input sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings -

1].

What are position IDs?

head_mask

head_mask (np.ndarray or tf.Tensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (np.ndarray or tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) —

Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you

want more control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding

lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the

value in the config will be used instead.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will

be used instead.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple. This argument can be used

in eager mode, in graph mode the value will always be set to True.

training

training (bool, optional, defaults to `False“) — Whether or not to use the model in training mode (some modules like

dropout modules have different behaviors between training and evaluation).

loss

loss (tf.Tensor of shape (n,), optional, where n is the number of non-masked labels, returned when next_sentence_label is

provided) — Next sentence prediction loss.

logits

logits (tf.Tensor of shape (batch_size, 2)) — Prediction scores of the next sequence prediction (classification) head

(scores of True/False continuation before SoftMax).

hidden_states

hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).


The TFBertForNextSentencePrediction forward method, overrides the __call__ special method.



class transformers.TFBertForSequenceClassification

TFBertForSequenceClassification

&lt; &gt;

( *args , **kwargs )

Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g.

for GLUE tasks.

This model inherits from TFPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all

matter related to general usage and behavior.

call

&lt; &gt;

( input_ids

 ,

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Examples:

&gt;&gt;&gt; import tensorflow as tf

&gt;&gt;&gt; from transformers import AutoTokenizer, TFBertForNextSentencePrediction

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = TFBertForNextSentencePrediction.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."

&gt;&gt;&gt; next_sentence = "The sky is blue due to the shorter wavelength of blue light."

&gt;&gt;&gt; encoding = tokenizer(prompt, next_sentence, return_tensors="tf")

&gt;&gt;&gt; logits = model(encoding["input_ids"], token_type_ids=encoding["token_type_ids"])[0]

&gt;&gt;&gt; assert logits[0][0] &lt; logits[0][1]  # the next sentence was random

TFBertForSequenceClassification





Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

TensorFlow models and layers in transformers accept two formats as input:

The reason the second format is supported is that Keras methods prefer this format when passing inputs to models and

layers. Because of this support, when using methods like model.fit() things should “just work” for you - just pass your

inputs and labels in any format that model.fit() supports! If, however, you want to use the second format outside of Keras

methods like fit() and predict(), such as when creating your own layers or models with the Keras Functional API, there

are three possibilities you can use to gather all the input Tensors in the first positional argument:

Note that when creating models and layers with subclassing then you donʼt need to worry about any of this, as you can just

pass inputs like you would to any other Python function!

having all inputs as keyword arguments (like PyTorch models), or

having all inputs as a list, tuple or dict in the first positional argument.

a single Tensor with input_ids only and nothing else: model(input_ids)

a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([input_ids,

attention_mask]) or model([input_ids, attention_mask, token_type_ids])

a dictionary with one or several input Tensors associated to the input names given in the docstring: model({"input_ids":

input_ids, "token_type_ids": token_type_ids})








attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , output_attentions

 , output_hidden_states

 , return_dict

 , labels

 , training

 ) → transformers.modeling_tf_outputs.TFSequenceClassifierOutput or tuple(tf.Tensor)





















Parameters

Parameters

Returns

Returns

transformers.modeling_tf_outputs.TFSequenceClassifierOutput

transformers.modeling_tf_outputs.TFSequenceClassifierOutput or 

 or tuple(tf.Tensor)

A transformers.modeling_tf_outputs.TFSequenceClassifierOutput or a tuple of tf.Tensor (if return_dict=False is passed or

when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and inputs.

input_ids

input_ids (np.ndarray, tf.Tensor, List[tf.Tensor] `Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each

example must have the shape (batch_size, sequence_length)) — Indices of input sequence tokens in the vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.call

call() and PreTrainedTokenizer.encode() for

details.

What are input IDs?

attention_mask

attention_mask (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Mask to avoid

performing attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Segment token indices

to indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Indices of positions of

each input sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings -

1].

What are position IDs?

head_mask

head_mask (np.ndarray or tf.Tensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (np.ndarray or tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) —

Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you

want more control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding

lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the

value in the config will be used instead.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will

be used instead.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple. This argument can be used

in eager mode, in graph mode the value will always be set to True.

training

training (bool, optional, defaults to `False“) — Whether or not to use the model in training mode (some modules like

dropout modules have different behaviors between training and evaluation).

labels

labels (tf.Tensor or np.ndarray of shape (batch_size,), optional) — Labels for computing the sequence

classification/regression loss. Indices should be in [0, ..., config.num_labels - 1]. If config.num_labels == 1 a

regression loss is computed (Mean-Square loss), If config.num_labels &gt; 1 a classification loss is computed (Cross-

Entropy).

loss

loss (tf.Tensor of shape (batch_size, ), optional, returned when labels is provided) — Classification (or regression if

config.num_labels==1) loss.

logits

logits (tf.Tensor of shape (batch_size, config.num_labels)) — Classification (or regression if config.num_labels==1)

scores (before SoftMax).

hidden_states

hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.


The TFBertForSequenceClassification forward method, overrides the __call__ special method.



class transformers.TFBertForMultipleChoice

TFBertForMultipleChoice

&lt; &gt;

( *args , **kwargs )

Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for

RocStories/SWAG tasks.

This model inherits from TFPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all

matter related to general usage and behavior.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, TFBertForSequenceClassification

&gt;&gt;&gt; import tensorflow as tf

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("ydshieh/bert-base-uncased-yelp-polarity")

&gt;&gt;&gt; model = TFBertForSequenceClassification.from_pretrained("ydshieh/bert-base-uncased-yelp-polarity")

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

&gt;&gt;&gt; logits = model(**inputs).logits

&gt;&gt;&gt; predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])

&gt;&gt;&gt; model.config.id2label[predicted_class_id]

'LABEL_1'

&gt;&gt;&gt; # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`

&gt;&gt;&gt; num_labels = len(model.config.id2label)

&gt;&gt;&gt; model = TFBertForSequenceClassification.from_pretrained("ydshieh/bert-base-uncased-yelp-polarity", num_labels=num_labels)

&gt;&gt;&gt; labels = tf.constant(1)

&gt;&gt;&gt; loss = model(**inputs, labels=labels).loss

&gt;&gt;&gt; round(float(loss), 2)

0.01

TFBertForMultipleChoice





Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

TensorFlow models and layers in transformers accept two formats as input:

The reason the second format is supported is that Keras methods prefer this format when passing inputs to models and

layers. Because of this support, when using methods like model.fit() things should “just work” for you - just pass your

inputs and labels in any format that model.fit() supports! If, however, you want to use the second format outside of Keras

methods like fit() and predict(), such as when creating your own layers or models with the Keras Functional API, there

are three possibilities you can use to gather all the input Tensors in the first positional argument:

Note that when creating models and layers with subclassing then you donʼt need to worry about any of this, as you can just

pass inputs like you would to any other Python function!

having all inputs as keyword arguments (like PyTorch models), or

having all inputs as a list, tuple or dict in the first positional argument.

a single Tensor with input_ids only and nothing else: model(input_ids)

a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([input_ids,

attention_mask]) or model([input_ids, attention_mask, token_type_ids])

a dictionary with one or several input Tensors associated to the input names given in the docstring: model({"input_ids":

input_ids, "token_type_ids": token_type_ids})


call

&lt; &gt;

( input_ids

 ,

attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , output_attentions

 , output_hidden_states

 , return_dict

 , labels

 , training

 ) → transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput or tuple(tf.Tensor)























Parameters

Parameters

Returns

Returns

transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput

transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput or 

 or tuple(tf.Tensor)

A transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput or a tuple of tf.Tensor (if return_dict=False is passed

or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and inputs.

input_ids

input_ids (np.ndarray, tf.Tensor, List[tf.Tensor] `Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each

example must have the shape (batch_size, num_choices, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.call

call() and PreTrainedTokenizer.encode() for

details.

What are input IDs?

attention_mask

attention_mask (np.ndarray or tf.Tensor of shape (batch_size, num_choices, sequence_length), optional) — Mask to

avoid performing attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (np.ndarray or tf.Tensor of shape (batch_size, num_choices, sequence_length), optional) —

Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (np.ndarray or tf.Tensor of shape (batch_size, num_choices, sequence_length), optional) — Indices of

positions of each input sequence tokens in the position embeddings. Selected in the range [0,

config.max_position_embeddings - 1].

What are position IDs?

head_mask

head_mask (np.ndarray or tf.Tensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (np.ndarray or tf.Tensor of shape (batch_size, num_choices, sequence_length, hidden_size),

optional) — Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is

useful if you want more control over how to convert input_ids indices into associated vectors than the modelʼs internal

embedding lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the

value in the config will be used instead.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will

be used instead.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple. This argument can be used

in eager mode, in graph mode the value will always be set to True.

training

training (bool, optional, defaults to `False“) — Whether or not to use the model in training mode (some modules like

dropout modules have different behaviors between training and evaluation).

labels

labels (tf.Tensor or np.ndarray of shape (batch_size,), optional) — Labels for computing the multiple choice

classification loss. Indices should be in [0, ..., num_choices] where num_choices is the size of the second dimension of

the input tensors. (See input_ids above)

loss

loss (tf.Tensor of shape (batch_size, ), optional, returned when labels is provided) — Classification loss.

logits

logits (tf.Tensor of shape (batch_size, num_choices)) — num_choices is the second dimension of the input tensors. (see

input_ids above).

Classification scores (before SoftMax).

hidden_states

hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).


The TFBertForMultipleChoice forward method, overrides the __call__ special method.



class transformers.TFBertForTokenClassification

TFBertForTokenClassification

&lt; &gt;

( *args , **kwargs )

Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-

Recognition (NER) tasks.

This model inherits from TFPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all

matter related to general usage and behavior.

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, TFBertForMultipleChoice

&gt;&gt;&gt; import tensorflow as tf

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = TFBertForMultipleChoice.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."

&gt;&gt;&gt; choice0 = "It is eaten with a fork and a knife."

&gt;&gt;&gt; choice1 = "It is eaten while held in the hand."

&gt;&gt;&gt; encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)

&gt;&gt;&gt; inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}

&gt;&gt;&gt; outputs = model(inputs)  # batch size is 1

&gt;&gt;&gt; # the linear classifier still needs to be trained

&gt;&gt;&gt; logits = outputs.logits

TFBertForTokenClassification





Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

TensorFlow models and layers in transformers accept two formats as input:

The reason the second format is supported is that Keras methods prefer this format when passing inputs to models and

layers. Because of this support, when using methods like model.fit() things should “just work” for you - just pass your

inputs and labels in any format that model.fit() supports! If, however, you want to use the second format outside of Keras

methods like fit() and predict(), such as when creating your own layers or models with the Keras Functional API, there

are three possibilities you can use to gather all the input Tensors in the first positional argument:

Note that when creating models and layers with subclassing then you donʼt need to worry about any of this, as you can just

pass inputs like you would to any other Python function!

having all inputs as keyword arguments (like PyTorch models), or

having all inputs as a list, tuple or dict in the first positional argument.

a single Tensor with input_ids only and nothing else: model(input_ids)

a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([input_ids,

attention_mask]) or model([input_ids, attention_mask, token_type_ids])

a dictionary with one or several input Tensors associated to the input names given in the docstring: model({"input_ids":

input_ids, "token_type_ids": token_type_ids})


call

&lt; &gt;

( input_ids

 ,

attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , output_attentions

 , output_hidden_states

 , return_dict

 , labels

 , training

 ) → transformers.modeling_tf_outputs.TFTokenClassifierOutput or tuple(tf.Tensor)























Parameters

Parameters

Returns

Returns

transformers.modeling_tf_outputs.TFTokenClassifierOutput

transformers.modeling_tf_outputs.TFTokenClassifierOutput or 

 or tuple(tf.Tensor)

A transformers.modeling_tf_outputs.TFTokenClassifierOutput or a tuple of tf.Tensor (if return_dict=False is passed or

when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and inputs.

input_ids

input_ids (np.ndarray, tf.Tensor, List[tf.Tensor] `Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each

example must have the shape (batch_size, sequence_length)) — Indices of input sequence tokens in the vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.call

call() and PreTrainedTokenizer.encode() for

details.

What are input IDs?

attention_mask

attention_mask (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Mask to avoid

performing attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Segment token indices

to indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Indices of positions of

each input sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings -

1].

What are position IDs?

head_mask

head_mask (np.ndarray or tf.Tensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (np.ndarray or tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) —

Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you

want more control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding

lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the

value in the config will be used instead.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will

be used instead.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple. This argument can be used

in eager mode, in graph mode the value will always be set to True.

training

training (bool, optional, defaults to `False“) — Whether or not to use the model in training mode (some modules like

dropout modules have different behaviors between training and evaluation).

labels

labels (tf.Tensor or np.ndarray of shape (batch_size, sequence_length), optional) — Labels for computing the token

classification loss. Indices should be in [0, ..., config.num_labels - 1].

loss

loss (tf.Tensor of shape (n,), optional, where n is the number of unmasked labels, returned when labels is provided) —

Classification loss.

logits

logits (tf.Tensor of shape (batch_size, sequence_length, config.num_labels)) — Classification scores (before SoftMax).

hidden_states

hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when


The TFBertForTokenClassification forward method, overrides the __call__ special method.



class transformers.TFBertForQuestionAnswering

TFBertForQuestionAnswering

&lt; &gt;

( *args , **kwargs )

Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layer on top of

the hidden-states output to compute span start logits and span end logits).

This model inherits from TFPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)

This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all

matter related to general usage and behavior.

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, TFBertForTokenClassification

&gt;&gt;&gt; import tensorflow as tf

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-large-cased-finetuned-conll03-english")

&gt;&gt;&gt; model = TFBertForTokenClassification.from_pretrained("dbmdz/bert-large-cased-finetuned-conll03-english")

&gt;&gt;&gt; inputs = tokenizer(

...  "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="tf"

... )

&gt;&gt;&gt; logits = model(**inputs).logits

&gt;&gt;&gt; predicted_token_class_ids = tf.math.argmax(logits, axis=-1)

&gt;&gt;&gt; # Note that tokens are classified rather then input words which means that

&gt;&gt;&gt; # there might be more predicted token classes than words.

&gt;&gt;&gt; # Multiple token classes might account for the same word

&gt;&gt;&gt; predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]

&gt;&gt;&gt; predicted_tokens_classes

['O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'I-LOC', 'I-LOC'] 

&gt;&gt;&gt; labels = predicted_token_class_ids

&gt;&gt;&gt; loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)

&gt;&gt;&gt; round(float(loss), 2)

0.01

TFBertForQuestionAnswering





Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

TensorFlow models and layers in transformers accept two formats as input:

The reason the second format is supported is that Keras methods prefer this format when passing inputs to models and

layers. Because of this support, when using methods like model.fit() things should “just work” for you - just pass your

inputs and labels in any format that model.fit() supports! If, however, you want to use the second format outside of Keras

methods like fit() and predict(), such as when creating your own layers or models with the Keras Functional API, there

are three possibilities you can use to gather all the input Tensors in the first positional argument:

having all inputs as keyword arguments (like PyTorch models), or

having all inputs as a list, tuple or dict in the first positional argument.

a single Tensor with input_ids only and nothing else: model(input_ids)

a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([input_ids,

attention_mask]) or model([input_ids, attention_mask, token_type_ids])

a dictionary with one or several input Tensors associated to the input names given in the docstring: model({"input_ids":

input_ids, "token_type_ids": token_type_ids})


call

&lt; &gt;

( input_ids

 ,

attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 , inputs_embeds

 , output_attentions

 , output_hidden_states

 , return_dict

 , start_positions

 ,

end_positions

 , training

 ) → transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput or tuple(tf.Tensor)

Note that when creating models and layers with subclassing then you donʼt need to worry about any of this, as you can just

pass inputs like you would to any other Python function!

























Parameters

Parameters

Returns

Returns

transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput

transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput or 

 or tuple(tf.Tensor)

input_ids

input_ids (np.ndarray, tf.Tensor, List[tf.Tensor] `Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each

example must have the shape (batch_size, sequence_length)) — Indices of input sequence tokens in the vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.call

call() and PreTrainedTokenizer.encode() for

details.

What are input IDs?

attention_mask

attention_mask (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Mask to avoid

performing attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Segment token indices

to indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (np.ndarray or tf.Tensor of shape (batch_size, sequence_length), optional) — Indices of positions of

each input sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings -

1].

What are position IDs?

head_mask

head_mask (np.ndarray or tf.Tensor of shape (num_heads,) or (num_layers, num_heads), optional) — Mask to nullify

selected heads of the self-attention modules. Mask values selected in [0, 1]:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

inputs_embeds

inputs_embeds (np.ndarray or tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) —

Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you

want more control over how to convert input_ids indices into associated vectors than the modelʼs internal embedding

lookup matrix.

output_attentions

output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See

attentions under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the

value in the config will be used instead.

output_hidden_states

output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under

returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will

be used instead.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple. This argument can be used

in eager mode, in graph mode the value will always be set to True.

training

training (bool, optional, defaults to `False“) — Whether or not to use the model in training mode (some modules like

dropout modules have different behaviors between training and evaluation).

start_positions

start_positions (tf.Tensor or np.ndarray of shape (batch_size,), optional) — Labels for position (index) of the start of

the labelled span for computing the token classification loss. Positions are clamped to the length of the sequence (

sequence_length). Position outside of the sequence are not taken into account for computing the loss.

end_positions

end_positions (tf.Tensor or np.ndarray of shape (batch_size,), optional) — Labels for position (index) of the end of the

labelled span for computing the token classification loss. Positions are clamped to the length of the sequence (

sequence_length). Position outside of the sequence are not taken into account for computing the loss.


The TFBertForQuestionAnswering forward method, overrides the __call__ special method.



class transformers.FlaxBertModel

FlaxBertModel

&lt; &gt;

( config

 , input_shape

 , seed

 , dtype

 ,

_do_init

 , gradient_checkpointing

 , **kwargs )

A transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput or a tuple of tf.Tensor (if return_dict=False is

passed or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and

inputs.

loss

loss (tf.Tensor of shape (batch_size, ), optional, returned when start_positions and end_positions are provided) —

Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.

start_logits

start_logits (tf.Tensor of shape (batch_size, sequence_length)) — Span-start scores (before SoftMax).

end_logits

end_logits (tf.Tensor of shape (batch_size, sequence_length)) — Span-end scores (before SoftMax).

hidden_states

hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, TFBertForQuestionAnswering

&gt;&gt;&gt; import tensorflow as tf

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("ydshieh/bert-base-cased-squad2")

&gt;&gt;&gt; model = TFBertForQuestionAnswering.from_pretrained("ydshieh/bert-base-cased-squad2")

&gt;&gt;&gt; question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

&gt;&gt;&gt; inputs = tokenizer(question, text, return_tensors="tf")

&gt;&gt;&gt; outputs = model(**inputs)

&gt;&gt;&gt; answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])

&gt;&gt;&gt; answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

&gt;&gt;&gt; predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]

&gt;&gt;&gt; tokenizer.decode(predict_answer_tokens)

'a nice puppet'

&gt;&gt;&gt; # target is "nice puppet"

&gt;&gt;&gt; target_start_index = tf.constant([14])

&gt;&gt;&gt; target_end_index = tf.constant([15])

&gt;&gt;&gt; outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)

&gt;&gt;&gt; loss = tf.math.reduce_mean(outputs.loss)

&gt;&gt;&gt; round(float(loss), 2)

7.41

FlaxBertModel















Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.


The bare Bert Model transformer outputting raw hidden-states without any specific head on top.

This model inherits from FlaxPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading, saving and converting weights from PyTorch models)

This model is also a Flax Linen flax.linen.Module subclass. Use it as a regular Flax linen Module and refer to the Flax

documentation for all matter related to general usage and behavior.

Finally, this model supports inherent JAX features such as:

__call__

&lt; &gt;

( input_ids , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 ,

encoder_hidden_states

 , encoder_attention_mask

 , params

 , dropout_rng

 , train

 , output_attentions

 , output_hidden_states

 ,

return_dict

 , past_key_values

 ) →

transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling or tuple(torch.FloatTensor)

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

Just-In-Time (JIT) compilation

Automatic Differentiation

Vectorization

Parallelization





























Parameters

Parameters

Returns

Returns

transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling

transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling or 

 or tuple(torch.FloatTensor)

A transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling or a tuple of torch.FloatTensor (if

return_dict=False is passed or when config.return_dict=False) comprising various elements depending on the

configuration (BertConfig) and inputs.

input_ids

input_ids (numpy.ndarray of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Segment token indices to indicate

first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

head_mask

head_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) -- Mask to nullify selected

heads of the attention modules. Mask values selected in [0, 1]`:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

last_hidden_state

last_hidden_state (jnp.ndarray of shape (batch_size, sequence_length, hidden_size)) — Sequence of hidden-states at

the output of the last layer of the model.

pooler_output

pooler_output (jnp.ndarray of shape (batch_size, hidden_size)) — Last layer hidden-state of the first token of the

sequence (classification token) further processed by a Linear layer and a Tanh activation function. The Linear layer weights

are trained from the next sentence prediction (classification) objective during pretraining.

hidden_states

hidden_states (tuple(jnp.ndarray), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of jnp.ndarray (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.


The FlaxBertPreTrainedModel forward method, overrides the __call__ special method.



class transformers.FlaxBertForPreTraining

FlaxBertForPreTraining

&lt; &gt;

( config

 , input_shape

 , seed

 , dtype

 ,

_do_init

 , gradient_checkpointing

 , **kwargs )

Bert Model with two heads on top as done during the pretraining: a masked language modeling head and a next sentence

prediction (classification) head.

This model inherits from FlaxPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading, saving and converting weights from PyTorch models)

This model is also a Flax Linen flax.linen.Module subclass. Use it as a regular Flax linen Module and refer to the Flax

documentation for all matter related to general usage and behavior.

Finally, this model supports inherent JAX features such as:

__call__

&lt; &gt;

attentions

attentions (tuple(jnp.ndarray), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of jnp.ndarray (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, FlaxBertModel

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = FlaxBertModel.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

&gt;&gt;&gt; outputs = model(**inputs)

&gt;&gt;&gt; last_hidden_states = outputs.last_hidden_state

FlaxBertForPreTraining















Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

Just-In-Time (JIT) compilation

Automatic Differentiation

Vectorization

Parallelization












( input_ids , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 ,

encoder_hidden_states

 , encoder_attention_mask

 , params

 , dropout_rng

 , train

 , output_attentions

 , output_hidden_states

 ,

return_dict

 , past_key_values

 ) →

transformers.models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput or tuple(torch.FloatTensor)

The FlaxBertPreTrainedModel forward method, overrides the __call__ special method.





























Parameters

Parameters

Returns

Returns

transformers.models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput

transformers.models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput or 

 or tuple(torch.FloatTensor)

A transformers.models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput or a tuple of torch.FloatTensor (if

return_dict=False is passed or when config.return_dict=False) comprising various elements depending on the

configuration (BertConfig) and inputs.

input_ids

input_ids (numpy.ndarray of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Segment token indices to indicate

first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

head_mask

head_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) -- Mask to nullify selected

heads of the attention modules. Mask values selected in [0, 1]`:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

prediction_logits

prediction_logits (jnp.ndarray of shape (batch_size, sequence_length, config.vocab_size)) — Prediction scores of the

language modeling head (scores for each vocabulary token before SoftMax).

seq_relationship_logits

seq_relationship_logits (jnp.ndarray of shape (batch_size, 2)) — Prediction scores of the next sequence prediction

(classification) head (scores of True/False continuation before SoftMax).

hidden_states

hidden_states (tuple(jnp.ndarray), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of jnp.ndarray (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(jnp.ndarray), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of jnp.ndarray (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, FlaxBertForPreTraining

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = FlaxBertForPreTraining.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="np")

&gt;&gt;&gt; outputs = model(**inputs)

&gt;&gt;&gt; prediction_logits = outputs.prediction_logits

&gt;&gt;&gt; seq_relationship_logits = outputs.seq_relationship_logits

FlaxBertForCausalLM




class transformers.FlaxBertForCausalLM

FlaxBertForCausalLM

&lt; &gt;

( config

 , input_shape

 , seed

 , dtype

 ,

_do_init

 , gradient_checkpointing

 , **kwargs )

Bert Model with a language modeling head on top (a linear layer on top of the hidden-states output) e.g for autoregressive

tasks.

This model inherits from FlaxPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading, saving and converting weights from PyTorch models)

This model is also a Flax Linen flax.linen.Module subclass. Use it as a regular Flax linen Module and refer to the Flax

documentation for all matter related to general usage and behavior.

Finally, this model supports inherent JAX features such as:

__call__

&lt; &gt;

( input_ids , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 ,

encoder_hidden_states

 , encoder_attention_mask

 , params

 , dropout_rng

 , train

 , output_attentions

 , output_hidden_states

 ,

return_dict

 , past_key_values

 ) →

transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)















Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

Just-In-Time (JIT) compilation

Automatic Differentiation

Vectorization

Parallelization





























Parameters

Parameters

input_ids

input_ids (numpy.ndarray of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Segment token indices to indicate

first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

head_mask

head_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) -- Mask to nullify selected

heads of the attention modules. Mask values selected in [0, 1]`:


The FlaxBertPreTrainedModel forward method, overrides the __call__ special method.



class transformers.FlaxBertForMaskedLM

FlaxBertForMaskedLM

&lt; &gt;

( config

 , input_shape

 , seed

 , dtype

 ,

_do_init

 , gradient_checkpointing

 , **kwargs )

Returns

Returns

transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions

transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or 

 or tuple(torch.FloatTensor)

A transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or a tuple of torch.FloatTensor (if

return_dict=False is passed or when config.return_dict=False) comprising various elements depending on the

configuration (BertConfig) and inputs.

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

logits

logits (jnp.ndarray of shape (batch_size, sequence_length, config.vocab_size)) — Prediction scores of the language

modeling head (scores for each vocabulary token before SoftMax).

hidden_states

hidden_states (tuple(jnp.ndarray), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of jnp.ndarray (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(jnp.ndarray), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of jnp.ndarray (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

cross_attentions

cross_attentions (tuple(jnp.ndarray), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of jnp.ndarray (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Cross attentions weights after the attention softmax, used to compute the weighted average in the cross-attention heads.

past_key_values

past_key_values (tuple(tuple(jnp.ndarray)), optional, returned when use_cache=True is passed or when

config.use_cache=True) — Tuple of jnp.ndarray tuples of length config.n_layers, with each tuple containing the cached

key, value states of the self-attention and the cross-attention layers if model is used in encoder-decoder setting. Only

relevant if config.is_decoder = True.

Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see past_key_values input)

to speed up sequential decoding.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, FlaxBertForCausalLM

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = FlaxBertForCausalLM.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="np")

&gt;&gt;&gt; outputs = model(**inputs)

&gt;&gt;&gt; # retrieve logts for next token

&gt;&gt;&gt; next_token_logits = outputs.logits[:, -1]

FlaxBertForMaskedLM















Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().


Bert Model with a language modeling head on top.

This model inherits from FlaxPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading, saving and converting weights from PyTorch models)

This model is also a Flax Linen flax.linen.Module subclass. Use it as a regular Flax linen Module and refer to the Flax

documentation for all matter related to general usage and behavior.

Finally, this model supports inherent JAX features such as:

__call__

&lt; &gt;

( input_ids , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 ,

encoder_hidden_states

 , encoder_attention_mask

 , params

 , dropout_rng

 , train

 , output_attentions

 , output_hidden_states

 ,

return_dict

 , past_key_values

 ) →

transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

Just-In-Time (JIT) compilation

Automatic Differentiation

Vectorization

Parallelization





























Parameters

Parameters

Returns

Returns

transformers.modeling_flax_outputs.FlaxMaskedLMOutput

transformers.modeling_flax_outputs.FlaxMaskedLMOutput or 

 or tuple(torch.FloatTensor)

A transformers.modeling_flax_outputs.FlaxMaskedLMOutput or a tuple of torch.FloatTensor (if return_dict=False is

passed or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and

inputs.

input_ids

input_ids (numpy.ndarray of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Segment token indices to indicate

first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

head_mask

head_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) -- Mask to nullify selected

heads of the attention modules. Mask values selected in [0, 1]`:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

logits

logits (jnp.ndarray of shape (batch_size, sequence_length, config.vocab_size)) — Prediction scores of the language

modeling head (scores for each vocabulary token before SoftMax).

hidden_states

hidden_states (tuple(jnp.ndarray), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of jnp.ndarray (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(jnp.ndarray), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of jnp.ndarray (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.


The FlaxBertPreTrainedModel forward method, overrides the __call__ special method.



class transformers.FlaxBertForNextSentencePrediction

FlaxBertForNextSentencePrediction

&lt; &gt;

( config

 , input_shape

 , seed

 , dtype

 ,

_do_init

 , gradient_checkpointing

 , **kwargs )

Bert Model with a next sentence prediction (classification) head on top.

This model inherits from FlaxPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading, saving and converting weights from PyTorch models)

This model is also a Flax Linen flax.linen.Module subclass. Use it as a regular Flax linen Module and refer to the Flax

documentation for all matter related to general usage and behavior.

Finally, this model supports inherent JAX features such as:

__call__

&lt; &gt;

( input_ids , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 ,

encoder_hidden_states

 , encoder_attention_mask

 , params

 , dropout_rng

 , train

 , output_attentions

 , output_hidden_states

 ,

return_dict

 , past_key_values

 ) →

transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput or tuple(torch.FloatTensor)

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, FlaxBertForMaskedLM

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = FlaxBertForMaskedLM.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("The capital of France is [MASK].", return_tensors="jax")

&gt;&gt;&gt; outputs = model(**inputs)

&gt;&gt;&gt; logits = outputs.logits

FlaxBertForNextSentencePrediction















Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

Just-In-Time (JIT) compilation

Automatic Differentiation

Vectorization

Parallelization





























Parameters

Parameters


The FlaxBertPreTrainedModel forward method, overrides the __call__ special method.



class transformers.FlaxBertForSequenceClassification

FlaxBertForSequenceClassification

&lt; &gt;

( config

 , input_shape

 , seed

 , dtype

 ,

Returns

Returns

transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput

transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput or 

 or tuple(torch.FloatTensor)

A transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput or a tuple of torch.FloatTensor (if

return_dict=False is passed or when config.return_dict=False) comprising various elements depending on the

configuration (BertConfig) and inputs.

input_ids

input_ids (numpy.ndarray of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Segment token indices to indicate

first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

head_mask

head_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) -- Mask to nullify selected

heads of the attention modules. Mask values selected in [0, 1]`:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

logits

logits (jnp.ndarray of shape (batch_size, 2)) — Prediction scores of the next sequence prediction (classification) head

(scores of True/False continuation before SoftMax).

hidden_states

hidden_states (tuple(jnp.ndarray), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of jnp.ndarray (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(jnp.ndarray), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of jnp.ndarray (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, FlaxBertForNextSentencePrediction

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = FlaxBertForNextSentencePrediction.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."

&gt;&gt;&gt; next_sentence = "The sky is blue due to the shorter wavelength of blue light."

&gt;&gt;&gt; encoding = tokenizer(prompt, next_sentence, return_tensors="jax")

&gt;&gt;&gt; outputs = model(**encoding)

&gt;&gt;&gt; logits = outputs.logits

&gt;&gt;&gt; assert logits[0, 0] &lt; logits[0, 1]  # next sentence was random

FlaxBertForSequenceClassification
















_do_init

 , gradient_checkpointing

 , **kwargs )

Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g.

for GLUE tasks.

This model inherits from FlaxPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading, saving and converting weights from PyTorch models)

This model is also a Flax Linen flax.linen.Module subclass. Use it as a regular Flax linen Module and refer to the Flax

documentation for all matter related to general usage and behavior.

Finally, this model supports inherent JAX features such as:

__call__

&lt; &gt;

( input_ids , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 ,

encoder_hidden_states

 , encoder_attention_mask

 , params

 , dropout_rng

 , train

 , output_attentions

 , output_hidden_states

 ,

return_dict

 , past_key_values

 ) →

transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput or tuple(torch.FloatTensor)







Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

Just-In-Time (JIT) compilation

Automatic Differentiation

Vectorization

Parallelization





























Parameters

Parameters

Returns

Returns

transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput

transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput or 

 or tuple(torch.FloatTensor)

input_ids

input_ids (numpy.ndarray of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Segment token indices to indicate

first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

head_mask

head_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) -- Mask to nullify selected

heads of the attention modules. Mask values selected in [0, 1]`:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.


The FlaxBertPreTrainedModel forward method, overrides the __call__ special method.



class transformers.FlaxBertForMultipleChoice

FlaxBertForMultipleChoice

&lt; &gt;

( config

 , input_shape

 , seed

 , dtype

 ,

_do_init

 , gradient_checkpointing

 , **kwargs )

Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for

RocStories/SWAG tasks.

This model inherits from FlaxPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading, saving and converting weights from PyTorch models)

This model is also a Flax Linen flax.linen.Module subclass. Use it as a regular Flax linen Module and refer to the Flax

documentation for all matter related to general usage and behavior.

Finally, this model supports inherent JAX features such as:

A transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput or a tuple of torch.FloatTensor (if

return_dict=False is passed or when config.return_dict=False) comprising various elements depending on the

configuration (BertConfig) and inputs.

logits

logits (jnp.ndarray of shape (batch_size, config.num_labels)) — Classification (or regression if config.num_labels==1)

scores (before SoftMax).

hidden_states

hidden_states (tuple(jnp.ndarray), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of jnp.ndarray (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(jnp.ndarray), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of jnp.ndarray (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, FlaxBertForSequenceClassification

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = FlaxBertForSequenceClassification.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

&gt;&gt;&gt; outputs = model(**inputs)

&gt;&gt;&gt; logits = outputs.logits

FlaxBertForMultipleChoice















Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

Just-In-Time (JIT) compilation

Automatic Differentiation


__call__

&lt; &gt;

( input_ids , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 ,

encoder_hidden_states

 , encoder_attention_mask

 , params

 , dropout_rng

 , train

 , output_attentions

 , output_hidden_states

 ,

return_dict

 , past_key_values

 ) →

transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput or tuple(torch.FloatTensor)

The FlaxBertPreTrainedModel forward method, overrides the __call__ special method.

Vectorization

Parallelization





























Parameters

Parameters

Returns

Returns

transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput

transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput or 

 or tuple(torch.FloatTensor)

A transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput or a tuple of torch.FloatTensor (if

return_dict=False is passed or when config.return_dict=False) comprising various elements depending on the

configuration (BertConfig) and inputs.

input_ids

input_ids (numpy.ndarray of shape (batch_size, num_choices, sequence_length)) — Indices of input sequence tokens

in the vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (numpy.ndarray of shape (batch_size, num_choices, sequence_length), optional) — Mask to avoid

performing attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (numpy.ndarray of shape (batch_size, num_choices, sequence_length), optional) — Segment token

indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (numpy.ndarray of shape (batch_size, num_choices, sequence_length), optional) — Indices of positions

of each input sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings

- 1].

head_mask

head_mask (numpy.ndarray of shape (batch_size, num_choices, sequence_length), optional) -- Mask to nullify

selected heads of the attention modules. Mask values selected in [0, 1]`:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

logits

logits (jnp.ndarray of shape (batch_size, num_choices)) — num_choices is the second dimension of the input tensors. (see

input_ids above).

Classification scores (before SoftMax).

hidden_states

hidden_states (tuple(jnp.ndarray), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of jnp.ndarray (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(jnp.ndarray), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of jnp.ndarray (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, FlaxBertForMultipleChoice

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = FlaxBertForMultipleChoice.from_pretrained("bert-base-uncased")




class transformers.FlaxBertForTokenClassification

FlaxBertForTokenClassification

&lt; &gt;

( config

 , input_shape

 , seed

 , dtype

 ,

_do_init

 , gradient_checkpointing

 , **kwargs )

Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-

Recognition (NER) tasks.

This model inherits from FlaxPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading, saving and converting weights from PyTorch models)

This model is also a Flax Linen flax.linen.Module subclass. Use it as a regular Flax linen Module and refer to the Flax

documentation for all matter related to general usage and behavior.

Finally, this model supports inherent JAX features such as:

__call__

&lt; &gt;

( input_ids , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 ,

encoder_hidden_states

 , encoder_attention_mask

 , params

 , dropout_rng

 , train

 , output_attentions

 , output_hidden_states

 ,

return_dict

 , past_key_values

 ) →

transformers.modeling_flax_outputs.FlaxTokenClassifierOutput or tuple(torch.FloatTensor)

&gt;&gt;&gt; prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."

&gt;&gt;&gt; choice0 = "It is eaten with a fork and a knife."

&gt;&gt;&gt; choice1 = "It is eaten while held in the hand."

&gt;&gt;&gt; encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="jax", padding=True)

&gt;&gt;&gt; outputs = model(**{k: v[None, :] for k, v in encoding.items()})

&gt;&gt;&gt; logits = outputs.logits

FlaxBertForTokenClassification















Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

Just-In-Time (JIT) compilation

Automatic Differentiation

Vectorization

Parallelization





























Parameters

Parameters

input_ids

input_ids (numpy.ndarray of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:


The FlaxBertPreTrainedModel forward method, overrides the __call__ special method.



class transformers.FlaxBertForQuestionAnswering

FlaxBertForQuestionAnswering

&lt; &gt;

( config

 , input_shape

 , seed

 , dtype

 ,

_do_init

 , gradient_checkpointing

 , **kwargs )

Returns

Returns

transformers.modeling_flax_outputs.FlaxTokenClassifierOutput

transformers.modeling_flax_outputs.FlaxTokenClassifierOutput or 

 or tuple(torch.FloatTensor)

A transformers.modeling_flax_outputs.FlaxTokenClassifierOutput or a tuple of torch.FloatTensor (if return_dict=False is

passed or when config.return_dict=False) comprising various elements depending on the configuration (BertConfig) and

inputs.

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Segment token indices to indicate

first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

head_mask

head_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) -- Mask to nullify selected

heads of the attention modules. Mask values selected in [0, 1]`:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

logits

logits (jnp.ndarray of shape (batch_size, sequence_length, config.num_labels)) — Classification scores (before

SoftMax).

hidden_states

hidden_states (tuple(jnp.ndarray), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of jnp.ndarray (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(jnp.ndarray), optional, returned when output_attentions=True is passed or when

config.output_attentions=True) — Tuple of jnp.ndarray (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, FlaxBertForTokenClassification

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = FlaxBertForTokenClassification.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

&gt;&gt;&gt; outputs = model(**inputs)

&gt;&gt;&gt; logits = outputs.logits

FlaxBertForQuestionAnswering















Parameters

Parameters

config

config (BertConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not

load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model

weights.

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.


Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of

the hidden-states output to compute span start logits and span end logits).

This model inherits from FlaxPreTrainedModel. Check the superclass documentation for the generic methods the library

implements for all its model (such as downloading, saving and converting weights from PyTorch models)

This model is also a Flax Linen flax.linen.Module subclass. Use it as a regular Flax linen Module and refer to the Flax

documentation for all matter related to general usage and behavior.

Finally, this model supports inherent JAX features such as:

__call__

&lt; &gt;

( input_ids , attention_mask

 , token_type_ids

 , position_ids

 , head_mask

 ,

encoder_hidden_states

 , encoder_attention_mask

 , params

 , dropout_rng

 , train

 , output_attentions

 , output_hidden_states

 ,

return_dict

 , past_key_values

 ) →

transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput or tuple(torch.FloatTensor)

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

dtype

dtype (jax.numpy.dtype, optional, defaults to jax.numpy.float32) — The data type of the computation. Can be one of

jax.numpy.float32, jax.numpy.float16 (on GPUs) and jax.numpy.bfloat16 (on TPUs).

This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If specified all the

computation will be performed with the given dtype.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

Note that this only specifies the dtype of the computation and does not influence the dtype of model parameters.

If you wish to change the dtype of the model parameters, see to_fp16() and to_bf16().

Just-In-Time (JIT) compilation

Automatic Differentiation

Vectorization

Parallelization





























Parameters

Parameters

Returns

Returns

transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput

transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput or 

 or tuple(torch.FloatTensor)

A transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput or a tuple of torch.FloatTensor (if

return_dict=False is passed or when config.return_dict=False) comprising various elements depending on the

configuration (BertConfig) and inputs.

input_ids

input_ids (numpy.ndarray of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the

vocabulary.

Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call

call() for

details.

What are input IDs?

attention_mask

attention_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) — Mask to avoid performing

attention on padding token indices. Mask values selected in [0, 1]:

What are attention masks?

1 for tokens that are not masked

not masked,

0 for tokens that are masked

masked.

token_type_ids

token_type_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Segment token indices to indicate

first and second portions of the inputs. Indices are selected in [0, 1]:

What are token type IDs?

0 corresponds to a sentence A token,

1 corresponds to a sentence B token.

position_ids

position_ids (numpy.ndarray of shape (batch_size, sequence_length), optional) — Indices of positions of each input

sequence tokens in the position embeddings. Selected in the range [0, config.max_position_embeddings - 1].

head_mask

head_mask (numpy.ndarray of shape (batch_size, sequence_length), optional) -- Mask to nullify selected

heads of the attention modules. Mask values selected in [0, 1]`:

1 indicates the head is not masked

not masked,

0 indicates the head is masked

masked.

return_dict

return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.

start_logits

start_logits (jnp.ndarray of shape (batch_size, sequence_length)) — Span-start scores (before SoftMax).

end_logits

end_logits (jnp.ndarray of shape (batch_size, sequence_length)) — Span-end scores (before SoftMax).

hidden_states

hidden_states (tuple(jnp.ndarray), optional, returned when output_hidden_states=True is passed or when

config.output_hidden_states=True) — Tuple of jnp.ndarray (one for the output of the embeddings + one for the output of

each layer) of shape (batch_size, sequence_length, hidden_size).

Hidden-states of the model at the output of each layer plus the initial embedding outputs.

attentions

attentions (tuple(jnp.ndarray), optional, returned when output_attentions=True is passed or when


← BARTpho

 BertGeneration →

The FlaxBertPreTrainedModel forward method, overrides the __call__ special method.

config.output_attentions=True) — Tuple of jnp.ndarray (one for each layer) of shape (batch_size, num_heads,

sequence_length, sequence_length).

Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.

Although the recipe for forward pass needs to be defined within this function, one should call the Module instance

afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently

ignores them.

Example:

&gt;&gt;&gt; from transformers import AutoTokenizer, FlaxBertForQuestionAnswering

&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; model = FlaxBertForQuestionAnswering.from_pretrained("bert-base-uncased")

&gt;&gt;&gt; question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

&gt;&gt;&gt; inputs = tokenizer(question, text, return_tensors="jax")

&gt;&gt;&gt; outputs = model(**inputs)

&gt;&gt;&gt; start_scores = outputs.start_logits

&gt;&gt;&gt; end_scores = outputs.end_logits

